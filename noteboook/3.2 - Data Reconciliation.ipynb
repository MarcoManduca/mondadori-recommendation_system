{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 - Data Reconciliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_cleaned = pd.read_parquet('../data/processed/cleaned_data.parquet', engine='pyarrow')\n",
    "original_columns = df_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model\n",
    "model_name = \"nickprock/sentence-bert-base-italian-xxl-uncased\"\n",
    "\n",
    "# enable model on mpu (- Apple Silicon GPU) or cuda (- Nvidia GPU) or cpu if no gpu is available\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) #tokenizer\n",
    "model = AutoModel.from_pretrained(model_name) #model\n",
    "model.to(device) # move model to device GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract topics list (unique)\n",
    "topics = []\n",
    "for _, row in df_cleaned.iterrows():\n",
    "    for topic in row['topics']:\n",
    "        if topic and topic not in topics:\n",
    "            topics.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to calculate the embeddings for each sentence\n",
    "def sentence_embedding(tokenizer, model, device, sentences_list):\n",
    "    # initialize list to store sentence embeddings\n",
    "    sentence_embeddings = []\n",
    "\n",
    "    for sentences in sentences_list:\n",
    "        # sentence tokenizer\n",
    "        encoded_input = tokenizer(sentences, padding = True, truncation = True, return_tensors = \"pt\")\n",
    "        encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
    "\n",
    "        # model inference\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "\n",
    "        # extract sentence embeddings\n",
    "        sentence_embedding = model_output.last_hidden_state.mean(dim = 1)\n",
    "        sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "    # average the resulsts to obtain a single embedding for the article\n",
    "    article_embedding = torch.cat(sentence_embeddings).mean(dim = 0).cpu().numpy()\n",
    "    \n",
    "    return article_embedding\n",
    "\n",
    "# calculate embeddings\n",
    "df_cleaned['sentence embeddings'] = df_cleaned['text chunked'].apply(\n",
    "    lambda sentences: sentence_embedding(tokenizer, model, device, sentences)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_embedding(tokenizer, model, topic):\n",
    "    encoded_input = tokenizer(topic, return_tensors = \"pt\", padding = True, truncation = True)\n",
    "    encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
    "\n",
    "    # model inference\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # embeddings for the topic\n",
    "    embedding = model_output.last_hidden_state.mean(dim = 1).squeeze().cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "df_topics = pd.DataFrame(topics, columns = ['topic'])\n",
    "df_topics['sentence embeddings'] = df_topics['topic'].apply(lambda topic: topic_embedding(tokenizer, model, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_embeddings = np.array(df_cleaned['sentence embeddings'].tolist())\n",
    "topic_embeddings = np.array(df_topics['sentence embeddings'].tolist())\n",
    "similarity_matrix = cosine_similarity(article_embeddings, topic_embeddings)\n",
    "most_similar_topics_indices = similarity_matrix.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['topic'] = df_topics.iloc[most_similar_topics_indices]['topic'].values\n",
    "df_cleaned['flag topic in topics'] = df_cleaned.apply(lambda row: 1 if row['topic'] in row['topics'] else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_fine_tuning = df_cleaned[df_cleaned['flag topic in topics'] == 1]\n",
    "df_for_fine_tuning = df_for_fine_tuning[original_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_fine_tuning.to_parquet('../data/processed/df_for_fine_tuning.parquet', engine='pyarrow', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
