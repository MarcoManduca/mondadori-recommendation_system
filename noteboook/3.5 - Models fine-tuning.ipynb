{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 - Models fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = pd.read_parquet('../data/processed/cleaned_data.parquet', engine='pyarrow')\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    if len(row['topics']) == 0:\n",
    "        df_cleaned.loc[index, 'flag'] = 1\n",
    "df_cleaned = df_cleaned[~(df_cleaned['flag'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nickprock/sentence-bert-base-italian-xxl-uncased\"\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "topic_encoded = mlb.fit_transform(df_cleaned['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "for i, article_sentences in enumerate(df_cleaned['text chunked']):\n",
    "    for sentence in article_sentences:\n",
    "        texts.append(sentence)\n",
    "        labels.append(topic_encoded[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(tokenizer, texts, max_length=128):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "inputs = encode_data(tokenizer, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'], labels)\n",
    "dataloader = DataLoader(dataset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [torch Loss Functions] (https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)\n",
    "##### [BCEWithLogitsLoss] (https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)\n",
    "\n",
    "###### BCELoss: Creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:\n",
    "\n",
    "###### BCEWithLogitsLoss: This loss combines a Sigmoid layer and the BCELoss in one single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nickprock/sentence-bert-base-italian-xxl-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = len(mlb.classes_))\n",
    "model.to(device) # move model to device GPU\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)\n",
    "loss_fn = BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Quante epoche attendere dopo l'ultima volta che si Ã¨ visto migliorare la loss di validazione.\n",
    "            verbose (bool): Se True, stampa un messaggio per ogni epoch in cui la loss di validazione migliora.\n",
    "            delta (float): Minimo cambiamento nella loss di validazione per qualificarsi come miglioramento.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"Salva il modello quando la loss di validazione diminuisce.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')\n",
    "        torch.save(model.state_dict(), '../models/checkpoints/checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisione del dataset in training e validation\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size = 0.2, random_state = 42)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss_fn, optimizer, epochs, early_stopping, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        step = 0\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            step += 1\n",
    "            inputs, masks, labels = batch\n",
    "            inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, attention_mask = masks)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            print(f\"Epoch: {epoch + 1} - Batch: {step}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "        val_loss = validate(model, val_loader, loss_fn, device)\n",
    "        print(f\"Epoch {epoch + 1} - Batch {step}, Training Loss: {total_loss / len(train_loader)}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "def validate(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, masks, labels = batch\n",
    "            inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "            outputs = model(inputs, attention_mask = masks)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "# Inizializzazione dell'EarlyStopping\n",
    "early_stopping = EarlyStopping(patience = 5, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Batch: 1, Training Loss: 0.0002931067303045472\n",
      "Epoch: 1 - Batch: 2, Training Loss: 0.0005330798983771607\n",
      "Epoch: 1 - Batch: 3, Training Loss: 0.0007044225931167603\n",
      "Epoch: 1 - Batch: 4, Training Loss: 0.0008180482223456969\n",
      "Epoch: 1 - Batch: 5, Training Loss: 0.0009122861373187297\n",
      "Epoch: 1 - Batch: 6, Training Loss: 0.00100462429633188\n",
      "Epoch: 1 - Batch: 7, Training Loss: 0.0010921313469089678\n",
      "Epoch: 1 - Batch: 8, Training Loss: 0.0011888883983219045\n",
      "Epoch: 1 - Batch: 9, Training Loss: 0.0012831542993066323\n",
      "Epoch: 1 - Batch: 10, Training Loss: 0.0013591723198420175\n",
      "Epoch: 1 - Batch: 11, Training Loss: 0.0014351644116155741\n",
      "Epoch: 1 - Batch: 12, Training Loss: 0.0015300974793497405\n",
      "Epoch: 1 - Batch: 13, Training Loss: 0.0016094569184788028\n",
      "Epoch: 1 - Batch: 14, Training Loss: 0.0017040945391848708\n",
      "Epoch: 1 - Batch: 15, Training Loss: 0.0018010287241358464\n",
      "Epoch: 1 - Batch: 16, Training Loss: 0.0018859861687344698\n",
      "Epoch: 1 - Batch: 17, Training Loss: 0.0019691622017529078\n",
      "Epoch: 1 - Batch: 18, Training Loss: 0.0020411692213755145\n",
      "Epoch: 1 - Batch: 19, Training Loss: 0.0021256498658834997\n",
      "Epoch: 1 - Batch: 20, Training Loss: 0.002213974095646224\n",
      "Epoch: 1 - Batch: 21, Training Loss: 0.0023107209102332495\n",
      "Epoch: 1 - Batch: 22, Training Loss: 0.003897209140544705\n",
      "Epoch: 1 - Batch: 23, Training Loss: 0.005257814152669749\n",
      "Epoch: 1 - Batch: 24, Training Loss: 0.006066922790326092\n",
      "Epoch: 1 - Batch: 25, Training Loss: 0.00617160537522626\n",
      "Epoch: 1 - Batch: 26, Training Loss: 0.0062768193730964\n",
      "Epoch: 1 - Batch: 27, Training Loss: 0.006379616238288025\n",
      "Epoch: 1 - Batch: 28, Training Loss: 0.006486516847440457\n",
      "Epoch: 1 - Batch: 29, Training Loss: 0.006582254818818265\n",
      "Epoch: 1 - Batch: 30, Training Loss: 0.006684703496014499\n",
      "Epoch: 1 - Batch: 31, Training Loss: 0.00677508566757142\n",
      "Epoch: 1 - Batch: 32, Training Loss: 0.006869256150465502\n",
      "Epoch: 1 - Batch: 33, Training Loss: 0.006956947884008066\n",
      "Epoch: 1 - Batch: 34, Training Loss: 0.007042004515233127\n",
      "Epoch: 1 - Batch: 35, Training Loss: 0.00713227838772051\n",
      "Epoch: 1 - Batch: 36, Training Loss: 0.007229693586108696\n",
      "Epoch: 1 - Batch: 37, Training Loss: 0.007314465501365773\n",
      "Epoch: 1 - Batch: 38, Training Loss: 0.007406457529633397\n",
      "Epoch: 1 - Batch: 39, Training Loss: 0.007496572326664901\n",
      "Epoch: 1 - Batch: 40, Training Loss: 0.007586696860110187\n",
      "Epoch: 1 - Batch: 41, Training Loss: 0.007681898241414755\n",
      "Epoch: 1 - Batch: 42, Training Loss: 0.007769970961569949\n",
      "Epoch: 1 - Batch: 43, Training Loss: 0.007863467012471821\n",
      "Epoch: 1 - Batch: 44, Training Loss: 0.007955373476443204\n",
      "Epoch: 1 - Batch: 45, Training Loss: 0.008044865204771953\n",
      "Epoch: 1 - Batch: 46, Training Loss: 0.00813773241678676\n",
      "Epoch: 1 - Batch: 47, Training Loss: 0.008229754062038947\n",
      "Epoch: 1 - Batch: 48, Training Loss: 0.0083088651608373\n",
      "Epoch: 1 - Batch: 49, Training Loss: 0.008396975436613928\n",
      "Epoch: 1 - Batch: 50, Training Loss: 0.00848652005418023\n",
      "Epoch: 1 - Batch: 51, Training Loss: 0.008581591328331091\n",
      "Epoch: 1 - Batch: 52, Training Loss: 0.008663548487138194\n",
      "Epoch: 1 - Batch: 53, Training Loss: 0.008761157091725525\n",
      "Epoch: 1 - Batch: 54, Training Loss: 0.008847011917364934\n",
      "Epoch: 1 - Batch: 55, Training Loss: 0.008929262070562907\n",
      "Epoch: 1 - Batch: 56, Training Loss: 0.009013431885003254\n",
      "Epoch: 1 - Batch: 57, Training Loss: 0.009109805305650578\n",
      "Epoch: 1 - Batch: 58, Training Loss: 0.009203475382592943\n",
      "Epoch: 1 - Batch: 59, Training Loss: 0.009299425658450197\n",
      "Epoch: 1 - Batch: 60, Training Loss: 0.009397868592149979\n",
      "Epoch: 1 - Batch: 61, Training Loss: 0.00948472612631657\n",
      "Epoch: 1 - Batch: 62, Training Loss: 0.009574551122945736\n",
      "Epoch: 1 - Batch: 63, Training Loss: 0.009658974053254768\n",
      "Epoch: 1 - Batch: 64, Training Loss: 0.00974477017622682\n",
      "Epoch: 1 - Batch: 65, Training Loss: 0.0098375313811832\n",
      "Epoch: 1 - Batch: 66, Training Loss: 0.009923567547529294\n",
      "Epoch: 1 - Batch: 67, Training Loss: 0.010013598528618045\n",
      "Epoch: 1 - Batch: 68, Training Loss: 0.010095783195784238\n",
      "Epoch: 1 - Batch: 69, Training Loss: 0.010181859543370963\n",
      "Epoch: 1 - Batch: 70, Training Loss: 0.010273610762201534\n",
      "Epoch: 1 - Batch: 71, Training Loss: 0.010361206564885466\n",
      "Epoch: 1 - Batch: 72, Training Loss: 0.010438997860670485\n",
      "Epoch: 1 - Batch: 73, Training Loss: 0.010527550414278733\n",
      "Epoch: 1 - Batch: 74, Training Loss: 0.010613375752670058\n",
      "Epoch: 1 - Batch: 75, Training Loss: 0.010702803125527764\n",
      "Epoch: 1 - Batch: 76, Training Loss: 0.010785516172301512\n",
      "Epoch: 1 - Batch: 77, Training Loss: 0.010870025556528351\n",
      "Epoch: 1 - Batch: 78, Training Loss: 0.010963589836461824\n",
      "Epoch: 1 - Batch: 79, Training Loss: 0.011055103198904698\n",
      "Epoch: 1 - Batch: 80, Training Loss: 0.011143410760618956\n",
      "Epoch: 1 - Batch: 81, Training Loss: 0.011230149122563563\n",
      "Epoch: 1 - Batch: 82, Training Loss: 0.011318116373140025\n",
      "Epoch: 1 - Batch: 83, Training Loss: 0.011401611823545365\n",
      "Epoch: 1 - Batch: 84, Training Loss: 0.01148472044919656\n",
      "Epoch: 1 - Batch: 85, Training Loss: 0.011576260012230074\n",
      "Epoch: 1 - Batch: 86, Training Loss: 0.011665153260007624\n",
      "Epoch: 1 - Batch: 87, Training Loss: 0.011754203402432637\n",
      "Epoch: 1 - Batch: 88, Training Loss: 0.011847641084275239\n",
      "Epoch: 1 - Batch: 89, Training Loss: 0.011928078547045959\n",
      "Epoch: 1 - Batch: 90, Training Loss: 0.012005513944782033\n",
      "Epoch: 1 - Batch: 91, Training Loss: 0.012093483783910128\n",
      "Epoch: 1 - Batch: 92, Training Loss: 0.012184041056466933\n",
      "Epoch: 1 - Batch: 93, Training Loss: 0.012278109727115379\n",
      "Epoch: 1 - Batch: 94, Training Loss: 0.01236216124042152\n",
      "Epoch: 1 - Batch: 95, Training Loss: 0.012450613390351606\n",
      "Epoch: 1 - Batch: 96, Training Loss: 0.012537108521111569\n",
      "Epoch: 1 - Batch: 97, Training Loss: 0.012623902558539044\n",
      "Epoch: 1 - Batch: 98, Training Loss: 0.012707969300436538\n",
      "Epoch: 1 - Batch: 99, Training Loss: 0.012791431680395829\n",
      "Epoch: 1 - Batch: 100, Training Loss: 0.012874818423988412\n",
      "Epoch: 1 - Batch: 101, Training Loss: 0.012957389829417762\n",
      "Epoch: 1 - Batch: 102, Training Loss: 0.013051912176174114\n",
      "Epoch: 1 - Batch: 103, Training Loss: 0.013138542652970325\n",
      "Epoch: 1 - Batch: 104, Training Loss: 0.013227292319534232\n",
      "Epoch: 1 - Batch: 105, Training Loss: 0.013320354414272862\n",
      "Epoch: 1 - Batch: 106, Training Loss: 0.013399468293138601\n",
      "Epoch: 1 - Batch: 107, Training Loss: 0.013488842381371392\n",
      "Epoch: 1 - Batch: 108, Training Loss: 0.01357039344029047\n",
      "Epoch: 1 - Batch: 109, Training Loss: 0.013669431536994368\n",
      "Epoch: 1 - Batch: 110, Training Loss: 0.013751700366067253\n",
      "Epoch: 1 - Batch: 111, Training Loss: 0.013850682367189211\n",
      "Epoch: 1 - Batch: 112, Training Loss: 0.013939144939283036\n",
      "Epoch: 1 - Batch: 113, Training Loss: 0.014031942173912751\n",
      "Epoch: 1 - Batch: 114, Training Loss: 0.014118976804549817\n",
      "Epoch: 1 - Batch: 115, Training Loss: 0.014203670482532698\n",
      "Epoch: 1 - Batch: 116, Training Loss: 0.014291514143668992\n",
      "Epoch: 1 - Batch: 117, Training Loss: 0.014378707009563793\n",
      "Epoch: 1 - Batch: 118, Training Loss: 0.01446916012844043\n",
      "Epoch: 1 - Batch: 119, Training Loss: 0.01456717422130096\n",
      "Epoch: 1 - Batch: 120, Training Loss: 0.014649062242921115\n",
      "Epoch: 1 - Batch: 121, Training Loss: 0.014732063860689626\n",
      "Epoch: 1 - Batch: 122, Training Loss: 0.014828577180505788\n",
      "Epoch: 1 - Batch: 123, Training Loss: 0.014912960442342173\n",
      "Epoch: 1 - Batch: 124, Training Loss: 0.015005443813541833\n",
      "Epoch: 1 - Batch: 125, Training Loss: 0.015101190359883047\n",
      "Epoch: 1 - Batch: 126, Training Loss: 0.015178350129382527\n",
      "Epoch: 1 - Batch: 127, Training Loss: 0.015268676656346219\n",
      "Epoch: 1 - Batch: 128, Training Loss: 0.015355400586652123\n",
      "Epoch: 1 - Batch: 129, Training Loss: 0.015450278090106117\n",
      "Epoch: 1 - Batch: 130, Training Loss: 0.01553732693615442\n",
      "Epoch: 1 - Batch: 131, Training Loss: 0.0156228244984229\n",
      "Epoch: 1 - Batch: 132, Training Loss: 0.01572328348756825\n",
      "Epoch: 1 - Batch: 133, Training Loss: 0.01581103899585667\n",
      "Epoch: 1 - Batch: 134, Training Loss: 0.0159014944252782\n",
      "Epoch: 1 - Batch: 135, Training Loss: 0.01599604737130959\n",
      "Epoch: 1 - Batch: 136, Training Loss: 0.016084467606066077\n",
      "Epoch: 1 - Batch: 137, Training Loss: 0.016175076612587987\n",
      "Epoch: 1 - Batch: 138, Training Loss: 0.016263692897696597\n",
      "Epoch: 1 - Batch: 139, Training Loss: 0.016353184366552392\n",
      "Epoch: 1 - Batch: 140, Training Loss: 0.01644319537440145\n",
      "Epoch: 1 - Batch: 141, Training Loss: 0.01653801647337713\n",
      "Epoch: 1 - Batch: 142, Training Loss: 0.01662460898063076\n",
      "Epoch: 1 - Batch: 143, Training Loss: 0.016712245193138645\n",
      "Epoch: 1 - Batch: 144, Training Loss: 0.01681040033426253\n",
      "Epoch: 1 - Batch: 145, Training Loss: 0.016890268371275212\n",
      "Epoch: 1 - Batch: 146, Training Loss: 0.016983702253692383\n",
      "Epoch: 1 - Batch: 147, Training Loss: 0.017063206732025986\n",
      "Epoch: 1 - Batch: 148, Training Loss: 0.01715739283889878\n",
      "Epoch: 1 - Batch: 149, Training Loss: 0.01725838486580904\n",
      "Epoch: 1 - Batch: 150, Training Loss: 0.01733853947834589\n",
      "Epoch: 1 - Batch: 151, Training Loss: 0.017422021794477306\n",
      "Epoch: 1 - Batch: 152, Training Loss: 0.01750684110061644\n",
      "Epoch: 1 - Batch: 153, Training Loss: 0.017589491138707346\n",
      "Epoch: 1 - Batch: 154, Training Loss: 0.017686275329755908\n",
      "Epoch: 1 - Batch: 155, Training Loss: 0.017771411440018595\n",
      "Epoch: 1 - Batch: 156, Training Loss: 0.017857493650458543\n",
      "Epoch: 1 - Batch: 157, Training Loss: 0.017941071619441855\n",
      "Epoch: 1 - Batch: 158, Training Loss: 0.0180264822273804\n",
      "Epoch: 1 - Batch: 159, Training Loss: 0.018114446839981806\n",
      "Epoch: 1 - Batch: 160, Training Loss: 0.01820815353967855\n",
      "Epoch: 1 - Batch: 161, Training Loss: 0.0182982398441083\n",
      "Epoch: 1 - Batch: 162, Training Loss: 0.01839415682093619\n",
      "Epoch: 1 - Batch: 163, Training Loss: 0.018478933128699734\n",
      "Epoch: 1 - Batch: 164, Training Loss: 0.018574082257093283\n",
      "Epoch: 1 - Batch: 165, Training Loss: 0.018663455949938713\n",
      "Epoch: 1 - Batch: 166, Training Loss: 0.018745952942231005\n",
      "Epoch: 1 - Batch: 167, Training Loss: 0.018833129072120137\n",
      "Epoch: 1 - Batch: 168, Training Loss: 0.01892491944665537\n",
      "Epoch: 1 - Batch: 169, Training Loss: 0.01901084707299275\n",
      "Epoch: 1 - Batch: 170, Training Loss: 0.01909838481551379\n",
      "Epoch: 1 - Batch: 171, Training Loss: 0.01918663577505605\n",
      "Epoch: 1 - Batch: 172, Training Loss: 0.01927870354471515\n",
      "Epoch: 1 - Batch: 173, Training Loss: 0.019364472806058318\n",
      "Epoch: 1 - Batch: 174, Training Loss: 0.019447141550913774\n",
      "Epoch: 1 - Batch: 175, Training Loss: 0.019533658390911063\n",
      "Epoch: 1 - Batch: 176, Training Loss: 0.019628054083975197\n",
      "Epoch: 1 - Batch: 177, Training Loss: 0.019720382049407927\n",
      "Epoch: 1 - Batch: 178, Training Loss: 0.019804345459289614\n",
      "Epoch: 1 - Batch: 179, Training Loss: 0.019886682313522495\n",
      "Epoch: 1 - Batch: 180, Training Loss: 0.019981733354960705\n",
      "Epoch: 1 - Batch: 181, Training Loss: 0.020078449211952894\n",
      "Epoch: 1 - Batch: 182, Training Loss: 0.020165478418369593\n",
      "Epoch: 1 - Batch: 183, Training Loss: 0.020252926272045123\n",
      "Epoch: 1 - Batch: 184, Training Loss: 0.020344706780430096\n",
      "Epoch: 1 - Batch: 185, Training Loss: 0.020433133569967687\n",
      "Epoch: 1 - Batch: 186, Training Loss: 0.020520053136704575\n",
      "Epoch: 1 - Batch: 187, Training Loss: 0.020606527559051468\n",
      "Epoch: 1 - Batch: 188, Training Loss: 0.02069409254005497\n",
      "Epoch: 1 - Batch: 189, Training Loss: 0.020781641143372602\n",
      "Epoch: 1 - Batch: 190, Training Loss: 0.020879251107103987\n",
      "Epoch: 1 - Batch: 191, Training Loss: 0.02096665154518575\n",
      "Epoch: 1 - Batch: 192, Training Loss: 0.021060370755185732\n",
      "Epoch: 1 - Batch: 193, Training Loss: 0.021149580115158956\n",
      "Epoch: 1 - Batch: 194, Training Loss: 0.021237015575911868\n",
      "Epoch: 1 - Batch: 195, Training Loss: 0.021320303651823927\n",
      "Epoch: 1 - Batch: 196, Training Loss: 0.021404840799657068\n",
      "Epoch: 1 - Batch: 197, Training Loss: 0.021497282303041883\n",
      "Epoch: 1 - Batch: 198, Training Loss: 0.021583318883309117\n",
      "Epoch: 1 - Batch: 199, Training Loss: 0.021673561557262494\n",
      "Epoch: 1 - Batch: 200, Training Loss: 0.021758702708714044\n",
      "Epoch: 1 - Batch: 201, Training Loss: 0.021845797920108434\n",
      "Epoch: 1 - Batch: 202, Training Loss: 0.021934212248766204\n",
      "Epoch: 1 - Batch: 203, Training Loss: 0.022013424931226877\n",
      "Epoch: 1 - Batch: 204, Training Loss: 0.022088729747096896\n",
      "Epoch: 1 - Batch: 205, Training Loss: 0.022172422613176344\n",
      "Epoch: 1 - Batch: 206, Training Loss: 0.02225106040241311\n",
      "Epoch: 1 - Batch: 207, Training Loss: 0.02234183834436323\n",
      "Epoch: 1 - Batch: 208, Training Loss: 0.022426306280874296\n",
      "Epoch: 1 - Batch: 209, Training Loss: 0.02251724227274433\n",
      "Epoch: 1 - Batch: 210, Training Loss: 0.02260752748172872\n",
      "Epoch: 1 - Batch: 211, Training Loss: 0.022691902656657978\n",
      "Epoch: 1 - Batch: 212, Training Loss: 0.02276966694254385\n",
      "Epoch: 1 - Batch: 213, Training Loss: 0.022855449239314096\n",
      "Epoch: 1 - Batch: 214, Training Loss: 0.022941317508123803\n",
      "Epoch: 1 - Batch: 215, Training Loss: 0.023026993278889712\n",
      "Epoch: 1 - Batch: 216, Training Loss: 0.02310682259883058\n",
      "Epoch: 1 - Batch: 217, Training Loss: 0.02319327567643787\n",
      "Epoch: 1 - Batch: 218, Training Loss: 0.02328110901425727\n",
      "Epoch: 1 - Batch: 219, Training Loss: 0.02336673520715478\n",
      "Epoch: 1 - Batch: 220, Training Loss: 0.02345587334180155\n",
      "Epoch: 1 - Batch: 221, Training Loss: 0.023546324767926043\n",
      "Epoch: 1 - Batch: 222, Training Loss: 0.02364133399097283\n",
      "Epoch: 1 - Batch: 223, Training Loss: 0.023731332018996156\n",
      "Epoch: 1 - Batch: 224, Training Loss: 0.023818593731715312\n",
      "Epoch: 1 - Batch: 225, Training Loss: 0.023906105440449753\n",
      "Epoch: 1 - Batch: 226, Training Loss: 0.023991071731840594\n",
      "Epoch: 1 - Batch: 227, Training Loss: 0.02407917864682465\n",
      "Epoch: 1 - Batch: 228, Training Loss: 0.02416900872171024\n",
      "Epoch: 1 - Batch: 229, Training Loss: 0.024247837859896284\n",
      "Epoch: 1 - Batch: 230, Training Loss: 0.024340742497796046\n",
      "Epoch: 1 - Batch: 231, Training Loss: 0.024426801658388394\n",
      "Epoch: 1 - Batch: 232, Training Loss: 0.02451967115005846\n",
      "Epoch: 1 - Batch: 233, Training Loss: 0.024611500062505603\n",
      "Epoch: 1 - Batch: 234, Training Loss: 0.024702304347138698\n",
      "Epoch: 1 - Batch: 235, Training Loss: 0.02478742972634127\n",
      "Epoch: 1 - Batch: 236, Training Loss: 0.024867903428834864\n",
      "Epoch: 1 - Batch: 237, Training Loss: 0.024956657926538096\n",
      "Epoch: 1 - Batch: 238, Training Loss: 0.025038382239256727\n",
      "Epoch: 1 - Batch: 239, Training Loss: 0.025130375812006233\n",
      "Epoch: 1 - Batch: 240, Training Loss: 0.025216882563447875\n",
      "Epoch: 1 - Batch: 241, Training Loss: 0.025299430140610753\n",
      "Epoch: 1 - Batch: 242, Training Loss: 0.025390336583453427\n",
      "Epoch: 1 - Batch: 243, Training Loss: 0.025489123003745753\n",
      "Epoch: 1 - Batch: 244, Training Loss: 0.025577290036142564\n",
      "Epoch: 1 - Batch: 245, Training Loss: 0.025666289846992017\n",
      "Epoch: 1 - Batch: 246, Training Loss: 0.025748498286823334\n",
      "Epoch: 1 - Batch: 247, Training Loss: 0.02584613514925117\n",
      "Epoch: 1 - Batch: 248, Training Loss: 0.025939014951882274\n",
      "Epoch: 1 - Batch: 249, Training Loss: 0.02601849247591808\n",
      "Epoch: 1 - Batch: 250, Training Loss: 0.026107006188895967\n",
      "Epoch: 1 - Batch: 251, Training Loss: 0.026195601771769435\n",
      "Epoch: 1 - Batch: 252, Training Loss: 0.026285982775698057\n",
      "Epoch: 1 - Batch: 253, Training Loss: 0.026375408387846418\n",
      "Epoch: 1 - Batch: 254, Training Loss: 0.026469732182189995\n",
      "Epoch: 1 - Batch: 255, Training Loss: 0.026562159303359527\n",
      "Epoch: 1 - Batch: 256, Training Loss: 0.026647237260790407\n",
      "Epoch: 1 - Batch: 257, Training Loss: 0.02672283676380344\n",
      "Epoch: 1 - Batch: 258, Training Loss: 0.026806579281886418\n",
      "Epoch: 1 - Batch: 259, Training Loss: 0.026899338719955528\n",
      "Epoch: 1 - Batch: 260, Training Loss: 0.02698435074036592\n",
      "Epoch: 1 - Batch: 261, Training Loss: 0.027077746875645906\n",
      "Epoch: 1 - Batch: 262, Training Loss: 0.02716624750995122\n",
      "Epoch: 1 - Batch: 263, Training Loss: 0.0272455146199534\n",
      "Epoch: 1 - Batch: 264, Training Loss: 0.027332627561999792\n",
      "Epoch: 1 - Batch: 265, Training Loss: 0.02742598917502076\n",
      "Epoch: 1 - Batch: 266, Training Loss: 0.027512580131614586\n",
      "Epoch: 1 - Batch: 267, Training Loss: 0.02760702948615721\n",
      "Epoch: 1 - Batch: 268, Training Loss: 0.02769922148776094\n",
      "Epoch: 1 - Batch: 269, Training Loss: 0.027790981127106727\n",
      "Epoch: 1 - Batch: 270, Training Loss: 0.027875995291257973\n",
      "Epoch: 1 - Batch: 271, Training Loss: 0.02796774482351433\n",
      "Epoch: 1 - Batch: 272, Training Loss: 0.028049560748720247\n",
      "Epoch: 1 - Batch: 273, Training Loss: 0.028131429958551085\n",
      "Epoch: 1 - Batch: 274, Training Loss: 0.02821999922345329\n",
      "Epoch: 1 - Batch: 275, Training Loss: 0.02831028614372361\n",
      "Epoch: 1 - Batch: 276, Training Loss: 0.028400108249082692\n",
      "Epoch: 1 - Batch: 277, Training Loss: 0.02849090583684235\n",
      "Epoch: 1 - Batch: 278, Training Loss: 0.028575506530838027\n",
      "Epoch: 1 - Batch: 279, Training Loss: 0.028665177054567323\n",
      "Epoch: 1 - Batch: 280, Training Loss: 0.028752439409790942\n",
      "Epoch: 1 - Batch: 281, Training Loss: 0.02884205166948573\n",
      "Epoch: 1 - Batch: 282, Training Loss: 0.028931515282066306\n",
      "Epoch: 1 - Batch: 283, Training Loss: 0.029009713400556872\n",
      "Epoch: 1 - Batch: 284, Training Loss: 0.029101784561898183\n",
      "Epoch: 1 - Batch: 285, Training Loss: 0.029193583882072473\n",
      "Epoch: 1 - Batch: 286, Training Loss: 0.029283975023980165\n",
      "Epoch: 1 - Batch: 287, Training Loss: 0.029375219325323405\n",
      "Epoch: 1 - Batch: 288, Training Loss: 0.0294558608388624\n",
      "Epoch: 1 - Batch: 289, Training Loss: 0.0295449910348723\n",
      "Epoch: 1 - Batch: 290, Training Loss: 0.029625929706725316\n",
      "Epoch: 1 - Batch: 291, Training Loss: 0.029719792170508783\n",
      "Epoch: 1 - Batch: 292, Training Loss: 0.02981357043531799\n",
      "Epoch: 1 - Batch: 293, Training Loss: 0.029903696371152824\n",
      "Epoch: 1 - Batch: 294, Training Loss: 0.02999278711491754\n",
      "Epoch: 1 - Batch: 295, Training Loss: 0.03007710408141364\n",
      "Epoch: 1 - Batch: 296, Training Loss: 0.030162208455662625\n",
      "Epoch: 1 - Batch: 297, Training Loss: 0.03024865625114188\n",
      "Epoch: 1 - Batch: 298, Training Loss: 0.030340961333531053\n",
      "Epoch: 1 - Batch: 299, Training Loss: 0.030427711371758683\n",
      "Epoch: 1 - Batch: 300, Training Loss: 0.03052474478040366\n",
      "Epoch: 1 - Batch: 301, Training Loss: 0.030617681228452258\n",
      "Epoch: 1 - Batch: 302, Training Loss: 0.030704548696726314\n",
      "Epoch: 1 - Batch: 303, Training Loss: 0.030793873423318168\n",
      "Epoch: 1 - Batch: 304, Training Loss: 0.03088304113170401\n",
      "Epoch: 1 - Batch: 305, Training Loss: 0.030976537343231997\n",
      "Epoch: 1 - Batch: 306, Training Loss: 0.03105906001099109\n",
      "Epoch: 1 - Batch: 307, Training Loss: 0.031144946486153215\n",
      "Epoch: 1 - Batch: 308, Training Loss: 0.03122483911749538\n",
      "Epoch: 1 - Batch: 309, Training Loss: 0.0313114628418168\n",
      "Epoch: 1 - Batch: 310, Training Loss: 0.03139755999666344\n",
      "Epoch: 1 - Batch: 311, Training Loss: 0.03148955051133882\n",
      "Epoch: 1 - Batch: 312, Training Loss: 0.03159225859971189\n",
      "Epoch: 1 - Batch: 313, Training Loss: 0.03168752989044435\n",
      "Epoch: 1 - Batch: 314, Training Loss: 0.03176852307632986\n",
      "Epoch: 1 - Batch: 315, Training Loss: 0.031847243767176105\n",
      "Epoch: 1 - Batch: 316, Training Loss: 0.03192912782874471\n",
      "Epoch: 1 - Batch: 317, Training Loss: 0.03202547159533991\n",
      "Epoch: 1 - Batch: 318, Training Loss: 0.03211598648113596\n",
      "Epoch: 1 - Batch: 319, Training Loss: 0.032204845058384224\n",
      "Epoch: 1 - Batch: 320, Training Loss: 0.03229827742103123\n",
      "Epoch: 1 - Batch: 321, Training Loss: 0.03238489838381905\n",
      "Epoch: 1 - Batch: 322, Training Loss: 0.03246810668415296\n",
      "Epoch: 1 - Batch: 323, Training Loss: 0.032549838645145865\n",
      "Epoch: 1 - Batch: 324, Training Loss: 0.03264262621749693\n",
      "Epoch: 1 - Batch: 325, Training Loss: 0.032727307743496366\n",
      "Epoch: 1 - Batch: 326, Training Loss: 0.03281633917915683\n",
      "Epoch: 1 - Batch: 327, Training Loss: 0.032901967077162336\n",
      "Epoch: 1 - Batch: 328, Training Loss: 0.03299059243764648\n",
      "Epoch: 1 - Batch: 329, Training Loss: 0.03307968350266343\n",
      "Epoch: 1 - Batch: 330, Training Loss: 0.03316696035130502\n",
      "Epoch: 1 - Batch: 331, Training Loss: 0.03325810285893641\n",
      "Epoch: 1 - Batch: 332, Training Loss: 0.03334392733859581\n",
      "Epoch: 1 - Batch: 333, Training Loss: 0.03342747429438293\n",
      "Epoch: 1 - Batch: 334, Training Loss: 0.03351294312311049\n",
      "Epoch: 1 - Batch: 335, Training Loss: 0.033605096932766254\n",
      "Epoch: 1 - Batch: 336, Training Loss: 0.03369508055386258\n",
      "Epoch: 1 - Batch: 337, Training Loss: 0.033782723951300185\n",
      "Epoch: 1 - Batch: 338, Training Loss: 0.03387242198534075\n",
      "Epoch: 1 - Batch: 339, Training Loss: 0.03396162923864069\n",
      "Epoch: 1 - Batch: 340, Training Loss: 0.034049944498052646\n",
      "Epoch: 1 - Batch: 341, Training Loss: 0.03414407336642691\n",
      "Epoch: 1 - Batch: 342, Training Loss: 0.03423145965705464\n",
      "Epoch: 1 - Batch: 343, Training Loss: 0.03431035434922967\n",
      "Epoch: 1 - Batch: 344, Training Loss: 0.03439595895027047\n",
      "Epoch: 1 - Batch: 345, Training Loss: 0.034481989933335365\n",
      "Epoch: 1 - Batch: 346, Training Loss: 0.03456344913882798\n",
      "Epoch: 1 - Batch: 347, Training Loss: 0.03466271515163419\n",
      "Epoch: 1 - Batch: 348, Training Loss: 0.0347619827151002\n",
      "Epoch: 1 - Batch: 349, Training Loss: 0.034845159335021754\n",
      "Epoch: 1 - Batch: 350, Training Loss: 0.03492504250177894\n",
      "Epoch: 1 - Batch: 351, Training Loss: 0.03500997491341523\n",
      "Epoch: 1 - Batch: 352, Training Loss: 0.03509724966156147\n",
      "Epoch: 1 - Batch: 353, Training Loss: 0.03517397736534353\n",
      "Epoch: 1 - Batch: 354, Training Loss: 0.03526706293620676\n",
      "Epoch: 1 - Batch: 355, Training Loss: 0.03535675835080605\n",
      "Epoch: 1 - Batch: 356, Training Loss: 0.03544893784828447\n",
      "Epoch: 1 - Batch: 357, Training Loss: 0.03553524263709734\n",
      "Epoch: 1 - Batch: 358, Training Loss: 0.035624193599073845\n",
      "Epoch: 1 - Batch: 359, Training Loss: 0.03570951415754076\n",
      "Epoch: 1 - Batch: 360, Training Loss: 0.03579749660581894\n",
      "Epoch: 1 - Batch: 361, Training Loss: 0.035886457699596586\n",
      "Epoch: 1 - Batch: 362, Training Loss: 0.0359771488725546\n",
      "Epoch: 1 - Batch: 363, Training Loss: 0.03606748745883282\n",
      "Epoch: 1 - Batch: 364, Training Loss: 0.036152657547696906\n",
      "Epoch: 1 - Batch: 365, Training Loss: 0.03625273017543268\n",
      "Epoch: 1 - Batch: 366, Training Loss: 0.03635070859076174\n",
      "Epoch: 1 - Batch: 367, Training Loss: 0.03644562304440027\n",
      "Epoch: 1 - Batch: 368, Training Loss: 0.03653801877552004\n",
      "Epoch: 1 - Batch: 369, Training Loss: 0.03661700285612845\n",
      "Epoch: 1 - Batch: 370, Training Loss: 0.03670080405216707\n",
      "Epoch: 1 - Batch: 371, Training Loss: 0.03679013709424938\n",
      "Epoch: 1 - Batch: 372, Training Loss: 0.03687282706646382\n",
      "Epoch: 1 - Batch: 373, Training Loss: 0.03696183774428779\n",
      "Epoch: 1 - Batch: 374, Training Loss: 0.037035776688338906\n",
      "Epoch: 1 - Batch: 375, Training Loss: 0.03712909303568489\n",
      "Epoch: 1 - Batch: 376, Training Loss: 0.03721302021819956\n",
      "Epoch: 1 - Batch: 377, Training Loss: 0.03729999728908586\n",
      "Epoch: 1 - Batch: 378, Training Loss: 0.03737875596800847\n",
      "Epoch: 1 - Batch: 379, Training Loss: 0.03746228638548361\n",
      "Epoch: 1 - Batch: 380, Training Loss: 0.037561922357301804\n",
      "Epoch: 1 - Batch: 381, Training Loss: 0.03764556358208506\n",
      "Epoch: 1 - Batch: 382, Training Loss: 0.03773741713210718\n",
      "Epoch: 1 - Batch: 383, Training Loss: 0.03782214514701125\n",
      "Epoch: 1 - Batch: 384, Training Loss: 0.03790626631027233\n",
      "Epoch: 1 - Batch: 385, Training Loss: 0.03799825300081057\n",
      "Epoch: 1 - Batch: 386, Training Loss: 0.038079336928095585\n",
      "Epoch: 1 - Batch: 387, Training Loss: 0.038165911000413485\n",
      "Epoch: 1 - Batch: 388, Training Loss: 0.03825125347198934\n",
      "Epoch: 1 - Batch: 389, Training Loss: 0.0383363195739773\n",
      "Epoch: 1 - Batch: 390, Training Loss: 0.038423211265905186\n",
      "Epoch: 1 - Batch: 391, Training Loss: 0.038505551320304525\n",
      "Epoch: 1 - Batch: 392, Training Loss: 0.0386000629668904\n",
      "Epoch: 1 - Batch: 393, Training Loss: 0.038687123625136134\n",
      "Epoch: 1 - Batch: 394, Training Loss: 0.03877548339651592\n",
      "Epoch: 1 - Batch: 395, Training Loss: 0.03886437733622135\n",
      "Epoch: 1 - Batch: 396, Training Loss: 0.038949767711447245\n",
      "Epoch: 1 - Batch: 397, Training Loss: 0.039045487124330765\n",
      "Epoch: 1 - Batch: 398, Training Loss: 0.039126222276717276\n",
      "Epoch: 1 - Batch: 399, Training Loss: 0.03921356464845822\n",
      "Epoch: 1 - Batch: 400, Training Loss: 0.039298769945331276\n",
      "Epoch: 1 - Batch: 401, Training Loss: 0.039383785901081506\n",
      "Epoch: 1 - Batch: 402, Training Loss: 0.03946698080149061\n",
      "Epoch: 1 - Batch: 403, Training Loss: 0.039558859501855684\n",
      "Epoch: 1 - Batch: 404, Training Loss: 0.039635116330179604\n",
      "Epoch: 1 - Batch: 405, Training Loss: 0.03972071106507015\n",
      "Epoch: 1 - Batch: 406, Training Loss: 0.0398179220644198\n",
      "Epoch: 1 - Batch: 407, Training Loss: 0.03991228887567275\n",
      "Epoch: 1 - Batch: 408, Training Loss: 0.04000892358558688\n",
      "Epoch: 1 - Batch: 409, Training Loss: 0.04009385789384692\n",
      "Epoch: 1 - Batch: 410, Training Loss: 0.04018692190374308\n",
      "Epoch: 1 - Batch: 411, Training Loss: 0.04028036519514387\n",
      "Epoch: 1 - Batch: 412, Training Loss: 0.040377699932203956\n",
      "Epoch: 1 - Batch: 413, Training Loss: 0.04046756694491823\n",
      "Epoch: 1 - Batch: 414, Training Loss: 0.04055823752066587\n",
      "Epoch: 1 - Batch: 415, Training Loss: 0.040643930929414866\n",
      "Epoch: 1 - Batch: 416, Training Loss: 0.04072884161328004\n",
      "Epoch: 1 - Batch: 417, Training Loss: 0.040819804627405074\n",
      "Epoch: 1 - Batch: 418, Training Loss: 0.04091131929956859\n",
      "Epoch: 1 - Batch: 419, Training Loss: 0.041008535105345856\n",
      "Epoch: 1 - Batch: 420, Training Loss: 0.041100779230124125\n",
      "Epoch: 1 - Batch: 421, Training Loss: 0.04117453240389452\n",
      "Epoch: 1 - Batch: 422, Training Loss: 0.04126677512010531\n",
      "Epoch: 1 - Batch: 423, Training Loss: 0.04135342581725832\n",
      "Epoch: 1 - Batch: 424, Training Loss: 0.041437518141953705\n",
      "Epoch: 1 - Batch: 425, Training Loss: 0.04153293751760897\n",
      "Epoch: 1 - Batch: 426, Training Loss: 0.04162123500930136\n",
      "Epoch: 1 - Batch: 427, Training Loss: 0.04171275151630935\n",
      "Epoch: 1 - Batch: 428, Training Loss: 0.041804159297970794\n",
      "Epoch: 1 - Batch: 429, Training Loss: 0.0418952371288789\n",
      "Epoch: 1 - Batch: 430, Training Loss: 0.04199086662202728\n",
      "Epoch: 1 - Batch: 431, Training Loss: 0.04208011487205428\n",
      "Epoch: 1 - Batch: 432, Training Loss: 0.04217619400094595\n",
      "Epoch: 1 - Batch: 433, Training Loss: 0.04226321921024354\n",
      "Epoch: 1 - Batch: 434, Training Loss: 0.042356976927265796\n",
      "Epoch: 1 - Batch: 435, Training Loss: 0.04243843499601974\n",
      "Epoch: 1 - Batch: 436, Training Loss: 0.0425250927298322\n",
      "Epoch: 1 - Batch: 437, Training Loss: 0.042625627064981665\n",
      "Epoch: 1 - Batch: 438, Training Loss: 0.04271673641567602\n",
      "Epoch: 1 - Batch: 439, Training Loss: 0.042811389796275204\n",
      "Epoch: 1 - Batch: 440, Training Loss: 0.04290121961786577\n",
      "Epoch: 1 - Batch: 441, Training Loss: 0.042983920790662815\n",
      "Epoch: 1 - Batch: 442, Training Loss: 0.04307562194974664\n",
      "Epoch: 1 - Batch: 443, Training Loss: 0.04316335973973891\n",
      "Epoch: 1 - Batch: 444, Training Loss: 0.04326242168942099\n",
      "Epoch: 1 - Batch: 445, Training Loss: 0.04335323617381243\n",
      "Epoch: 1 - Batch: 446, Training Loss: 0.043435574609594754\n",
      "Epoch: 1 - Batch: 447, Training Loss: 0.043528563378511576\n",
      "Epoch: 1 - Batch: 448, Training Loss: 0.04361861640862367\n",
      "Epoch: 1 - Batch: 449, Training Loss: 0.04372132181577619\n",
      "Epoch: 1 - Batch: 450, Training Loss: 0.043811831308241504\n",
      "Epoch: 1 - Batch: 451, Training Loss: 0.04389761212848708\n",
      "Epoch: 1 - Batch: 452, Training Loss: 0.0439812554105202\n",
      "Epoch: 1 - Batch: 453, Training Loss: 0.04405950875968285\n",
      "Epoch: 1 - Batch: 454, Training Loss: 0.04414329124277899\n",
      "Epoch: 1 - Batch: 455, Training Loss: 0.04421785956319687\n",
      "Epoch: 1 - Batch: 456, Training Loss: 0.04430290299219081\n",
      "Epoch: 1 - Batch: 457, Training Loss: 0.044395111532106526\n",
      "Epoch: 1 - Batch: 458, Training Loss: 0.0444821672171206\n",
      "Epoch: 1 - Batch: 459, Training Loss: 0.044579848121993775\n",
      "Epoch: 1 - Batch: 460, Training Loss: 0.04466630192859651\n",
      "Epoch: 1 - Batch: 461, Training Loss: 0.044752886188316895\n",
      "Epoch: 1 - Batch: 462, Training Loss: 0.04484825553511506\n",
      "Epoch: 1 - Batch: 463, Training Loss: 0.04493055600135481\n",
      "Epoch: 1 - Batch: 464, Training Loss: 0.04502801232620653\n",
      "Epoch: 1 - Batch: 465, Training Loss: 0.04512669682922846\n",
      "Epoch: 1 - Batch: 466, Training Loss: 0.045219873067356066\n",
      "Epoch: 1 - Batch: 467, Training Loss: 0.045310068375139095\n",
      "Epoch: 1 - Batch: 468, Training Loss: 0.04539472528827526\n",
      "Epoch: 1 - Batch: 469, Training Loss: 0.0454806454516762\n",
      "Epoch: 1 - Batch: 470, Training Loss: 0.04556848003743695\n",
      "Epoch: 1 - Batch: 471, Training Loss: 0.045656649516293064\n",
      "Epoch: 1 - Batch: 472, Training Loss: 0.04574152077262477\n",
      "Epoch: 1 - Batch: 473, Training Loss: 0.04583263200758701\n",
      "Epoch: 1 - Batch: 474, Training Loss: 0.04591464610835213\n",
      "Epoch: 1 - Batch: 475, Training Loss: 0.04599836653416627\n",
      "Epoch: 1 - Batch: 476, Training Loss: 0.046078359705101\n",
      "Epoch: 1 - Batch: 477, Training Loss: 0.046170823850590195\n",
      "Epoch: 1 - Batch: 478, Training Loss: 0.04626314835020559\n",
      "Epoch: 1 - Batch: 479, Training Loss: 0.04635068893457329\n",
      "Epoch: 1 - Batch: 480, Training Loss: 0.04644195414820121\n",
      "Epoch: 1 - Batch: 481, Training Loss: 0.0465348016958727\n",
      "Epoch: 1 - Batch: 482, Training Loss: 0.04662252046402612\n",
      "Epoch: 1 - Batch: 483, Training Loss: 0.04670851281651019\n",
      "Epoch: 1 - Batch: 484, Training Loss: 0.04679902695477701\n",
      "Epoch: 1 - Batch: 485, Training Loss: 0.04688486895503887\n",
      "Epoch: 1 - Batch: 486, Training Loss: 0.046981771768473866\n",
      "Epoch: 1 - Batch: 487, Training Loss: 0.04707472999989492\n",
      "Epoch: 1 - Batch: 488, Training Loss: 0.04715745561257326\n",
      "Epoch: 1 - Batch: 489, Training Loss: 0.04724963040864883\n",
      "Epoch: 1 - Batch: 490, Training Loss: 0.04734577924309678\n",
      "Epoch: 1 - Batch: 491, Training Loss: 0.047435518125594156\n",
      "Epoch: 1 - Batch: 492, Training Loss: 0.04753342338190545\n",
      "Epoch: 1 - Batch: 493, Training Loss: 0.047626929187754886\n",
      "Epoch: 1 - Batch: 494, Training Loss: 0.047709110580619496\n",
      "Epoch: 1 - Batch: 495, Training Loss: 0.04779563809607555\n",
      "Epoch: 1 - Batch: 496, Training Loss: 0.047887978380295765\n",
      "Epoch: 1 - Batch: 497, Training Loss: 0.04797803503067339\n",
      "Epoch: 1 - Batch: 498, Training Loss: 0.048061909719337874\n",
      "Epoch: 1 - Batch: 499, Training Loss: 0.048153989257948906\n",
      "Epoch: 1 - Batch: 500, Training Loss: 0.04824920122204333\n",
      "Epoch: 1 - Batch: 501, Training Loss: 0.04833860383994544\n",
      "Epoch: 1 - Batch: 502, Training Loss: 0.04842603689130661\n",
      "Epoch: 1 - Batch: 503, Training Loss: 0.04851816146083139\n",
      "Epoch: 1 - Batch: 504, Training Loss: 0.048608586501027416\n",
      "Epoch: 1 - Batch: 505, Training Loss: 0.04869328581945813\n",
      "Epoch: 1 - Batch: 506, Training Loss: 0.04878967432660448\n",
      "Epoch: 1 - Batch: 507, Training Loss: 0.04887749525296747\n",
      "Epoch: 1 - Batch: 508, Training Loss: 0.0489604657895826\n",
      "Epoch: 1 - Batch: 509, Training Loss: 0.04904257619805992\n",
      "Epoch: 1 - Batch: 510, Training Loss: 0.04912685506527697\n",
      "Epoch: 1 - Batch: 511, Training Loss: 0.04920976616405136\n",
      "Epoch: 1 - Batch: 512, Training Loss: 0.04929489810421297\n",
      "Epoch: 1 - Batch: 513, Training Loss: 0.04938326855105152\n",
      "Epoch: 1 - Batch: 514, Training Loss: 0.04946404872199592\n",
      "Epoch: 1 - Batch: 515, Training Loss: 0.049559877539798\n",
      "Epoch: 1 - Batch: 516, Training Loss: 0.04964634895967211\n",
      "Epoch: 1 - Batch: 517, Training Loss: 0.049737252010832574\n",
      "Epoch: 1 - Batch: 518, Training Loss: 0.04982513993064167\n",
      "Epoch: 1 - Batch: 519, Training Loss: 0.049904184110129056\n",
      "Epoch: 1 - Batch: 520, Training Loss: 0.049990920142994985\n",
      "Epoch: 1 - Batch: 521, Training Loss: 0.050072210777557115\n",
      "Epoch: 1 - Batch: 522, Training Loss: 0.05015490143552151\n",
      "Epoch: 1 - Batch: 523, Training Loss: 0.050243921559396665\n",
      "Epoch: 1 - Batch: 524, Training Loss: 0.05033603189497642\n",
      "Epoch: 1 - Batch: 525, Training Loss: 0.05042478683749044\n",
      "Epoch: 1 - Batch: 526, Training Loss: 0.050519292182590235\n",
      "Epoch: 1 - Batch: 527, Training Loss: 0.05060079193382121\n",
      "Epoch: 1 - Batch: 528, Training Loss: 0.05069306802937443\n",
      "Epoch: 1 - Batch: 529, Training Loss: 0.0507772099072265\n",
      "Epoch: 1 - Batch: 530, Training Loss: 0.05086472629883001\n",
      "Epoch: 1 - Batch: 531, Training Loss: 0.05095841623780937\n",
      "Epoch: 1 - Batch: 532, Training Loss: 0.05105013442326146\n",
      "Epoch: 1 - Batch: 533, Training Loss: 0.051133659212644615\n",
      "Epoch: 1 - Batch: 534, Training Loss: 0.05122889242880973\n",
      "Epoch: 1 - Batch: 535, Training Loss: 0.051318933813528436\n",
      "Epoch: 1 - Batch: 536, Training Loss: 0.05139652350500448\n",
      "Epoch: 1 - Batch: 537, Training Loss: 0.05149562157085088\n",
      "Epoch: 1 - Batch: 538, Training Loss: 0.05159242459222254\n",
      "Epoch: 1 - Batch: 539, Training Loss: 0.05167614716795547\n",
      "Epoch: 1 - Batch: 540, Training Loss: 0.05176550433484476\n",
      "Epoch: 1 - Batch: 541, Training Loss: 0.05184909644510418\n",
      "Epoch: 1 - Batch: 542, Training Loss: 0.051941405969423246\n",
      "Epoch: 1 - Batch: 543, Training Loss: 0.052035459195252876\n",
      "Epoch: 1 - Batch: 544, Training Loss: 0.05212116631282305\n",
      "Epoch: 1 - Batch: 545, Training Loss: 0.0522150264845954\n",
      "Epoch: 1 - Batch: 546, Training Loss: 0.0523030051890495\n",
      "Epoch: 1 - Batch: 547, Training Loss: 0.05239753175533035\n",
      "Epoch: 1 - Batch: 548, Training Loss: 0.05249224939873167\n",
      "Epoch: 1 - Batch: 549, Training Loss: 0.05259289468945951\n",
      "Epoch: 1 - Batch: 550, Training Loss: 0.052688327279701755\n",
      "Epoch: 1 - Batch: 551, Training Loss: 0.052779632056777555\n",
      "Epoch: 1 - Batch: 552, Training Loss: 0.05286780889354535\n",
      "Epoch: 1 - Batch: 553, Training Loss: 0.05296063555364387\n",
      "Epoch: 1 - Batch: 554, Training Loss: 0.05304529160804812\n",
      "Epoch: 1 - Batch: 555, Training Loss: 0.05313186590894933\n",
      "Epoch: 1 - Batch: 556, Training Loss: 0.05321412230185411\n",
      "Epoch: 1 - Batch: 557, Training Loss: 0.053304047343841636\n",
      "Epoch: 1 - Batch: 558, Training Loss: 0.053386647594014604\n",
      "Epoch: 1 - Batch: 559, Training Loss: 0.05346705318485722\n",
      "Epoch: 1 - Batch: 560, Training Loss: 0.05355758218103973\n",
      "Epoch: 1 - Batch: 561, Training Loss: 0.05365097142793053\n",
      "Epoch: 1 - Batch: 562, Training Loss: 0.053743617927306524\n",
      "Epoch: 1 - Batch: 563, Training Loss: 0.053834660593451156\n",
      "Epoch: 1 - Batch: 564, Training Loss: 0.05391625212397346\n",
      "Epoch: 1 - Batch: 565, Training Loss: 0.05400758681457434\n",
      "Epoch: 1 - Batch: 566, Training Loss: 0.05409061696201217\n",
      "Epoch: 1 - Batch: 567, Training Loss: 0.054175695339542126\n",
      "Epoch: 1 - Batch: 568, Training Loss: 0.054267391704554184\n",
      "Epoch: 1 - Batch: 569, Training Loss: 0.05435274167613406\n",
      "Epoch: 1 - Batch: 570, Training Loss: 0.054443132484433665\n",
      "Epoch: 1 - Batch: 571, Training Loss: 0.05453761608940649\n",
      "Epoch: 1 - Batch: 572, Training Loss: 0.05462230858122729\n",
      "Epoch: 1 - Batch: 573, Training Loss: 0.054712965188680796\n",
      "Epoch: 1 - Batch: 574, Training Loss: 0.054802222928004476\n",
      "Epoch: 1 - Batch: 575, Training Loss: 0.054898257316345005\n",
      "Epoch: 1 - Batch: 576, Training Loss: 0.05498322469505109\n",
      "Epoch: 1 - Batch: 577, Training Loss: 0.055069025853034075\n",
      "Epoch: 1 - Batch: 578, Training Loss: 0.05514954356708337\n",
      "Epoch: 1 - Batch: 579, Training Loss: 0.05523457594994289\n",
      "Epoch: 1 - Batch: 580, Training Loss: 0.05533096758929849\n",
      "Epoch: 1 - Batch: 581, Training Loss: 0.055426841116762084\n",
      "Epoch: 1 - Batch: 582, Training Loss: 0.05550568769523753\n",
      "Epoch: 1 - Batch: 583, Training Loss: 0.05560267100089027\n",
      "Epoch: 1 - Batch: 584, Training Loss: 0.05568398142335427\n",
      "Epoch: 1 - Batch: 585, Training Loss: 0.05576617695749498\n",
      "Epoch: 1 - Batch: 586, Training Loss: 0.055852845397655844\n",
      "Epoch: 1 - Batch: 587, Training Loss: 0.055933095137920746\n",
      "Epoch: 1 - Batch: 588, Training Loss: 0.056028153574338206\n",
      "Epoch: 1 - Batch: 589, Training Loss: 0.056119091376341\n",
      "Epoch: 1 - Batch: 590, Training Loss: 0.05620593859336862\n",
      "Epoch: 1 - Batch: 591, Training Loss: 0.056297083572171024\n",
      "Epoch: 1 - Batch: 592, Training Loss: 0.05638880866454608\n",
      "Epoch: 1 - Batch: 593, Training Loss: 0.0564710992028563\n",
      "Epoch: 1 - Batch: 594, Training Loss: 0.056560654650329556\n",
      "Epoch: 1 - Batch: 595, Training Loss: 0.05665412403729622\n",
      "Epoch: 1 - Batch: 596, Training Loss: 0.05674248041699379\n",
      "Epoch: 1 - Batch: 597, Training Loss: 0.05682655671614517\n",
      "Epoch: 1 - Batch: 598, Training Loss: 0.05691967688340255\n",
      "Epoch: 1 - Batch: 599, Training Loss: 0.057015566893576786\n",
      "Epoch: 1 - Batch: 600, Training Loss: 0.05711070139263795\n",
      "Epoch: 1 - Batch: 601, Training Loss: 0.05720097068110311\n",
      "Epoch: 1 - Batch: 602, Training Loss: 0.05729861826223521\n",
      "Epoch: 1 - Batch: 603, Training Loss: 0.05739861823754326\n",
      "Epoch: 1 - Batch: 604, Training Loss: 0.057501713286584884\n",
      "Epoch: 1 - Batch: 605, Training Loss: 0.05758587633001666\n",
      "Epoch: 1 - Batch: 606, Training Loss: 0.057673102186044455\n",
      "Epoch: 1 - Batch: 607, Training Loss: 0.05776380357111667\n",
      "Epoch: 1 - Batch: 608, Training Loss: 0.05785096793823179\n",
      "Epoch: 1 - Batch: 609, Training Loss: 0.0579443794750851\n",
      "Epoch: 1 - Batch: 610, Training Loss: 0.05803351778889177\n",
      "Epoch: 1 - Batch: 611, Training Loss: 0.05812315596499253\n",
      "Epoch: 1 - Batch: 612, Training Loss: 0.0582148095972799\n",
      "Epoch: 1 - Batch: 613, Training Loss: 0.058301946948317945\n",
      "Epoch: 1 - Batch: 614, Training Loss: 0.05839098285679793\n",
      "Epoch: 1 - Batch: 615, Training Loss: 0.05848679792253335\n",
      "Epoch: 1 - Batch: 616, Training Loss: 0.058579025539889265\n",
      "Epoch: 1 - Batch: 617, Training Loss: 0.058662304058547436\n",
      "Epoch: 1 - Batch: 618, Training Loss: 0.058758166298937445\n",
      "Epoch: 1 - Batch: 619, Training Loss: 0.05884542246387768\n",
      "Epoch: 1 - Batch: 620, Training Loss: 0.058938299894184615\n",
      "Epoch: 1 - Batch: 621, Training Loss: 0.05902188363953016\n",
      "Epoch: 1 - Batch: 622, Training Loss: 0.059109255634433595\n",
      "Epoch: 1 - Batch: 623, Training Loss: 0.05919753026779414\n",
      "Epoch: 1 - Batch: 624, Training Loss: 0.059291907013154545\n",
      "Epoch: 1 - Batch: 625, Training Loss: 0.05939144647956685\n",
      "Epoch: 1 - Batch: 626, Training Loss: 0.05947322791958132\n",
      "Epoch: 1 - Batch: 627, Training Loss: 0.05957012551884548\n",
      "Epoch: 1 - Batch: 628, Training Loss: 0.059662616229423045\n",
      "Epoch: 1 - Batch: 629, Training Loss: 0.05974886565164943\n",
      "Epoch: 1 - Batch: 630, Training Loss: 0.059837106164316235\n",
      "Epoch: 1 - Batch: 631, Training Loss: 0.05993825831075213\n",
      "Epoch: 1 - Batch: 632, Training Loss: 0.06002305101473533\n",
      "Epoch: 1 - Batch: 633, Training Loss: 0.060113612574773836\n",
      "Epoch: 1 - Batch: 634, Training Loss: 0.060192711824307195\n",
      "Epoch: 1 - Batch: 635, Training Loss: 0.06028944478180278\n",
      "Epoch: 1 - Batch: 636, Training Loss: 0.06037710214135659\n",
      "Epoch: 1 - Batch: 637, Training Loss: 0.060478164355701476\n",
      "Epoch: 1 - Batch: 638, Training Loss: 0.06056591040558285\n",
      "Epoch: 1 - Batch: 639, Training Loss: 0.06065435802709206\n",
      "Epoch: 1 - Batch: 640, Training Loss: 0.0607443620724168\n",
      "Epoch: 1 - Batch: 641, Training Loss: 0.06083195521241397\n",
      "Epoch: 1 - Batch: 642, Training Loss: 0.06092407287501577\n",
      "Epoch: 1 - Batch: 643, Training Loss: 0.06101238157964662\n",
      "Epoch: 1 - Batch: 644, Training Loss: 0.061105413501387214\n",
      "Epoch: 1 - Batch: 645, Training Loss: 0.0611881586733841\n",
      "Epoch: 1 - Batch: 646, Training Loss: 0.06126918119824743\n",
      "Epoch: 1 - Batch: 647, Training Loss: 0.06135075544292851\n",
      "Epoch: 1 - Batch: 648, Training Loss: 0.061428453958597945\n",
      "Epoch: 1 - Batch: 649, Training Loss: 0.06151433468210954\n",
      "Epoch: 1 - Batch: 650, Training Loss: 0.06160217201704805\n",
      "Epoch: 1 - Batch: 651, Training Loss: 0.06169073916909904\n",
      "Epoch: 1 - Batch: 652, Training Loss: 0.061783056767969385\n",
      "Epoch: 1 - Batch: 653, Training Loss: 0.06186977598451659\n",
      "Epoch: 1 - Batch: 654, Training Loss: 0.061967004609493476\n",
      "Epoch: 1 - Batch: 655, Training Loss: 0.062061042273123666\n",
      "Epoch: 1 - Batch: 656, Training Loss: 0.062153293990574864\n",
      "Epoch: 1 - Batch: 657, Training Loss: 0.062241309101555876\n",
      "Epoch: 1 - Batch: 658, Training Loss: 0.06233193958255387\n",
      "Epoch: 1 - Batch: 659, Training Loss: 0.06242575025059295\n",
      "Epoch: 1 - Batch: 660, Training Loss: 0.06251788794821372\n",
      "Epoch: 1 - Batch: 661, Training Loss: 0.06260662038453776\n",
      "Epoch: 1 - Batch: 662, Training Loss: 0.06270710202814335\n",
      "Epoch: 1 - Batch: 663, Training Loss: 0.06279160642060475\n",
      "Epoch: 1 - Batch: 664, Training Loss: 0.06288169780329092\n",
      "Epoch: 1 - Batch: 665, Training Loss: 0.06297493339276827\n",
      "Epoch: 1 - Batch: 666, Training Loss: 0.06308118818608881\n",
      "Epoch: 1 - Batch: 667, Training Loss: 0.06316528367388308\n",
      "Epoch: 1 - Batch: 668, Training Loss: 0.06325186631498646\n",
      "Epoch: 1 - Batch: 669, Training Loss: 0.06334651403660403\n",
      "Epoch: 1 - Batch: 670, Training Loss: 0.06342932048746405\n",
      "Epoch: 1 - Batch: 671, Training Loss: 0.06351684528722693\n",
      "Epoch: 1 - Batch: 672, Training Loss: 0.06360330516673242\n",
      "Epoch: 1 - Batch: 673, Training Loss: 0.06369646470592193\n",
      "Epoch: 1 - Batch: 674, Training Loss: 0.06378286426414305\n",
      "Epoch: 1 - Batch: 675, Training Loss: 0.06387762703730494\n",
      "Epoch: 1 - Batch: 676, Training Loss: 0.06397349224927809\n",
      "Epoch: 1 - Batch: 677, Training Loss: 0.06406049182635437\n",
      "Epoch: 1 - Batch: 678, Training Loss: 0.06414432322628068\n",
      "Epoch: 1 - Batch: 679, Training Loss: 0.06422324492217692\n",
      "Epoch: 1 - Batch: 680, Training Loss: 0.06431269898368154\n",
      "Epoch: 1 - Batch: 681, Training Loss: 0.06439739930911444\n",
      "Epoch: 1 - Batch: 682, Training Loss: 0.0644834006134155\n",
      "Epoch: 1 - Batch: 683, Training Loss: 0.06457095372662022\n",
      "Epoch: 1 - Batch: 684, Training Loss: 0.06465592122888486\n",
      "Epoch: 1 - Batch: 685, Training Loss: 0.0647470921858725\n",
      "Epoch: 1 - Batch: 686, Training Loss: 0.0648373158565208\n",
      "Epoch: 1 - Batch: 687, Training Loss: 0.06492328235437819\n",
      "Epoch: 1 - Batch: 688, Training Loss: 0.06501593611899695\n",
      "Epoch: 1 - Batch: 689, Training Loss: 0.0650902366139007\n",
      "Epoch: 1 - Batch: 690, Training Loss: 0.06516982891147409\n",
      "Epoch: 1 - Batch: 691, Training Loss: 0.06525895772362823\n",
      "Epoch: 1 - Batch: 692, Training Loss: 0.06533916693051063\n",
      "Epoch: 1 - Batch: 693, Training Loss: 0.06543949884586468\n",
      "Epoch: 1 - Batch: 694, Training Loss: 0.06552357097491499\n",
      "Epoch: 1 - Batch: 695, Training Loss: 0.06560590928096083\n",
      "Epoch: 1 - Batch: 696, Training Loss: 0.06569443003059817\n",
      "Epoch: 1 - Batch: 697, Training Loss: 0.06578010698752616\n",
      "Epoch: 1 - Batch: 698, Training Loss: 0.06587207429790576\n",
      "Epoch: 1 - Batch: 699, Training Loss: 0.06595638531505765\n",
      "Epoch: 1 - Batch: 700, Training Loss: 0.06604423220725004\n",
      "Epoch: 1 - Batch: 701, Training Loss: 0.06613800770434772\n",
      "Epoch: 1 - Batch: 702, Training Loss: 0.06623417087158753\n",
      "Epoch: 1 - Batch: 703, Training Loss: 0.06633111005448188\n",
      "Epoch: 1 - Batch: 704, Training Loss: 0.06641723215579987\n",
      "Epoch: 1 - Batch: 705, Training Loss: 0.06650880926209896\n",
      "Epoch: 1 - Batch: 706, Training Loss: 0.06658891967156436\n",
      "Epoch: 1 - Batch: 707, Training Loss: 0.06667625781760286\n",
      "Epoch: 1 - Batch: 708, Training Loss: 0.06677096709609032\n",
      "Epoch: 1 - Batch: 709, Training Loss: 0.06685304649754939\n",
      "Epoch: 1 - Batch: 710, Training Loss: 0.06693660458596191\n",
      "Epoch: 1 - Batch: 711, Training Loss: 0.0670396355513613\n",
      "Epoch: 1 - Batch: 712, Training Loss: 0.06712052045671106\n",
      "Epoch: 1 - Batch: 713, Training Loss: 0.06720100813985463\n",
      "Epoch: 1 - Batch: 714, Training Loss: 0.06728253575971668\n",
      "Epoch: 1 - Batch: 715, Training Loss: 0.0673667941100364\n",
      "Epoch: 1 - Batch: 716, Training Loss: 0.06745383676580133\n",
      "Epoch: 1 - Batch: 717, Training Loss: 0.06754234166905457\n",
      "Epoch: 1 - Batch: 718, Training Loss: 0.06762536885726511\n",
      "Epoch: 1 - Batch: 719, Training Loss: 0.0677106865121061\n",
      "Epoch: 1 - Batch: 720, Training Loss: 0.06781008981081779\n",
      "Epoch: 1 - Batch: 721, Training Loss: 0.0679007740027177\n",
      "Epoch: 1 - Batch: 722, Training Loss: 0.06799123433741369\n",
      "Epoch: 1 - Batch: 723, Training Loss: 0.06807652334520473\n",
      "Epoch: 1 - Batch: 724, Training Loss: 0.06817378232233955\n",
      "Epoch: 1 - Batch: 725, Training Loss: 0.06826703609963555\n",
      "Epoch: 1 - Batch: 726, Training Loss: 0.06836000679811435\n",
      "Epoch: 1 - Batch: 727, Training Loss: 0.06845869644117197\n",
      "Epoch: 1 - Batch: 728, Training Loss: 0.06854474049055359\n",
      "Epoch: 1 - Batch: 729, Training Loss: 0.06863363742655387\n",
      "Epoch: 1 - Batch: 730, Training Loss: 0.06872391360340822\n",
      "Epoch: 1 - Batch: 731, Training Loss: 0.06881075027471356\n",
      "Epoch: 1 - Batch: 732, Training Loss: 0.06889639675592506\n",
      "Epoch: 1 - Batch: 733, Training Loss: 0.06898118611147154\n",
      "Epoch: 1 - Batch: 734, Training Loss: 0.06907036843683391\n",
      "Epoch: 1 - Batch: 735, Training Loss: 0.06915561633051727\n",
      "Epoch: 1 - Batch: 736, Training Loss: 0.06924346887551334\n",
      "Epoch: 1 - Batch: 737, Training Loss: 0.0693295944920721\n",
      "Epoch: 1 - Batch: 738, Training Loss: 0.0694182724514312\n",
      "Epoch: 1 - Batch: 739, Training Loss: 0.06950242797014725\n",
      "Epoch: 1 - Batch: 740, Training Loss: 0.06959062712158927\n",
      "Epoch: 1 - Batch: 741, Training Loss: 0.06967515783202194\n",
      "Epoch: 1 - Batch: 742, Training Loss: 0.06976714039570459\n",
      "Epoch: 1 - Batch: 743, Training Loss: 0.06985827240703711\n",
      "Epoch: 1 - Batch: 744, Training Loss: 0.06994849823996005\n",
      "Epoch: 1 - Batch: 745, Training Loss: 0.07004434078511709\n",
      "Epoch: 1 - Batch: 746, Training Loss: 0.07013281393382284\n",
      "Epoch: 1 - Batch: 747, Training Loss: 0.07021561255107077\n",
      "Epoch: 1 - Batch: 748, Training Loss: 0.07030522233341661\n",
      "Epoch: 1 - Batch: 749, Training Loss: 0.0703903746916287\n",
      "Epoch: 1 - Batch: 750, Training Loss: 0.07047907686228579\n",
      "Epoch: 1 - Batch: 751, Training Loss: 0.0705734846702658\n",
      "Epoch: 1 - Batch: 752, Training Loss: 0.07066460012475254\n",
      "Epoch: 1 - Batch: 753, Training Loss: 0.07075088395185732\n",
      "Epoch: 1 - Batch: 754, Training Loss: 0.07084159018042471\n",
      "Epoch: 1 - Batch: 755, Training Loss: 0.0709259413479276\n",
      "Epoch: 1 - Batch: 756, Training Loss: 0.0710120921580748\n",
      "Epoch: 1 - Batch: 757, Training Loss: 0.07110228834930146\n",
      "Epoch: 1 - Batch: 758, Training Loss: 0.0711928960152131\n",
      "Epoch: 1 - Batch: 759, Training Loss: 0.07129095925966503\n",
      "Epoch: 1 - Batch: 760, Training Loss: 0.0713817195945217\n",
      "Epoch: 1 - Batch: 761, Training Loss: 0.07147571456249475\n",
      "Epoch: 1 - Batch: 762, Training Loss: 0.07156243757846739\n",
      "Epoch: 1 - Batch: 763, Training Loss: 0.07164455594411537\n",
      "Epoch: 1 - Batch: 764, Training Loss: 0.07174128826528442\n",
      "Epoch: 1 - Batch: 765, Training Loss: 0.07183876414327676\n",
      "Epoch: 1 - Batch: 766, Training Loss: 0.0719426200349829\n",
      "Epoch: 1 - Batch: 767, Training Loss: 0.07202977812532367\n",
      "Epoch: 1 - Batch: 768, Training Loss: 0.07211927222597658\n",
      "Epoch: 1 - Batch: 769, Training Loss: 0.07220809576819785\n",
      "Epoch: 1 - Batch: 770, Training Loss: 0.07229948686327704\n",
      "Epoch: 1 - Batch: 771, Training Loss: 0.07237732757630436\n",
      "Epoch: 1 - Batch: 772, Training Loss: 0.07247106973730509\n",
      "Epoch: 1 - Batch: 773, Training Loss: 0.07255685093440424\n",
      "Epoch: 1 - Batch: 774, Training Loss: 0.07264886616696765\n",
      "Epoch: 1 - Batch: 775, Training Loss: 0.0727366739281671\n",
      "Epoch: 1 - Batch: 776, Training Loss: 0.07281819645741686\n",
      "Epoch: 1 - Batch: 777, Training Loss: 0.07289625101940549\n",
      "Epoch: 1 - Batch: 778, Training Loss: 0.07297915699667797\n",
      "Epoch: 1 - Batch: 779, Training Loss: 0.07306762791515188\n",
      "Epoch: 1 - Batch: 780, Training Loss: 0.07314851479743843\n",
      "Epoch: 1 - Batch: 781, Training Loss: 0.07324180406075015\n",
      "Epoch: 1 - Batch: 782, Training Loss: 0.07332700986421326\n",
      "Epoch: 1 - Batch: 783, Training Loss: 0.0734197103277862\n",
      "Epoch: 1 - Batch: 784, Training Loss: 0.07351377812116894\n",
      "Epoch: 1 - Batch: 785, Training Loss: 0.07359969926068241\n",
      "Epoch: 1 - Batch: 786, Training Loss: 0.07369409514526229\n",
      "Epoch: 1 - Batch: 787, Training Loss: 0.07377756202275282\n",
      "Epoch: 1 - Batch: 788, Training Loss: 0.07385773819625674\n",
      "Epoch: 1 - Batch: 789, Training Loss: 0.07394428310875671\n",
      "Epoch: 1 - Batch: 790, Training Loss: 0.07403127445683352\n",
      "Epoch: 1 - Batch: 791, Training Loss: 0.0741161666934071\n",
      "Epoch: 1 - Batch: 792, Training Loss: 0.07419983437207603\n",
      "Epoch: 1 - Batch: 793, Training Loss: 0.07429098842007603\n",
      "Epoch: 1 - Batch: 794, Training Loss: 0.07436930188047353\n",
      "Epoch: 1 - Batch: 795, Training Loss: 0.07445485041682202\n",
      "Epoch: 1 - Batch: 796, Training Loss: 0.07453880322538008\n",
      "Epoch: 1 - Batch: 797, Training Loss: 0.074627708205042\n",
      "Epoch: 1 - Batch: 798, Training Loss: 0.07470739131815003\n",
      "Epoch: 1 - Batch: 799, Training Loss: 0.07478782448108319\n",
      "Epoch: 1 - Batch: 800, Training Loss: 0.07487444280589596\n",
      "Epoch: 1 - Batch: 801, Training Loss: 0.07495595149658806\n",
      "Epoch: 1 - Batch: 802, Training Loss: 0.07504110699936525\n",
      "Epoch: 1 - Batch: 803, Training Loss: 0.07513455148310606\n",
      "Epoch: 1 - Batch: 804, Training Loss: 0.07523595910537895\n",
      "Epoch: 1 - Batch: 805, Training Loss: 0.07531715504466796\n",
      "Epoch: 1 - Batch: 806, Training Loss: 0.0754081750687082\n",
      "Epoch: 1 - Batch: 807, Training Loss: 0.07549682365202193\n",
      "Epoch: 1 - Batch: 808, Training Loss: 0.07558125927797202\n",
      "Epoch: 1 - Batch: 809, Training Loss: 0.07566633402288059\n",
      "Epoch: 1 - Batch: 810, Training Loss: 0.07575931425390156\n",
      "Epoch: 1 - Batch: 811, Training Loss: 0.07584884804773884\n",
      "Epoch: 1 - Batch: 812, Training Loss: 0.07593869033638716\n",
      "Epoch: 1 - Batch: 813, Training Loss: 0.07602909135062304\n",
      "Epoch: 1 - Batch: 814, Training Loss: 0.07611705734090228\n",
      "Epoch: 1 - Batch: 815, Training Loss: 0.07619710585988972\n",
      "Epoch: 1 - Batch: 816, Training Loss: 0.07628068843760696\n",
      "Epoch: 1 - Batch: 817, Training Loss: 0.07637685157395714\n",
      "Epoch: 1 - Batch: 818, Training Loss: 0.07648073944261616\n",
      "Epoch: 1 - Batch: 819, Training Loss: 0.07656165615193682\n",
      "Epoch: 1 - Batch: 820, Training Loss: 0.07664105437609489\n",
      "Epoch: 1 - Batch: 821, Training Loss: 0.07673113061050277\n",
      "Epoch: 1 - Batch: 822, Training Loss: 0.07681606687098791\n",
      "Epoch: 1 - Batch: 823, Training Loss: 0.07689585097467722\n",
      "Epoch: 1 - Batch: 824, Training Loss: 0.07697682523757071\n",
      "Epoch: 1 - Batch: 825, Training Loss: 0.07706138313088448\n",
      "Epoch: 1 - Batch: 826, Training Loss: 0.0771522790341828\n",
      "Epoch: 1 - Batch: 827, Training Loss: 0.07723725957805244\n",
      "Epoch: 1 - Batch: 828, Training Loss: 0.07732776878015517\n",
      "Epoch: 1 - Batch: 829, Training Loss: 0.0774217701175715\n",
      "Epoch: 1 - Batch: 830, Training Loss: 0.07751446161079367\n",
      "Epoch: 1 - Batch: 831, Training Loss: 0.07760818992088091\n",
      "Epoch: 1 - Batch: 832, Training Loss: 0.07769881291423074\n",
      "Epoch: 1 - Batch: 833, Training Loss: 0.07777963791839519\n",
      "Epoch: 1 - Batch: 834, Training Loss: 0.0778751217174787\n",
      "Epoch: 1 - Batch: 835, Training Loss: 0.07796536365008078\n",
      "Epoch: 1 - Batch: 836, Training Loss: 0.07807221895031273\n",
      "Epoch: 1 - Batch: 837, Training Loss: 0.07815596308701271\n",
      "Epoch: 1 - Batch: 838, Training Loss: 0.07824923505212734\n",
      "Epoch: 1 - Batch: 839, Training Loss: 0.07834100807286416\n",
      "Epoch: 1 - Batch: 840, Training Loss: 0.0784302985243141\n",
      "Epoch: 1 - Batch: 841, Training Loss: 0.07852165329317944\n",
      "Epoch: 1 - Batch: 842, Training Loss: 0.07860770547271367\n",
      "Epoch: 1 - Batch: 843, Training Loss: 0.07869743911632851\n",
      "Epoch: 1 - Batch: 844, Training Loss: 0.07878787325043386\n",
      "Epoch: 1 - Batch: 845, Training Loss: 0.07888176882173094\n",
      "Epoch: 1 - Batch: 846, Training Loss: 0.0789751440323704\n",
      "Epoch: 1 - Batch: 847, Training Loss: 0.07907251773361938\n",
      "Epoch: 1 - Batch: 848, Training Loss: 0.07916062343772964\n",
      "Epoch: 1 - Batch: 849, Training Loss: 0.07924967139316831\n",
      "Epoch: 1 - Batch: 850, Training Loss: 0.07933062513298657\n",
      "Epoch: 1 - Batch: 851, Training Loss: 0.07942350475645776\n",
      "Epoch: 1 - Batch: 852, Training Loss: 0.0795181361312791\n",
      "Epoch: 1 - Batch: 853, Training Loss: 0.07959698427894815\n",
      "Epoch: 1 - Batch: 854, Training Loss: 0.07969919807133984\n",
      "Epoch: 1 - Batch: 855, Training Loss: 0.07979168644666079\n",
      "Epoch: 1 - Batch: 856, Training Loss: 0.07987322767649717\n",
      "Epoch: 1 - Batch: 857, Training Loss: 0.079961538599004\n",
      "Epoch: 1 - Batch: 858, Training Loss: 0.0800442511638994\n",
      "Epoch: 1 - Batch: 859, Training Loss: 0.08013726171001076\n",
      "Epoch: 1 - Batch: 860, Training Loss: 0.08022592671425584\n",
      "Epoch: 1 - Batch: 861, Training Loss: 0.08032395739485178\n",
      "Epoch: 1 - Batch: 862, Training Loss: 0.08041403463008392\n",
      "Epoch: 1 - Batch: 863, Training Loss: 0.0805019475952112\n",
      "Epoch: 1 - Batch: 864, Training Loss: 0.08059332918863787\n",
      "Epoch: 1 - Batch: 865, Training Loss: 0.08067857455554886\n",
      "Epoch: 1 - Batch: 866, Training Loss: 0.08076735210927763\n",
      "Epoch: 1 - Batch: 867, Training Loss: 0.0808608863150599\n",
      "Epoch: 1 - Batch: 868, Training Loss: 0.08094630541764879\n",
      "Epoch: 1 - Batch: 869, Training Loss: 0.08103479472163502\n",
      "Epoch: 1 - Batch: 870, Training Loss: 0.08112720697871093\n",
      "Epoch: 1 - Batch: 871, Training Loss: 0.08121399436868838\n",
      "Epoch: 1 - Batch: 872, Training Loss: 0.08129930302228897\n",
      "Epoch: 1 - Batch: 873, Training Loss: 0.08138146883442034\n",
      "Epoch: 1 - Batch: 874, Training Loss: 0.08146498787833091\n",
      "Epoch: 1 - Batch: 875, Training Loss: 0.081553974591974\n",
      "Epoch: 1 - Batch: 876, Training Loss: 0.08164507581251573\n",
      "Epoch: 1 - Batch: 877, Training Loss: 0.08173247649153667\n",
      "Epoch: 1 - Batch: 878, Training Loss: 0.08182201390563948\n",
      "Epoch: 1 - Batch: 879, Training Loss: 0.08191602809314506\n",
      "Epoch: 1 - Batch: 880, Training Loss: 0.08199987542594646\n",
      "Epoch: 1 - Batch: 881, Training Loss: 0.08208811479569668\n",
      "Epoch: 1 - Batch: 882, Training Loss: 0.08217724315068417\n",
      "Epoch: 1 - Batch: 883, Training Loss: 0.08226502358073223\n",
      "Epoch: 1 - Batch: 884, Training Loss: 0.08235449384076284\n",
      "Epoch: 1 - Batch: 885, Training Loss: 0.08243997615590618\n",
      "Epoch: 1 - Batch: 886, Training Loss: 0.08253135191119132\n",
      "Epoch: 1 - Batch: 887, Training Loss: 0.08261177014207366\n",
      "Epoch: 1 - Batch: 888, Training Loss: 0.08270036909191762\n",
      "Epoch: 1 - Batch: 889, Training Loss: 0.08278267492059847\n",
      "Epoch: 1 - Batch: 890, Training Loss: 0.08286732983826404\n",
      "Epoch: 1 - Batch: 891, Training Loss: 0.0829586867199806\n",
      "Epoch: 1 - Batch: 892, Training Loss: 0.08304809373038918\n",
      "Epoch: 1 - Batch: 893, Training Loss: 0.08313715603715349\n",
      "Epoch: 1 - Batch: 894, Training Loss: 0.08322105336792236\n",
      "Epoch: 1 - Batch: 895, Training Loss: 0.08331096460224187\n",
      "Epoch: 1 - Batch: 896, Training Loss: 0.08339163359520249\n",
      "Epoch: 1 - Batch: 897, Training Loss: 0.083492689192987\n",
      "Epoch: 1 - Batch: 898, Training Loss: 0.08358412633216006\n",
      "Epoch: 1 - Batch: 899, Training Loss: 0.08366132803743158\n",
      "Epoch: 1 - Batch: 900, Training Loss: 0.0837511586127293\n",
      "Epoch: 1 - Batch: 901, Training Loss: 0.08384211558484121\n",
      "Epoch: 1 - Batch: 902, Training Loss: 0.08392903627596092\n",
      "Epoch: 1 - Batch: 903, Training Loss: 0.08402403714027175\n",
      "Epoch: 1 - Batch: 904, Training Loss: 0.08411094697677278\n",
      "Epoch: 1 - Batch: 905, Training Loss: 0.08419557267901909\n",
      "Epoch: 1 - Batch: 906, Training Loss: 0.08428409391671271\n",
      "Epoch: 1 - Batch: 907, Training Loss: 0.08437132436911858\n",
      "Epoch: 1 - Batch: 908, Training Loss: 0.08446258940976453\n",
      "Epoch: 1 - Batch: 909, Training Loss: 0.08454983079710213\n",
      "Epoch: 1 - Batch: 910, Training Loss: 0.08462812765740835\n",
      "Epoch: 1 - Batch: 911, Training Loss: 0.08471561204116934\n",
      "Epoch: 1 - Batch: 912, Training Loss: 0.0847997518741274\n",
      "Epoch: 1 - Batch: 913, Training Loss: 0.08488070407564169\n",
      "Epoch: 1 - Batch: 914, Training Loss: 0.0849696003876713\n",
      "Epoch: 1 - Batch: 915, Training Loss: 0.08506417331929823\n",
      "Epoch: 1 - Batch: 916, Training Loss: 0.08514969186798653\n",
      "Epoch: 1 - Batch: 917, Training Loss: 0.0852446669518058\n",
      "Epoch: 1 - Batch: 918, Training Loss: 0.08533445428161084\n",
      "Epoch: 1 - Batch: 919, Training Loss: 0.08542021053223862\n",
      "Epoch: 1 - Batch: 920, Training Loss: 0.0855160251160957\n",
      "Epoch: 1 - Batch: 921, Training Loss: 0.08560505049120926\n",
      "Epoch: 1 - Batch: 922, Training Loss: 0.08568686069565429\n",
      "Epoch: 1 - Batch: 923, Training Loss: 0.08577915347161183\n",
      "Epoch: 1 - Batch: 924, Training Loss: 0.08588075165823719\n",
      "Epoch: 1 - Batch: 925, Training Loss: 0.08596210503583128\n",
      "Epoch: 1 - Batch: 926, Training Loss: 0.08604626305043006\n",
      "Epoch: 1 - Batch: 927, Training Loss: 0.0861404386424702\n",
      "Epoch: 1 - Batch: 928, Training Loss: 0.08621714113413002\n",
      "Epoch: 1 - Batch: 929, Training Loss: 0.08630120657866273\n",
      "Epoch: 1 - Batch: 930, Training Loss: 0.08639262360250377\n",
      "Epoch: 1 - Batch: 931, Training Loss: 0.08648364520809346\n",
      "Epoch: 1 - Batch: 932, Training Loss: 0.08657608202860921\n",
      "Epoch: 1 - Batch: 933, Training Loss: 0.08666382252453374\n",
      "Epoch: 1 - Batch: 934, Training Loss: 0.08675286642107402\n",
      "Epoch: 1 - Batch: 935, Training Loss: 0.08684202108167692\n",
      "Epoch: 1 - Batch: 936, Training Loss: 0.08692995597844694\n",
      "Epoch: 1 - Batch: 937, Training Loss: 0.08701798673811836\n",
      "Epoch: 1 - Batch: 938, Training Loss: 0.08712547098844008\n",
      "Epoch: 1 - Batch: 939, Training Loss: 0.08721119859819586\n",
      "Epoch: 1 - Batch: 940, Training Loss: 0.08730309001273578\n",
      "Epoch: 1 - Batch: 941, Training Loss: 0.0873990644504676\n",
      "Epoch: 1 - Batch: 942, Training Loss: 0.08749265476418767\n",
      "Epoch: 1 - Batch: 943, Training Loss: 0.08758588505300322\n",
      "Epoch: 1 - Batch: 944, Training Loss: 0.0876757579718162\n",
      "Epoch: 1 - Batch: 945, Training Loss: 0.08776476515911112\n",
      "Epoch: 1 - Batch: 946, Training Loss: 0.08785223718701705\n",
      "Epoch: 1 - Batch: 947, Training Loss: 0.08793069023150907\n",
      "Epoch: 1 - Batch: 948, Training Loss: 0.08801717531078095\n",
      "Epoch: 1 - Batch: 949, Training Loss: 0.08810228893338744\n",
      "Epoch: 1 - Batch: 950, Training Loss: 0.0881818055451707\n",
      "Epoch: 1 - Batch: 951, Training Loss: 0.08825856862376578\n",
      "Epoch: 1 - Batch: 952, Training Loss: 0.08834541535273713\n",
      "Epoch: 1 - Batch: 953, Training Loss: 0.088432642481418\n",
      "Epoch: 1 - Batch: 954, Training Loss: 0.08852327000936663\n",
      "Epoch: 1 - Batch: 955, Training Loss: 0.08860878454240202\n",
      "Epoch: 1 - Batch: 956, Training Loss: 0.08869919455787831\n",
      "Epoch: 1 - Batch: 957, Training Loss: 0.08878994994421503\n",
      "Epoch: 1 - Batch: 958, Training Loss: 0.08887634220012583\n",
      "Epoch: 1 - Batch: 959, Training Loss: 0.08895830758175446\n",
      "Epoch: 1 - Batch: 960, Training Loss: 0.0890506701559372\n",
      "Epoch: 1 - Batch: 961, Training Loss: 0.08913989735776512\n",
      "Epoch: 1 - Batch: 962, Training Loss: 0.08923313916841549\n",
      "Epoch: 1 - Batch: 963, Training Loss: 0.08932244908033714\n",
      "Epoch: 1 - Batch: 964, Training Loss: 0.08940337955408033\n",
      "Epoch: 1 - Batch: 965, Training Loss: 0.08949131300521529\n",
      "Epoch: 1 - Batch: 966, Training Loss: 0.08957290064982118\n",
      "Epoch: 1 - Batch: 967, Training Loss: 0.0896646474576115\n",
      "Epoch: 1 - Batch: 968, Training Loss: 0.08975069436119563\n",
      "Epoch: 1 - Batch: 969, Training Loss: 0.08984416184536062\n",
      "Epoch: 1 - Batch: 970, Training Loss: 0.08993078052849318\n",
      "Epoch: 1 - Batch: 971, Training Loss: 0.09000954640470137\n",
      "Epoch: 1 - Batch: 972, Training Loss: 0.09008756876733173\n",
      "Epoch: 1 - Batch: 973, Training Loss: 0.09016981075330952\n",
      "Epoch: 1 - Batch: 974, Training Loss: 0.09025768164057241\n",
      "Epoch: 1 - Batch: 975, Training Loss: 0.09034374393337402\n",
      "Epoch: 1 - Batch: 976, Training Loss: 0.09044290416152719\n",
      "Epoch: 1 - Batch: 977, Training Loss: 0.09054193406629918\n",
      "Epoch: 1 - Batch: 978, Training Loss: 0.09063489864862974\n",
      "Epoch: 1 - Batch: 979, Training Loss: 0.09071661064256088\n",
      "Epoch: 1 - Batch: 980, Training Loss: 0.09080029860336587\n",
      "Epoch: 1 - Batch: 981, Training Loss: 0.09089993223374954\n",
      "Epoch: 1 - Batch: 982, Training Loss: 0.09099148307767871\n",
      "Epoch: 1 - Batch: 983, Training Loss: 0.09108752584946689\n",
      "Epoch: 1 - Batch: 984, Training Loss: 0.09118128361591257\n",
      "Epoch: 1 - Batch: 985, Training Loss: 0.09127131378769282\n",
      "Epoch: 1 - Batch: 986, Training Loss: 0.09135994246572404\n",
      "Epoch: 1 - Batch: 987, Training Loss: 0.09144971013118576\n",
      "Epoch: 1 - Batch: 988, Training Loss: 0.09153937917839038\n",
      "Epoch: 1 - Batch: 989, Training Loss: 0.09163334721059942\n",
      "Epoch: 1 - Batch: 990, Training Loss: 0.09172228305518726\n",
      "Epoch: 1 - Batch: 991, Training Loss: 0.0918129286288622\n",
      "Epoch: 1 - Batch: 992, Training Loss: 0.09189917897159978\n",
      "Epoch: 1 - Batch: 993, Training Loss: 0.0919842785270653\n",
      "Epoch: 1 - Batch: 994, Training Loss: 0.09206610811166897\n",
      "Epoch: 1 - Batch: 995, Training Loss: 0.09214893943486523\n",
      "Epoch: 1 - Batch: 996, Training Loss: 0.09224352716262858\n",
      "Epoch: 1 - Batch: 997, Training Loss: 0.09233725500937125\n",
      "Epoch: 1 - Batch: 998, Training Loss: 0.09242258638538926\n",
      "Epoch: 1 - Batch: 999, Training Loss: 0.09250770128064884\n",
      "Epoch: 1 - Batch: 1000, Training Loss: 0.09259687449974603\n",
      "Epoch: 1 - Batch: 1001, Training Loss: 0.09268205953919473\n",
      "Epoch: 1 - Batch: 1002, Training Loss: 0.09277149670406756\n",
      "Epoch: 1 - Batch: 1003, Training Loss: 0.09285853879764108\n",
      "Epoch: 1 - Batch: 1004, Training Loss: 0.09296039476447042\n",
      "Epoch: 1 - Batch: 1005, Training Loss: 0.09304892426205313\n",
      "Epoch: 1 - Batch: 1006, Training Loss: 0.09312990971935131\n",
      "Epoch: 1 - Batch: 1007, Training Loss: 0.09322204648410502\n",
      "Epoch: 1 - Batch: 1008, Training Loss: 0.09330645686086533\n",
      "Epoch: 1 - Batch: 1009, Training Loss: 0.09339094045842267\n",
      "Epoch: 1 - Batch: 1010, Training Loss: 0.09348175997782505\n",
      "Epoch: 1 - Batch: 1011, Training Loss: 0.09357949991897366\n",
      "Epoch: 1 - Batch: 1012, Training Loss: 0.0936698687400292\n",
      "Epoch: 1 - Batch: 1013, Training Loss: 0.09376782533491825\n",
      "Epoch: 1 - Batch: 1014, Training Loss: 0.09385632321386789\n",
      "Epoch: 1 - Batch: 1015, Training Loss: 0.09394696004577537\n",
      "Epoch: 1 - Batch: 1016, Training Loss: 0.09403905408398232\n",
      "Epoch: 1 - Batch: 1017, Training Loss: 0.09413055900737619\n",
      "Epoch: 1 - Batch: 1018, Training Loss: 0.09421811643895225\n",
      "Epoch: 1 - Batch: 1019, Training Loss: 0.09431327645903204\n",
      "Epoch: 1 - Batch: 1020, Training Loss: 0.09440279089742237\n",
      "Epoch: 1 - Batch: 1021, Training Loss: 0.09448920392970343\n",
      "Epoch: 1 - Batch: 1022, Training Loss: 0.09457908729539187\n",
      "Epoch: 1 - Batch: 1023, Training Loss: 0.0946768695676999\n",
      "Epoch: 1 - Batch: 1024, Training Loss: 0.09476366910966079\n",
      "Epoch: 1 - Batch: 1025, Training Loss: 0.09483933248626653\n",
      "Epoch: 1 - Batch: 1026, Training Loss: 0.09493913764063003\n",
      "Epoch: 1 - Batch: 1027, Training Loss: 0.09502822596882508\n",
      "Epoch: 1 - Batch: 1028, Training Loss: 0.09510641918821912\n",
      "Epoch: 1 - Batch: 1029, Training Loss: 0.09519858537844164\n",
      "Epoch: 1 - Batch: 1030, Training Loss: 0.09528649137486668\n",
      "Epoch: 1 - Batch: 1031, Training Loss: 0.09538145365802012\n",
      "Epoch: 1 - Batch: 1032, Training Loss: 0.09547366130213635\n",
      "Epoch: 1 - Batch: 1033, Training Loss: 0.09555920695957063\n",
      "Epoch: 1 - Batch: 1034, Training Loss: 0.09564399165078181\n",
      "Epoch: 1 - Batch: 1035, Training Loss: 0.09573544183773781\n",
      "Epoch: 1 - Batch: 1036, Training Loss: 0.09582221625406746\n",
      "Epoch: 1 - Batch: 1037, Training Loss: 0.0959133918321449\n",
      "Epoch: 1 - Batch: 1038, Training Loss: 0.09600387119485766\n",
      "Epoch: 1 - Batch: 1039, Training Loss: 0.09608655185075739\n",
      "Epoch: 1 - Batch: 1040, Training Loss: 0.0961724136327432\n",
      "Epoch: 1 - Batch: 1041, Training Loss: 0.09625821396906183\n",
      "Epoch: 1 - Batch: 1042, Training Loss: 0.09634537299226963\n",
      "Epoch: 1 - Batch: 1043, Training Loss: 0.09642669829736104\n",
      "Epoch: 1 - Batch: 1044, Training Loss: 0.09651841425258129\n",
      "Epoch: 1 - Batch: 1045, Training Loss: 0.09660508644274415\n",
      "Epoch: 1 - Batch: 1046, Training Loss: 0.09668974758158276\n",
      "Epoch: 1 - Batch: 1047, Training Loss: 0.09677450576330694\n",
      "Epoch: 1 - Batch: 1048, Training Loss: 0.09685850478894081\n",
      "Epoch: 1 - Batch: 1049, Training Loss: 0.09694838173576255\n",
      "Epoch: 1 - Batch: 1050, Training Loss: 0.09704389576344545\n",
      "Epoch: 1 - Batch: 1051, Training Loss: 0.09712725120347926\n",
      "Epoch: 1 - Batch: 1052, Training Loss: 0.09721571983611998\n",
      "Epoch: 1 - Batch: 1053, Training Loss: 0.0973030124879002\n",
      "Epoch: 1 - Batch: 1054, Training Loss: 0.09738234635658723\n",
      "Epoch: 1 - Batch: 1055, Training Loss: 0.09746237617498804\n",
      "Epoch: 1 - Batch: 1056, Training Loss: 0.0975489391764598\n",
      "Epoch: 1 - Batch: 1057, Training Loss: 0.09762856115279704\n",
      "Epoch: 1 - Batch: 1058, Training Loss: 0.09771242675990806\n",
      "Epoch: 1 - Batch: 1059, Training Loss: 0.0977973092909575\n",
      "Epoch: 1 - Batch: 1060, Training Loss: 0.09787948495070535\n",
      "Epoch: 1 - Batch: 1061, Training Loss: 0.0979756947860097\n",
      "Epoch: 1 - Batch: 1062, Training Loss: 0.09806224772363753\n",
      "Epoch: 1 - Batch: 1063, Training Loss: 0.0981390651123935\n",
      "Epoch: 1 - Batch: 1064, Training Loss: 0.0982406541495082\n",
      "Epoch: 1 - Batch: 1065, Training Loss: 0.09831790293676541\n",
      "Epoch: 1 - Batch: 1066, Training Loss: 0.09841266619798358\n",
      "Epoch: 1 - Batch: 1067, Training Loss: 0.09850172936347983\n",
      "Epoch: 1 - Batch: 1068, Training Loss: 0.09859246639519387\n",
      "Epoch: 1 - Batch: 1069, Training Loss: 0.09867984897906508\n",
      "Epoch: 1 - Batch: 1070, Training Loss: 0.09877336739406459\n",
      "Epoch: 1 - Batch: 1071, Training Loss: 0.09886740561988619\n",
      "Epoch: 1 - Batch: 1072, Training Loss: 0.0989607619446012\n",
      "Epoch: 1 - Batch: 1073, Training Loss: 0.09904099197109936\n",
      "Epoch: 1 - Batch: 1074, Training Loss: 0.09912734448440237\n",
      "Epoch: 1 - Batch: 1075, Training Loss: 0.09921322962513215\n",
      "Epoch: 1 - Batch: 1076, Training Loss: 0.09930343329782905\n",
      "Epoch: 1 - Batch: 1077, Training Loss: 0.0993959716093382\n",
      "Epoch: 1 - Batch: 1078, Training Loss: 0.09948248709019143\n",
      "Epoch: 1 - Batch: 1079, Training Loss: 0.09957866203873905\n",
      "Epoch: 1 - Batch: 1080, Training Loss: 0.09966910448215692\n",
      "Epoch: 1 - Batch: 1081, Training Loss: 0.09975829229969686\n",
      "Epoch: 1 - Batch: 1082, Training Loss: 0.09984907007484294\n",
      "Epoch: 1 - Batch: 1083, Training Loss: 0.0999309121141485\n",
      "Epoch: 1 - Batch: 1084, Training Loss: 0.10001810329964703\n",
      "Epoch: 1 - Batch: 1085, Training Loss: 0.10010179559735714\n",
      "Epoch: 1 - Batch: 1086, Training Loss: 0.10018640630577334\n",
      "Epoch: 1 - Batch: 1087, Training Loss: 0.10027378711040143\n",
      "Epoch: 1 - Batch: 1088, Training Loss: 0.1003686489898767\n",
      "Epoch: 1 - Batch: 1089, Training Loss: 0.1004518933322398\n",
      "Epoch: 1 - Batch: 1090, Training Loss: 0.10054213277513709\n",
      "Epoch: 1 - Batch: 1091, Training Loss: 0.10063284564497657\n",
      "Epoch: 1 - Batch: 1092, Training Loss: 0.1007249056119132\n",
      "Epoch: 1 - Batch: 1093, Training Loss: 0.1008122562558991\n",
      "Epoch: 1 - Batch: 1094, Training Loss: 0.10090180512387954\n",
      "Epoch: 1 - Batch: 1095, Training Loss: 0.1009869094919506\n",
      "Epoch: 1 - Batch: 1096, Training Loss: 0.10107521089427111\n",
      "Epoch: 1 - Batch: 1097, Training Loss: 0.10115950991388775\n",
      "Epoch: 1 - Batch: 1098, Training Loss: 0.10124071416248927\n",
      "Epoch: 1 - Batch: 1099, Training Loss: 0.10132799197488757\n",
      "Epoch: 1 - Batch: 1100, Training Loss: 0.10141827489186085\n",
      "Epoch: 1 - Batch: 1101, Training Loss: 0.10150515765668346\n",
      "Epoch: 1 - Batch: 1102, Training Loss: 0.10159293585649969\n",
      "Epoch: 1 - Batch: 1103, Training Loss: 0.10168445321606166\n",
      "Epoch: 1 - Batch: 1104, Training Loss: 0.10176287334455582\n",
      "Epoch: 1 - Batch: 1105, Training Loss: 0.1018471205993177\n",
      "Epoch: 1 - Batch: 1106, Training Loss: 0.10193116226500737\n",
      "Epoch: 1 - Batch: 1107, Training Loss: 0.10201763556297146\n",
      "Epoch: 1 - Batch: 1108, Training Loss: 0.10210716000342646\n",
      "Epoch: 1 - Batch: 1109, Training Loss: 0.10218902387552792\n",
      "Epoch: 1 - Batch: 1110, Training Loss: 0.10227169641363087\n",
      "Epoch: 1 - Batch: 1111, Training Loss: 0.10234902589697743\n",
      "Epoch: 1 - Batch: 1112, Training Loss: 0.10243369660542577\n",
      "Epoch: 1 - Batch: 1113, Training Loss: 0.10253042104973722\n",
      "Epoch: 1 - Batch: 1114, Training Loss: 0.10262089340050225\n",
      "Epoch: 1 - Batch: 1115, Training Loss: 0.10271848944536291\n",
      "Epoch: 1 - Batch: 1116, Training Loss: 0.10279804447149954\n",
      "Epoch: 1 - Batch: 1117, Training Loss: 0.10288969893780711\n",
      "Epoch: 1 - Batch: 1118, Training Loss: 0.10297710901345582\n",
      "Epoch: 1 - Batch: 1119, Training Loss: 0.10305828182876209\n",
      "Epoch: 1 - Batch: 1120, Training Loss: 0.10314964200949195\n",
      "Epoch: 1 - Batch: 1121, Training Loss: 0.10324286977153513\n",
      "Epoch: 1 - Batch: 1122, Training Loss: 0.10333220367852729\n",
      "Epoch: 1 - Batch: 1123, Training Loss: 0.10342218560069355\n",
      "Epoch: 1 - Batch: 1124, Training Loss: 0.10350694330671731\n",
      "Epoch: 1 - Batch: 1125, Training Loss: 0.10359459750935016\n",
      "Epoch: 1 - Batch: 1126, Training Loss: 0.10368154664364818\n",
      "Epoch: 1 - Batch: 1127, Training Loss: 0.10377033402028171\n",
      "Epoch: 1 - Batch: 1128, Training Loss: 0.10386463027342438\n",
      "Epoch: 1 - Batch: 1129, Training Loss: 0.10394632114502131\n",
      "Epoch: 1 - Batch: 1130, Training Loss: 0.10403284046236753\n",
      "Epoch: 1 - Batch: 1131, Training Loss: 0.10411161055192228\n",
      "Epoch: 1 - Batch: 1132, Training Loss: 0.10419187691699015\n",
      "Epoch: 1 - Batch: 1133, Training Loss: 0.10427303016062202\n",
      "Epoch: 1 - Batch: 1134, Training Loss: 0.10435895762633328\n",
      "Epoch: 1 - Batch: 1135, Training Loss: 0.10444589760494272\n",
      "Epoch: 1 - Batch: 1136, Training Loss: 0.10453421938824614\n",
      "Epoch: 1 - Batch: 1137, Training Loss: 0.10462530512690149\n",
      "Epoch: 1 - Batch: 1138, Training Loss: 0.10471098593267833\n",
      "Epoch: 1 - Batch: 1139, Training Loss: 0.10479877041544684\n",
      "Epoch: 1 - Batch: 1140, Training Loss: 0.10488661585582627\n",
      "Epoch: 1 - Batch: 1141, Training Loss: 0.10496814886901905\n",
      "Epoch: 1 - Batch: 1142, Training Loss: 0.10505139116648814\n",
      "Epoch: 1 - Batch: 1143, Training Loss: 0.105144987719915\n",
      "Epoch: 1 - Batch: 1144, Training Loss: 0.10523184817624132\n",
      "Epoch: 1 - Batch: 1145, Training Loss: 0.10532076997222196\n",
      "Epoch: 1 - Batch: 1146, Training Loss: 0.10540725391475518\n",
      "Epoch: 1 - Batch: 1147, Training Loss: 0.10549497682212004\n",
      "Epoch: 1 - Batch: 1148, Training Loss: 0.10557814984030985\n",
      "Epoch: 1 - Batch: 1149, Training Loss: 0.105663705054998\n",
      "Epoch: 1 - Batch: 1150, Training Loss: 0.10575836259952034\n",
      "Epoch: 1 - Batch: 1151, Training Loss: 0.10585359795942631\n",
      "Epoch: 1 - Batch: 1152, Training Loss: 0.1059443099334663\n",
      "Epoch: 1 - Batch: 1153, Training Loss: 0.10603496870937237\n",
      "Epoch: 1 - Batch: 1154, Training Loss: 0.10612652157966178\n",
      "Epoch: 1 - Batch: 1155, Training Loss: 0.1062125230322331\n",
      "Epoch: 1 - Batch: 1156, Training Loss: 0.10629903422560462\n",
      "Epoch: 1 - Batch: 1157, Training Loss: 0.10638202209748439\n",
      "Epoch: 1 - Batch: 1158, Training Loss: 0.10647221563838015\n",
      "Epoch: 1 - Batch: 1159, Training Loss: 0.10655726653031053\n",
      "Epoch: 1 - Batch: 1160, Training Loss: 0.10664744561485588\n",
      "Epoch: 1 - Batch: 1161, Training Loss: 0.10673403859780993\n",
      "Epoch: 1 - Batch: 1162, Training Loss: 0.10681945332642613\n",
      "Epoch: 1 - Batch: 1163, Training Loss: 0.10691310308416486\n",
      "Epoch: 1 - Batch: 1164, Training Loss: 0.10699727145297017\n",
      "Epoch: 1 - Batch: 1165, Training Loss: 0.10708122790650547\n",
      "Epoch: 1 - Batch: 1166, Training Loss: 0.10716942805877175\n",
      "Epoch: 1 - Batch: 1167, Training Loss: 0.10726420813013073\n",
      "Epoch: 1 - Batch: 1168, Training Loss: 0.10734551343109279\n",
      "Epoch: 1 - Batch: 1169, Training Loss: 0.10743961173108166\n",
      "Epoch: 1 - Batch: 1170, Training Loss: 0.10752306833467871\n",
      "Epoch: 1 - Batch: 1171, Training Loss: 0.10760599574318178\n",
      "Epoch: 1 - Batch: 1172, Training Loss: 0.10769859320834699\n",
      "Epoch: 1 - Batch: 1173, Training Loss: 0.10778410690736218\n",
      "Epoch: 1 - Batch: 1174, Training Loss: 0.10787577218504292\n",
      "Epoch: 1 - Batch: 1175, Training Loss: 0.10797211152206408\n",
      "Epoch: 1 - Batch: 1176, Training Loss: 0.10805625761944065\n",
      "Epoch: 1 - Batch: 1177, Training Loss: 0.10815052048706297\n",
      "Epoch: 1 - Batch: 1178, Training Loss: 0.10822984064075089\n",
      "Epoch: 1 - Batch: 1179, Training Loss: 0.10832495320199141\n",
      "Epoch: 1 - Batch: 1180, Training Loss: 0.10841771758858046\n",
      "Epoch: 1 - Batch: 1181, Training Loss: 0.10850918328831247\n",
      "Epoch: 1 - Batch: 1182, Training Loss: 0.10859834181012008\n",
      "Epoch: 1 - Batch: 1183, Training Loss: 0.10869234847290994\n",
      "Epoch: 1 - Batch: 1184, Training Loss: 0.10878055343160384\n",
      "Epoch: 1 - Batch: 1185, Training Loss: 0.1088666530390877\n",
      "Epoch: 1 - Batch: 1186, Training Loss: 0.10895547092232735\n",
      "Epoch: 1 - Batch: 1187, Training Loss: 0.10904859629757764\n",
      "Epoch: 1 - Batch: 1188, Training Loss: 0.10913208209188226\n",
      "Epoch: 1 - Batch: 1189, Training Loss: 0.10922018177251318\n",
      "Epoch: 1 - Batch: 1190, Training Loss: 0.10931124790860448\n",
      "Epoch: 1 - Batch: 1191, Training Loss: 0.10940816785961044\n",
      "Epoch: 1 - Batch: 1192, Training Loss: 0.10949957881672663\n",
      "Epoch: 1 - Batch: 1193, Training Loss: 0.10958688843309583\n",
      "Epoch: 1 - Batch: 1194, Training Loss: 0.10967326987763344\n",
      "Epoch: 1 - Batch: 1195, Training Loss: 0.10975746259982906\n",
      "Epoch: 1 - Batch: 1196, Training Loss: 0.10984284063155576\n",
      "Epoch: 1 - Batch: 1197, Training Loss: 0.10992787712891501\n",
      "Epoch: 1 - Batch: 1198, Training Loss: 0.11002428607264561\n",
      "Epoch: 1 - Batch: 1199, Training Loss: 0.1101119413625937\n",
      "Epoch: 1 - Batch: 1200, Training Loss: 0.11019856416640392\n",
      "Epoch: 1 - Batch: 1201, Training Loss: 0.11027879609570376\n",
      "Epoch: 1 - Batch: 1202, Training Loss: 0.11035934940697147\n",
      "Epoch: 1 - Batch: 1203, Training Loss: 0.11044360961684738\n",
      "Epoch: 1 - Batch: 1204, Training Loss: 0.11053406069083001\n",
      "Epoch: 1 - Batch: 1205, Training Loss: 0.11062280950483991\n",
      "Epoch: 1 - Batch: 1206, Training Loss: 0.11070463739051352\n",
      "Epoch: 1 - Batch: 1207, Training Loss: 0.11078257119255279\n",
      "Epoch: 1 - Batch: 1208, Training Loss: 0.11086511631707845\n",
      "Epoch: 1 - Batch: 1209, Training Loss: 0.11095465014180536\n",
      "Epoch: 1 - Batch: 1210, Training Loss: 0.11104940461801059\n",
      "Epoch: 1 - Batch: 1211, Training Loss: 0.11114416317164799\n",
      "Epoch: 1 - Batch: 1212, Training Loss: 0.11122309927447123\n",
      "Epoch: 1 - Batch: 1213, Training Loss: 0.11130270379816319\n",
      "Epoch: 1 - Batch: 1214, Training Loss: 0.11139361106267023\n",
      "Epoch: 1 - Batch: 1215, Training Loss: 0.11148605489513372\n",
      "Epoch: 1 - Batch: 1216, Training Loss: 0.11157822445232675\n",
      "Epoch: 1 - Batch: 1217, Training Loss: 0.111667041347098\n",
      "Epoch: 1 - Batch: 1218, Training Loss: 0.11175893719738988\n",
      "Epoch: 1 - Batch: 1219, Training Loss: 0.11185187365161642\n",
      "Epoch: 1 - Batch: 1220, Training Loss: 0.11193731990619678\n",
      "Epoch: 1 - Batch: 1221, Training Loss: 0.11203260646876609\n",
      "Epoch: 1 - Batch: 1222, Training Loss: 0.11212404581345926\n",
      "Epoch: 1 - Batch: 1223, Training Loss: 0.11221280440712846\n",
      "Epoch: 1 - Batch: 1224, Training Loss: 0.11230416645359241\n",
      "Epoch: 1 - Batch: 1225, Training Loss: 0.11239008853832881\n",
      "Epoch: 1 - Batch: 1226, Training Loss: 0.11248450492127222\n",
      "Epoch: 1 - Batch: 1227, Training Loss: 0.11256763255640642\n",
      "Epoch: 1 - Batch: 1228, Training Loss: 0.11264830589862802\n",
      "Epoch: 1 - Batch: 1229, Training Loss: 0.11273494650722539\n",
      "Epoch: 1 - Batch: 1230, Training Loss: 0.11281248308088056\n",
      "Epoch: 1 - Batch: 1231, Training Loss: 0.11291426737757267\n",
      "Epoch: 1 - Batch: 1232, Training Loss: 0.11301431354524484\n",
      "Epoch: 1 - Batch: 1233, Training Loss: 0.11310412079886616\n",
      "Epoch: 1 - Batch: 1234, Training Loss: 0.1131883574955499\n",
      "Epoch: 1 - Batch: 1235, Training Loss: 0.11327703293431457\n",
      "Epoch: 1 - Batch: 1236, Training Loss: 0.11336723142373029\n",
      "Epoch: 1 - Batch: 1237, Training Loss: 0.11345925389682475\n",
      "Epoch: 1 - Batch: 1238, Training Loss: 0.11354714809958615\n",
      "Epoch: 1 - Batch: 1239, Training Loss: 0.113638341031462\n",
      "Epoch: 1 - Batch: 1240, Training Loss: 0.11372870494106516\n",
      "Epoch: 1 - Batch: 1241, Training Loss: 0.11381496591601602\n",
      "Epoch: 1 - Batch: 1242, Training Loss: 0.1138997436261691\n",
      "Epoch: 1 - Batch: 1243, Training Loss: 0.11398207609407345\n",
      "Epoch: 1 - Batch: 1244, Training Loss: 0.11407089390317797\n",
      "Epoch: 1 - Batch: 1245, Training Loss: 0.11416215029472537\n",
      "Epoch: 1 - Batch: 1246, Training Loss: 0.11425480331181491\n",
      "Epoch: 1 - Batch: 1247, Training Loss: 0.11434279751668917\n",
      "Epoch: 1 - Batch: 1248, Training Loss: 0.11443132947308705\n",
      "Epoch: 1 - Batch: 1249, Training Loss: 0.11451935525952682\n",
      "Epoch: 1 - Batch: 1250, Training Loss: 0.11462057666053029\n",
      "Epoch: 1 - Batch: 1251, Training Loss: 0.11470639194125559\n",
      "Epoch: 1 - Batch: 1252, Training Loss: 0.11479478937945951\n",
      "Epoch: 1 - Batch: 1253, Training Loss: 0.11488777285389244\n",
      "Epoch: 1 - Batch: 1254, Training Loss: 0.1149795999747406\n",
      "Epoch: 1 - Batch: 1255, Training Loss: 0.11506058299793533\n",
      "Epoch: 1 - Batch: 1256, Training Loss: 0.11515496422847112\n",
      "Epoch: 1 - Batch: 1257, Training Loss: 0.1152512757401067\n",
      "Epoch: 1 - Batch: 1258, Training Loss: 0.11534176325150589\n",
      "Epoch: 1 - Batch: 1259, Training Loss: 0.11543448887159971\n",
      "Epoch: 1 - Batch: 1260, Training Loss: 0.11552396356738226\n",
      "Epoch: 1 - Batch: 1261, Training Loss: 0.11561647176767266\n",
      "Epoch: 1 - Batch: 1262, Training Loss: 0.11569187550451823\n",
      "Epoch: 1 - Batch: 1263, Training Loss: 0.11578553901432363\n",
      "Epoch: 1 - Batch: 1264, Training Loss: 0.11587601543016497\n",
      "Epoch: 1 - Batch: 1265, Training Loss: 0.11595557787599255\n",
      "Epoch: 1 - Batch: 1266, Training Loss: 0.11604855198468735\n",
      "Epoch: 1 - Batch: 1267, Training Loss: 0.11613680371647055\n",
      "Epoch: 1 - Batch: 1268, Training Loss: 0.11621970320691911\n",
      "Epoch: 1 - Batch: 1269, Training Loss: 0.1163019130306062\n",
      "Epoch: 1 - Batch: 1270, Training Loss: 0.11639000927630942\n",
      "Epoch: 1 - Batch: 1271, Training Loss: 0.11647448097517835\n",
      "Epoch: 1 - Batch: 1272, Training Loss: 0.11655348530085526\n",
      "Epoch: 1 - Batch: 1273, Training Loss: 0.1166408041964716\n",
      "Epoch: 1 - Batch: 1274, Training Loss: 0.11672214350074678\n",
      "Epoch: 1 - Batch: 1275, Training Loss: 0.11680553887490412\n",
      "Epoch: 1 - Batch: 1276, Training Loss: 0.11689717208296307\n",
      "Epoch: 1 - Batch: 1277, Training Loss: 0.1169877612089142\n",
      "Epoch: 1 - Batch: 1278, Training Loss: 0.11707855631932493\n",
      "Epoch: 1 - Batch: 1279, Training Loss: 0.11717440903952861\n",
      "Epoch: 1 - Batch: 1280, Training Loss: 0.11726550654095796\n",
      "Epoch: 1 - Batch: 1281, Training Loss: 0.1173472884010715\n",
      "Epoch: 1 - Batch: 1282, Training Loss: 0.11744157645610434\n",
      "Epoch: 1 - Batch: 1283, Training Loss: 0.11753134617881593\n",
      "Epoch: 1 - Batch: 1284, Training Loss: 0.11762048845267414\n",
      "Epoch: 1 - Batch: 1285, Training Loss: 0.11771188730723031\n",
      "Epoch: 1 - Batch: 1286, Training Loss: 0.11780591502439719\n",
      "Epoch: 1 - Batch: 1287, Training Loss: 0.11789417316269123\n",
      "Epoch: 1 - Batch: 1288, Training Loss: 0.11798776785656193\n",
      "Epoch: 1 - Batch: 1289, Training Loss: 0.11808884546012427\n",
      "Epoch: 1 - Batch: 1290, Training Loss: 0.11817910114479302\n",
      "Epoch: 1 - Batch: 1291, Training Loss: 0.11828389728000113\n",
      "Epoch: 1 - Batch: 1292, Training Loss: 0.11837394570137928\n",
      "Epoch: 1 - Batch: 1293, Training Loss: 0.11846333861993517\n",
      "Epoch: 1 - Batch: 1294, Training Loss: 0.11854840735372026\n",
      "Epoch: 1 - Batch: 1295, Training Loss: 0.1186407780086322\n",
      "Epoch: 1 - Batch: 1296, Training Loss: 0.11873215955881337\n",
      "Epoch: 1 - Batch: 1297, Training Loss: 0.11882316613968331\n",
      "Epoch: 1 - Batch: 1298, Training Loss: 0.11890516907075546\n",
      "Epoch: 1 - Batch: 1299, Training Loss: 0.11899187884742941\n",
      "Epoch: 1 - Batch: 1300, Training Loss: 0.11908571194752335\n",
      "Epoch: 1 - Batch: 1301, Training Loss: 0.11917544092680289\n",
      "Epoch: 1 - Batch: 1302, Training Loss: 0.11926687135056872\n",
      "Epoch: 1 - Batch: 1303, Training Loss: 0.11935890972564865\n",
      "Epoch: 1 - Batch: 1304, Training Loss: 0.11944985503383339\n",
      "Epoch: 1 - Batch: 1305, Training Loss: 0.1195350863768489\n",
      "Epoch: 1 - Batch: 1306, Training Loss: 0.11962875449538823\n",
      "Epoch: 1 - Batch: 1307, Training Loss: 0.11972203261989661\n",
      "Epoch: 1 - Batch: 1308, Training Loss: 0.11980713294760308\n",
      "Epoch: 1 - Batch: 1309, Training Loss: 0.11990349228257563\n",
      "Epoch: 1 - Batch: 1310, Training Loss: 0.11999141881443177\n",
      "Epoch: 1 - Batch: 1311, Training Loss: 0.12007456388367745\n",
      "Epoch: 1 - Batch: 1312, Training Loss: 0.12016851212798461\n",
      "Epoch: 1 - Batch: 1313, Training Loss: 0.12025773591959654\n",
      "Epoch: 1 - Batch: 1314, Training Loss: 0.12034410115325234\n",
      "Epoch: 1 - Batch: 1315, Training Loss: 0.12042572301739879\n",
      "Epoch: 1 - Batch: 1316, Training Loss: 0.1205225971281825\n",
      "Epoch: 1 - Batch: 1317, Training Loss: 0.12061072226187483\n",
      "Epoch: 1 - Batch: 1318, Training Loss: 0.12069269188773968\n",
      "Epoch: 1 - Batch: 1319, Training Loss: 0.1207753674468492\n",
      "Epoch: 1 - Batch: 1320, Training Loss: 0.12086029730700142\n",
      "Epoch: 1 - Batch: 1321, Training Loss: 0.12094595762677059\n",
      "Epoch: 1 - Batch: 1322, Training Loss: 0.12103694564296831\n",
      "Epoch: 1 - Batch: 1323, Training Loss: 0.12112310824060124\n",
      "Epoch: 1 - Batch: 1324, Training Loss: 0.12120458641233135\n",
      "Epoch: 1 - Batch: 1325, Training Loss: 0.121284639601832\n",
      "Epoch: 1 - Batch: 1326, Training Loss: 0.12137002921717281\n",
      "Epoch: 1 - Batch: 1327, Training Loss: 0.12145390889760274\n",
      "Epoch: 1 - Batch: 1328, Training Loss: 0.12155300242885032\n",
      "Epoch: 1 - Batch: 1329, Training Loss: 0.12163655331997729\n",
      "Epoch: 1 - Batch: 1330, Training Loss: 0.12172750903796992\n",
      "Epoch: 1 - Batch: 1331, Training Loss: 0.12181465493308173\n",
      "Epoch: 1 - Batch: 1332, Training Loss: 0.12190115399918153\n",
      "Epoch: 1 - Batch: 1333, Training Loss: 0.12199149301791468\n",
      "Epoch: 1 - Batch: 1334, Training Loss: 0.12207480693609758\n",
      "Epoch: 1 - Batch: 1335, Training Loss: 0.12216214450758883\n",
      "Epoch: 1 - Batch: 1336, Training Loss: 0.12224497756678271\n",
      "Epoch: 1 - Batch: 1337, Training Loss: 0.12233234072724979\n",
      "Epoch: 1 - Batch: 1338, Training Loss: 0.1224167385195717\n",
      "Epoch: 1 - Batch: 1339, Training Loss: 0.12250989885571387\n",
      "Epoch: 1 - Batch: 1340, Training Loss: 0.12259904895700625\n",
      "Epoch: 1 - Batch: 1341, Training Loss: 0.12269813157185591\n",
      "Epoch: 1 - Batch: 1342, Training Loss: 0.12278325131061066\n",
      "Epoch: 1 - Batch: 1343, Training Loss: 0.12287976453157998\n",
      "Epoch: 1 - Batch: 1344, Training Loss: 0.12296506874942859\n",
      "Epoch: 1 - Batch: 1345, Training Loss: 0.12304610717009945\n",
      "Epoch: 1 - Batch: 1346, Training Loss: 0.12313312757914735\n",
      "Epoch: 1 - Batch: 1347, Training Loss: 0.12321230760508312\n",
      "Epoch: 1 - Batch: 1348, Training Loss: 0.12329410316783991\n",
      "Epoch: 1 - Batch: 1349, Training Loss: 0.12338124620257128\n",
      "Epoch: 1 - Batch: 1350, Training Loss: 0.12347433325613712\n",
      "Epoch: 1 - Batch: 1351, Training Loss: 0.12356786984659941\n",
      "Epoch: 1 - Batch: 1352, Training Loss: 0.12365629085359683\n",
      "Epoch: 1 - Batch: 1353, Training Loss: 0.12374933640384556\n",
      "Epoch: 1 - Batch: 1354, Training Loss: 0.12383365063969769\n",
      "Epoch: 1 - Batch: 1355, Training Loss: 0.12391860820785486\n",
      "Epoch: 1 - Batch: 1356, Training Loss: 0.12399993199935402\n",
      "Epoch: 1 - Batch: 1357, Training Loss: 0.1240931348952389\n",
      "Epoch: 1 - Batch: 1358, Training Loss: 0.12418493372735692\n",
      "Epoch: 1 - Batch: 1359, Training Loss: 0.12427703089777312\n",
      "Epoch: 1 - Batch: 1360, Training Loss: 0.12436623358558462\n",
      "Epoch: 1 - Batch: 1361, Training Loss: 0.12446667955165874\n",
      "Epoch: 1 - Batch: 1362, Training Loss: 0.12455079146038438\n",
      "Epoch: 1 - Batch: 1363, Training Loss: 0.1246369957343164\n",
      "Epoch: 1 - Batch: 1364, Training Loss: 0.12472100456432125\n",
      "Epoch: 1 - Batch: 1365, Training Loss: 0.12480735461263119\n",
      "Epoch: 1 - Batch: 1366, Training Loss: 0.12489679369266156\n",
      "Epoch: 1 - Batch: 1367, Training Loss: 0.12497798034652549\n",
      "Epoch: 1 - Batch: 1368, Training Loss: 0.12507817283841113\n",
      "Epoch: 1 - Batch: 1369, Training Loss: 0.12516053281662673\n",
      "Epoch: 1 - Batch: 1370, Training Loss: 0.12525349107275952\n",
      "Epoch: 1 - Batch: 1371, Training Loss: 0.12533056580308657\n",
      "Epoch: 1 - Batch: 1372, Training Loss: 0.12541797611967445\n",
      "Epoch: 1 - Batch: 1373, Training Loss: 0.12549954696031748\n",
      "Epoch: 1 - Batch: 1374, Training Loss: 0.12558909289378234\n",
      "Epoch: 1 - Batch: 1375, Training Loss: 0.1256691334926667\n",
      "Epoch: 1 - Batch: 1376, Training Loss: 0.12575663666968323\n",
      "Epoch: 1 - Batch: 1377, Training Loss: 0.12585978241479812\n",
      "Epoch: 1 - Batch: 1378, Training Loss: 0.12595143048077279\n",
      "Epoch: 1 - Batch: 1379, Training Loss: 0.12604310244297112\n",
      "Epoch: 1 - Batch: 1380, Training Loss: 0.12612457467771288\n",
      "Epoch: 1 - Batch: 1381, Training Loss: 0.12620568437931154\n",
      "Epoch: 1 - Batch: 1382, Training Loss: 0.12629561031092063\n",
      "Epoch: 1 - Batch: 1383, Training Loss: 0.12637918794011793\n",
      "Epoch: 1 - Batch: 1384, Training Loss: 0.12646252209719142\n",
      "Epoch: 1 - Batch: 1385, Training Loss: 0.1265546468026306\n",
      "Epoch: 1 - Batch: 1386, Training Loss: 0.12663474090854523\n",
      "Epoch: 1 - Batch: 1387, Training Loss: 0.1267217349810881\n",
      "Epoch: 1 - Batch: 1388, Training Loss: 0.12680974417979246\n",
      "Epoch: 1 - Batch: 1389, Training Loss: 0.1268911072134873\n",
      "Epoch: 1 - Batch: 1390, Training Loss: 0.1269749039428843\n",
      "Epoch: 1 - Batch: 1391, Training Loss: 0.12705577328603462\n",
      "Epoch: 1 - Batch: 1392, Training Loss: 0.12713656864232487\n",
      "Epoch: 1 - Batch: 1393, Training Loss: 0.12722247004953782\n",
      "Epoch: 1 - Batch: 1394, Training Loss: 0.12731197299080504\n",
      "Epoch: 1 - Batch: 1395, Training Loss: 0.12739619272907773\n",
      "Epoch: 1 - Batch: 1396, Training Loss: 0.12748076327478708\n",
      "Epoch: 1 - Batch: 1397, Training Loss: 0.12757400151459533\n",
      "Epoch: 1 - Batch: 1398, Training Loss: 0.12767041722315658\n",
      "Epoch: 1 - Batch: 1399, Training Loss: 0.1277592040561325\n",
      "Epoch: 1 - Batch: 1400, Training Loss: 0.12784862682025627\n",
      "Epoch: 1 - Batch: 1401, Training Loss: 0.12793420632037752\n",
      "Epoch: 1 - Batch: 1402, Training Loss: 0.12802034723966277\n",
      "Epoch: 1 - Batch: 1403, Training Loss: 0.12810113812931143\n",
      "Epoch: 1 - Batch: 1404, Training Loss: 0.12819613257475557\n",
      "Epoch: 1 - Batch: 1405, Training Loss: 0.1282810071362785\n",
      "Epoch: 1 - Batch: 1406, Training Loss: 0.12836357587284314\n",
      "Epoch: 1 - Batch: 1407, Training Loss: 0.12845106968412154\n",
      "Epoch: 1 - Batch: 1408, Training Loss: 0.1285314390414192\n",
      "Epoch: 1 - Batch: 1409, Training Loss: 0.1286190351653554\n",
      "Epoch: 1 - Batch: 1410, Training Loss: 0.12871333431594606\n",
      "Epoch: 1 - Batch: 1411, Training Loss: 0.12880133052864676\n",
      "Epoch: 1 - Batch: 1412, Training Loss: 0.12888574756521293\n",
      "Epoch: 1 - Batch: 1413, Training Loss: 0.12896694304115736\n",
      "Epoch: 1 - Batch: 1414, Training Loss: 0.1290656365021742\n",
      "Epoch: 1 - Batch: 1415, Training Loss: 0.12914625030770824\n",
      "Epoch: 1 - Batch: 1416, Training Loss: 0.1292438249151604\n",
      "Epoch: 1 - Batch: 1417, Training Loss: 0.12932986910663433\n",
      "Epoch: 1 - Batch: 1418, Training Loss: 0.12941716415545043\n",
      "Epoch: 1 - Batch: 1419, Training Loss: 0.1295088069948984\n",
      "Epoch: 1 - Batch: 1420, Training Loss: 0.12959069203875748\n",
      "Epoch: 1 - Batch: 1421, Training Loss: 0.12967452386496078\n",
      "Epoch: 1 - Batch: 1422, Training Loss: 0.12975927151949646\n",
      "Epoch: 1 - Batch: 1423, Training Loss: 0.12984985218168688\n",
      "Epoch: 1 - Batch: 1424, Training Loss: 0.12993823870967078\n",
      "Epoch: 1 - Batch: 1425, Training Loss: 0.13002230912125723\n",
      "Epoch: 1 - Batch: 1426, Training Loss: 0.13010830104746432\n",
      "Epoch: 1 - Batch: 1427, Training Loss: 0.13020006190386182\n",
      "Epoch: 1 - Batch: 1428, Training Loss: 0.1302812838522257\n",
      "Epoch: 1 - Batch: 1429, Training Loss: 0.13036048784852028\n",
      "Epoch: 1 - Batch: 1430, Training Loss: 0.13043854608637578\n",
      "Epoch: 1 - Batch: 1431, Training Loss: 0.130522793995998\n",
      "Epoch: 1 - Batch: 1432, Training Loss: 0.1306050606504404\n",
      "Epoch: 1 - Batch: 1433, Training Loss: 0.13068940646785804\n",
      "Epoch: 1 - Batch: 1434, Training Loss: 0.13077668350183747\n",
      "Epoch: 1 - Batch: 1435, Training Loss: 0.1308666982967561\n",
      "Epoch: 1 - Batch: 1436, Training Loss: 0.13095115481645708\n",
      "Epoch: 1 - Batch: 1437, Training Loss: 0.13103704805644986\n",
      "Epoch: 1 - Batch: 1438, Training Loss: 0.13112224425323568\n",
      "Epoch: 1 - Batch: 1439, Training Loss: 0.13121004226566546\n",
      "Epoch: 1 - Batch: 1440, Training Loss: 0.13130520992166367\n",
      "Epoch: 1 - Batch: 1441, Training Loss: 0.131389009011029\n",
      "Epoch: 1 - Batch: 1442, Training Loss: 0.13148367715069706\n",
      "Epoch: 1 - Batch: 1443, Training Loss: 0.13157174925923742\n",
      "Epoch: 1 - Batch: 1444, Training Loss: 0.13165975811815578\n",
      "Epoch: 1 - Batch: 1445, Training Loss: 0.13175167232307036\n",
      "Epoch: 1 - Batch: 1446, Training Loss: 0.1318506266965004\n",
      "Epoch: 1 - Batch: 1447, Training Loss: 0.13194558625518782\n",
      "Epoch: 1 - Batch: 1448, Training Loss: 0.1320338848465513\n",
      "Epoch: 1 - Batch: 1449, Training Loss: 0.1321184923918686\n",
      "Epoch: 1 - Batch: 1450, Training Loss: 0.1322103516132282\n",
      "Epoch: 1 - Batch: 1451, Training Loss: 0.1322928021475055\n",
      "Epoch: 1 - Batch: 1452, Training Loss: 0.13238207519573358\n",
      "Epoch: 1 - Batch: 1453, Training Loss: 0.132466422310516\n",
      "Epoch: 1 - Batch: 1454, Training Loss: 0.1325513889231591\n",
      "Epoch: 1 - Batch: 1455, Training Loss: 0.13264247044228994\n",
      "Epoch: 1 - Batch: 1456, Training Loss: 0.13273586270400936\n",
      "Epoch: 1 - Batch: 1457, Training Loss: 0.13282472169517878\n",
      "Epoch: 1 - Batch: 1458, Training Loss: 0.13291831797677683\n",
      "Epoch: 1 - Batch: 1459, Training Loss: 0.13300933329941425\n",
      "Epoch: 1 - Batch: 1460, Training Loss: 0.1330996220236394\n",
      "Epoch: 1 - Batch: 1461, Training Loss: 0.1331719402440053\n",
      "Epoch: 1 - Batch: 1462, Training Loss: 0.13326063809629105\n",
      "Epoch: 1 - Batch: 1463, Training Loss: 0.1333451628388457\n",
      "Epoch: 1 - Batch: 1464, Training Loss: 0.13343398310676538\n",
      "Epoch: 1 - Batch: 1465, Training Loss: 0.13351836319932495\n",
      "Epoch: 1 - Batch: 1466, Training Loss: 0.1336069348303734\n",
      "Epoch: 1 - Batch: 1467, Training Loss: 0.13369232817718243\n",
      "Epoch: 1 - Batch: 1468, Training Loss: 0.13377536405155907\n",
      "Epoch: 1 - Batch: 1469, Training Loss: 0.13387289477101408\n",
      "Epoch: 1 - Batch: 1470, Training Loss: 0.13395657796863694\n",
      "Epoch: 1 - Batch: 1471, Training Loss: 0.13405353017460253\n",
      "Epoch: 1 - Batch: 1472, Training Loss: 0.134141621793038\n",
      "Epoch: 1 - Batch: 1473, Training Loss: 0.13423520225914162\n",
      "Epoch: 1 - Batch: 1474, Training Loss: 0.13431758313110811\n",
      "Epoch: 1 - Batch: 1475, Training Loss: 0.1344096545086769\n",
      "Epoch: 1 - Batch: 1476, Training Loss: 0.1344984733247836\n",
      "Epoch: 1 - Batch: 1477, Training Loss: 0.13458322805922424\n",
      "Epoch: 1 - Batch: 1478, Training Loss: 0.13467386422391556\n",
      "Epoch: 1 - Batch: 1479, Training Loss: 0.134757878459607\n",
      "Epoch: 1 - Batch: 1480, Training Loss: 0.1348439739896587\n",
      "Epoch: 1 - Batch: 1481, Training Loss: 0.1349310557455269\n",
      "Epoch: 1 - Batch: 1482, Training Loss: 0.13502262314012395\n",
      "Epoch: 1 - Batch: 1483, Training Loss: 0.13510655124932774\n",
      "Epoch: 1 - Batch: 1484, Training Loss: 0.13518918178370146\n",
      "Epoch: 1 - Batch: 1485, Training Loss: 0.13527888406197824\n",
      "Epoch: 1 - Batch: 1486, Training Loss: 0.13535021228181388\n",
      "Epoch: 1 - Batch: 1487, Training Loss: 0.13544948277979546\n",
      "Epoch: 1 - Batch: 1488, Training Loss: 0.13553449071041782\n",
      "Epoch: 1 - Batch: 1489, Training Loss: 0.13562768665613426\n",
      "Epoch: 1 - Batch: 1490, Training Loss: 0.13571372735707915\n",
      "Epoch: 1 - Batch: 1491, Training Loss: 0.1358005226918714\n",
      "Epoch: 1 - Batch: 1492, Training Loss: 0.13587601279614378\n",
      "Epoch: 1 - Batch: 1493, Training Loss: 0.1359612983875409\n",
      "Epoch: 1 - Batch: 1494, Training Loss: 0.13605176755049533\n",
      "Epoch: 1 - Batch: 1495, Training Loss: 0.1361400528469588\n",
      "Epoch: 1 - Batch: 1496, Training Loss: 0.1362313354576306\n",
      "Epoch: 1 - Batch: 1497, Training Loss: 0.136321918763974\n",
      "Epoch: 1 - Batch: 1498, Training Loss: 0.1364169697312771\n",
      "Epoch: 1 - Batch: 1499, Training Loss: 0.1365039611225994\n",
      "Epoch: 1 - Batch: 1500, Training Loss: 0.1365882394399809\n",
      "Epoch: 1 - Batch: 1501, Training Loss: 0.1366743426244848\n",
      "Epoch: 1 - Batch: 1502, Training Loss: 0.1367724006794776\n",
      "Epoch: 1 - Batch: 1503, Training Loss: 0.13685977214307926\n",
      "Epoch: 1 - Batch: 1504, Training Loss: 0.1369440187553367\n",
      "Epoch: 1 - Batch: 1505, Training Loss: 0.1370317843151132\n",
      "Epoch: 1 - Batch: 1506, Training Loss: 0.1371121369116935\n",
      "Epoch: 1 - Batch: 1507, Training Loss: 0.13720432288981788\n",
      "Epoch: 1 - Batch: 1508, Training Loss: 0.13727959176250557\n",
      "Epoch: 1 - Batch: 1509, Training Loss: 0.13736564554830097\n",
      "Epoch: 1 - Batch: 1510, Training Loss: 0.13745279166581817\n",
      "Epoch: 1 - Batch: 1511, Training Loss: 0.13753479179828915\n",
      "Epoch: 1 - Batch: 1512, Training Loss: 0.1376238122557724\n",
      "Epoch: 1 - Batch: 1513, Training Loss: 0.13771625808988439\n",
      "Epoch: 1 - Batch: 1514, Training Loss: 0.13780359357941407\n",
      "Epoch: 1 - Batch: 1515, Training Loss: 0.13788964777295268\n",
      "Epoch: 1 - Batch: 1516, Training Loss: 0.13797622542229063\n",
      "Epoch: 1 - Batch: 1517, Training Loss: 0.1380653133303866\n",
      "Epoch: 1 - Batch: 1518, Training Loss: 0.13815933706896816\n",
      "Epoch: 1 - Batch: 1519, Training Loss: 0.13824866144341813\n",
      "Epoch: 1 - Batch: 1520, Training Loss: 0.13833624144296344\n",
      "Epoch: 1 - Batch: 1521, Training Loss: 0.13842419264946212\n",
      "Epoch: 1 - Batch: 1522, Training Loss: 0.1385089239448457\n",
      "Epoch: 1 - Batch: 1523, Training Loss: 0.13859428803916793\n",
      "Epoch: 1 - Batch: 1524, Training Loss: 0.13868182151891897\n",
      "Epoch: 1 - Batch: 1525, Training Loss: 0.1387683553358611\n",
      "Epoch: 1 - Batch: 1526, Training Loss: 0.13885393139487673\n",
      "Epoch: 1 - Batch: 1527, Training Loss: 0.13893930213664896\n",
      "Epoch: 1 - Batch: 1528, Training Loss: 0.13903006557282524\n",
      "Epoch: 1 - Batch: 1529, Training Loss: 0.13911897393799144\n",
      "Epoch: 1 - Batch: 1530, Training Loss: 0.13920478478914272\n",
      "Epoch: 1 - Batch: 1531, Training Loss: 0.13929139204310936\n",
      "Epoch: 1 - Batch: 1532, Training Loss: 0.1393775653903362\n",
      "Epoch: 1 - Batch: 1533, Training Loss: 0.13947544890034258\n",
      "Epoch: 1 - Batch: 1534, Training Loss: 0.1395698322004841\n",
      "Epoch: 1 - Batch: 1535, Training Loss: 0.1396549201043783\n",
      "Epoch: 1 - Batch: 1536, Training Loss: 0.13974016311132098\n",
      "Epoch: 1 - Batch: 1537, Training Loss: 0.13982610263634676\n",
      "Epoch: 1 - Batch: 1538, Training Loss: 0.13991063552140992\n",
      "Epoch: 1 - Batch: 1539, Training Loss: 0.14000018031195818\n",
      "Epoch: 1 - Batch: 1540, Training Loss: 0.1400866409575267\n",
      "Epoch: 1 - Batch: 1541, Training Loss: 0.14016722385488933\n",
      "Epoch: 1 - Batch: 1542, Training Loss: 0.14025232844536578\n",
      "Epoch: 1 - Batch: 1543, Training Loss: 0.1403324225698142\n",
      "Epoch: 1 - Batch: 1544, Training Loss: 0.14041054125869057\n",
      "Epoch: 1 - Batch: 1545, Training Loss: 0.14049501810995105\n",
      "Epoch: 1 - Batch: 1546, Training Loss: 0.14058967484516488\n",
      "Epoch: 1 - Batch: 1547, Training Loss: 0.1406769385039312\n",
      "Epoch: 1 - Batch: 1548, Training Loss: 0.14076467206822105\n",
      "Epoch: 1 - Batch: 1549, Training Loss: 0.14085224905911567\n",
      "Epoch: 1 - Batch: 1550, Training Loss: 0.1409420510614986\n",
      "Epoch: 1 - Batch: 1551, Training Loss: 0.14103733856311287\n",
      "Epoch: 1 - Batch: 1552, Training Loss: 0.14112385986357384\n",
      "Epoch: 1 - Batch: 1553, Training Loss: 0.14122689519059006\n",
      "Epoch: 1 - Batch: 1554, Training Loss: 0.1413190248877947\n",
      "Epoch: 1 - Batch: 1555, Training Loss: 0.14140541217665173\n",
      "Epoch: 1 - Batch: 1556, Training Loss: 0.14148887553890152\n",
      "Epoch: 1 - Batch: 1557, Training Loss: 0.1415735920443266\n",
      "Epoch: 1 - Batch: 1558, Training Loss: 0.14165837499336223\n",
      "Epoch: 1 - Batch: 1559, Training Loss: 0.1417499652895366\n",
      "Epoch: 1 - Batch: 1560, Training Loss: 0.14184046746733572\n",
      "Epoch: 1 - Batch: 1561, Training Loss: 0.14192395708577749\n",
      "Epoch: 1 - Batch: 1562, Training Loss: 0.1420150883372249\n",
      "Epoch: 1 - Batch: 1563, Training Loss: 0.14209342982687959\n",
      "Epoch: 1 - Batch: 1564, Training Loss: 0.14218148099224565\n",
      "Epoch: 1 - Batch: 1565, Training Loss: 0.14226109249081778\n",
      "Epoch: 1 - Batch: 1566, Training Loss: 0.14235600507254426\n",
      "Epoch: 1 - Batch: 1567, Training Loss: 0.14244467701460195\n",
      "Epoch: 1 - Batch: 1568, Training Loss: 0.14253362901447028\n",
      "Epoch: 1 - Batch: 1569, Training Loss: 0.14262651922073136\n",
      "Epoch: 1 - Batch: 1570, Training Loss: 0.14272284621425332\n",
      "Epoch: 1 - Batch: 1571, Training Loss: 0.14281514307999887\n",
      "Epoch: 1 - Batch: 1572, Training Loss: 0.142905523614405\n",
      "Epoch: 1 - Batch: 1573, Training Loss: 0.14299586585801632\n",
      "Epoch: 1 - Batch: 1574, Training Loss: 0.14308505515208095\n",
      "Epoch: 1 - Batch: 1575, Training Loss: 0.14317139432723844\n",
      "Epoch: 1 - Batch: 1576, Training Loss: 0.14326900583545168\n",
      "Epoch: 1 - Batch: 1577, Training Loss: 0.14336360386182023\n",
      "Epoch: 1 - Batch: 1578, Training Loss: 0.14344838929438275\n",
      "Epoch: 1 - Batch: 1579, Training Loss: 0.14353736034697956\n",
      "Epoch: 1 - Batch: 1580, Training Loss: 0.14361718199393445\n",
      "Epoch: 1 - Batch: 1581, Training Loss: 0.1437065412860308\n",
      "Epoch: 1 - Batch: 1582, Training Loss: 0.1437970658341055\n",
      "Epoch: 1 - Batch: 1583, Training Loss: 0.14388309029945687\n",
      "Epoch: 1 - Batch: 1584, Training Loss: 0.14396989890443745\n",
      "Epoch: 1 - Batch: 1585, Training Loss: 0.14404908793779156\n",
      "Epoch: 1 - Batch: 1586, Training Loss: 0.14413348195539977\n",
      "Epoch: 1 - Batch: 1587, Training Loss: 0.1442196835172216\n",
      "Epoch: 1 - Batch: 1588, Training Loss: 0.14430602379205018\n",
      "Epoch: 1 - Batch: 1589, Training Loss: 0.14439547799564711\n",
      "Epoch: 1 - Batch: 1590, Training Loss: 0.144487789070626\n",
      "Epoch: 1 - Batch: 1591, Training Loss: 0.14456706162338234\n",
      "Epoch: 1 - Batch: 1592, Training Loss: 0.14465915252320208\n",
      "Epoch: 1 - Batch: 1593, Training Loss: 0.14475553988743184\n",
      "Epoch: 1 - Batch: 1594, Training Loss: 0.14485093224701004\n",
      "Epoch: 1 - Batch: 1595, Training Loss: 0.1449427733237965\n",
      "Epoch: 1 - Batch: 1596, Training Loss: 0.14503101287888454\n",
      "Epoch: 1 - Batch: 1597, Training Loss: 0.1451226800470111\n",
      "Epoch: 1 - Batch: 1598, Training Loss: 0.14520504009936183\n",
      "Epoch: 1 - Batch: 1599, Training Loss: 0.14529860836405856\n",
      "Epoch: 1 - Batch: 1600, Training Loss: 0.14538412058654906\n",
      "Epoch: 1 - Batch: 1601, Training Loss: 0.14546977112665896\n",
      "Epoch: 1 - Batch: 1602, Training Loss: 0.14555423213771326\n",
      "Epoch: 1 - Batch: 1603, Training Loss: 0.14565286476467776\n",
      "Epoch: 1 - Batch: 1604, Training Loss: 0.14574903014979354\n",
      "Epoch: 1 - Batch: 1605, Training Loss: 0.1458415198286572\n",
      "Epoch: 1 - Batch: 1606, Training Loss: 0.14593041992775638\n",
      "Epoch: 1 - Batch: 1607, Training Loss: 0.1460205528817169\n",
      "Epoch: 1 - Batch: 1608, Training Loss: 0.1461049386147243\n",
      "Epoch: 1 - Batch: 1609, Training Loss: 0.1461893225746665\n",
      "Epoch: 1 - Batch: 1610, Training Loss: 0.14628051601931033\n",
      "Epoch: 1 - Batch: 1611, Training Loss: 0.14636922533782956\n",
      "Epoch: 1 - Batch: 1612, Training Loss: 0.1464594664302335\n",
      "Epoch: 1 - Batch: 1613, Training Loss: 0.14654389655165015\n",
      "Epoch: 1 - Batch: 1614, Training Loss: 0.14663100673216295\n",
      "Epoch: 1 - Batch: 1615, Training Loss: 0.1467183783069673\n",
      "Epoch: 1 - Batch: 1616, Training Loss: 0.14679870656251315\n",
      "Epoch: 1 - Batch: 1617, Training Loss: 0.14687477268439225\n",
      "Epoch: 1 - Batch: 1618, Training Loss: 0.14696140147461426\n",
      "Epoch: 1 - Batch: 1619, Training Loss: 0.1470576249322488\n",
      "Epoch: 1 - Batch: 1620, Training Loss: 0.14714711968776203\n",
      "Epoch: 1 - Batch: 1621, Training Loss: 0.14723316811111634\n",
      "Epoch: 1 - Batch: 1622, Training Loss: 0.14731510027402867\n",
      "Epoch: 1 - Batch: 1623, Training Loss: 0.14740293747305278\n",
      "Epoch: 1 - Batch: 1624, Training Loss: 0.1474851118725034\n",
      "Epoch: 1 - Batch: 1625, Training Loss: 0.14757112317269122\n",
      "Epoch: 1 - Batch: 1626, Training Loss: 0.14766473289128165\n",
      "Epoch: 1 - Batch: 1627, Training Loss: 0.1477585655650986\n",
      "Epoch: 1 - Batch: 1628, Training Loss: 0.14784829951760978\n",
      "Epoch: 1 - Batch: 1629, Training Loss: 0.1479368967932354\n",
      "Epoch: 1 - Batch: 1630, Training Loss: 0.14802536430767124\n",
      "Epoch: 1 - Batch: 1631, Training Loss: 0.14811860634690494\n",
      "Epoch: 1 - Batch: 1632, Training Loss: 0.14820198786397082\n",
      "Epoch: 1 - Batch: 1633, Training Loss: 0.1482898971902988\n",
      "Epoch: 1 - Batch: 1634, Training Loss: 0.14837331368679035\n",
      "Epoch: 1 - Batch: 1635, Training Loss: 0.1484784383394726\n",
      "Epoch: 1 - Batch: 1636, Training Loss: 0.14856346670667925\n",
      "Epoch: 1 - Batch: 1637, Training Loss: 0.1486541578734593\n",
      "Epoch: 1 - Batch: 1638, Training Loss: 0.14874231027652376\n",
      "Epoch: 1 - Batch: 1639, Training Loss: 0.14883418868652623\n",
      "Epoch: 1 - Batch: 1640, Training Loss: 0.14891771570614123\n",
      "Epoch: 1 - Batch: 1641, Training Loss: 0.14900877260623088\n",
      "Epoch: 1 - Batch: 1642, Training Loss: 0.1491015102115041\n",
      "Epoch: 1 - Batch: 1643, Training Loss: 0.14919917672798408\n",
      "Epoch: 1 - Batch: 1644, Training Loss: 0.14928122724763196\n",
      "Epoch: 1 - Batch: 1645, Training Loss: 0.1493730747432851\n",
      "Epoch: 1 - Batch: 1646, Training Loss: 0.14945860390565288\n",
      "Epoch: 1 - Batch: 1647, Training Loss: 0.14954151196488694\n",
      "Epoch: 1 - Batch: 1648, Training Loss: 0.14962685140310036\n",
      "Epoch: 1 - Batch: 1649, Training Loss: 0.1497142625166409\n",
      "Epoch: 1 - Batch: 1650, Training Loss: 0.1498052924253652\n",
      "Epoch: 1 - Batch: 1651, Training Loss: 0.14989298409095056\n",
      "Epoch: 1 - Batch: 1652, Training Loss: 0.14997565851949934\n",
      "Epoch: 1 - Batch: 1653, Training Loss: 0.15006745547970532\n",
      "Epoch: 1 - Batch: 1654, Training Loss: 0.15015151220100437\n",
      "Epoch: 1 - Batch: 1655, Training Loss: 0.15024489081568188\n",
      "Epoch: 1 - Batch: 1656, Training Loss: 0.1503264773667946\n",
      "Epoch: 1 - Batch: 1657, Training Loss: 0.15041826502304173\n",
      "Epoch: 1 - Batch: 1658, Training Loss: 0.1504968242352183\n",
      "Epoch: 1 - Batch: 1659, Training Loss: 0.1505884829334062\n",
      "Epoch: 1 - Batch: 1660, Training Loss: 0.1506749564105302\n",
      "Epoch: 1 - Batch: 1661, Training Loss: 0.15075846448244146\n",
      "Epoch: 1 - Batch: 1662, Training Loss: 0.15084261339695298\n",
      "Epoch: 1 - Batch: 1663, Training Loss: 0.1509368559515496\n",
      "Epoch: 1 - Batch: 1664, Training Loss: 0.15102667921835905\n",
      "Epoch: 1 - Batch: 1665, Training Loss: 0.1511058571994008\n",
      "Epoch: 1 - Batch: 1666, Training Loss: 0.1511859472958405\n",
      "Epoch: 1 - Batch: 1667, Training Loss: 0.15127059187783334\n",
      "Epoch: 1 - Batch: 1668, Training Loss: 0.15135466002212036\n",
      "Epoch: 1 - Batch: 1669, Training Loss: 0.15144258516394282\n",
      "Epoch: 1 - Batch: 1670, Training Loss: 0.15153852944721036\n",
      "Epoch: 1 - Batch: 1671, Training Loss: 0.15162896340215581\n",
      "Epoch: 1 - Batch: 1672, Training Loss: 0.15172164586324793\n",
      "Epoch: 1 - Batch: 1673, Training Loss: 0.1518205835377399\n",
      "Epoch: 1 - Batch: 1674, Training Loss: 0.1519103452538574\n",
      "Epoch: 1 - Batch: 1675, Training Loss: 0.15200839981214323\n",
      "Epoch: 1 - Batch: 1676, Training Loss: 0.1521007557944773\n",
      "Epoch: 1 - Batch: 1677, Training Loss: 0.1522055122921024\n",
      "Epoch: 1 - Batch: 1678, Training Loss: 0.1523043895773528\n",
      "Epoch: 1 - Batch: 1679, Training Loss: 0.15239708858293483\n",
      "Epoch: 1 - Batch: 1680, Training Loss: 0.15248485195572498\n",
      "Epoch: 1 - Batch: 1681, Training Loss: 0.15257817066921722\n",
      "Epoch: 1 - Batch: 1682, Training Loss: 0.1526781216597379\n",
      "Epoch: 1 - Batch: 1683, Training Loss: 0.15276073642186264\n",
      "Epoch: 1 - Batch: 1684, Training Loss: 0.15286042671001965\n",
      "Epoch: 1 - Batch: 1685, Training Loss: 0.1529476721006543\n",
      "Epoch: 1 - Batch: 1686, Training Loss: 0.1530377353490585\n",
      "Epoch: 1 - Batch: 1687, Training Loss: 0.15313086966376993\n",
      "Epoch: 1 - Batch: 1688, Training Loss: 0.15320675028719713\n",
      "Epoch: 1 - Batch: 1689, Training Loss: 0.15328904435310395\n",
      "Epoch: 1 - Batch: 1690, Training Loss: 0.15337917993886158\n",
      "Epoch: 1 - Batch: 1691, Training Loss: 0.1534661086736429\n",
      "Epoch: 1 - Batch: 1692, Training Loss: 0.15356305882235863\n",
      "Epoch: 1 - Batch: 1693, Training Loss: 0.1536403010733685\n",
      "Epoch: 1 - Batch: 1694, Training Loss: 0.15372924724745118\n",
      "Epoch: 1 - Batch: 1695, Training Loss: 0.15381672727577325\n",
      "Epoch: 1 - Batch: 1696, Training Loss: 0.15390835573918388\n",
      "Epoch: 1 - Batch: 1697, Training Loss: 0.15398941316933773\n",
      "Epoch: 1 - Batch: 1698, Training Loss: 0.15407035105041603\n",
      "Epoch: 1 - Batch: 1699, Training Loss: 0.15416506039068276\n",
      "Epoch: 1 - Batch: 1700, Training Loss: 0.15424994380517584\n",
      "Epoch: 1 - Batch: 1701, Training Loss: 0.1543446916215752\n",
      "Epoch: 1 - Batch: 1702, Training Loss: 0.15444085320108764\n",
      "Epoch: 1 - Batch: 1703, Training Loss: 0.15452993643249247\n",
      "Epoch: 1 - Batch: 1704, Training Loss: 0.15460982480106464\n",
      "Epoch: 1 - Batch: 1705, Training Loss: 0.1546981578776196\n",
      "Epoch: 1 - Batch: 1706, Training Loss: 0.1547842816037325\n",
      "Epoch: 1 - Batch: 1707, Training Loss: 0.15486904009435307\n",
      "Epoch: 1 - Batch: 1708, Training Loss: 0.15495816207371937\n",
      "Epoch: 1 - Batch: 1709, Training Loss: 0.15504710221196683\n",
      "Epoch: 1 - Batch: 1710, Training Loss: 0.15513653515113723\n",
      "Epoch: 1 - Batch: 1711, Training Loss: 0.15522915795816117\n",
      "Epoch: 1 - Batch: 1712, Training Loss: 0.15531613436800923\n",
      "Epoch: 1 - Batch: 1713, Training Loss: 0.1554071959880355\n",
      "Epoch: 1 - Batch: 1714, Training Loss: 0.15549061252777255\n",
      "Epoch: 1 - Batch: 1715, Training Loss: 0.1555803246536658\n",
      "Epoch: 1 - Batch: 1716, Training Loss: 0.15566913149694897\n",
      "Epoch: 1 - Batch: 1717, Training Loss: 0.15576463563994783\n",
      "Epoch: 1 - Batch: 1718, Training Loss: 0.15585443607313715\n",
      "Epoch: 1 - Batch: 1719, Training Loss: 0.15594313821290462\n",
      "Epoch: 1 - Batch: 1720, Training Loss: 0.15603067663499767\n",
      "Epoch: 1 - Batch: 1721, Training Loss: 0.15611498240091118\n",
      "Epoch: 1 - Batch: 1722, Training Loss: 0.15620933312484084\n",
      "Epoch: 1 - Batch: 1723, Training Loss: 0.15629730492237195\n",
      "Epoch: 1 - Batch: 1724, Training Loss: 0.1563927778174134\n",
      "Epoch: 1 - Batch: 1725, Training Loss: 0.15647415286717722\n",
      "Epoch: 1 - Batch: 1726, Training Loss: 0.15656189007026639\n",
      "Epoch: 1 - Batch: 1727, Training Loss: 0.15665410615960954\n",
      "Epoch: 1 - Batch: 1728, Training Loss: 0.1567389799674253\n",
      "Epoch: 1 - Batch: 1729, Training Loss: 0.15682638553936484\n",
      "Epoch: 1 - Batch: 1730, Training Loss: 0.15692316240339138\n",
      "Epoch: 1 - Batch: 1731, Training Loss: 0.15701108535822747\n",
      "Epoch: 1 - Batch: 1732, Training Loss: 0.15709652773306934\n",
      "Epoch: 1 - Batch: 1733, Training Loss: 0.15718086727989056\n",
      "Epoch: 1 - Batch: 1734, Training Loss: 0.15726498497115637\n",
      "Epoch: 1 - Batch: 1735, Training Loss: 0.1573639606646143\n",
      "Epoch: 1 - Batch: 1736, Training Loss: 0.1574491402242413\n",
      "Epoch: 1 - Batch: 1737, Training Loss: 0.15752929242738642\n",
      "Epoch: 1 - Batch: 1738, Training Loss: 0.15762443737950096\n",
      "Epoch: 1 - Batch: 1739, Training Loss: 0.1577141109427706\n",
      "Epoch: 1 - Batch: 1740, Training Loss: 0.1577933489732679\n",
      "Epoch: 1 - Batch: 1741, Training Loss: 0.15787288261141944\n",
      "Epoch: 1 - Batch: 1742, Training Loss: 0.1579687896850295\n",
      "Epoch: 1 - Batch: 1743, Training Loss: 0.15805329162485365\n",
      "Epoch: 1 - Batch: 1744, Training Loss: 0.1581463127846445\n",
      "Epoch: 1 - Batch: 1745, Training Loss: 0.15822273749197102\n",
      "Epoch: 1 - Batch: 1746, Training Loss: 0.1583114152227468\n",
      "Epoch: 1 - Batch: 1747, Training Loss: 0.15840627778797206\n",
      "Epoch: 1 - Batch: 1748, Training Loss: 0.15849696576694153\n",
      "Epoch: 1 - Batch: 1749, Training Loss: 0.1585759032906881\n",
      "Epoch: 1 - Batch: 1750, Training Loss: 0.1586602667584447\n",
      "Epoch: 1 - Batch: 1751, Training Loss: 0.15874562339600837\n",
      "Epoch: 1 - Batch: 1752, Training Loss: 0.15883214556755712\n",
      "Epoch: 1 - Batch: 1753, Training Loss: 0.15891461900958967\n",
      "Epoch: 1 - Batch: 1754, Training Loss: 0.1590039775541568\n",
      "Epoch: 1 - Batch: 1755, Training Loss: 0.1590885038288375\n",
      "Epoch: 1 - Batch: 1756, Training Loss: 0.15917498146988465\n",
      "Epoch: 1 - Batch: 1757, Training Loss: 0.15926343871660492\n",
      "Epoch: 1 - Batch: 1758, Training Loss: 0.15935311944009256\n",
      "Epoch: 1 - Batch: 1759, Training Loss: 0.159443288714089\n",
      "Epoch: 1 - Batch: 1760, Training Loss: 0.15952927412876047\n",
      "Epoch: 1 - Batch: 1761, Training Loss: 0.1596101949834705\n",
      "Epoch: 1 - Batch: 1762, Training Loss: 0.15969465438826363\n",
      "Epoch: 1 - Batch: 1763, Training Loss: 0.1597850054786672\n",
      "Epoch: 1 - Batch: 1764, Training Loss: 0.1598810405404018\n",
      "Epoch: 1 - Batch: 1765, Training Loss: 0.15997049828395124\n",
      "Epoch: 1 - Batch: 1766, Training Loss: 0.16005531054107505\n",
      "Epoch: 1 - Batch: 1767, Training Loss: 0.16014270214472046\n",
      "Epoch: 1 - Batch: 1768, Training Loss: 0.1602365418181293\n",
      "Epoch: 1 - Batch: 1769, Training Loss: 0.160320076875228\n",
      "Epoch: 1 - Batch: 1770, Training Loss: 0.16040176567104128\n",
      "Epoch: 1 - Batch: 1771, Training Loss: 0.16048531617913672\n",
      "Epoch: 1 - Batch: 1772, Training Loss: 0.16056607630888423\n",
      "Epoch: 1 - Batch: 1773, Training Loss: 0.16065015100795238\n",
      "Epoch: 1 - Batch: 1774, Training Loss: 0.16073831989372744\n",
      "Epoch: 1 - Batch: 1775, Training Loss: 0.1608306566812407\n",
      "Epoch: 1 - Batch: 1776, Training Loss: 0.16091659516837467\n",
      "Epoch: 1 - Batch: 1777, Training Loss: 0.1609991982109709\n",
      "Epoch: 1 - Batch: 1778, Training Loss: 0.16110049907865612\n",
      "Epoch: 1 - Batch: 1779, Training Loss: 0.1611873744979229\n",
      "Epoch: 1 - Batch: 1780, Training Loss: 0.16126844349315708\n",
      "Epoch: 1 - Batch: 1781, Training Loss: 0.16135798578782262\n",
      "Epoch: 1 - Batch: 1782, Training Loss: 0.1614396385601405\n",
      "Epoch: 1 - Batch: 1783, Training Loss: 0.1615319574934432\n",
      "Epoch: 1 - Batch: 1784, Training Loss: 0.16161843334289136\n",
      "Epoch: 1 - Batch: 1785, Training Loss: 0.1617071791606064\n",
      "Epoch: 1 - Batch: 1786, Training Loss: 0.1617944965179188\n",
      "Epoch: 1 - Batch: 1787, Training Loss: 0.1618829769689349\n",
      "Epoch: 1 - Batch: 1788, Training Loss: 0.16197026862606875\n",
      "Epoch: 1 - Batch: 1789, Training Loss: 0.16205826641414098\n",
      "Epoch: 1 - Batch: 1790, Training Loss: 0.1621422177955978\n",
      "Epoch: 1 - Batch: 1791, Training Loss: 0.16223070069307316\n",
      "Epoch: 1 - Batch: 1792, Training Loss: 0.16231632384643033\n",
      "Epoch: 1 - Batch: 1793, Training Loss: 0.1624204015133788\n",
      "Epoch: 1 - Batch: 1794, Training Loss: 0.1625061186391916\n",
      "Epoch: 1 - Batch: 1795, Training Loss: 0.16259050886973023\n",
      "Epoch: 1 - Batch: 1796, Training Loss: 0.16266992979159403\n",
      "Epoch: 1 - Batch: 1797, Training Loss: 0.16276780762408502\n",
      "Epoch: 1 - Batch: 1798, Training Loss: 0.16286038084088472\n",
      "Epoch: 1 - Batch: 1799, Training Loss: 0.16295344729724018\n",
      "Epoch: 1 - Batch: 1800, Training Loss: 0.16303935111384488\n",
      "Epoch: 1 - Batch: 1801, Training Loss: 0.1631323020244218\n",
      "Epoch: 1 - Batch: 1802, Training Loss: 0.16321197198472212\n",
      "Epoch: 1 - Batch: 1803, Training Loss: 0.16330111084836435\n",
      "Epoch: 1 - Batch: 1804, Training Loss: 0.16337935664192163\n",
      "Epoch: 1 - Batch: 1805, Training Loss: 0.16346540539888402\n",
      "Epoch: 1 - Batch: 1806, Training Loss: 0.16355881281505968\n",
      "Epoch: 1 - Batch: 1807, Training Loss: 0.1636536742744359\n",
      "Epoch: 1 - Batch: 1808, Training Loss: 0.16374089127749353\n",
      "Epoch: 1 - Batch: 1809, Training Loss: 0.16382951629201373\n",
      "Epoch: 1 - Batch: 1810, Training Loss: 0.163925958510062\n",
      "Epoch: 1 - Batch: 1811, Training Loss: 0.16401021598311602\n",
      "Epoch: 1 - Batch: 1812, Training Loss: 0.16409282685932433\n",
      "Epoch: 1 - Batch: 1813, Training Loss: 0.16418491404620963\n",
      "Epoch: 1 - Batch: 1814, Training Loss: 0.16427703128253446\n",
      "Epoch: 1 - Batch: 1815, Training Loss: 0.16436742499446\n",
      "Epoch: 1 - Batch: 1816, Training Loss: 0.16446402478672775\n",
      "Epoch: 1 - Batch: 1817, Training Loss: 0.16456341553806864\n",
      "Epoch: 1 - Batch: 1818, Training Loss: 0.16465282650235083\n",
      "Epoch: 1 - Batch: 1819, Training Loss: 0.16473565405083335\n",
      "Epoch: 1 - Batch: 1820, Training Loss: 0.16481956380541446\n",
      "Epoch: 1 - Batch: 1821, Training Loss: 0.1649000467377318\n",
      "Epoch: 1 - Batch: 1822, Training Loss: 0.16499266813001032\n",
      "Epoch: 1 - Batch: 1823, Training Loss: 0.1650742947820506\n",
      "Epoch: 1 - Batch: 1824, Training Loss: 0.16517865601587256\n",
      "Epoch: 1 - Batch: 1825, Training Loss: 0.16526633038326086\n",
      "Epoch: 1 - Batch: 1826, Training Loss: 0.16536831144694467\n",
      "Epoch: 1 - Batch: 1827, Training Loss: 0.16545970299919052\n",
      "Epoch: 1 - Batch: 1828, Training Loss: 0.16555696451545354\n",
      "Epoch: 1 - Batch: 1829, Training Loss: 0.16565586329989174\n",
      "Epoch: 1 - Batch: 1830, Training Loss: 0.1657448262655814\n",
      "Epoch: 1 - Batch: 1831, Training Loss: 0.16583230377330907\n",
      "Epoch: 1 - Batch: 1832, Training Loss: 0.16591546082155623\n",
      "Epoch: 1 - Batch: 1833, Training Loss: 0.16600273686706724\n",
      "Epoch: 1 - Batch: 1834, Training Loss: 0.16608599373826735\n",
      "Epoch: 1 - Batch: 1835, Training Loss: 0.16616869589335487\n",
      "Epoch: 1 - Batch: 1836, Training Loss: 0.16626869236577801\n",
      "Epoch: 1 - Batch: 1837, Training Loss: 0.16635088730065978\n",
      "Epoch: 1 - Batch: 1838, Training Loss: 0.1664397833964619\n",
      "Epoch: 1 - Batch: 1839, Training Loss: 0.16652756120089077\n",
      "Epoch: 1 - Batch: 1840, Training Loss: 0.1666236516106781\n",
      "Epoch: 1 - Batch: 1841, Training Loss: 0.16671712334525723\n",
      "Epoch: 1 - Batch: 1842, Training Loss: 0.16680756000613495\n",
      "Epoch: 1 - Batch: 1843, Training Loss: 0.1668963167278921\n",
      "Epoch: 1 - Batch: 1844, Training Loss: 0.16697705247335964\n",
      "Epoch: 1 - Batch: 1845, Training Loss: 0.16706325065585512\n",
      "Epoch: 1 - Batch: 1846, Training Loss: 0.16715178791909274\n",
      "Epoch: 1 - Batch: 1847, Training Loss: 0.16723937085933155\n",
      "Epoch: 1 - Batch: 1848, Training Loss: 0.16732659630761612\n",
      "Epoch: 1 - Batch: 1849, Training Loss: 0.16742819559391262\n",
      "Epoch: 1 - Batch: 1850, Training Loss: 0.1675132116732214\n",
      "Epoch: 1 - Batch: 1851, Training Loss: 0.16759616439887146\n",
      "Epoch: 1 - Batch: 1852, Training Loss: 0.1676854248008819\n",
      "Epoch: 1 - Batch: 1853, Training Loss: 0.1677658036104087\n",
      "Epoch: 1 - Batch: 1854, Training Loss: 0.16785870344805875\n",
      "Epoch: 1 - Batch: 1855, Training Loss: 0.16795114623027457\n",
      "Epoch: 1 - Batch: 1856, Training Loss: 0.16803684564396912\n",
      "Epoch: 1 - Batch: 1857, Training Loss: 0.16813472943816019\n",
      "Epoch: 1 - Batch: 1858, Training Loss: 0.16822423199639588\n",
      "Epoch: 1 - Batch: 1859, Training Loss: 0.16830078654522523\n",
      "Epoch: 1 - Batch: 1860, Training Loss: 0.16839091363633252\n",
      "Epoch: 1 - Batch: 1861, Training Loss: 0.16848627353707948\n",
      "Epoch: 1 - Batch: 1862, Training Loss: 0.16857319743454358\n",
      "Epoch: 1 - Batch: 1863, Training Loss: 0.16866859483531063\n",
      "Epoch: 1 - Batch: 1864, Training Loss: 0.16875714814880396\n",
      "Epoch: 1 - Batch: 1865, Training Loss: 0.16884681036939866\n",
      "Epoch: 1 - Batch: 1866, Training Loss: 0.16894180428354102\n",
      "Epoch: 1 - Batch: 1867, Training Loss: 0.16903198864403648\n",
      "Epoch: 1 - Batch: 1868, Training Loss: 0.16912267997762062\n",
      "Epoch: 1 - Batch: 1869, Training Loss: 0.16921549992848986\n",
      "Epoch: 1 - Batch: 1870, Training Loss: 0.16930257284374378\n",
      "Epoch: 1 - Batch: 1871, Training Loss: 0.169387920912522\n",
      "Epoch: 1 - Batch: 1872, Training Loss: 0.16947149321634575\n",
      "Epoch: 1 - Batch: 1873, Training Loss: 0.1695675066676899\n",
      "Epoch: 1 - Batch: 1874, Training Loss: 0.16965924918799852\n",
      "Epoch: 1 - Batch: 1875, Training Loss: 0.1697543947146603\n",
      "Epoch: 1 - Batch: 1876, Training Loss: 0.16984743275254915\n",
      "Epoch: 1 - Batch: 1877, Training Loss: 0.16994158249614053\n",
      "Epoch: 1 - Batch: 1878, Training Loss: 0.17002530003686253\n",
      "Epoch: 1 - Batch: 1879, Training Loss: 0.17011847364937094\n",
      "Epoch: 1 - Batch: 1880, Training Loss: 0.17020467578574003\n",
      "Epoch: 1 - Batch: 1881, Training Loss: 0.1702977717355215\n",
      "Epoch: 1 - Batch: 1882, Training Loss: 0.17037770905799138\n",
      "Epoch: 1 - Batch: 1883, Training Loss: 0.17046241577757928\n",
      "Epoch: 1 - Batch: 1884, Training Loss: 0.1705442905969683\n",
      "Epoch: 1 - Batch: 1885, Training Loss: 0.1706313859257433\n",
      "Epoch: 1 - Batch: 1886, Training Loss: 0.170726752585143\n",
      "Epoch: 1 - Batch: 1887, Training Loss: 0.17081438434954307\n",
      "Epoch: 1 - Batch: 1888, Training Loss: 0.17089995868491692\n",
      "Epoch: 1 - Batch: 1889, Training Loss: 0.17098041715264123\n",
      "Epoch: 1 - Batch: 1890, Training Loss: 0.17107419600435356\n",
      "Epoch: 1 - Batch: 1891, Training Loss: 0.17115932024063954\n",
      "Epoch: 1 - Batch: 1892, Training Loss: 0.17124232767183784\n",
      "Epoch: 1 - Batch: 1893, Training Loss: 0.1713247087600318\n",
      "Epoch: 1 - Batch: 1894, Training Loss: 0.17141967914244824\n",
      "Epoch: 1 - Batch: 1895, Training Loss: 0.1715139501649349\n",
      "Epoch: 1 - Batch: 1896, Training Loss: 0.17160688734815685\n",
      "Epoch: 1 - Batch: 1897, Training Loss: 0.17169567476185796\n",
      "Epoch: 1 - Batch: 1898, Training Loss: 0.1717828043225294\n",
      "Epoch: 1 - Batch: 1899, Training Loss: 0.17187576843533744\n",
      "Epoch: 1 - Batch: 1900, Training Loss: 0.17196490175120074\n",
      "Epoch: 1 - Batch: 1901, Training Loss: 0.17205134702608557\n",
      "Epoch: 1 - Batch: 1902, Training Loss: 0.17213298520613862\n",
      "Epoch: 1 - Batch: 1903, Training Loss: 0.17222347220476983\n",
      "Epoch: 1 - Batch: 1904, Training Loss: 0.17230352848323424\n",
      "Epoch: 1 - Batch: 1905, Training Loss: 0.17239467787307688\n",
      "Epoch: 1 - Batch: 1906, Training Loss: 0.17248345996140446\n",
      "Epoch: 1 - Batch: 1907, Training Loss: 0.17257691230346908\n",
      "Epoch: 1 - Batch: 1908, Training Loss: 0.1726672846644771\n",
      "Epoch: 1 - Batch: 1909, Training Loss: 0.1727560194298798\n",
      "Epoch: 1 - Batch: 1910, Training Loss: 0.17284439668479448\n",
      "Epoch: 1 - Batch: 1911, Training Loss: 0.1729323773476516\n",
      "Epoch: 1 - Batch: 1912, Training Loss: 0.17301720729835987\n",
      "Epoch: 1 - Batch: 1913, Training Loss: 0.17311351200809724\n",
      "Epoch: 1 - Batch: 1914, Training Loss: 0.17320853645355744\n",
      "Epoch: 1 - Batch: 1915, Training Loss: 0.17329027739107905\n",
      "Epoch: 1 - Batch: 1916, Training Loss: 0.17337003003058346\n",
      "Epoch: 1 - Batch: 1917, Training Loss: 0.17346122799747027\n",
      "Epoch: 1 - Batch: 1918, Training Loss: 0.1735497711915183\n",
      "Epoch: 1 - Batch: 1919, Training Loss: 0.1736400932950859\n",
      "Epoch: 1 - Batch: 1920, Training Loss: 0.17373832381310353\n",
      "Epoch: 1 - Batch: 1921, Training Loss: 0.17381986404211563\n",
      "Epoch: 1 - Batch: 1922, Training Loss: 0.17391130440616687\n",
      "Epoch: 1 - Batch: 1923, Training Loss: 0.17400230647097178\n",
      "Epoch: 1 - Batch: 1924, Training Loss: 0.17409094267371875\n",
      "Epoch: 1 - Batch: 1925, Training Loss: 0.17418403041921246\n",
      "Epoch: 1 - Batch: 1926, Training Loss: 0.1742764947500395\n",
      "Epoch: 1 - Batch: 1927, Training Loss: 0.1743607475340465\n",
      "Epoch: 1 - Batch: 1928, Training Loss: 0.17444520575267758\n",
      "Epoch: 1 - Batch: 1929, Training Loss: 0.17453717062895374\n",
      "Epoch: 1 - Batch: 1930, Training Loss: 0.1746131685588688\n",
      "Epoch: 1 - Batch: 1931, Training Loss: 0.17470335518666366\n",
      "Epoch: 1 - Batch: 1932, Training Loss: 0.17479631928711586\n",
      "Epoch: 1 - Batch: 1933, Training Loss: 0.17489317105851362\n",
      "Epoch: 1 - Batch: 1934, Training Loss: 0.17498990462762404\n",
      "Epoch: 1 - Batch: 1935, Training Loss: 0.17508722550141476\n",
      "Epoch: 1 - Batch: 1936, Training Loss: 0.17517132878575356\n",
      "Epoch: 1 - Batch: 1937, Training Loss: 0.17525429794469086\n",
      "Epoch: 1 - Batch: 1938, Training Loss: 0.17533890340186867\n",
      "Epoch: 1 - Batch: 1939, Training Loss: 0.17542318185516456\n",
      "Epoch: 1 - Batch: 1940, Training Loss: 0.17551103485732727\n",
      "Epoch: 1 - Batch: 1941, Training Loss: 0.17559532770519432\n",
      "Epoch: 1 - Batch: 1942, Training Loss: 0.17568206783402618\n",
      "Epoch: 1 - Batch: 1943, Training Loss: 0.17577065187653104\n",
      "Epoch: 1 - Batch: 1944, Training Loss: 0.17585348924791833\n",
      "Epoch: 1 - Batch: 1945, Training Loss: 0.17594752135141375\n",
      "Epoch: 1 - Batch: 1946, Training Loss: 0.17603604525179412\n",
      "Epoch: 1 - Batch: 1947, Training Loss: 0.17611876121454967\n",
      "Epoch: 1 - Batch: 1948, Training Loss: 0.17620948070094952\n",
      "Epoch: 1 - Batch: 1949, Training Loss: 0.17630157020455767\n",
      "Epoch: 1 - Batch: 1950, Training Loss: 0.17637594746638888\n",
      "Epoch: 1 - Batch: 1951, Training Loss: 0.176465881664065\n",
      "Epoch: 1 - Batch: 1952, Training Loss: 0.17655563524385195\n",
      "Epoch: 1 - Batch: 1953, Training Loss: 0.17663782481011467\n",
      "Epoch: 1 - Batch: 1954, Training Loss: 0.1767360338474783\n",
      "Epoch: 1 - Batch: 1955, Training Loss: 0.17682013920760076\n",
      "Epoch: 1 - Batch: 1956, Training Loss: 0.17689744780955227\n",
      "Epoch: 1 - Batch: 1957, Training Loss: 0.17698506037295358\n",
      "Epoch: 1 - Batch: 1958, Training Loss: 0.177076571301293\n",
      "Epoch: 1 - Batch: 1959, Training Loss: 0.1771758565459876\n",
      "Epoch: 1 - Batch: 1960, Training Loss: 0.17725676090563114\n",
      "Epoch: 1 - Batch: 1961, Training Loss: 0.17734791665256122\n",
      "Epoch: 1 - Batch: 1962, Training Loss: 0.1774347445388536\n",
      "Epoch: 1 - Batch: 1963, Training Loss: 0.177520173149273\n",
      "Epoch: 1 - Batch: 1964, Training Loss: 0.17760977119355653\n",
      "Epoch: 1 - Batch: 1965, Training Loss: 0.17769411627580078\n",
      "Epoch: 1 - Batch: 1966, Training Loss: 0.17777760965660042\n",
      "Epoch: 1 - Batch: 1967, Training Loss: 0.17786797292987702\n",
      "Epoch: 1 - Batch: 1968, Training Loss: 0.17794245946689033\n",
      "Epoch: 1 - Batch: 1969, Training Loss: 0.1780354461662014\n",
      "Epoch: 1 - Batch: 1970, Training Loss: 0.17812703816748376\n",
      "Epoch: 1 - Batch: 1971, Training Loss: 0.17821192126690255\n",
      "Epoch: 1 - Batch: 1972, Training Loss: 0.1782945032242321\n",
      "Epoch: 1 - Batch: 1973, Training Loss: 0.17836569346623438\n",
      "Epoch: 1 - Batch: 1974, Training Loss: 0.17844539071444057\n",
      "Epoch: 1 - Batch: 1975, Training Loss: 0.17854147925231587\n",
      "Epoch: 1 - Batch: 1976, Training Loss: 0.17863993077951285\n",
      "Epoch: 1 - Batch: 1977, Training Loss: 0.17873373734540804\n",
      "Epoch: 1 - Batch: 1978, Training Loss: 0.17882676466459263\n",
      "Epoch: 1 - Batch: 1979, Training Loss: 0.17891090949549407\n",
      "Epoch: 1 - Batch: 1980, Training Loss: 0.1789992693224752\n",
      "Epoch: 1 - Batch: 1981, Training Loss: 0.17908013442756723\n",
      "Epoch: 1 - Batch: 1982, Training Loss: 0.17916274306118785\n",
      "Epoch: 1 - Batch: 1983, Training Loss: 0.17925180530617288\n",
      "Epoch: 1 - Batch: 1984, Training Loss: 0.17933774971793937\n",
      "Epoch: 1 - Batch: 1985, Training Loss: 0.17941803386308267\n",
      "Epoch: 1 - Batch: 1986, Training Loss: 0.17950564961429458\n",
      "Epoch: 1 - Batch: 1987, Training Loss: 0.17959554214802745\n",
      "Epoch: 1 - Batch: 1988, Training Loss: 0.1796770680380698\n",
      "Epoch: 1 - Batch: 1989, Training Loss: 0.1797592767497299\n",
      "Epoch: 1 - Batch: 1990, Training Loss: 0.17984979589211803\n",
      "Epoch: 1 - Batch: 1991, Training Loss: 0.17994656376725998\n",
      "Epoch: 1 - Batch: 1992, Training Loss: 0.18003489246984225\n",
      "Epoch: 1 - Batch: 1993, Training Loss: 0.18012568879730467\n",
      "Epoch: 1 - Batch: 1994, Training Loss: 0.18020957539701343\n",
      "Epoch: 1 - Batch: 1995, Training Loss: 0.18029158250436458\n",
      "Epoch: 1 - Batch: 1996, Training Loss: 0.18038047111869648\n",
      "Epoch: 1 - Batch: 1997, Training Loss: 0.18047295019623652\n",
      "Epoch: 1 - Batch: 1998, Training Loss: 0.18056104871541706\n",
      "Epoch: 1 - Batch: 1999, Training Loss: 0.1806490432538994\n",
      "Epoch: 1 - Batch: 2000, Training Loss: 0.18073250807414007\n",
      "Epoch: 1 - Batch: 2001, Training Loss: 0.18082495421714845\n",
      "Epoch: 1 - Batch: 2002, Training Loss: 0.18091013631590366\n",
      "Epoch: 1 - Batch: 2003, Training Loss: 0.18100746326877507\n",
      "Epoch: 1 - Batch: 2004, Training Loss: 0.18109800597377876\n",
      "Epoch: 1 - Batch: 2005, Training Loss: 0.18118711621508274\n",
      "Epoch: 1 - Batch: 2006, Training Loss: 0.18127246004580266\n",
      "Epoch: 1 - Batch: 2007, Training Loss: 0.18135586989484417\n",
      "Epoch: 1 - Batch: 2008, Training Loss: 0.1814401725470822\n",
      "Epoch: 1 - Batch: 2009, Training Loss: 0.18152845182624425\n",
      "Epoch: 1 - Batch: 2010, Training Loss: 0.18162210842912668\n",
      "Epoch: 1 - Batch: 2011, Training Loss: 0.18170593477550825\n",
      "Epoch: 1 - Batch: 2012, Training Loss: 0.18179865616372173\n",
      "Epoch: 1 - Batch: 2013, Training Loss: 0.18188561303278503\n",
      "Epoch: 1 - Batch: 2014, Training Loss: 0.1819722676961675\n",
      "Epoch: 1 - Batch: 2015, Training Loss: 0.18205566443564683\n",
      "Epoch: 1 - Batch: 2016, Training Loss: 0.1821458130691874\n",
      "Epoch: 1 - Batch: 2017, Training Loss: 0.1822255600558881\n",
      "Epoch: 1 - Batch: 2018, Training Loss: 0.182319328460725\n",
      "Epoch: 1 - Batch: 2019, Training Loss: 0.1824011862908133\n",
      "Epoch: 1 - Batch: 2020, Training Loss: 0.1824856949892902\n",
      "Epoch: 1 - Batch: 2021, Training Loss: 0.18256836068842738\n",
      "Epoch: 1 - Batch: 2022, Training Loss: 0.1826612278571967\n",
      "Epoch: 1 - Batch: 2023, Training Loss: 0.1827499694615652\n",
      "Epoch: 1 - Batch: 2024, Training Loss: 0.18283482615652172\n",
      "Epoch: 1 - Batch: 2025, Training Loss: 0.1829230572602049\n",
      "Epoch: 1 - Batch: 2026, Training Loss: 0.1830090496374007\n",
      "Epoch: 1 - Batch: 2027, Training Loss: 0.18310132842653032\n",
      "Epoch: 1 - Batch: 2028, Training Loss: 0.18319097580156515\n",
      "Epoch: 1 - Batch: 2029, Training Loss: 0.18327282640347828\n",
      "Epoch: 1 - Batch: 2030, Training Loss: 0.1833652125835221\n",
      "Epoch: 1 - Batch: 2031, Training Loss: 0.18345660248008336\n",
      "Epoch: 1 - Batch: 2032, Training Loss: 0.18354692184682905\n",
      "Epoch: 1 - Batch: 2033, Training Loss: 0.18364315743791326\n",
      "Epoch: 1 - Batch: 2034, Training Loss: 0.1837428374768885\n",
      "Epoch: 1 - Batch: 2035, Training Loss: 0.1838312366202004\n",
      "Epoch: 1 - Batch: 2036, Training Loss: 0.1839191029481528\n",
      "Epoch: 1 - Batch: 2037, Training Loss: 0.18400284348312104\n",
      "Epoch: 1 - Batch: 2038, Training Loss: 0.18409352324544692\n",
      "Epoch: 1 - Batch: 2039, Training Loss: 0.1841835800008493\n",
      "Epoch: 1 - Batch: 2040, Training Loss: 0.18427160117855515\n",
      "Epoch: 1 - Batch: 2041, Training Loss: 0.18435370802236828\n",
      "Epoch: 1 - Batch: 2042, Training Loss: 0.18444099886401574\n",
      "Epoch: 1 - Batch: 2043, Training Loss: 0.1845247212667667\n",
      "Epoch: 1 - Batch: 2044, Training Loss: 0.18461279174433418\n",
      "Epoch: 1 - Batch: 2045, Training Loss: 0.18470543716257287\n",
      "Epoch: 1 - Batch: 2046, Training Loss: 0.18477911597262964\n",
      "Epoch: 1 - Batch: 2047, Training Loss: 0.1848597772740705\n",
      "Epoch: 1 - Batch: 2048, Training Loss: 0.18494531938537437\n",
      "Epoch: 1 - Batch: 2049, Training Loss: 0.18503575704707634\n",
      "Epoch: 1 - Batch: 2050, Training Loss: 0.18513709926980843\n",
      "Epoch: 1 - Batch: 2051, Training Loss: 0.18522731495857436\n",
      "Epoch: 1 - Batch: 2052, Training Loss: 0.18531440970044627\n",
      "Epoch: 1 - Batch: 2053, Training Loss: 0.185398801117147\n",
      "Epoch: 1 - Batch: 2054, Training Loss: 0.1854858123272608\n",
      "Epoch: 1 - Batch: 2055, Training Loss: 0.18557546817841222\n",
      "Epoch: 1 - Batch: 2056, Training Loss: 0.18566807492282458\n",
      "Epoch: 1 - Batch: 2057, Training Loss: 0.18575606143000115\n",
      "Epoch: 1 - Batch: 2058, Training Loss: 0.18584054156174115\n",
      "Epoch: 1 - Batch: 2059, Training Loss: 0.18592445981715053\n",
      "Epoch: 1 - Batch: 2060, Training Loss: 0.18601953634871773\n",
      "Epoch: 1 - Batch: 2061, Training Loss: 0.1861114605680528\n",
      "Epoch: 1 - Batch: 2062, Training Loss: 0.18621058307995844\n",
      "Epoch: 1 - Batch: 2063, Training Loss: 0.1863065553121701\n",
      "Epoch: 1 - Batch: 2064, Training Loss: 0.1863901876098481\n",
      "Epoch: 1 - Batch: 2065, Training Loss: 0.18648189447115904\n",
      "Epoch: 1 - Batch: 2066, Training Loss: 0.1865665473192189\n",
      "Epoch: 1 - Batch: 2067, Training Loss: 0.1866632452478654\n",
      "Epoch: 1 - Batch: 2068, Training Loss: 0.1867578620489557\n",
      "Epoch: 1 - Batch: 2069, Training Loss: 0.18685532093764734\n",
      "Epoch: 1 - Batch: 2070, Training Loss: 0.1869391210340149\n",
      "Epoch: 1 - Batch: 2071, Training Loss: 0.18703692695542948\n",
      "Epoch: 1 - Batch: 2072, Training Loss: 0.18712453931495918\n",
      "Epoch: 1 - Batch: 2073, Training Loss: 0.1872133772085061\n",
      "Epoch: 1 - Batch: 2074, Training Loss: 0.18730677826759431\n",
      "Epoch: 1 - Batch: 2075, Training Loss: 0.18738780512318484\n",
      "Epoch: 1 - Batch: 2076, Training Loss: 0.18748013092634294\n",
      "Epoch: 1 - Batch: 2077, Training Loss: 0.1875721500510007\n",
      "Epoch: 1 - Batch: 2078, Training Loss: 0.1876589674101046\n",
      "Epoch: 1 - Batch: 2079, Training Loss: 0.18775278279190238\n",
      "Epoch: 1 - Batch: 2080, Training Loss: 0.18784002002767267\n",
      "Epoch: 1 - Batch: 2081, Training Loss: 0.1879202842057542\n",
      "Epoch: 1 - Batch: 2082, Training Loss: 0.18801026939604412\n",
      "Epoch: 1 - Batch: 2083, Training Loss: 0.18809027090718103\n",
      "Epoch: 1 - Batch: 2084, Training Loss: 0.1881783238085447\n",
      "Epoch: 1 - Batch: 2085, Training Loss: 0.18826295872825888\n",
      "Epoch: 1 - Batch: 2086, Training Loss: 0.1883450403167043\n",
      "Epoch: 1 - Batch: 2087, Training Loss: 0.18843196537561877\n",
      "Epoch: 1 - Batch: 2088, Training Loss: 0.18851671072578746\n",
      "Epoch: 1 - Batch: 2089, Training Loss: 0.18859816000335056\n",
      "Epoch: 1 - Batch: 2090, Training Loss: 0.18869080940635247\n",
      "Epoch: 1 - Batch: 2091, Training Loss: 0.1887923424034866\n",
      "Epoch: 1 - Batch: 2092, Training Loss: 0.18888552939738604\n",
      "Epoch: 1 - Batch: 2093, Training Loss: 0.18897649445022716\n",
      "Epoch: 1 - Batch: 2094, Training Loss: 0.18906678094422047\n",
      "Epoch: 1 - Batch: 2095, Training Loss: 0.18915681449532706\n",
      "Epoch: 1 - Batch: 2096, Training Loss: 0.1892411695549243\n",
      "Epoch: 1 - Batch: 2097, Training Loss: 0.18933239112149425\n",
      "Epoch: 1 - Batch: 2098, Training Loss: 0.18942178185924172\n",
      "Epoch: 1 - Batch: 2099, Training Loss: 0.18951111672381263\n",
      "Epoch: 1 - Batch: 2100, Training Loss: 0.1895982604998953\n",
      "Epoch: 1 - Batch: 2101, Training Loss: 0.18968730221562718\n",
      "Epoch: 1 - Batch: 2102, Training Loss: 0.1897739237158947\n",
      "Epoch: 1 - Batch: 2103, Training Loss: 0.18985166320357946\n",
      "Epoch: 1 - Batch: 2104, Training Loss: 0.1899286845257231\n",
      "Epoch: 1 - Batch: 2105, Training Loss: 0.19001355186524874\n",
      "Epoch: 1 - Batch: 2106, Training Loss: 0.19011074209440604\n",
      "Epoch: 1 - Batch: 2107, Training Loss: 0.19019679512031637\n",
      "Epoch: 1 - Batch: 2108, Training Loss: 0.19027656144393024\n",
      "Epoch: 1 - Batch: 2109, Training Loss: 0.19037081960397179\n",
      "Epoch: 1 - Batch: 2110, Training Loss: 0.19046830389012348\n",
      "Epoch: 1 - Batch: 2111, Training Loss: 0.19054818536412854\n",
      "Epoch: 1 - Batch: 2112, Training Loss: 0.1906320946368313\n",
      "Epoch: 1 - Batch: 2113, Training Loss: 0.19071120290613885\n",
      "Epoch: 1 - Batch: 2114, Training Loss: 0.19079871480021113\n",
      "Epoch: 1 - Batch: 2115, Training Loss: 0.19089163964611183\n",
      "Epoch: 1 - Batch: 2116, Training Loss: 0.1909746009899708\n",
      "Epoch: 1 - Batch: 2117, Training Loss: 0.1910736657073644\n",
      "Epoch: 1 - Batch: 2118, Training Loss: 0.19116880108986922\n",
      "Epoch: 1 - Batch: 2119, Training Loss: 0.19125041223531142\n",
      "Epoch: 1 - Batch: 2120, Training Loss: 0.1913382327292195\n",
      "Epoch: 1 - Batch: 2121, Training Loss: 0.19143261563397362\n",
      "Epoch: 1 - Batch: 2122, Training Loss: 0.19152064353326462\n",
      "Epoch: 1 - Batch: 2123, Training Loss: 0.19160154301234542\n",
      "Epoch: 1 - Batch: 2124, Training Loss: 0.1916834948087114\n",
      "Epoch: 1 - Batch: 2125, Training Loss: 0.19176846624631588\n",
      "Epoch: 1 - Batch: 2126, Training Loss: 0.19185580263164506\n",
      "Epoch: 1 - Batch: 2127, Training Loss: 0.19195466004250258\n",
      "Epoch: 1 - Batch: 2128, Training Loss: 0.19203514034920072\n",
      "Epoch: 1 - Batch: 2129, Training Loss: 0.19212971421987263\n",
      "Epoch: 1 - Batch: 2130, Training Loss: 0.19221239915707614\n",
      "Epoch: 1 - Batch: 2131, Training Loss: 0.19231037996326314\n",
      "Epoch: 1 - Batch: 2132, Training Loss: 0.19240227864922377\n",
      "Epoch: 1 - Batch: 2133, Training Loss: 0.19248838501536036\n",
      "Epoch: 1 - Batch: 2134, Training Loss: 0.19257551772677484\n",
      "Epoch: 1 - Batch: 2135, Training Loss: 0.19266708224245763\n",
      "Epoch: 1 - Batch: 2136, Training Loss: 0.19275300020033842\n",
      "Epoch: 1 - Batch: 2137, Training Loss: 0.19282987810807245\n",
      "Epoch: 1 - Batch: 2138, Training Loss: 0.19291756060561335\n",
      "Epoch: 1 - Batch: 2139, Training Loss: 0.19300862106418926\n",
      "Epoch: 1 - Batch: 2140, Training Loss: 0.19309929310410573\n",
      "Epoch: 1 - Batch: 2141, Training Loss: 0.19319426839797454\n",
      "Epoch: 1 - Batch: 2142, Training Loss: 0.19328624072483128\n",
      "Epoch: 1 - Batch: 2143, Training Loss: 0.19338622046742668\n",
      "Epoch: 1 - Batch: 2144, Training Loss: 0.19347579027240353\n",
      "Epoch: 1 - Batch: 2145, Training Loss: 0.1935694403575724\n",
      "Epoch: 1 - Batch: 2146, Training Loss: 0.19365172161045754\n",
      "Epoch: 1 - Batch: 2147, Training Loss: 0.19374248694325757\n",
      "Epoch: 1 - Batch: 2148, Training Loss: 0.19383839982411952\n",
      "Epoch: 1 - Batch: 2149, Training Loss: 0.19392929305237522\n",
      "Epoch: 1 - Batch: 2150, Training Loss: 0.19401704354418647\n",
      "Epoch: 1 - Batch: 2151, Training Loss: 0.19409710944168407\n",
      "Epoch: 1 - Batch: 2152, Training Loss: 0.19418412078771227\n",
      "Epoch: 1 - Batch: 2153, Training Loss: 0.19426359588382255\n",
      "Epoch: 1 - Batch: 2154, Training Loss: 0.19436411565141892\n",
      "Epoch: 1 - Batch: 2155, Training Loss: 0.19446182076141214\n",
      "Epoch: 1 - Batch: 2156, Training Loss: 0.19456019421813894\n",
      "Epoch: 1 - Batch: 2157, Training Loss: 0.19464600822003328\n",
      "Epoch: 1 - Batch: 2158, Training Loss: 0.19473779198418606\n",
      "Epoch: 1 - Batch: 2159, Training Loss: 0.19482895961127075\n",
      "Epoch: 1 - Batch: 2160, Training Loss: 0.19491476839899424\n",
      "Epoch: 1 - Batch: 2161, Training Loss: 0.19500389083552716\n",
      "Epoch: 1 - Batch: 2162, Training Loss: 0.19508315573383128\n",
      "Epoch: 1 - Batch: 2163, Training Loss: 0.19516152045867138\n",
      "Epoch: 1 - Batch: 2164, Training Loss: 0.1952414740599803\n",
      "Epoch: 1 - Batch: 2165, Training Loss: 0.19532825453067892\n",
      "Epoch: 1 - Batch: 2166, Training Loss: 0.19542272051002454\n",
      "Epoch: 1 - Batch: 2167, Training Loss: 0.1955080725017867\n",
      "Epoch: 1 - Batch: 2168, Training Loss: 0.19560735872259385\n",
      "Epoch: 1 - Batch: 2169, Training Loss: 0.1956980945434341\n",
      "Epoch: 1 - Batch: 2170, Training Loss: 0.19579087651858282\n",
      "Epoch: 1 - Batch: 2171, Training Loss: 0.19587935260180414\n",
      "Epoch: 1 - Batch: 2172, Training Loss: 0.1959730431462204\n",
      "Epoch: 1 - Batch: 2173, Training Loss: 0.19606462551611376\n",
      "Epoch: 1 - Batch: 2174, Training Loss: 0.19616128456706233\n",
      "Epoch: 1 - Batch: 2175, Training Loss: 0.19625527977597457\n",
      "Epoch: 1 - Batch: 2176, Training Loss: 0.19634026620740913\n",
      "Epoch: 1 - Batch: 2177, Training Loss: 0.1964266004402246\n",
      "Epoch: 1 - Batch: 2178, Training Loss: 0.19651736393818017\n",
      "Epoch: 1 - Batch: 2179, Training Loss: 0.19660063205938633\n",
      "Epoch: 1 - Batch: 2180, Training Loss: 0.1967025196386708\n",
      "Epoch: 1 - Batch: 2181, Training Loss: 0.19679496620177828\n",
      "Epoch: 1 - Batch: 2182, Training Loss: 0.19688286159070176\n",
      "Epoch: 1 - Batch: 2183, Training Loss: 0.19696584129536132\n",
      "Epoch: 1 - Batch: 2184, Training Loss: 0.1970515844796724\n",
      "Epoch: 1 - Batch: 2185, Training Loss: 0.1971391685998954\n",
      "Epoch: 1 - Batch: 2186, Training Loss: 0.19722231369385276\n",
      "Epoch: 1 - Batch: 2187, Training Loss: 0.1973229175739324\n",
      "Epoch: 1 - Batch: 2188, Training Loss: 0.1974153940876325\n",
      "Epoch: 1 - Batch: 2189, Training Loss: 0.19749993900729848\n",
      "Epoch: 1 - Batch: 2190, Training Loss: 0.19759036690787493\n",
      "Epoch: 1 - Batch: 2191, Training Loss: 0.19768147817990475\n",
      "Epoch: 1 - Batch: 2192, Training Loss: 0.19776808895529008\n",
      "Epoch: 1 - Batch: 2193, Training Loss: 0.19785594819098168\n",
      "Epoch: 1 - Batch: 2194, Training Loss: 0.19794377921825618\n",
      "Epoch: 1 - Batch: 2195, Training Loss: 0.19803020046718084\n",
      "Epoch: 1 - Batch: 2196, Training Loss: 0.19811007730156233\n",
      "Epoch: 1 - Batch: 2197, Training Loss: 0.19820763563635338\n",
      "Epoch: 1 - Batch: 2198, Training Loss: 0.19828826376850134\n",
      "Epoch: 1 - Batch: 2199, Training Loss: 0.198370123532281\n",
      "Epoch: 1 - Batch: 2200, Training Loss: 0.19847105487340916\n",
      "Epoch: 1 - Batch: 2201, Training Loss: 0.19856498426268152\n",
      "Epoch: 1 - Batch: 2202, Training Loss: 0.19865831137197726\n",
      "Epoch: 1 - Batch: 2203, Training Loss: 0.19874525042389757\n",
      "Epoch: 1 - Batch: 2204, Training Loss: 0.19883648503825044\n",
      "Epoch: 1 - Batch: 2205, Training Loss: 0.198916410029577\n",
      "Epoch: 1 - Batch: 2206, Training Loss: 0.19901331131706387\n",
      "Epoch: 1 - Batch: 2207, Training Loss: 0.1991026166882088\n",
      "Epoch: 1 - Batch: 2208, Training Loss: 0.19919055083744958\n",
      "Epoch: 1 - Batch: 2209, Training Loss: 0.19928041271012814\n",
      "Epoch: 1 - Batch: 2210, Training Loss: 0.19938149176550346\n",
      "Epoch: 1 - Batch: 2211, Training Loss: 0.19947345524556798\n",
      "Epoch: 1 - Batch: 2212, Training Loss: 0.1995626158060917\n",
      "Epoch: 1 - Batch: 2213, Training Loss: 0.19965168908485528\n",
      "Epoch: 1 - Batch: 2214, Training Loss: 0.1997412842255129\n",
      "Epoch: 1 - Batch: 2215, Training Loss: 0.19983263248284264\n",
      "Epoch: 1 - Batch: 2216, Training Loss: 0.19991313465322627\n",
      "Epoch: 1 - Batch: 2217, Training Loss: 0.20001256429462091\n",
      "Epoch: 1 - Batch: 2218, Training Loss: 0.2001025427200802\n",
      "Epoch: 1 - Batch: 2219, Training Loss: 0.20019318973939018\n",
      "Epoch: 1 - Batch: 2220, Training Loss: 0.2002886586743801\n",
      "Epoch: 1 - Batch: 2221, Training Loss: 0.20037220657539012\n",
      "Epoch: 1 - Batch: 2222, Training Loss: 0.20045642180995363\n",
      "Epoch: 1 - Batch: 2223, Training Loss: 0.2005419616972036\n",
      "Epoch: 1 - Batch: 2224, Training Loss: 0.20063840988930778\n",
      "Epoch: 1 - Batch: 2225, Training Loss: 0.20072415164033966\n",
      "Epoch: 1 - Batch: 2226, Training Loss: 0.20081197058976585\n",
      "Epoch: 1 - Batch: 2227, Training Loss: 0.20090158542565642\n",
      "Epoch: 1 - Batch: 2228, Training Loss: 0.20099559771036035\n",
      "Epoch: 1 - Batch: 2229, Training Loss: 0.2010916243392239\n",
      "Epoch: 1 - Batch: 2230, Training Loss: 0.20118676892066278\n",
      "Epoch: 1 - Batch: 2231, Training Loss: 0.20127448390174663\n",
      "Epoch: 1 - Batch: 2232, Training Loss: 0.20136766382191904\n",
      "Epoch: 1 - Batch: 2233, Training Loss: 0.20145286158172052\n",
      "Epoch: 1 - Batch: 2234, Training Loss: 0.20155052651665104\n",
      "Epoch: 1 - Batch: 2235, Training Loss: 0.20163593294213264\n",
      "Epoch: 1 - Batch: 2236, Training Loss: 0.2017220525846355\n",
      "Epoch: 1 - Batch: 2237, Training Loss: 0.20181936185010038\n",
      "Epoch: 1 - Batch: 2238, Training Loss: 0.20190493787822636\n",
      "Epoch: 1 - Batch: 2239, Training Loss: 0.2019977080288218\n",
      "Epoch: 1 - Batch: 2240, Training Loss: 0.20208442493482412\n",
      "Epoch: 1 - Batch: 2241, Training Loss: 0.20217199163329147\n",
      "Epoch: 1 - Batch: 2242, Training Loss: 0.20225727352411\n",
      "Epoch: 1 - Batch: 2243, Training Loss: 0.2023461326511938\n",
      "Epoch: 1 - Batch: 2244, Training Loss: 0.20243020760973493\n",
      "Epoch: 1 - Batch: 2245, Training Loss: 0.20251862017768335\n",
      "Epoch: 1 - Batch: 2246, Training Loss: 0.20260990847204852\n",
      "Epoch: 1 - Batch: 2247, Training Loss: 0.20269686212241156\n",
      "Epoch: 1 - Batch: 2248, Training Loss: 0.2027885403922739\n",
      "Epoch: 1 - Batch: 2249, Training Loss: 0.202884487783039\n",
      "Epoch: 1 - Batch: 2250, Training Loss: 0.20297318755048227\n",
      "Epoch: 1 - Batch: 2251, Training Loss: 0.20306927471067973\n",
      "Epoch: 1 - Batch: 2252, Training Loss: 0.20316197727474802\n",
      "Epoch: 1 - Batch: 2253, Training Loss: 0.20325285954953823\n",
      "Epoch: 1 - Batch: 2254, Training Loss: 0.20334103098679734\n",
      "Epoch: 1 - Batch: 2255, Training Loss: 0.20343951794440868\n",
      "Epoch: 1 - Batch: 2256, Training Loss: 0.20353006328738743\n",
      "Epoch: 1 - Batch: 2257, Training Loss: 0.20361560166104517\n",
      "Epoch: 1 - Batch: 2258, Training Loss: 0.20369750943967754\n",
      "Epoch: 1 - Batch: 2259, Training Loss: 0.20378296861230438\n",
      "Epoch: 1 - Batch: 2260, Training Loss: 0.2038698789430396\n",
      "Epoch: 1 - Batch: 2261, Training Loss: 0.20396322373974185\n",
      "Epoch: 1 - Batch: 2262, Training Loss: 0.20405876878679885\n",
      "Epoch: 1 - Batch: 2263, Training Loss: 0.2041542081048042\n",
      "Epoch: 1 - Batch: 2264, Training Loss: 0.20425004609065073\n",
      "Epoch: 1 - Batch: 2265, Training Loss: 0.20433477330860214\n",
      "Epoch: 1 - Batch: 2266, Training Loss: 0.20442441829895697\n",
      "Epoch: 1 - Batch: 2267, Training Loss: 0.20451295082372417\n",
      "Epoch: 1 - Batch: 2268, Training Loss: 0.20459365090401613\n",
      "Epoch: 1 - Batch: 2269, Training Loss: 0.20467965550115255\n",
      "Epoch: 1 - Batch: 2270, Training Loss: 0.2047756489360115\n",
      "Epoch: 1 - Batch: 2271, Training Loss: 0.2048586844582463\n",
      "Epoch: 1 - Batch: 2272, Training Loss: 0.20494849540626825\n",
      "Epoch: 1 - Batch: 2273, Training Loss: 0.20503431213880652\n",
      "Epoch: 1 - Batch: 2274, Training Loss: 0.20511940185901142\n",
      "Epoch: 1 - Batch: 2275, Training Loss: 0.2052032169368532\n",
      "Epoch: 1 - Batch: 2276, Training Loss: 0.20528903792598355\n",
      "Epoch: 1 - Batch: 2277, Training Loss: 0.2053784818927546\n",
      "Epoch: 1 - Batch: 2278, Training Loss: 0.2054692553001059\n",
      "Epoch: 1 - Batch: 2279, Training Loss: 0.20557086452668777\n",
      "Epoch: 1 - Batch: 2280, Training Loss: 0.20566706048018896\n",
      "Epoch: 1 - Batch: 2281, Training Loss: 0.2057599169240761\n",
      "Epoch: 1 - Batch: 2282, Training Loss: 0.2058381349107816\n",
      "Epoch: 1 - Batch: 2283, Training Loss: 0.20592315217625246\n",
      "Epoch: 1 - Batch: 2284, Training Loss: 0.20601394570511372\n",
      "Epoch: 1 - Batch: 2285, Training Loss: 0.2060942961023518\n",
      "Epoch: 1 - Batch: 2286, Training Loss: 0.20618852046912978\n",
      "Epoch: 1 - Batch: 2287, Training Loss: 0.2062681105982506\n",
      "Epoch: 1 - Batch: 2288, Training Loss: 0.2063561435943318\n",
      "Epoch: 1 - Batch: 2289, Training Loss: 0.20644381885751958\n",
      "Epoch: 1 - Batch: 2290, Training Loss: 0.2065264090376707\n",
      "Epoch: 1 - Batch: 2291, Training Loss: 0.20661784694290675\n",
      "Epoch: 1 - Batch: 2292, Training Loss: 0.20670254467978802\n",
      "Epoch: 1 - Batch: 2293, Training Loss: 0.2067957255390054\n",
      "Epoch: 1 - Batch: 2294, Training Loss: 0.206880748710575\n",
      "Epoch: 1 - Batch: 2295, Training Loss: 0.20698068637530603\n",
      "Epoch: 1 - Batch: 2296, Training Loss: 0.2070656435171862\n",
      "Epoch: 1 - Batch: 2297, Training Loss: 0.20715698765383828\n",
      "Epoch: 1 - Batch: 2298, Training Loss: 0.20724509791725904\n",
      "Epoch: 1 - Batch: 2299, Training Loss: 0.2073299573799271\n",
      "Epoch: 1 - Batch: 2300, Training Loss: 0.20742609854138905\n",
      "Epoch: 1 - Batch: 2301, Training Loss: 0.2075146787226892\n",
      "Epoch: 1 - Batch: 2302, Training Loss: 0.20760196687076024\n",
      "Epoch: 1 - Batch: 2303, Training Loss: 0.2076899701386542\n",
      "Epoch: 1 - Batch: 2304, Training Loss: 0.20777757924859402\n",
      "Epoch: 1 - Batch: 2305, Training Loss: 0.20786633961532838\n",
      "Epoch: 1 - Batch: 2306, Training Loss: 0.20794762856949423\n",
      "Epoch: 1 - Batch: 2307, Training Loss: 0.20804852445549632\n",
      "Epoch: 1 - Batch: 2308, Training Loss: 0.20813640557958515\n",
      "Epoch: 1 - Batch: 2309, Training Loss: 0.2082227538610078\n",
      "Epoch: 1 - Batch: 2310, Training Loss: 0.20831027840747565\n",
      "Epoch: 1 - Batch: 2311, Training Loss: 0.20839120654596222\n",
      "Epoch: 1 - Batch: 2312, Training Loss: 0.20847663801676203\n",
      "Epoch: 1 - Batch: 2313, Training Loss: 0.20856345127989998\n",
      "Epoch: 1 - Batch: 2314, Training Loss: 0.20864405329177035\n",
      "Epoch: 1 - Batch: 2315, Training Loss: 0.20874576702417424\n",
      "Epoch: 1 - Batch: 2316, Training Loss: 0.20883627853682188\n",
      "Epoch: 1 - Batch: 2317, Training Loss: 0.20893162744703578\n",
      "Epoch: 1 - Batch: 2318, Training Loss: 0.2090193561122291\n",
      "Epoch: 1 - Batch: 2319, Training Loss: 0.2091038444111893\n",
      "Epoch: 1 - Batch: 2320, Training Loss: 0.20919468263735622\n",
      "Epoch: 1 - Batch: 2321, Training Loss: 0.2092904915745875\n",
      "Epoch: 1 - Batch: 2322, Training Loss: 0.20939059360308038\n",
      "Epoch: 1 - Batch: 2323, Training Loss: 0.20948529680248715\n",
      "Epoch: 1 - Batch: 2324, Training Loss: 0.20958352458986082\n",
      "Epoch: 1 - Batch: 2325, Training Loss: 0.2096768023807611\n",
      "Epoch: 1 - Batch: 2326, Training Loss: 0.20977490413223532\n",
      "Epoch: 1 - Batch: 2327, Training Loss: 0.20987330593329362\n",
      "Epoch: 1 - Batch: 2328, Training Loss: 0.20997122091984077\n",
      "Epoch: 1 - Batch: 2329, Training Loss: 0.21007354402448208\n",
      "Epoch: 1 - Batch: 2330, Training Loss: 0.21014724534690676\n",
      "Epoch: 1 - Batch: 2331, Training Loss: 0.2102271664090714\n",
      "Epoch: 1 - Batch: 2332, Training Loss: 0.2103223390012337\n",
      "Epoch: 1 - Batch: 2333, Training Loss: 0.21040903129437274\n",
      "Epoch: 1 - Batch: 2334, Training Loss: 0.2105017339819996\n",
      "Epoch: 1 - Batch: 2335, Training Loss: 0.2105854907587393\n",
      "Epoch: 1 - Batch: 2336, Training Loss: 0.2106690931428922\n",
      "Epoch: 1 - Batch: 2337, Training Loss: 0.21076200686234542\n",
      "Epoch: 1 - Batch: 2338, Training Loss: 0.21085146150457523\n",
      "Epoch: 1 - Batch: 2339, Training Loss: 0.21094970369269797\n",
      "Epoch: 1 - Batch: 2340, Training Loss: 0.21103819778457802\n",
      "Epoch: 1 - Batch: 2341, Training Loss: 0.21112245624610046\n",
      "Epoch: 1 - Batch: 2342, Training Loss: 0.2112144235441242\n",
      "Epoch: 1 - Batch: 2343, Training Loss: 0.2113043225214355\n",
      "Epoch: 1 - Batch: 2344, Training Loss: 0.21140266589165524\n",
      "Epoch: 1 - Batch: 2345, Training Loss: 0.21149909537699488\n",
      "Epoch: 1 - Batch: 2346, Training Loss: 0.21157964980646746\n",
      "Epoch: 1 - Batch: 2347, Training Loss: 0.21166757910603512\n",
      "Epoch: 1 - Batch: 2348, Training Loss: 0.21175286137370722\n",
      "Epoch: 1 - Batch: 2349, Training Loss: 0.2118485880703673\n",
      "Epoch: 1 - Batch: 2350, Training Loss: 0.21193470646492876\n",
      "Epoch: 1 - Batch: 2351, Training Loss: 0.21202199298820487\n",
      "Epoch: 1 - Batch: 2352, Training Loss: 0.2121134676644656\n",
      "Epoch: 1 - Batch: 2353, Training Loss: 0.21220486172494998\n",
      "Epoch: 1 - Batch: 2354, Training Loss: 0.2122840247492292\n",
      "Epoch: 1 - Batch: 2355, Training Loss: 0.21237563335678075\n",
      "Epoch: 1 - Batch: 2356, Training Loss: 0.21246267187333423\n",
      "Epoch: 1 - Batch: 2357, Training Loss: 0.21255504771152736\n",
      "Epoch: 1 - Batch: 2358, Training Loss: 0.2126397160167619\n",
      "Epoch: 1 - Batch: 2359, Training Loss: 0.21272737254229548\n",
      "Epoch: 1 - Batch: 2360, Training Loss: 0.2128171000326945\n",
      "Epoch: 1 - Batch: 2361, Training Loss: 0.21291112071320192\n",
      "Epoch: 1 - Batch: 2362, Training Loss: 0.21300531946629236\n",
      "Epoch: 1 - Batch: 2363, Training Loss: 0.21309612520891635\n",
      "Epoch: 1 - Batch: 2364, Training Loss: 0.21318947308840444\n",
      "Epoch: 1 - Batch: 2365, Training Loss: 0.21327577400983466\n",
      "Epoch: 1 - Batch: 2366, Training Loss: 0.21335803855142585\n",
      "Epoch: 1 - Batch: 2367, Training Loss: 0.21344363222371288\n",
      "Epoch: 1 - Batch: 2368, Training Loss: 0.21353647280886004\n",
      "Epoch: 1 - Batch: 2369, Training Loss: 0.21361898316400957\n",
      "Epoch: 1 - Batch: 2370, Training Loss: 0.2137123204607078\n",
      "Epoch: 1 - Batch: 2371, Training Loss: 0.21380331437682631\n",
      "Epoch: 1 - Batch: 2372, Training Loss: 0.2138961458989538\n",
      "Epoch: 1 - Batch: 2373, Training Loss: 0.21399044995481892\n",
      "Epoch: 1 - Batch: 2374, Training Loss: 0.21408932448471363\n",
      "Epoch: 1 - Batch: 2375, Training Loss: 0.21417612493482988\n",
      "Epoch: 1 - Batch: 2376, Training Loss: 0.21426728116981622\n",
      "Epoch: 1 - Batch: 2377, Training Loss: 0.2143554846828751\n",
      "Epoch: 1 - Batch: 2378, Training Loss: 0.2144477163158839\n",
      "Epoch: 1 - Batch: 2379, Training Loss: 0.21452911043691003\n",
      "Epoch: 1 - Batch: 2380, Training Loss: 0.2146108557236926\n",
      "Epoch: 1 - Batch: 2381, Training Loss: 0.2147065404722248\n",
      "Epoch: 1 - Batch: 2382, Training Loss: 0.21480143613260777\n",
      "Epoch: 1 - Batch: 2383, Training Loss: 0.21489826997565986\n",
      "Epoch: 1 - Batch: 2384, Training Loss: 0.214984958592932\n",
      "Epoch: 1 - Batch: 2385, Training Loss: 0.2150740008028981\n",
      "Epoch: 1 - Batch: 2386, Training Loss: 0.21516366410463011\n",
      "Epoch: 1 - Batch: 2387, Training Loss: 0.2152472607865559\n",
      "Epoch: 1 - Batch: 2388, Training Loss: 0.2153371493973048\n",
      "Epoch: 1 - Batch: 2389, Training Loss: 0.21542017872307825\n",
      "Epoch: 1 - Batch: 2390, Training Loss: 0.2155100836806234\n",
      "Epoch: 1 - Batch: 2391, Training Loss: 0.215593241371375\n",
      "Epoch: 1 - Batch: 2392, Training Loss: 0.21568194171954347\n",
      "Epoch: 1 - Batch: 2393, Training Loss: 0.21576892118746566\n",
      "Epoch: 1 - Batch: 2394, Training Loss: 0.2158619260745954\n",
      "Epoch: 1 - Batch: 2395, Training Loss: 0.2159469750204786\n",
      "Epoch: 1 - Batch: 2396, Training Loss: 0.2160339103903541\n",
      "Epoch: 1 - Batch: 2397, Training Loss: 0.21611995341419976\n",
      "Epoch: 1 - Batch: 2398, Training Loss: 0.21620310133764795\n",
      "Epoch: 1 - Batch: 2399, Training Loss: 0.2162909724349604\n",
      "Epoch: 1 - Batch: 2400, Training Loss: 0.21637964782430164\n",
      "Epoch: 1 - Batch: 2401, Training Loss: 0.21647819252700157\n",
      "Epoch: 1 - Batch: 2402, Training Loss: 0.21656863167140614\n",
      "Epoch: 1 - Batch: 2403, Training Loss: 0.21666096161377568\n",
      "Epoch: 1 - Batch: 2404, Training Loss: 0.21674005597039045\n",
      "Epoch: 1 - Batch: 2405, Training Loss: 0.2168331321878714\n",
      "Epoch: 1 - Batch: 2406, Training Loss: 0.2169180915167379\n",
      "Epoch: 1 - Batch: 2407, Training Loss: 0.21700292916514388\n",
      "Epoch: 1 - Batch: 2408, Training Loss: 0.2171043400493625\n",
      "Epoch: 1 - Batch: 2409, Training Loss: 0.21720416909377174\n",
      "Epoch: 1 - Batch: 2410, Training Loss: 0.21728938927945016\n",
      "Epoch: 1 - Batch: 2411, Training Loss: 0.21737991712653815\n",
      "Epoch: 1 - Batch: 2412, Training Loss: 0.21751351529117643\n",
      "Epoch 1 - Batch 2412, Training Loss: 0.21751351529117643, Validation Loss: 0.21270342611060608\n",
      "Validation loss decreased (inf --> 0.212703). Saving model...\n",
      "Epoch: 2 - Batch: 1, Training Loss: 8.222397731904366e-05\n",
      "Epoch: 2 - Batch: 2, Training Loss: 0.00017441389078326882\n",
      "Epoch: 2 - Batch: 3, Training Loss: 0.0002607511878507845\n",
      "Epoch: 2 - Batch: 4, Training Loss: 0.00034133396783278357\n",
      "Epoch: 2 - Batch: 5, Training Loss: 0.00043688128218919683\n",
      "Epoch: 2 - Batch: 6, Training Loss: 0.0005389102000996446\n",
      "Epoch: 2 - Batch: 7, Training Loss: 0.0006265223804694503\n",
      "Epoch: 2 - Batch: 8, Training Loss: 0.0007191810441847464\n",
      "Epoch: 2 - Batch: 9, Training Loss: 0.0008225347198064054\n",
      "Epoch: 2 - Batch: 10, Training Loss: 0.000913554885938986\n",
      "Epoch: 2 - Batch: 11, Training Loss: 0.0010044796648705579\n",
      "Epoch: 2 - Batch: 12, Training Loss: 0.0010939833660228533\n",
      "Epoch: 2 - Batch: 13, Training Loss: 0.0011847473335009112\n",
      "Epoch: 2 - Batch: 14, Training Loss: 0.0012786249332068175\n",
      "Epoch: 2 - Batch: 15, Training Loss: 0.00136569776196978\n",
      "Epoch: 2 - Batch: 16, Training Loss: 0.0014585293644103245\n",
      "Epoch: 2 - Batch: 17, Training Loss: 0.0015450523514455032\n",
      "Epoch: 2 - Batch: 18, Training Loss: 0.0016312964730100647\n",
      "Epoch: 2 - Batch: 19, Training Loss: 0.0017156481718147176\n",
      "Epoch: 2 - Batch: 20, Training Loss: 0.0018120242798605171\n",
      "Epoch: 2 - Batch: 21, Training Loss: 0.0019000182808631687\n",
      "Epoch: 2 - Batch: 22, Training Loss: 0.001995153496563929\n",
      "Epoch: 2 - Batch: 23, Training Loss: 0.002092703194612294\n",
      "Epoch: 2 - Batch: 24, Training Loss: 0.002179047262688379\n",
      "Epoch: 2 - Batch: 25, Training Loss: 0.0022663435718016838\n",
      "Epoch: 2 - Batch: 26, Training Loss: 0.0023511347869043524\n",
      "Epoch: 2 - Batch: 27, Training Loss: 0.0024324117620687182\n",
      "Epoch: 2 - Batch: 28, Training Loss: 0.002527780880283558\n",
      "Epoch: 2 - Batch: 29, Training Loss: 0.002620841133999785\n",
      "Epoch: 2 - Batch: 30, Training Loss: 0.0027133198655758728\n",
      "Epoch: 2 - Batch: 31, Training Loss: 0.0028024729199176207\n",
      "Epoch: 2 - Batch: 32, Training Loss: 0.0028982136281173227\n",
      "Epoch: 2 - Batch: 33, Training Loss: 0.0029871985501614376\n",
      "Epoch: 2 - Batch: 34, Training Loss: 0.0030783152340557642\n",
      "Epoch: 2 - Batch: 35, Training Loss: 0.0031651969671644777\n",
      "Epoch: 2 - Batch: 36, Training Loss: 0.003246241528695298\n",
      "Epoch: 2 - Batch: 37, Training Loss: 0.0033325504072961917\n",
      "Epoch: 2 - Batch: 38, Training Loss: 0.0034127104193416996\n",
      "Epoch: 2 - Batch: 39, Training Loss: 0.003494464237842196\n",
      "Epoch: 2 - Batch: 40, Training Loss: 0.003596006125014022\n",
      "Epoch: 2 - Batch: 41, Training Loss: 0.0036874238654946412\n",
      "Epoch: 2 - Batch: 42, Training Loss: 0.003769966803033949\n",
      "Epoch: 2 - Batch: 43, Training Loss: 0.0038543621241848664\n",
      "Epoch: 2 - Batch: 44, Training Loss: 0.003942890491206848\n",
      "Epoch: 2 - Batch: 45, Training Loss: 0.004030458202854318\n",
      "Epoch: 2 - Batch: 46, Training Loss: 0.004128963274149159\n",
      "Epoch: 2 - Batch: 47, Training Loss: 0.004217486000723309\n",
      "Epoch: 2 - Batch: 48, Training Loss: 0.004302812743295682\n",
      "Epoch: 2 - Batch: 49, Training Loss: 0.004384777074676644\n",
      "Epoch: 2 - Batch: 50, Training Loss: 0.004462203990118223\n",
      "Epoch: 2 - Batch: 51, Training Loss: 0.004549195912742298\n",
      "Epoch: 2 - Batch: 52, Training Loss: 0.004634380791564881\n",
      "Epoch: 2 - Batch: 53, Training Loss: 0.004732263022740287\n",
      "Epoch: 2 - Batch: 54, Training Loss: 0.00482028709171621\n",
      "Epoch: 2 - Batch: 55, Training Loss: 0.004911956385049852\n",
      "Epoch: 2 - Batch: 56, Training Loss: 0.004990403764382326\n",
      "Epoch: 2 - Batch: 57, Training Loss: 0.005070267285329983\n",
      "Epoch: 2 - Batch: 58, Training Loss: 0.00515231340011554\n",
      "Epoch: 2 - Batch: 59, Training Loss: 0.005243934382055925\n",
      "Epoch: 2 - Batch: 60, Training Loss: 0.005338402233313565\n",
      "Epoch: 2 - Batch: 61, Training Loss: 0.005426720668180269\n",
      "Epoch: 2 - Batch: 62, Training Loss: 0.0055121661505197015\n",
      "Epoch: 2 - Batch: 63, Training Loss: 0.005608678352130981\n",
      "Epoch: 2 - Batch: 64, Training Loss: 0.005695926504299217\n",
      "Epoch: 2 - Batch: 65, Training Loss: 0.005796116957468773\n",
      "Epoch: 2 - Batch: 66, Training Loss: 0.005888383909689253\n",
      "Epoch: 2 - Batch: 67, Training Loss: 0.005988803619571388\n",
      "Epoch: 2 - Batch: 68, Training Loss: 0.006086857207922595\n",
      "Epoch: 2 - Batch: 69, Training Loss: 0.006182592362165451\n",
      "Epoch: 2 - Batch: 70, Training Loss: 0.006270372526562629\n",
      "Epoch: 2 - Batch: 71, Training Loss: 0.006360268316063319\n",
      "Epoch: 2 - Batch: 72, Training Loss: 0.006448171563310607\n",
      "Epoch: 2 - Batch: 73, Training Loss: 0.00652991627554593\n",
      "Epoch: 2 - Batch: 74, Training Loss: 0.006612978053972694\n",
      "Epoch: 2 - Batch: 75, Training Loss: 0.006707956270001223\n",
      "Epoch: 2 - Batch: 76, Training Loss: 0.006789009437385088\n",
      "Epoch: 2 - Batch: 77, Training Loss: 0.006866927911986166\n",
      "Epoch: 2 - Batch: 78, Training Loss: 0.006952366419445421\n",
      "Epoch: 2 - Batch: 79, Training Loss: 0.007042242692873055\n",
      "Epoch: 2 - Batch: 80, Training Loss: 0.007133973561610353\n",
      "Epoch: 2 - Batch: 81, Training Loss: 0.0072320405128188\n",
      "Epoch: 2 - Batch: 82, Training Loss: 0.007316510333597759\n",
      "Epoch: 2 - Batch: 83, Training Loss: 0.007405816371958845\n",
      "Epoch: 2 - Batch: 84, Training Loss: 0.007494458326356328\n",
      "Epoch: 2 - Batch: 85, Training Loss: 0.007586214166276689\n",
      "Epoch: 2 - Batch: 86, Training Loss: 0.00767992960774088\n",
      "Epoch: 2 - Batch: 87, Training Loss: 0.007771243040093142\n",
      "Epoch: 2 - Batch: 88, Training Loss: 0.007856554418159757\n",
      "Epoch: 2 - Batch: 89, Training Loss: 0.007945852802068637\n",
      "Epoch: 2 - Batch: 90, Training Loss: 0.008027500409639099\n",
      "Epoch: 2 - Batch: 91, Training Loss: 0.00811651221802381\n",
      "Epoch: 2 - Batch: 92, Training Loss: 0.008198512943575828\n",
      "Epoch: 2 - Batch: 93, Training Loss: 0.008281364400637882\n",
      "Epoch: 2 - Batch: 94, Training Loss: 0.008360331362209114\n",
      "Epoch: 2 - Batch: 95, Training Loss: 0.008450175782242423\n",
      "Epoch: 2 - Batch: 96, Training Loss: 0.00853418671892057\n",
      "Epoch: 2 - Batch: 97, Training Loss: 0.008622258481496997\n",
      "Epoch: 2 - Batch: 98, Training Loss: 0.008726142396282398\n",
      "Epoch: 2 - Batch: 99, Training Loss: 0.008820151196635184\n",
      "Epoch: 2 - Batch: 100, Training Loss: 0.008906402503129459\n",
      "Epoch: 2 - Batch: 101, Training Loss: 0.009000999886993547\n",
      "Epoch: 2 - Batch: 102, Training Loss: 0.009088090335205815\n",
      "Epoch: 2 - Batch: 103, Training Loss: 0.009174922756097012\n",
      "Epoch: 2 - Batch: 104, Training Loss: 0.009256313683134605\n",
      "Epoch: 2 - Batch: 105, Training Loss: 0.009339958583784736\n",
      "Epoch: 2 - Batch: 106, Training Loss: 0.009431579738707091\n",
      "Epoch: 2 - Batch: 107, Training Loss: 0.009522982102326097\n",
      "Epoch: 2 - Batch: 108, Training Loss: 0.009618065065115838\n",
      "Epoch: 2 - Batch: 109, Training Loss: 0.009703828661299464\n",
      "Epoch: 2 - Batch: 110, Training Loss: 0.009789363142862842\n",
      "Epoch: 2 - Batch: 111, Training Loss: 0.009878044419769031\n",
      "Epoch: 2 - Batch: 112, Training Loss: 0.009958121517849207\n",
      "Epoch: 2 - Batch: 113, Training Loss: 0.010046287802716788\n",
      "Epoch: 2 - Batch: 114, Training Loss: 0.010137258848147606\n",
      "Epoch: 2 - Batch: 115, Training Loss: 0.010236589333805477\n",
      "Epoch: 2 - Batch: 116, Training Loss: 0.010319474590309026\n",
      "Epoch: 2 - Batch: 117, Training Loss: 0.01040605788009479\n",
      "Epoch: 2 - Batch: 118, Training Loss: 0.010488332664689811\n",
      "Epoch: 2 - Batch: 119, Training Loss: 0.010573787790774112\n",
      "Epoch: 2 - Batch: 120, Training Loss: 0.010659126771820916\n",
      "Epoch: 2 - Batch: 121, Training Loss: 0.01075144794153337\n",
      "Epoch: 2 - Batch: 122, Training Loss: 0.010854047693769334\n",
      "Epoch: 2 - Batch: 123, Training Loss: 0.010940395165883486\n",
      "Epoch: 2 - Batch: 124, Training Loss: 0.011033863916523619\n",
      "Epoch: 2 - Batch: 125, Training Loss: 0.011121907600419438\n",
      "Epoch: 2 - Batch: 126, Training Loss: 0.011210223446734509\n",
      "Epoch: 2 - Batch: 127, Training Loss: 0.011292593649420177\n",
      "Epoch: 2 - Batch: 128, Training Loss: 0.011382075561022087\n",
      "Epoch: 2 - Batch: 129, Training Loss: 0.011473728328399595\n",
      "Epoch: 2 - Batch: 130, Training Loss: 0.01157098965314688\n",
      "Epoch: 2 - Batch: 131, Training Loss: 0.011658607338050112\n",
      "Epoch: 2 - Batch: 132, Training Loss: 0.011739699228683712\n",
      "Epoch: 2 - Batch: 133, Training Loss: 0.011825171683854725\n",
      "Epoch: 2 - Batch: 134, Training Loss: 0.011919857572708557\n",
      "Epoch: 2 - Batch: 135, Training Loss: 0.01200369476135295\n",
      "Epoch: 2 - Batch: 136, Training Loss: 0.012089450412721777\n",
      "Epoch: 2 - Batch: 137, Training Loss: 0.01218204649403123\n",
      "Epoch: 2 - Batch: 138, Training Loss: 0.01227205509866648\n",
      "Epoch: 2 - Batch: 139, Training Loss: 0.012356809480965236\n",
      "Epoch: 2 - Batch: 140, Training Loss: 0.012446269263230746\n",
      "Epoch: 2 - Batch: 141, Training Loss: 0.01252986942950766\n",
      "Epoch: 2 - Batch: 142, Training Loss: 0.01262129595500122\n",
      "Epoch: 2 - Batch: 143, Training Loss: 0.012706744470703068\n",
      "Epoch: 2 - Batch: 144, Training Loss: 0.012793181467165005\n",
      "Epoch: 2 - Batch: 145, Training Loss: 0.012881229864819528\n",
      "Epoch: 2 - Batch: 146, Training Loss: 0.012983005673039217\n",
      "Epoch: 2 - Batch: 147, Training Loss: 0.013066258176444577\n",
      "Epoch: 2 - Batch: 148, Training Loss: 0.01314800933225831\n",
      "Epoch: 2 - Batch: 149, Training Loss: 0.01324424691881311\n",
      "Epoch: 2 - Batch: 150, Training Loss: 0.013323972696688637\n",
      "Epoch: 2 - Batch: 151, Training Loss: 0.013428356479946061\n",
      "Epoch: 2 - Batch: 152, Training Loss: 0.013525805156523513\n",
      "Epoch: 2 - Batch: 153, Training Loss: 0.013614323076670046\n",
      "Epoch: 2 - Batch: 154, Training Loss: 0.013707022329369193\n",
      "Epoch: 2 - Batch: 155, Training Loss: 0.013785702357096459\n",
      "Epoch: 2 - Batch: 156, Training Loss: 0.013868709985988452\n",
      "Epoch: 2 - Batch: 157, Training Loss: 0.013956140621779965\n",
      "Epoch: 2 - Batch: 158, Training Loss: 0.01404475092665473\n",
      "Epoch: 2 - Batch: 159, Training Loss: 0.014135762363375716\n",
      "Epoch: 2 - Batch: 160, Training Loss: 0.014233042240439362\n",
      "Epoch: 2 - Batch: 161, Training Loss: 0.014318424905611705\n",
      "Epoch: 2 - Batch: 162, Training Loss: 0.014408604416434049\n",
      "Epoch: 2 - Batch: 163, Training Loss: 0.014503439397212877\n",
      "Epoch: 2 - Batch: 164, Training Loss: 0.014591433231411486\n",
      "Epoch: 2 - Batch: 165, Training Loss: 0.014676992610022796\n",
      "Epoch: 2 - Batch: 166, Training Loss: 0.014765705400537298\n",
      "Epoch: 2 - Batch: 167, Training Loss: 0.014852171822468044\n",
      "Epoch: 2 - Batch: 168, Training Loss: 0.014932387602665333\n",
      "Epoch: 2 - Batch: 169, Training Loss: 0.015017124711478725\n",
      "Epoch: 2 - Batch: 170, Training Loss: 0.015101915599151234\n",
      "Epoch: 2 - Batch: 171, Training Loss: 0.015194112024151073\n",
      "Epoch: 2 - Batch: 172, Training Loss: 0.015285204589416337\n",
      "Epoch: 2 - Batch: 173, Training Loss: 0.015378231173427544\n",
      "Epoch: 2 - Batch: 174, Training Loss: 0.015468909434517028\n",
      "Epoch: 2 - Batch: 175, Training Loss: 0.015554994968681984\n",
      "Epoch: 2 - Batch: 176, Training Loss: 0.015636803202368134\n",
      "Epoch: 2 - Batch: 177, Training Loss: 0.01571982917970488\n",
      "Epoch: 2 - Batch: 178, Training Loss: 0.015807275773083194\n",
      "Epoch: 2 - Batch: 179, Training Loss: 0.015889602384312237\n",
      "Epoch: 2 - Batch: 180, Training Loss: 0.015969645380232465\n",
      "Epoch: 2 - Batch: 181, Training Loss: 0.01606277869411962\n",
      "Epoch: 2 - Batch: 182, Training Loss: 0.0161500839303382\n",
      "Epoch: 2 - Batch: 183, Training Loss: 0.016241012867014997\n",
      "Epoch: 2 - Batch: 184, Training Loss: 0.016334608215746\n",
      "Epoch: 2 - Batch: 185, Training Loss: 0.01642514644793017\n",
      "Epoch: 2 - Batch: 186, Training Loss: 0.01651545564580715\n",
      "Epoch: 2 - Batch: 187, Training Loss: 0.016599411648353733\n",
      "Epoch: 2 - Batch: 188, Training Loss: 0.016690863206809632\n",
      "Epoch: 2 - Batch: 189, Training Loss: 0.016781169865558398\n",
      "Epoch: 2 - Batch: 190, Training Loss: 0.016868029877073927\n",
      "Epoch: 2 - Batch: 191, Training Loss: 0.016947989959028822\n",
      "Epoch: 2 - Batch: 192, Training Loss: 0.017029869770183295\n",
      "Epoch: 2 - Batch: 193, Training Loss: 0.017121711421516996\n",
      "Epoch: 2 - Batch: 194, Training Loss: 0.017215003749080755\n",
      "Epoch: 2 - Batch: 195, Training Loss: 0.01730679132501482\n",
      "Epoch: 2 - Batch: 196, Training Loss: 0.01739475048868415\n",
      "Epoch: 2 - Batch: 197, Training Loss: 0.017488016374718095\n",
      "Epoch: 2 - Batch: 198, Training Loss: 0.017577725751433602\n",
      "Epoch: 2 - Batch: 199, Training Loss: 0.017661477252223202\n",
      "Epoch: 2 - Batch: 200, Training Loss: 0.01774802924462812\n",
      "Epoch: 2 - Batch: 201, Training Loss: 0.01784012553160068\n",
      "Epoch: 2 - Batch: 202, Training Loss: 0.0179218938311633\n",
      "Epoch: 2 - Batch: 203, Training Loss: 0.018003685958992387\n",
      "Epoch: 2 - Batch: 204, Training Loss: 0.018090427341943553\n",
      "Epoch: 2 - Batch: 205, Training Loss: 0.018178218984930078\n",
      "Epoch: 2 - Batch: 206, Training Loss: 0.018275072968025902\n",
      "Epoch: 2 - Batch: 207, Training Loss: 0.018357593924499073\n",
      "Epoch: 2 - Batch: 208, Training Loss: 0.01844686751020686\n",
      "Epoch: 2 - Batch: 209, Training Loss: 0.018542305901523056\n",
      "Epoch: 2 - Batch: 210, Training Loss: 0.01863898995454434\n",
      "Epoch: 2 - Batch: 211, Training Loss: 0.018715734153392897\n",
      "Epoch: 2 - Batch: 212, Training Loss: 0.018815748739499556\n",
      "Epoch: 2 - Batch: 213, Training Loss: 0.018902429343999716\n",
      "Epoch: 2 - Batch: 214, Training Loss: 0.018978986938547337\n",
      "Epoch: 2 - Batch: 215, Training Loss: 0.019064604970402584\n",
      "Epoch: 2 - Batch: 216, Training Loss: 0.01916275378532868\n",
      "Epoch: 2 - Batch: 217, Training Loss: 0.019265344381876055\n",
      "Epoch: 2 - Batch: 218, Training Loss: 0.01935363568946299\n",
      "Epoch: 2 - Batch: 219, Training Loss: 0.019444604652932233\n",
      "Epoch: 2 - Batch: 220, Training Loss: 0.019541466191633425\n",
      "Epoch: 2 - Batch: 221, Training Loss: 0.01964975746462792\n",
      "Epoch: 2 - Batch: 222, Training Loss: 0.019734613374987647\n",
      "Epoch: 2 - Batch: 223, Training Loss: 0.01982181340727838\n",
      "Epoch: 2 - Batch: 224, Training Loss: 0.019906694208508106\n",
      "Epoch: 2 - Batch: 225, Training Loss: 0.0199967020594362\n",
      "Epoch: 2 - Batch: 226, Training Loss: 0.020082324298460092\n",
      "Epoch: 2 - Batch: 227, Training Loss: 0.020172494363231246\n",
      "Epoch: 2 - Batch: 228, Training Loss: 0.020259995711581227\n",
      "Epoch: 2 - Batch: 229, Training Loss: 0.02035225169798035\n",
      "Epoch: 2 - Batch: 230, Training Loss: 0.020438949755125772\n",
      "Epoch: 2 - Batch: 231, Training Loss: 0.020526332610825797\n",
      "Epoch: 2 - Batch: 232, Training Loss: 0.02060954202936459\n",
      "Epoch: 2 - Batch: 233, Training Loss: 0.02070733804756136\n",
      "Epoch: 2 - Batch: 234, Training Loss: 0.020787592328602996\n",
      "Epoch: 2 - Batch: 235, Training Loss: 0.020874390851205855\n",
      "Epoch: 2 - Batch: 236, Training Loss: 0.02096568828890375\n",
      "Epoch: 2 - Batch: 237, Training Loss: 0.021052099467806555\n",
      "Epoch: 2 - Batch: 238, Training Loss: 0.02113606388469043\n",
      "Epoch: 2 - Batch: 239, Training Loss: 0.021211078578905876\n",
      "Epoch: 2 - Batch: 240, Training Loss: 0.02130399980454105\n",
      "Epoch: 2 - Batch: 241, Training Loss: 0.021391631816058215\n",
      "Epoch: 2 - Batch: 242, Training Loss: 0.021471117383755657\n",
      "Epoch: 2 - Batch: 243, Training Loss: 0.0215599859198231\n",
      "Epoch: 2 - Batch: 244, Training Loss: 0.021652780139624184\n",
      "Epoch: 2 - Batch: 245, Training Loss: 0.021732094733177332\n",
      "Epoch: 2 - Batch: 246, Training Loss: 0.021820580664015134\n",
      "Epoch: 2 - Batch: 247, Training Loss: 0.02191249580179676\n",
      "Epoch: 2 - Batch: 248, Training Loss: 0.021997928650274404\n",
      "Epoch: 2 - Batch: 249, Training Loss: 0.02208850839813155\n",
      "Epoch: 2 - Batch: 250, Training Loss: 0.022182957863876872\n",
      "Epoch: 2 - Batch: 251, Training Loss: 0.022274525116381557\n",
      "Epoch: 2 - Batch: 252, Training Loss: 0.02237089400243008\n",
      "Epoch: 2 - Batch: 253, Training Loss: 0.02246697011649312\n",
      "Epoch: 2 - Batch: 254, Training Loss: 0.022559078339221664\n",
      "Epoch: 2 - Batch: 255, Training Loss: 0.022645208212372477\n",
      "Epoch: 2 - Batch: 256, Training Loss: 0.022737140166472835\n",
      "Epoch: 2 - Batch: 257, Training Loss: 0.022825301206279947\n",
      "Epoch: 2 - Batch: 258, Training Loss: 0.02290077729901271\n",
      "Epoch: 2 - Batch: 259, Training Loss: 0.022983411478363656\n",
      "Epoch: 2 - Batch: 260, Training Loss: 0.02306816464361069\n",
      "Epoch: 2 - Batch: 261, Training Loss: 0.02315639040338657\n",
      "Epoch: 2 - Batch: 262, Training Loss: 0.02325180602903983\n",
      "Epoch: 2 - Batch: 263, Training Loss: 0.023341281824493486\n",
      "Epoch: 2 - Batch: 264, Training Loss: 0.023433789481126253\n",
      "Epoch: 2 - Batch: 265, Training Loss: 0.023523297982528238\n",
      "Epoch: 2 - Batch: 266, Training Loss: 0.02362100473585018\n",
      "Epoch: 2 - Batch: 267, Training Loss: 0.023709324702842913\n",
      "Epoch: 2 - Batch: 268, Training Loss: 0.02379172627086663\n",
      "Epoch: 2 - Batch: 269, Training Loss: 0.023888529211925233\n",
      "Epoch: 2 - Batch: 270, Training Loss: 0.023976734065594366\n",
      "Epoch: 2 - Batch: 271, Training Loss: 0.024068640807572487\n",
      "Epoch: 2 - Batch: 272, Training Loss: 0.0241632104340082\n",
      "Epoch: 2 - Batch: 273, Training Loss: 0.024261398645578136\n",
      "Epoch: 2 - Batch: 274, Training Loss: 0.02435581363230993\n",
      "Epoch: 2 - Batch: 275, Training Loss: 0.02443927102050378\n",
      "Epoch: 2 - Batch: 276, Training Loss: 0.024519714642668246\n",
      "Epoch: 2 - Batch: 277, Training Loss: 0.024614696954662726\n",
      "Epoch: 2 - Batch: 278, Training Loss: 0.0246966464095942\n",
      "Epoch: 2 - Batch: 279, Training Loss: 0.024788996930640333\n",
      "Epoch: 2 - Batch: 280, Training Loss: 0.024883485434709696\n",
      "Epoch: 2 - Batch: 281, Training Loss: 0.024968219324822846\n",
      "Epoch: 2 - Batch: 282, Training Loss: 0.02505291409012097\n",
      "Epoch: 2 - Batch: 283, Training Loss: 0.02513819978654286\n",
      "Epoch: 2 - Batch: 284, Training Loss: 0.025226803703440562\n",
      "Epoch: 2 - Batch: 285, Training Loss: 0.02531088776824684\n",
      "Epoch: 2 - Batch: 286, Training Loss: 0.025387548799835034\n",
      "Epoch: 2 - Batch: 287, Training Loss: 0.025459710032884556\n",
      "Epoch: 2 - Batch: 288, Training Loss: 0.025536756320033303\n",
      "Epoch: 2 - Batch: 289, Training Loss: 0.025627821436766566\n",
      "Epoch: 2 - Batch: 290, Training Loss: 0.025713776363365685\n",
      "Epoch: 2 - Batch: 291, Training Loss: 0.025792785977348562\n",
      "Epoch: 2 - Batch: 292, Training Loss: 0.025879723138823044\n",
      "Epoch: 2 - Batch: 293, Training Loss: 0.025967189069114514\n",
      "Epoch: 2 - Batch: 294, Training Loss: 0.026051264330374066\n",
      "Epoch: 2 - Batch: 295, Training Loss: 0.026152024567621463\n",
      "Epoch: 2 - Batch: 296, Training Loss: 0.026236596398339737\n",
      "Epoch: 2 - Batch: 297, Training Loss: 0.026323995600835995\n",
      "Epoch: 2 - Batch: 298, Training Loss: 0.026411395550861486\n",
      "Epoch: 2 - Batch: 299, Training Loss: 0.02650027906840318\n",
      "Epoch: 2 - Batch: 300, Training Loss: 0.02658692027008158\n",
      "Epoch: 2 - Batch: 301, Training Loss: 0.026667996487313043\n",
      "Epoch: 2 - Batch: 302, Training Loss: 0.026760625589645128\n",
      "Epoch: 2 - Batch: 303, Training Loss: 0.026846148611152944\n",
      "Epoch: 2 - Batch: 304, Training Loss: 0.02693529889160523\n",
      "Epoch: 2 - Batch: 305, Training Loss: 0.027019190279207815\n",
      "Epoch: 2 - Batch: 306, Training Loss: 0.027099431654558845\n",
      "Epoch: 2 - Batch: 307, Training Loss: 0.027189330848097604\n",
      "Epoch: 2 - Batch: 308, Training Loss: 0.02727225792299258\n",
      "Epoch: 2 - Batch: 309, Training Loss: 0.027363023422596663\n",
      "Epoch: 2 - Batch: 310, Training Loss: 0.02745246183541086\n",
      "Epoch: 2 - Batch: 311, Training Loss: 0.02754231546673411\n",
      "Epoch: 2 - Batch: 312, Training Loss: 0.027624532895449975\n",
      "Epoch: 2 - Batch: 313, Training Loss: 0.02771724683308285\n",
      "Epoch: 2 - Batch: 314, Training Loss: 0.027798125844689745\n",
      "Epoch: 2 - Batch: 315, Training Loss: 0.027887135935610603\n",
      "Epoch: 2 - Batch: 316, Training Loss: 0.027970481719543683\n",
      "Epoch: 2 - Batch: 317, Training Loss: 0.0280521732830685\n",
      "Epoch: 2 - Batch: 318, Training Loss: 0.028141339168965718\n",
      "Epoch: 2 - Batch: 319, Training Loss: 0.028232870552423187\n",
      "Epoch: 2 - Batch: 320, Training Loss: 0.028318954276455376\n",
      "Epoch: 2 - Batch: 321, Training Loss: 0.02840408931175868\n",
      "Epoch: 2 - Batch: 322, Training Loss: 0.028491321302468505\n",
      "Epoch: 2 - Batch: 323, Training Loss: 0.028580856857015127\n",
      "Epoch: 2 - Batch: 324, Training Loss: 0.02866404633938179\n",
      "Epoch: 2 - Batch: 325, Training Loss: 0.028749873926538733\n",
      "Epoch: 2 - Batch: 326, Training Loss: 0.02883055924158389\n",
      "Epoch: 2 - Batch: 327, Training Loss: 0.028922009465607443\n",
      "Epoch: 2 - Batch: 328, Training Loss: 0.029009627453229124\n",
      "Epoch: 2 - Batch: 329, Training Loss: 0.029095668889347396\n",
      "Epoch: 2 - Batch: 330, Training Loss: 0.029186248136792412\n",
      "Epoch: 2 - Batch: 331, Training Loss: 0.02927969289236203\n",
      "Epoch: 2 - Batch: 332, Training Loss: 0.029369908198096463\n",
      "Epoch: 2 - Batch: 333, Training Loss: 0.02945482992191813\n",
      "Epoch: 2 - Batch: 334, Training Loss: 0.029535454958924408\n",
      "Epoch: 2 - Batch: 335, Training Loss: 0.029632518373763385\n",
      "Epoch: 2 - Batch: 336, Training Loss: 0.029718259031302104\n",
      "Epoch: 2 - Batch: 337, Training Loss: 0.029800268616002195\n",
      "Epoch: 2 - Batch: 338, Training Loss: 0.029892548634539395\n",
      "Epoch: 2 - Batch: 339, Training Loss: 0.02997289350253235\n",
      "Epoch: 2 - Batch: 340, Training Loss: 0.03005334237593521\n",
      "Epoch: 2 - Batch: 341, Training Loss: 0.030139539736766325\n",
      "Epoch: 2 - Batch: 342, Training Loss: 0.03022011128527608\n",
      "Epoch: 2 - Batch: 343, Training Loss: 0.030299199451607455\n",
      "Epoch: 2 - Batch: 344, Training Loss: 0.0303876935620213\n",
      "Epoch: 2 - Batch: 345, Training Loss: 0.030471477496930417\n",
      "Epoch: 2 - Batch: 346, Training Loss: 0.03056503278797934\n",
      "Epoch: 2 - Batch: 347, Training Loss: 0.030639913316142697\n",
      "Epoch: 2 - Batch: 348, Training Loss: 0.030723333469967345\n",
      "Epoch: 2 - Batch: 349, Training Loss: 0.03080705361159682\n",
      "Epoch: 2 - Batch: 350, Training Loss: 0.03088712875127397\n",
      "Epoch: 2 - Batch: 351, Training Loss: 0.030981568968661784\n",
      "Epoch: 2 - Batch: 352, Training Loss: 0.031071816819718427\n",
      "Epoch: 2 - Batch: 353, Training Loss: 0.031154631413086928\n",
      "Epoch: 2 - Batch: 354, Training Loss: 0.03124863417760452\n",
      "Epoch: 2 - Batch: 355, Training Loss: 0.03134317745517933\n",
      "Epoch: 2 - Batch: 356, Training Loss: 0.03144130853945343\n",
      "Epoch: 2 - Batch: 357, Training Loss: 0.03153352845911165\n",
      "Epoch: 2 - Batch: 358, Training Loss: 0.03162127024004511\n",
      "Epoch: 2 - Batch: 359, Training Loss: 0.031706195855961114\n",
      "Epoch: 2 - Batch: 360, Training Loss: 0.03179597380149424\n",
      "Epoch: 2 - Batch: 361, Training Loss: 0.03187893225408312\n",
      "Epoch: 2 - Batch: 362, Training Loss: 0.03196449648236754\n",
      "Epoch: 2 - Batch: 363, Training Loss: 0.03205559677004221\n",
      "Epoch: 2 - Batch: 364, Training Loss: 0.03214974978175725\n",
      "Epoch: 2 - Batch: 365, Training Loss: 0.03223667731184272\n",
      "Epoch: 2 - Batch: 366, Training Loss: 0.032321243458867666\n",
      "Epoch: 2 - Batch: 367, Training Loss: 0.03240386674653238\n",
      "Epoch: 2 - Batch: 368, Training Loss: 0.03248514021881184\n",
      "Epoch: 2 - Batch: 369, Training Loss: 0.03256878384063098\n",
      "Epoch: 2 - Batch: 370, Training Loss: 0.032654215614149225\n",
      "Epoch: 2 - Batch: 371, Training Loss: 0.032746985560873056\n",
      "Epoch: 2 - Batch: 372, Training Loss: 0.0328369798883178\n",
      "Epoch: 2 - Batch: 373, Training Loss: 0.03292436651255361\n",
      "Epoch: 2 - Batch: 374, Training Loss: 0.033013839299356564\n",
      "Epoch: 2 - Batch: 375, Training Loss: 0.03309510033546791\n",
      "Epoch: 2 - Batch: 376, Training Loss: 0.033176469979545174\n",
      "Epoch: 2 - Batch: 377, Training Loss: 0.03326720832097985\n",
      "Epoch: 2 - Batch: 378, Training Loss: 0.03335527260826397\n",
      "Epoch: 2 - Batch: 379, Training Loss: 0.03344237643786726\n",
      "Epoch: 2 - Batch: 380, Training Loss: 0.03352026861920879\n",
      "Epoch: 2 - Batch: 381, Training Loss: 0.03360313822084399\n",
      "Epoch: 2 - Batch: 382, Training Loss: 0.03368764004946546\n",
      "Epoch: 2 - Batch: 383, Training Loss: 0.0337679963836919\n",
      "Epoch: 2 - Batch: 384, Training Loss: 0.033852401855177745\n",
      "Epoch: 2 - Batch: 385, Training Loss: 0.033940110114676444\n",
      "Epoch: 2 - Batch: 386, Training Loss: 0.03402149604994859\n",
      "Epoch: 2 - Batch: 387, Training Loss: 0.03411247133343769\n",
      "Epoch: 2 - Batch: 388, Training Loss: 0.03419874548590796\n",
      "Epoch: 2 - Batch: 389, Training Loss: 0.03429418240069947\n",
      "Epoch: 2 - Batch: 390, Training Loss: 0.03438530913608189\n",
      "Epoch: 2 - Batch: 391, Training Loss: 0.03446830036875423\n",
      "Epoch: 2 - Batch: 392, Training Loss: 0.034555566171261404\n",
      "Epoch: 2 - Batch: 393, Training Loss: 0.03464647197159962\n",
      "Epoch: 2 - Batch: 394, Training Loss: 0.03472996718724371\n",
      "Epoch: 2 - Batch: 395, Training Loss: 0.034815991325164906\n",
      "Epoch: 2 - Batch: 396, Training Loss: 0.03490282473452451\n",
      "Epoch: 2 - Batch: 397, Training Loss: 0.035002337116902545\n",
      "Epoch: 2 - Batch: 398, Training Loss: 0.035094761766901066\n",
      "Epoch: 2 - Batch: 399, Training Loss: 0.03517932756178414\n",
      "Epoch: 2 - Batch: 400, Training Loss: 0.0352689724471142\n",
      "Epoch: 2 - Batch: 401, Training Loss: 0.035359820249068794\n",
      "Epoch: 2 - Batch: 402, Training Loss: 0.03544332169824176\n",
      "Epoch: 2 - Batch: 403, Training Loss: 0.03552685224545338\n",
      "Epoch: 2 - Batch: 404, Training Loss: 0.03561382923602662\n",
      "Epoch: 2 - Batch: 405, Training Loss: 0.035698656289286874\n",
      "Epoch: 2 - Batch: 406, Training Loss: 0.03578544353099407\n",
      "Epoch: 2 - Batch: 407, Training Loss: 0.0358663703782938\n",
      "Epoch: 2 - Batch: 408, Training Loss: 0.0359542630118616\n",
      "Epoch: 2 - Batch: 409, Training Loss: 0.036062468794694984\n",
      "Epoch: 2 - Batch: 410, Training Loss: 0.036152407008023996\n",
      "Epoch: 2 - Batch: 411, Training Loss: 0.036238428928069215\n",
      "Epoch: 2 - Batch: 412, Training Loss: 0.036330010253892805\n",
      "Epoch: 2 - Batch: 413, Training Loss: 0.0364121544556337\n",
      "Epoch: 2 - Batch: 414, Training Loss: 0.036504326440752244\n",
      "Epoch: 2 - Batch: 415, Training Loss: 0.03658705030507709\n",
      "Epoch: 2 - Batch: 416, Training Loss: 0.03667329139327924\n",
      "Epoch: 2 - Batch: 417, Training Loss: 0.03676105459308743\n",
      "Epoch: 2 - Batch: 418, Training Loss: 0.03684203909280684\n",
      "Epoch: 2 - Batch: 419, Training Loss: 0.03693310846613217\n",
      "Epoch: 2 - Batch: 420, Training Loss: 0.03702558116187306\n",
      "Epoch: 2 - Batch: 421, Training Loss: 0.037103865338894065\n",
      "Epoch: 2 - Batch: 422, Training Loss: 0.037187603415047156\n",
      "Epoch: 2 - Batch: 423, Training Loss: 0.037267863305646985\n",
      "Epoch: 2 - Batch: 424, Training Loss: 0.03735545425865781\n",
      "Epoch: 2 - Batch: 425, Training Loss: 0.03744026222829993\n",
      "Epoch: 2 - Batch: 426, Training Loss: 0.037527499068682864\n",
      "Epoch: 2 - Batch: 427, Training Loss: 0.03761641715790699\n",
      "Epoch: 2 - Batch: 428, Training Loss: 0.03770195349902656\n",
      "Epoch: 2 - Batch: 429, Training Loss: 0.03778890398011279\n",
      "Epoch: 2 - Batch: 430, Training Loss: 0.03786682660010324\n",
      "Epoch: 2 - Batch: 431, Training Loss: 0.03796154442884238\n",
      "Epoch: 2 - Batch: 432, Training Loss: 0.038047142061180934\n",
      "Epoch: 2 - Batch: 433, Training Loss: 0.03813311820278318\n",
      "Epoch: 2 - Batch: 434, Training Loss: 0.03822131151608369\n",
      "Epoch: 2 - Batch: 435, Training Loss: 0.03831235389186573\n",
      "Epoch: 2 - Batch: 436, Training Loss: 0.03839894119730439\n",
      "Epoch: 2 - Batch: 437, Training Loss: 0.03848982428140308\n",
      "Epoch: 2 - Batch: 438, Training Loss: 0.03857825220767934\n",
      "Epoch: 2 - Batch: 439, Training Loss: 0.038668387824326605\n",
      "Epoch: 2 - Batch: 440, Training Loss: 0.0387591070018301\n",
      "Epoch: 2 - Batch: 441, Training Loss: 0.03885748954628831\n",
      "Epoch: 2 - Batch: 442, Training Loss: 0.0389393851852931\n",
      "Epoch: 2 - Batch: 443, Training Loss: 0.03902183509559971\n",
      "Epoch: 2 - Batch: 444, Training Loss: 0.03910111694613697\n",
      "Epoch: 2 - Batch: 445, Training Loss: 0.0391881919680642\n",
      "Epoch: 2 - Batch: 446, Training Loss: 0.03927909872598118\n",
      "Epoch: 2 - Batch: 447, Training Loss: 0.03936924903374011\n",
      "Epoch: 2 - Batch: 448, Training Loss: 0.03945033086052977\n",
      "Epoch: 2 - Batch: 449, Training Loss: 0.03952820059128266\n",
      "Epoch: 2 - Batch: 450, Training Loss: 0.03961637706973067\n",
      "Epoch: 2 - Batch: 451, Training Loss: 0.03970696064790288\n",
      "Epoch: 2 - Batch: 452, Training Loss: 0.03979775983574576\n",
      "Epoch: 2 - Batch: 453, Training Loss: 0.039880347365565956\n",
      "Epoch: 2 - Batch: 454, Training Loss: 0.039970087631919095\n",
      "Epoch: 2 - Batch: 455, Training Loss: 0.04006456539875042\n",
      "Epoch: 2 - Batch: 456, Training Loss: 0.0401487115887959\n",
      "Epoch: 2 - Batch: 457, Training Loss: 0.04023670604696519\n",
      "Epoch: 2 - Batch: 458, Training Loss: 0.04032728864902485\n",
      "Epoch: 2 - Batch: 459, Training Loss: 0.04041773277283901\n",
      "Epoch: 2 - Batch: 460, Training Loss: 0.0404996537042791\n",
      "Epoch: 2 - Batch: 461, Training Loss: 0.04058802024048953\n",
      "Epoch: 2 - Batch: 462, Training Loss: 0.0406634688661565\n",
      "Epoch: 2 - Batch: 463, Training Loss: 0.0407460047546805\n",
      "Epoch: 2 - Batch: 464, Training Loss: 0.04083959100950219\n",
      "Epoch: 2 - Batch: 465, Training Loss: 0.04092330435541139\n",
      "Epoch: 2 - Batch: 466, Training Loss: 0.04101502168830948\n",
      "Epoch: 2 - Batch: 467, Training Loss: 0.041095016249277895\n",
      "Epoch: 2 - Batch: 468, Training Loss: 0.041184280067682266\n",
      "Epoch: 2 - Batch: 469, Training Loss: 0.04127087160500128\n",
      "Epoch: 2 - Batch: 470, Training Loss: 0.04135613796451001\n",
      "Epoch: 2 - Batch: 471, Training Loss: 0.0414427514458276\n",
      "Epoch: 2 - Batch: 472, Training Loss: 0.04153257840703771\n",
      "Epoch: 2 - Batch: 473, Training Loss: 0.04161875606645794\n",
      "Epoch: 2 - Batch: 474, Training Loss: 0.041703279005057776\n",
      "Epoch: 2 - Batch: 475, Training Loss: 0.04179166778798522\n",
      "Epoch: 2 - Batch: 476, Training Loss: 0.04186607462355549\n",
      "Epoch: 2 - Batch: 477, Training Loss: 0.04195360903617359\n",
      "Epoch: 2 - Batch: 478, Training Loss: 0.0420424973601429\n",
      "Epoch: 2 - Batch: 479, Training Loss: 0.04212417030957208\n",
      "Epoch: 2 - Batch: 480, Training Loss: 0.04221662544146501\n",
      "Epoch: 2 - Batch: 481, Training Loss: 0.042289240949881415\n",
      "Epoch: 2 - Batch: 482, Training Loss: 0.04237150049970716\n",
      "Epoch: 2 - Batch: 483, Training Loss: 0.042448437752612984\n",
      "Epoch: 2 - Batch: 484, Training Loss: 0.04253061741565788\n",
      "Epoch: 2 - Batch: 485, Training Loss: 0.042618635244927004\n",
      "Epoch: 2 - Batch: 486, Training Loss: 0.042715025512782695\n",
      "Epoch: 2 - Batch: 487, Training Loss: 0.042809313932313256\n",
      "Epoch: 2 - Batch: 488, Training Loss: 0.042906434783344445\n",
      "Epoch: 2 - Batch: 489, Training Loss: 0.04298850198339665\n",
      "Epoch: 2 - Batch: 490, Training Loss: 0.043068233624125395\n",
      "Epoch: 2 - Batch: 491, Training Loss: 0.0431566436961911\n",
      "Epoch: 2 - Batch: 492, Training Loss: 0.04324917294468057\n",
      "Epoch: 2 - Batch: 493, Training Loss: 0.04333688538663621\n",
      "Epoch: 2 - Batch: 494, Training Loss: 0.043424305223410405\n",
      "Epoch: 2 - Batch: 495, Training Loss: 0.0435130406127838\n",
      "Epoch: 2 - Batch: 496, Training Loss: 0.04359685883519068\n",
      "Epoch: 2 - Batch: 497, Training Loss: 0.04367816025641427\n",
      "Epoch: 2 - Batch: 498, Training Loss: 0.0437655321833605\n",
      "Epoch: 2 - Batch: 499, Training Loss: 0.043852844826914185\n",
      "Epoch: 2 - Batch: 500, Training Loss: 0.04394585442162469\n",
      "Epoch: 2 - Batch: 501, Training Loss: 0.04402653227991132\n",
      "Epoch: 2 - Batch: 502, Training Loss: 0.044116046594743115\n",
      "Epoch: 2 - Batch: 503, Training Loss: 0.04419163256191693\n",
      "Epoch: 2 - Batch: 504, Training Loss: 0.044287473043646185\n",
      "Epoch: 2 - Batch: 505, Training Loss: 0.044373747535902466\n",
      "Epoch: 2 - Batch: 506, Training Loss: 0.04446079751868944\n",
      "Epoch: 2 - Batch: 507, Training Loss: 0.0445459473810188\n",
      "Epoch: 2 - Batch: 508, Training Loss: 0.04464097575564092\n",
      "Epoch: 2 - Batch: 509, Training Loss: 0.044729255596994366\n",
      "Epoch: 2 - Batch: 510, Training Loss: 0.04481260953579178\n",
      "Epoch: 2 - Batch: 511, Training Loss: 0.04491650226030184\n",
      "Epoch: 2 - Batch: 512, Training Loss: 0.044998572598741224\n",
      "Epoch: 2 - Batch: 513, Training Loss: 0.04508244062759983\n",
      "Epoch: 2 - Batch: 514, Training Loss: 0.045167935403622995\n",
      "Epoch: 2 - Batch: 515, Training Loss: 0.04525361906978028\n",
      "Epoch: 2 - Batch: 516, Training Loss: 0.045345363109859066\n",
      "Epoch: 2 - Batch: 517, Training Loss: 0.04543508608419306\n",
      "Epoch: 2 - Batch: 518, Training Loss: 0.04552120211260829\n",
      "Epoch: 2 - Batch: 519, Training Loss: 0.045607329007998035\n",
      "Epoch: 2 - Batch: 520, Training Loss: 0.04569629283241965\n",
      "Epoch: 2 - Batch: 521, Training Loss: 0.04578346903644391\n",
      "Epoch: 2 - Batch: 522, Training Loss: 0.045870342973008085\n",
      "Epoch: 2 - Batch: 523, Training Loss: 0.045965375431240296\n",
      "Epoch: 2 - Batch: 524, Training Loss: 0.046057461209558136\n",
      "Epoch: 2 - Batch: 525, Training Loss: 0.04614212687681761\n",
      "Epoch: 2 - Batch: 526, Training Loss: 0.04622518286034835\n",
      "Epoch: 2 - Batch: 527, Training Loss: 0.04630747948391718\n",
      "Epoch: 2 - Batch: 528, Training Loss: 0.04639045341221452\n",
      "Epoch: 2 - Batch: 529, Training Loss: 0.04647709985895339\n",
      "Epoch: 2 - Batch: 530, Training Loss: 0.046559390835896455\n",
      "Epoch: 2 - Batch: 531, Training Loss: 0.046657573740626644\n",
      "Epoch: 2 - Batch: 532, Training Loss: 0.04673796353450858\n",
      "Epoch: 2 - Batch: 533, Training Loss: 0.046821815957922244\n",
      "Epoch: 2 - Batch: 534, Training Loss: 0.04690711465270365\n",
      "Epoch: 2 - Batch: 535, Training Loss: 0.04698398917493338\n",
      "Epoch: 2 - Batch: 536, Training Loss: 0.04706846231943734\n",
      "Epoch: 2 - Batch: 537, Training Loss: 0.04715088553491912\n",
      "Epoch: 2 - Batch: 538, Training Loss: 0.04723619596776282\n",
      "Epoch: 2 - Batch: 539, Training Loss: 0.047327494418640835\n",
      "Epoch: 2 - Batch: 540, Training Loss: 0.04741774322727624\n",
      "Epoch: 2 - Batch: 541, Training Loss: 0.0474997705796959\n",
      "Epoch: 2 - Batch: 542, Training Loss: 0.04758686071901179\n",
      "Epoch: 2 - Batch: 543, Training Loss: 0.04767582006443594\n",
      "Epoch: 2 - Batch: 544, Training Loss: 0.04776238965157846\n",
      "Epoch: 2 - Batch: 545, Training Loss: 0.04785158955932257\n",
      "Epoch: 2 - Batch: 546, Training Loss: 0.04794192837418411\n",
      "Epoch: 2 - Batch: 547, Training Loss: 0.04802401772828442\n",
      "Epoch: 2 - Batch: 548, Training Loss: 0.04810626984606335\n",
      "Epoch: 2 - Batch: 549, Training Loss: 0.04818879548540558\n",
      "Epoch: 2 - Batch: 550, Training Loss: 0.04827589376722995\n",
      "Epoch: 2 - Batch: 551, Training Loss: 0.04836477853271301\n",
      "Epoch: 2 - Batch: 552, Training Loss: 0.04845224101572092\n",
      "Epoch: 2 - Batch: 553, Training Loss: 0.048533929274054505\n",
      "Epoch: 2 - Batch: 554, Training Loss: 0.04861453321343828\n",
      "Epoch: 2 - Batch: 555, Training Loss: 0.04870365670639682\n",
      "Epoch: 2 - Batch: 556, Training Loss: 0.04879475774160072\n",
      "Epoch: 2 - Batch: 557, Training Loss: 0.04888142274065595\n",
      "Epoch: 2 - Batch: 558, Training Loss: 0.04896907981602509\n",
      "Epoch: 2 - Batch: 559, Training Loss: 0.049058217023982734\n",
      "Epoch: 2 - Batch: 560, Training Loss: 0.0491494366445055\n",
      "Epoch: 2 - Batch: 561, Training Loss: 0.049243235240874206\n",
      "Epoch: 2 - Batch: 562, Training Loss: 0.0493309532182529\n",
      "Epoch: 2 - Batch: 563, Training Loss: 0.04942055144787428\n",
      "Epoch: 2 - Batch: 564, Training Loss: 0.04951482178461097\n",
      "Epoch: 2 - Batch: 565, Training Loss: 0.049599909917088486\n",
      "Epoch: 2 - Batch: 566, Training Loss: 0.04969178671465189\n",
      "Epoch: 2 - Batch: 567, Training Loss: 0.049770085866969224\n",
      "Epoch: 2 - Batch: 568, Training Loss: 0.04985136303364934\n",
      "Epoch: 2 - Batch: 569, Training Loss: 0.04994355184126454\n",
      "Epoch: 2 - Batch: 570, Training Loss: 0.050032923143252014\n",
      "Epoch: 2 - Batch: 571, Training Loss: 0.05012148110254685\n",
      "Epoch: 2 - Batch: 572, Training Loss: 0.05020480206356713\n",
      "Epoch: 2 - Batch: 573, Training Loss: 0.05029208761073068\n",
      "Epoch: 2 - Batch: 574, Training Loss: 0.050381431130578073\n",
      "Epoch: 2 - Batch: 575, Training Loss: 0.050476103712176014\n",
      "Epoch: 2 - Batch: 576, Training Loss: 0.05055748463714894\n",
      "Epoch: 2 - Batch: 577, Training Loss: 0.05064513342173934\n",
      "Epoch: 2 - Batch: 578, Training Loss: 0.05072891735664845\n",
      "Epoch: 2 - Batch: 579, Training Loss: 0.05081534737205229\n",
      "Epoch: 2 - Batch: 580, Training Loss: 0.0508942576140709\n",
      "Epoch: 2 - Batch: 581, Training Loss: 0.050974011075239675\n",
      "Epoch: 2 - Batch: 582, Training Loss: 0.05106412161567911\n",
      "Epoch: 2 - Batch: 583, Training Loss: 0.05115837815710363\n",
      "Epoch: 2 - Batch: 584, Training Loss: 0.051246416274686755\n",
      "Epoch: 2 - Batch: 585, Training Loss: 0.05133588081395646\n",
      "Epoch: 2 - Batch: 586, Training Loss: 0.051423257720312275\n",
      "Epoch: 2 - Batch: 587, Training Loss: 0.05151325268408355\n",
      "Epoch: 2 - Batch: 588, Training Loss: 0.05159972846557449\n",
      "Epoch: 2 - Batch: 589, Training Loss: 0.05169592461059145\n",
      "Epoch: 2 - Batch: 590, Training Loss: 0.051791939396367934\n",
      "Epoch: 2 - Batch: 591, Training Loss: 0.051880706762694205\n",
      "Epoch: 2 - Batch: 592, Training Loss: 0.05196281170370567\n",
      "Epoch: 2 - Batch: 593, Training Loss: 0.05204573740710074\n",
      "Epoch: 2 - Batch: 594, Training Loss: 0.05213407102113537\n",
      "Epoch: 2 - Batch: 595, Training Loss: 0.05222604677674189\n",
      "Epoch: 2 - Batch: 596, Training Loss: 0.052316921119073136\n",
      "Epoch: 2 - Batch: 597, Training Loss: 0.052410732701445494\n",
      "Epoch: 2 - Batch: 598, Training Loss: 0.052497396909726005\n",
      "Epoch: 2 - Batch: 599, Training Loss: 0.052581849858584884\n",
      "Epoch: 2 - Batch: 600, Training Loss: 0.052665566929784385\n",
      "Epoch: 2 - Batch: 601, Training Loss: 0.052749678134226284\n",
      "Epoch: 2 - Batch: 602, Training Loss: 0.05284496855800029\n",
      "Epoch: 2 - Batch: 603, Training Loss: 0.05294121407083611\n",
      "Epoch: 2 - Batch: 604, Training Loss: 0.05302604125383284\n",
      "Epoch: 2 - Batch: 605, Training Loss: 0.05311111986612404\n",
      "Epoch: 2 - Batch: 606, Training Loss: 0.05319379641370196\n",
      "Epoch: 2 - Batch: 607, Training Loss: 0.05327764056487068\n",
      "Epoch: 2 - Batch: 608, Training Loss: 0.05337055730533046\n",
      "Epoch: 2 - Batch: 609, Training Loss: 0.053458618497472896\n",
      "Epoch: 2 - Batch: 610, Training Loss: 0.053536808782351356\n",
      "Epoch: 2 - Batch: 611, Training Loss: 0.053624613930287446\n",
      "Epoch: 2 - Batch: 612, Training Loss: 0.053711054670573466\n",
      "Epoch: 2 - Batch: 613, Training Loss: 0.05380367981903193\n",
      "Epoch: 2 - Batch: 614, Training Loss: 0.05389364198418597\n",
      "Epoch: 2 - Batch: 615, Training Loss: 0.05397810794376022\n",
      "Epoch: 2 - Batch: 616, Training Loss: 0.05405832694883568\n",
      "Epoch: 2 - Batch: 617, Training Loss: 0.054140676375151076\n",
      "Epoch: 2 - Batch: 618, Training Loss: 0.0542303665879354\n",
      "Epoch: 2 - Batch: 619, Training Loss: 0.054315001044305006\n",
      "Epoch: 2 - Batch: 620, Training Loss: 0.05439915046540659\n",
      "Epoch: 2 - Batch: 621, Training Loss: 0.0544931740618958\n",
      "Epoch: 2 - Batch: 622, Training Loss: 0.054582222258273645\n",
      "Epoch: 2 - Batch: 623, Training Loss: 0.05466499482320118\n",
      "Epoch: 2 - Batch: 624, Training Loss: 0.054753819199442666\n",
      "Epoch: 2 - Batch: 625, Training Loss: 0.05483437374011794\n",
      "Epoch: 2 - Batch: 626, Training Loss: 0.05492456739221639\n",
      "Epoch: 2 - Batch: 627, Training Loss: 0.05501157227613242\n",
      "Epoch: 2 - Batch: 628, Training Loss: 0.05510647221781919\n",
      "Epoch: 2 - Batch: 629, Training Loss: 0.05519588131018934\n",
      "Epoch: 2 - Batch: 630, Training Loss: 0.05528624711023832\n",
      "Epoch: 2 - Batch: 631, Training Loss: 0.05537709211359174\n",
      "Epoch: 2 - Batch: 632, Training Loss: 0.05546451061593359\n",
      "Epoch: 2 - Batch: 633, Training Loss: 0.055546767076795574\n",
      "Epoch: 2 - Batch: 634, Training Loss: 0.05563322473842509\n",
      "Epoch: 2 - Batch: 635, Training Loss: 0.05571749408545581\n",
      "Epoch: 2 - Batch: 636, Training Loss: 0.05580487907254083\n",
      "Epoch: 2 - Batch: 637, Training Loss: 0.055889995333122375\n",
      "Epoch: 2 - Batch: 638, Training Loss: 0.05597790245393022\n",
      "Epoch: 2 - Batch: 639, Training Loss: 0.05606816264848606\n",
      "Epoch: 2 - Batch: 640, Training Loss: 0.0561591422503465\n",
      "Epoch: 2 - Batch: 641, Training Loss: 0.056245896792283305\n",
      "Epoch: 2 - Batch: 642, Training Loss: 0.05633255888153467\n",
      "Epoch: 2 - Batch: 643, Training Loss: 0.05641757242171523\n",
      "Epoch: 2 - Batch: 644, Training Loss: 0.05650980692746034\n",
      "Epoch: 2 - Batch: 645, Training Loss: 0.05659351028983866\n",
      "Epoch: 2 - Batch: 646, Training Loss: 0.056676862109606936\n",
      "Epoch: 2 - Batch: 647, Training Loss: 0.05677098791372914\n",
      "Epoch: 2 - Batch: 648, Training Loss: 0.056847870473442585\n",
      "Epoch: 2 - Batch: 649, Training Loss: 0.05694617631893055\n",
      "Epoch: 2 - Batch: 650, Training Loss: 0.05703664777389609\n",
      "Epoch: 2 - Batch: 651, Training Loss: 0.05712516545310345\n",
      "Epoch: 2 - Batch: 652, Training Loss: 0.05721377075385692\n",
      "Epoch: 2 - Batch: 653, Training Loss: 0.05730524418217626\n",
      "Epoch: 2 - Batch: 654, Training Loss: 0.05739263225204711\n",
      "Epoch: 2 - Batch: 655, Training Loss: 0.05748356375256383\n",
      "Epoch: 2 - Batch: 656, Training Loss: 0.057572702233174546\n",
      "Epoch: 2 - Batch: 657, Training Loss: 0.05766222642033452\n",
      "Epoch: 2 - Batch: 658, Training Loss: 0.05774369504098868\n",
      "Epoch: 2 - Batch: 659, Training Loss: 0.05782617738541481\n",
      "Epoch: 2 - Batch: 660, Training Loss: 0.05791481651032149\n",
      "Epoch: 2 - Batch: 661, Training Loss: 0.058000143790373555\n",
      "Epoch: 2 - Batch: 662, Training Loss: 0.058089498554049046\n",
      "Epoch: 2 - Batch: 663, Training Loss: 0.058173465676865176\n",
      "Epoch: 2 - Batch: 664, Training Loss: 0.05825569869249219\n",
      "Epoch: 2 - Batch: 665, Training Loss: 0.058334696389252864\n",
      "Epoch: 2 - Batch: 666, Training Loss: 0.05842453757823008\n",
      "Epoch: 2 - Batch: 667, Training Loss: 0.05851588708350117\n",
      "Epoch: 2 - Batch: 668, Training Loss: 0.058593936103888807\n",
      "Epoch: 2 - Batch: 669, Training Loss: 0.05868002466523825\n",
      "Epoch: 2 - Batch: 670, Training Loss: 0.058767216882450665\n",
      "Epoch: 2 - Batch: 671, Training Loss: 0.0588523484025132\n",
      "Epoch: 2 - Batch: 672, Training Loss: 0.05893311132468394\n",
      "Epoch: 2 - Batch: 673, Training Loss: 0.05901492498876839\n",
      "Epoch: 2 - Batch: 674, Training Loss: 0.05909147199434823\n",
      "Epoch: 2 - Batch: 675, Training Loss: 0.05917622688941497\n",
      "Epoch: 2 - Batch: 676, Training Loss: 0.0592637389317575\n",
      "Epoch: 2 - Batch: 677, Training Loss: 0.059343613091096356\n",
      "Epoch: 2 - Batch: 678, Training Loss: 0.05943508629083238\n",
      "Epoch: 2 - Batch: 679, Training Loss: 0.059529271557507035\n",
      "Epoch: 2 - Batch: 680, Training Loss: 0.05961409758523131\n",
      "Epoch: 2 - Batch: 681, Training Loss: 0.05970497472616373\n",
      "Epoch: 2 - Batch: 682, Training Loss: 0.05979711459223706\n",
      "Epoch: 2 - Batch: 683, Training Loss: 0.05989184595063749\n",
      "Epoch: 2 - Batch: 684, Training Loss: 0.05998846719884754\n",
      "Epoch: 2 - Batch: 685, Training Loss: 0.06007282543389952\n",
      "Epoch: 2 - Batch: 686, Training Loss: 0.060154325290155256\n",
      "Epoch: 2 - Batch: 687, Training Loss: 0.06023684877957871\n",
      "Epoch: 2 - Batch: 688, Training Loss: 0.060327134186256785\n",
      "Epoch: 2 - Batch: 689, Training Loss: 0.06041159596955203\n",
      "Epoch: 2 - Batch: 690, Training Loss: 0.06049431021484372\n",
      "Epoch: 2 - Batch: 691, Training Loss: 0.06057557151660595\n",
      "Epoch: 2 - Batch: 692, Training Loss: 0.06066987135912451\n",
      "Epoch: 2 - Batch: 693, Training Loss: 0.06076302300910056\n",
      "Epoch: 2 - Batch: 694, Training Loss: 0.06085690799760779\n",
      "Epoch: 2 - Batch: 695, Training Loss: 0.060944936934790604\n",
      "Epoch: 2 - Batch: 696, Training Loss: 0.0610294161892649\n",
      "Epoch: 2 - Batch: 697, Training Loss: 0.061110064313186344\n",
      "Epoch: 2 - Batch: 698, Training Loss: 0.06119956150280302\n",
      "Epoch: 2 - Batch: 699, Training Loss: 0.061289657796941586\n",
      "Epoch: 2 - Batch: 700, Training Loss: 0.06137428276401452\n",
      "Epoch: 2 - Batch: 701, Training Loss: 0.06145949002546853\n",
      "Epoch: 2 - Batch: 702, Training Loss: 0.06154958775288628\n",
      "Epoch: 2 - Batch: 703, Training Loss: 0.061638889960932294\n",
      "Epoch: 2 - Batch: 704, Training Loss: 0.06172607924980706\n",
      "Epoch: 2 - Batch: 705, Training Loss: 0.061812804872865106\n",
      "Epoch: 2 - Batch: 706, Training Loss: 0.06189665796349495\n",
      "Epoch: 2 - Batch: 707, Training Loss: 0.06198107531513543\n",
      "Epoch: 2 - Batch: 708, Training Loss: 0.0620700101094756\n",
      "Epoch: 2 - Batch: 709, Training Loss: 0.0621562253554069\n",
      "Epoch: 2 - Batch: 710, Training Loss: 0.062238390179069875\n",
      "Epoch: 2 - Batch: 711, Training Loss: 0.062327691516028114\n",
      "Epoch: 2 - Batch: 712, Training Loss: 0.062416215570856685\n",
      "Epoch: 2 - Batch: 713, Training Loss: 0.06250542570924877\n",
      "Epoch: 2 - Batch: 714, Training Loss: 0.06258922094358733\n",
      "Epoch: 2 - Batch: 715, Training Loss: 0.06266870122297882\n",
      "Epoch: 2 - Batch: 716, Training Loss: 0.06275523568860333\n",
      "Epoch: 2 - Batch: 717, Training Loss: 0.0628470384560612\n",
      "Epoch: 2 - Batch: 718, Training Loss: 0.06294461554086228\n",
      "Epoch: 2 - Batch: 719, Training Loss: 0.0630260036260532\n",
      "Epoch: 2 - Batch: 720, Training Loss: 0.06311944235814348\n",
      "Epoch: 2 - Batch: 721, Training Loss: 0.06320991086005967\n",
      "Epoch: 2 - Batch: 722, Training Loss: 0.06329327215676878\n",
      "Epoch: 2 - Batch: 723, Training Loss: 0.06337723610413015\n",
      "Epoch: 2 - Batch: 724, Training Loss: 0.06347200550003036\n",
      "Epoch: 2 - Batch: 725, Training Loss: 0.0635567877077147\n",
      "Epoch: 2 - Batch: 726, Training Loss: 0.06364016543153309\n",
      "Epoch: 2 - Batch: 727, Training Loss: 0.06372556071821137\n",
      "Epoch: 2 - Batch: 728, Training Loss: 0.06380613666540552\n",
      "Epoch: 2 - Batch: 729, Training Loss: 0.06389709488545879\n",
      "Epoch: 2 - Batch: 730, Training Loss: 0.0639834916327229\n",
      "Epoch: 2 - Batch: 731, Training Loss: 0.06406259996380972\n",
      "Epoch: 2 - Batch: 732, Training Loss: 0.0641510215453544\n",
      "Epoch: 2 - Batch: 733, Training Loss: 0.06423880397087306\n",
      "Epoch: 2 - Batch: 734, Training Loss: 0.06433338959814107\n",
      "Epoch: 2 - Batch: 735, Training Loss: 0.06442267164143163\n",
      "Epoch: 2 - Batch: 736, Training Loss: 0.06450960875966062\n",
      "Epoch: 2 - Batch: 737, Training Loss: 0.06459606852796343\n",
      "Epoch: 2 - Batch: 738, Training Loss: 0.06468630475833839\n",
      "Epoch: 2 - Batch: 739, Training Loss: 0.06477063587228853\n",
      "Epoch: 2 - Batch: 740, Training Loss: 0.06485311948936773\n",
      "Epoch: 2 - Batch: 741, Training Loss: 0.0649357156991761\n",
      "Epoch: 2 - Batch: 742, Training Loss: 0.06501324933831569\n",
      "Epoch: 2 - Batch: 743, Training Loss: 0.06509981484184811\n",
      "Epoch: 2 - Batch: 744, Training Loss: 0.06518162292108606\n",
      "Epoch: 2 - Batch: 745, Training Loss: 0.06526507233975341\n",
      "Epoch: 2 - Batch: 746, Training Loss: 0.06535647009463848\n",
      "Epoch: 2 - Batch: 747, Training Loss: 0.06544975383488298\n",
      "Epoch: 2 - Batch: 748, Training Loss: 0.06553763348244711\n",
      "Epoch: 2 - Batch: 749, Training Loss: 0.06562762349152051\n",
      "Epoch: 2 - Batch: 750, Training Loss: 0.0657162037037103\n",
      "Epoch: 2 - Batch: 751, Training Loss: 0.06579891469323418\n",
      "Epoch: 2 - Batch: 752, Training Loss: 0.06589382493352614\n",
      "Epoch: 2 - Batch: 753, Training Loss: 0.0659733595910357\n",
      "Epoch: 2 - Batch: 754, Training Loss: 0.06606369234535034\n",
      "Epoch: 2 - Batch: 755, Training Loss: 0.0661518738217417\n",
      "Epoch: 2 - Batch: 756, Training Loss: 0.06624087393530961\n",
      "Epoch: 2 - Batch: 757, Training Loss: 0.06632594178565106\n",
      "Epoch: 2 - Batch: 758, Training Loss: 0.06641728821431424\n",
      "Epoch: 2 - Batch: 759, Training Loss: 0.06650346957431306\n",
      "Epoch: 2 - Batch: 760, Training Loss: 0.0665869292730518\n",
      "Epoch: 2 - Batch: 761, Training Loss: 0.06667656022715529\n",
      "Epoch: 2 - Batch: 762, Training Loss: 0.06676877292481623\n",
      "Epoch: 2 - Batch: 763, Training Loss: 0.06686088281558521\n",
      "Epoch: 2 - Batch: 764, Training Loss: 0.06695300032991675\n",
      "Epoch: 2 - Batch: 765, Training Loss: 0.06704011876414072\n",
      "Epoch: 2 - Batch: 766, Training Loss: 0.06711886925113142\n",
      "Epoch: 2 - Batch: 767, Training Loss: 0.06720490493559916\n",
      "Epoch: 2 - Batch: 768, Training Loss: 0.06728410644218893\n",
      "Epoch: 2 - Batch: 769, Training Loss: 0.06736145909894165\n",
      "Epoch: 2 - Batch: 770, Training Loss: 0.0674482441536625\n",
      "Epoch: 2 - Batch: 771, Training Loss: 0.06753107918979319\n",
      "Epoch: 2 - Batch: 772, Training Loss: 0.06761046445265931\n",
      "Epoch: 2 - Batch: 773, Training Loss: 0.0676953994219576\n",
      "Epoch: 2 - Batch: 774, Training Loss: 0.06778519651288813\n",
      "Epoch: 2 - Batch: 775, Training Loss: 0.06787342150042307\n",
      "Epoch: 2 - Batch: 776, Training Loss: 0.06795935514406186\n",
      "Epoch: 2 - Batch: 777, Training Loss: 0.0680419822558637\n",
      "Epoch: 2 - Batch: 778, Training Loss: 0.06813815089056939\n",
      "Epoch: 2 - Batch: 779, Training Loss: 0.06821877560014551\n",
      "Epoch: 2 - Batch: 780, Training Loss: 0.06831223703611945\n",
      "Epoch: 2 - Batch: 781, Training Loss: 0.06840412674555138\n",
      "Epoch: 2 - Batch: 782, Training Loss: 0.06850050163654546\n",
      "Epoch: 2 - Batch: 783, Training Loss: 0.06859036585683649\n",
      "Epoch: 2 - Batch: 784, Training Loss: 0.06868277173211325\n",
      "Epoch: 2 - Batch: 785, Training Loss: 0.06877221930061604\n",
      "Epoch: 2 - Batch: 786, Training Loss: 0.06885539231262792\n",
      "Epoch: 2 - Batch: 787, Training Loss: 0.06894716771186683\n",
      "Epoch: 2 - Batch: 788, Training Loss: 0.06903075053670117\n",
      "Epoch: 2 - Batch: 789, Training Loss: 0.06912936700220725\n",
      "Epoch: 2 - Batch: 790, Training Loss: 0.06921732782773908\n",
      "Epoch: 2 - Batch: 791, Training Loss: 0.0693013452883087\n",
      "Epoch: 2 - Batch: 792, Training Loss: 0.06938394514927224\n",
      "Epoch: 2 - Batch: 793, Training Loss: 0.0694672144875301\n",
      "Epoch: 2 - Batch: 794, Training Loss: 0.06955662131260086\n",
      "Epoch: 2 - Batch: 795, Training Loss: 0.06964671261497397\n",
      "Epoch: 2 - Batch: 796, Training Loss: 0.06972964556507803\n",
      "Epoch: 2 - Batch: 797, Training Loss: 0.06982078139436976\n",
      "Epoch: 2 - Batch: 798, Training Loss: 0.06990944063460847\n",
      "Epoch: 2 - Batch: 799, Training Loss: 0.06999138323821831\n",
      "Epoch: 2 - Batch: 800, Training Loss: 0.07007661302439609\n",
      "Epoch: 2 - Batch: 801, Training Loss: 0.07016113143457504\n",
      "Epoch: 2 - Batch: 802, Training Loss: 0.07024484167916463\n",
      "Epoch: 2 - Batch: 803, Training Loss: 0.07033723194899646\n",
      "Epoch: 2 - Batch: 804, Training Loss: 0.07043185532340165\n",
      "Epoch: 2 - Batch: 805, Training Loss: 0.07051881686297816\n",
      "Epoch: 2 - Batch: 806, Training Loss: 0.07060819576999441\n",
      "Epoch: 2 - Batch: 807, Training Loss: 0.07069804167890825\n",
      "Epoch: 2 - Batch: 808, Training Loss: 0.07078658543514771\n",
      "Epoch: 2 - Batch: 809, Training Loss: 0.07087347303110964\n",
      "Epoch: 2 - Batch: 810, Training Loss: 0.07095715589512443\n",
      "Epoch: 2 - Batch: 811, Training Loss: 0.0710426061900694\n",
      "Epoch: 2 - Batch: 812, Training Loss: 0.07112589797273797\n",
      "Epoch: 2 - Batch: 813, Training Loss: 0.07120502278035751\n",
      "Epoch: 2 - Batch: 814, Training Loss: 0.07128734577749894\n",
      "Epoch: 2 - Batch: 815, Training Loss: 0.07136583740314836\n",
      "Epoch: 2 - Batch: 816, Training Loss: 0.07145096443803552\n",
      "Epoch: 2 - Batch: 817, Training Loss: 0.07153253596442849\n",
      "Epoch: 2 - Batch: 818, Training Loss: 0.07162734405886673\n",
      "Epoch: 2 - Batch: 819, Training Loss: 0.0717194818244447\n",
      "Epoch: 2 - Batch: 820, Training Loss: 0.07180374182427107\n",
      "Epoch: 2 - Batch: 821, Training Loss: 0.07190092265062269\n",
      "Epoch: 2 - Batch: 822, Training Loss: 0.07198352815203406\n",
      "Epoch: 2 - Batch: 823, Training Loss: 0.07207012157362097\n",
      "Epoch: 2 - Batch: 824, Training Loss: 0.07215819649929629\n",
      "Epoch: 2 - Batch: 825, Training Loss: 0.07223875850908594\n",
      "Epoch: 2 - Batch: 826, Training Loss: 0.07232621107652017\n",
      "Epoch: 2 - Batch: 827, Training Loss: 0.07241460895335694\n",
      "Epoch: 2 - Batch: 828, Training Loss: 0.07249570189423822\n",
      "Epoch: 2 - Batch: 829, Training Loss: 0.0725867054046782\n",
      "Epoch: 2 - Batch: 830, Training Loss: 0.072675129191743\n",
      "Epoch: 2 - Batch: 831, Training Loss: 0.07275254232422827\n",
      "Epoch: 2 - Batch: 832, Training Loss: 0.07284801657223583\n",
      "Epoch: 2 - Batch: 833, Training Loss: 0.07295021019987206\n",
      "Epoch: 2 - Batch: 834, Training Loss: 0.07303710914468686\n",
      "Epoch: 2 - Batch: 835, Training Loss: 0.07313010733494316\n",
      "Epoch: 2 - Batch: 836, Training Loss: 0.07321201325401935\n",
      "Epoch: 2 - Batch: 837, Training Loss: 0.07330075017551878\n",
      "Epoch: 2 - Batch: 838, Training Loss: 0.07339196068359845\n",
      "Epoch: 2 - Batch: 839, Training Loss: 0.07347871102454452\n",
      "Epoch: 2 - Batch: 840, Training Loss: 0.07356082295896996\n",
      "Epoch: 2 - Batch: 841, Training Loss: 0.07364849712866456\n",
      "Epoch: 2 - Batch: 842, Training Loss: 0.07373325934457542\n",
      "Epoch: 2 - Batch: 843, Training Loss: 0.07381658343162703\n",
      "Epoch: 2 - Batch: 844, Training Loss: 0.0739038488511027\n",
      "Epoch: 2 - Batch: 845, Training Loss: 0.07398417682246387\n",
      "Epoch: 2 - Batch: 846, Training Loss: 0.07407273978588\n",
      "Epoch: 2 - Batch: 847, Training Loss: 0.0741564523471924\n",
      "Epoch: 2 - Batch: 848, Training Loss: 0.07424614166417723\n",
      "Epoch: 2 - Batch: 849, Training Loss: 0.07434055065831932\n",
      "Epoch: 2 - Batch: 850, Training Loss: 0.07443009979812858\n",
      "Epoch: 2 - Batch: 851, Training Loss: 0.07451414737609488\n",
      "Epoch: 2 - Batch: 852, Training Loss: 0.07460263161418054\n",
      "Epoch: 2 - Batch: 853, Training Loss: 0.07469421707303765\n",
      "Epoch: 2 - Batch: 854, Training Loss: 0.07478377325568429\n",
      "Epoch: 2 - Batch: 855, Training Loss: 0.07487282019176492\n",
      "Epoch: 2 - Batch: 856, Training Loss: 0.07495302170094961\n",
      "Epoch: 2 - Batch: 857, Training Loss: 0.07503335578228111\n",
      "Epoch: 2 - Batch: 858, Training Loss: 0.07511601924500853\n",
      "Epoch: 2 - Batch: 859, Training Loss: 0.07520792460313089\n",
      "Epoch: 2 - Batch: 860, Training Loss: 0.07531051831953166\n",
      "Epoch: 2 - Batch: 861, Training Loss: 0.0753945231277057\n",
      "Epoch: 2 - Batch: 862, Training Loss: 0.07548543095440413\n",
      "Epoch: 2 - Batch: 863, Training Loss: 0.07557646454517324\n",
      "Epoch: 2 - Batch: 864, Training Loss: 0.07566137214625257\n",
      "Epoch: 2 - Batch: 865, Training Loss: 0.0757493650661179\n",
      "Epoch: 2 - Batch: 866, Training Loss: 0.07583357267094093\n",
      "Epoch: 2 - Batch: 867, Training Loss: 0.07592317201258926\n",
      "Epoch: 2 - Batch: 868, Training Loss: 0.07600384720201121\n",
      "Epoch: 2 - Batch: 869, Training Loss: 0.07609606229670803\n",
      "Epoch: 2 - Batch: 870, Training Loss: 0.0761833987252827\n",
      "Epoch: 2 - Batch: 871, Training Loss: 0.07626514482137378\n",
      "Epoch: 2 - Batch: 872, Training Loss: 0.07635130665912161\n",
      "Epoch: 2 - Batch: 873, Training Loss: 0.0764332184101614\n",
      "Epoch: 2 - Batch: 874, Training Loss: 0.07652420362158004\n",
      "Epoch: 2 - Batch: 875, Training Loss: 0.07660815744949613\n",
      "Epoch: 2 - Batch: 876, Training Loss: 0.07669571186006563\n",
      "Epoch: 2 - Batch: 877, Training Loss: 0.07677944979412639\n",
      "Epoch: 2 - Batch: 878, Training Loss: 0.07686884561013028\n",
      "Epoch: 2 - Batch: 879, Training Loss: 0.07694031210433983\n",
      "Epoch: 2 - Batch: 880, Training Loss: 0.07703322696670964\n",
      "Epoch: 2 - Batch: 881, Training Loss: 0.07712082091601531\n",
      "Epoch: 2 - Batch: 882, Training Loss: 0.07720212445009011\n",
      "Epoch: 2 - Batch: 883, Training Loss: 0.07729077680811755\n",
      "Epoch: 2 - Batch: 884, Training Loss: 0.077380053235672\n",
      "Epoch: 2 - Batch: 885, Training Loss: 0.0774588537700536\n",
      "Epoch: 2 - Batch: 886, Training Loss: 0.07755428429686806\n",
      "Epoch: 2 - Batch: 887, Training Loss: 0.07765071723566917\n",
      "Epoch: 2 - Batch: 888, Training Loss: 0.07773580137720551\n",
      "Epoch: 2 - Batch: 889, Training Loss: 0.07782245952493912\n",
      "Epoch: 2 - Batch: 890, Training Loss: 0.07790558265636414\n",
      "Epoch: 2 - Batch: 891, Training Loss: 0.07800082743760958\n",
      "Epoch: 2 - Batch: 892, Training Loss: 0.07807923325184565\n",
      "Epoch: 2 - Batch: 893, Training Loss: 0.07815782568891645\n",
      "Epoch: 2 - Batch: 894, Training Loss: 0.0782428128740582\n",
      "Epoch: 2 - Batch: 895, Training Loss: 0.07831921792939725\n",
      "Epoch: 2 - Batch: 896, Training Loss: 0.07840273874637302\n",
      "Epoch: 2 - Batch: 897, Training Loss: 0.07848796216928544\n",
      "Epoch: 2 - Batch: 898, Training Loss: 0.0785681071133657\n",
      "Epoch: 2 - Batch: 899, Training Loss: 0.07864900987292602\n",
      "Epoch: 2 - Batch: 900, Training Loss: 0.07873240571660585\n",
      "Epoch: 2 - Batch: 901, Training Loss: 0.0788192047519767\n",
      "Epoch: 2 - Batch: 902, Training Loss: 0.07890096121463017\n",
      "Epoch: 2 - Batch: 903, Training Loss: 0.0789839392265376\n",
      "Epoch: 2 - Batch: 904, Training Loss: 0.07907356128442544\n",
      "Epoch: 2 - Batch: 905, Training Loss: 0.07915629796794992\n",
      "Epoch: 2 - Batch: 906, Training Loss: 0.07924500073786596\n",
      "Epoch: 2 - Batch: 907, Training Loss: 0.07933692944237644\n",
      "Epoch: 2 - Batch: 908, Training Loss: 0.07942073207169426\n",
      "Epoch: 2 - Batch: 909, Training Loss: 0.07950426877829961\n",
      "Epoch: 2 - Batch: 910, Training Loss: 0.07958480030619486\n",
      "Epoch: 2 - Batch: 911, Training Loss: 0.07967292515570251\n",
      "Epoch: 2 - Batch: 912, Training Loss: 0.07975453142058198\n",
      "Epoch: 2 - Batch: 913, Training Loss: 0.07985502735991186\n",
      "Epoch: 2 - Batch: 914, Training Loss: 0.07994264235123869\n",
      "Epoch: 2 - Batch: 915, Training Loss: 0.08003179780879424\n",
      "Epoch: 2 - Batch: 916, Training Loss: 0.08011372556684425\n",
      "Epoch: 2 - Batch: 917, Training Loss: 0.08020459169253188\n",
      "Epoch: 2 - Batch: 918, Training Loss: 0.0802957930325671\n",
      "Epoch: 2 - Batch: 919, Training Loss: 0.08038939721573447\n",
      "Epoch: 2 - Batch: 920, Training Loss: 0.08047516474108\n",
      "Epoch: 2 - Batch: 921, Training Loss: 0.08056240339777362\n",
      "Epoch: 2 - Batch: 922, Training Loss: 0.08065724115861984\n",
      "Epoch: 2 - Batch: 923, Training Loss: 0.08074504405779032\n",
      "Epoch: 2 - Batch: 924, Training Loss: 0.08082406911659201\n",
      "Epoch: 2 - Batch: 925, Training Loss: 0.08090841524908396\n",
      "Epoch: 2 - Batch: 926, Training Loss: 0.08099173263926214\n",
      "Epoch: 2 - Batch: 927, Training Loss: 0.08107416819206517\n",
      "Epoch: 2 - Batch: 928, Training Loss: 0.08116025473941024\n",
      "Epoch: 2 - Batch: 929, Training Loss: 0.08124797837577057\n",
      "Epoch: 2 - Batch: 930, Training Loss: 0.0813366240245687\n",
      "Epoch: 2 - Batch: 931, Training Loss: 0.08142302542381223\n",
      "Epoch: 2 - Batch: 932, Training Loss: 0.0815088722490355\n",
      "Epoch: 2 - Batch: 933, Training Loss: 0.08160518378538278\n",
      "Epoch: 2 - Batch: 934, Training Loss: 0.08169342591048868\n",
      "Epoch: 2 - Batch: 935, Training Loss: 0.08178891236608103\n",
      "Epoch: 2 - Batch: 936, Training Loss: 0.08187996537407635\n",
      "Epoch: 2 - Batch: 937, Training Loss: 0.08196824228091422\n",
      "Epoch: 2 - Batch: 938, Training Loss: 0.08205879168896928\n",
      "Epoch: 2 - Batch: 939, Training Loss: 0.08214941778363873\n",
      "Epoch: 2 - Batch: 940, Training Loss: 0.08223766077365448\n",
      "Epoch: 2 - Batch: 941, Training Loss: 0.08231810330232578\n",
      "Epoch: 2 - Batch: 942, Training Loss: 0.08240224115834703\n",
      "Epoch: 2 - Batch: 943, Training Loss: 0.0824939826160521\n",
      "Epoch: 2 - Batch: 944, Training Loss: 0.08259399636813855\n",
      "Epoch: 2 - Batch: 945, Training Loss: 0.08268145911061942\n",
      "Epoch: 2 - Batch: 946, Training Loss: 0.08277205115873026\n",
      "Epoch: 2 - Batch: 947, Training Loss: 0.08286572265041804\n",
      "Epoch: 2 - Batch: 948, Training Loss: 0.0829573620981838\n",
      "Epoch: 2 - Batch: 949, Training Loss: 0.08305458534218581\n",
      "Epoch: 2 - Batch: 950, Training Loss: 0.083145738778571\n",
      "Epoch: 2 - Batch: 951, Training Loss: 0.0832342199771163\n",
      "Epoch: 2 - Batch: 952, Training Loss: 0.08332018976163112\n",
      "Epoch: 2 - Batch: 953, Training Loss: 0.0834137030057053\n",
      "Epoch: 2 - Batch: 954, Training Loss: 0.08350905635167118\n",
      "Epoch: 2 - Batch: 955, Training Loss: 0.08359614576816954\n",
      "Epoch: 2 - Batch: 956, Training Loss: 0.08368433410206047\n",
      "Epoch: 2 - Batch: 957, Training Loss: 0.08377721517116672\n",
      "Epoch: 2 - Batch: 958, Training Loss: 0.08386182278444122\n",
      "Epoch: 2 - Batch: 959, Training Loss: 0.0839550669735937\n",
      "Epoch: 2 - Batch: 960, Training Loss: 0.08403976062686487\n",
      "Epoch: 2 - Batch: 961, Training Loss: 0.0841243001964456\n",
      "Epoch: 2 - Batch: 962, Training Loss: 0.08421273774793295\n",
      "Epoch: 2 - Batch: 963, Training Loss: 0.08429902008615718\n",
      "Epoch: 2 - Batch: 964, Training Loss: 0.08438146225552061\n",
      "Epoch: 2 - Batch: 965, Training Loss: 0.08447375402447596\n",
      "Epoch: 2 - Batch: 966, Training Loss: 0.08455596072830963\n",
      "Epoch: 2 - Batch: 967, Training Loss: 0.08464065210192555\n",
      "Epoch: 2 - Batch: 968, Training Loss: 0.08472916207107936\n",
      "Epoch: 2 - Batch: 969, Training Loss: 0.08481045093257629\n",
      "Epoch: 2 - Batch: 970, Training Loss: 0.08489974767933438\n",
      "Epoch: 2 - Batch: 971, Training Loss: 0.08498276044826207\n",
      "Epoch: 2 - Batch: 972, Training Loss: 0.0850695105976924\n",
      "Epoch: 2 - Batch: 973, Training Loss: 0.08516107859772631\n",
      "Epoch: 2 - Batch: 974, Training Loss: 0.0852532737747848\n",
      "Epoch: 2 - Batch: 975, Training Loss: 0.08533239680933913\n",
      "Epoch: 2 - Batch: 976, Training Loss: 0.0854181632659032\n",
      "Epoch: 2 - Batch: 977, Training Loss: 0.08550856184603563\n",
      "Epoch: 2 - Batch: 978, Training Loss: 0.08558729541168283\n",
      "Epoch: 2 - Batch: 979, Training Loss: 0.08568225474178692\n",
      "Epoch: 2 - Batch: 980, Training Loss: 0.08576891308721421\n",
      "Epoch: 2 - Batch: 981, Training Loss: 0.08585183113615706\n",
      "Epoch: 2 - Batch: 982, Training Loss: 0.08593812804193442\n",
      "Epoch: 2 - Batch: 983, Training Loss: 0.08601784543636229\n",
      "Epoch: 2 - Batch: 984, Training Loss: 0.08610963979566078\n",
      "Epoch: 2 - Batch: 985, Training Loss: 0.08619573965026174\n",
      "Epoch: 2 - Batch: 986, Training Loss: 0.08628361751858275\n",
      "Epoch: 2 - Batch: 987, Training Loss: 0.08636937169960482\n",
      "Epoch: 2 - Batch: 988, Training Loss: 0.0864655223812531\n",
      "Epoch: 2 - Batch: 989, Training Loss: 0.08655335325407942\n",
      "Epoch: 2 - Batch: 990, Training Loss: 0.08663993056980927\n",
      "Epoch: 2 - Batch: 991, Training Loss: 0.0867292891576219\n",
      "Epoch: 2 - Batch: 992, Training Loss: 0.08681571333488422\n",
      "Epoch: 2 - Batch: 993, Training Loss: 0.08690035026860277\n",
      "Epoch: 2 - Batch: 994, Training Loss: 0.08700051182760528\n",
      "Epoch: 2 - Batch: 995, Training Loss: 0.08709246642381002\n",
      "Epoch: 2 - Batch: 996, Training Loss: 0.08718231988832922\n",
      "Epoch: 2 - Batch: 997, Training Loss: 0.08727056748707891\n",
      "Epoch: 2 - Batch: 998, Training Loss: 0.08736186900838691\n",
      "Epoch: 2 - Batch: 999, Training Loss: 0.08744925414992011\n",
      "Epoch: 2 - Batch: 1000, Training Loss: 0.08753615369399388\n",
      "Epoch: 2 - Batch: 1001, Training Loss: 0.0876268963537999\n",
      "Epoch: 2 - Batch: 1002, Training Loss: 0.08771108071108165\n",
      "Epoch: 2 - Batch: 1003, Training Loss: 0.08779477905698281\n",
      "Epoch: 2 - Batch: 1004, Training Loss: 0.08788130993323144\n",
      "Epoch: 2 - Batch: 1005, Training Loss: 0.08797385751780981\n",
      "Epoch: 2 - Batch: 1006, Training Loss: 0.08805591173803629\n",
      "Epoch: 2 - Batch: 1007, Training Loss: 0.08814131346829297\n",
      "Epoch: 2 - Batch: 1008, Training Loss: 0.08822083355207151\n",
      "Epoch: 2 - Batch: 1009, Training Loss: 0.08831422860621417\n",
      "Epoch: 2 - Batch: 1010, Training Loss: 0.08838992488028398\n",
      "Epoch: 2 - Batch: 1011, Training Loss: 0.08849500583030691\n",
      "Epoch: 2 - Batch: 1012, Training Loss: 0.08858774843970144\n",
      "Epoch: 2 - Batch: 1013, Training Loss: 0.08868020274375209\n",
      "Epoch: 2 - Batch: 1014, Training Loss: 0.08877583598072454\n",
      "Epoch: 2 - Batch: 1015, Training Loss: 0.08885849970910284\n",
      "Epoch: 2 - Batch: 1016, Training Loss: 0.08893836272346044\n",
      "Epoch: 2 - Batch: 1017, Training Loss: 0.08902894721596592\n",
      "Epoch: 2 - Batch: 1018, Training Loss: 0.08911646517058511\n",
      "Epoch: 2 - Batch: 1019, Training Loss: 0.08919877324185363\n",
      "Epoch: 2 - Batch: 1020, Training Loss: 0.08928696828497386\n",
      "Epoch: 2 - Batch: 1021, Training Loss: 0.0893757198234498\n",
      "Epoch: 2 - Batch: 1022, Training Loss: 0.08946450938089175\n",
      "Epoch: 2 - Batch: 1023, Training Loss: 0.08955016789670607\n",
      "Epoch: 2 - Batch: 1024, Training Loss: 0.08963621622739147\n",
      "Epoch: 2 - Batch: 1025, Training Loss: 0.08972188614766001\n",
      "Epoch: 2 - Batch: 1026, Training Loss: 0.08981189989233096\n",
      "Epoch: 2 - Batch: 1027, Training Loss: 0.08989729538288085\n",
      "Epoch: 2 - Batch: 1028, Training Loss: 0.08998403341121737\n",
      "Epoch: 2 - Batch: 1029, Training Loss: 0.0900648710924891\n",
      "Epoch: 2 - Batch: 1030, Training Loss: 0.09014617975424376\n",
      "Epoch: 2 - Batch: 1031, Training Loss: 0.09023748209721612\n",
      "Epoch: 2 - Batch: 1032, Training Loss: 0.09032822316311682\n",
      "Epoch: 2 - Batch: 1033, Training Loss: 0.09040955754358378\n",
      "Epoch: 2 - Batch: 1034, Training Loss: 0.09049398159827561\n",
      "Epoch: 2 - Batch: 1035, Training Loss: 0.09058311312871786\n",
      "Epoch: 2 - Batch: 1036, Training Loss: 0.09067093874412785\n",
      "Epoch: 2 - Batch: 1037, Training Loss: 0.09076964158323866\n",
      "Epoch: 2 - Batch: 1038, Training Loss: 0.09084769666417321\n",
      "Epoch: 2 - Batch: 1039, Training Loss: 0.09093215174441709\n",
      "Epoch: 2 - Batch: 1040, Training Loss: 0.09101892653142239\n",
      "Epoch: 2 - Batch: 1041, Training Loss: 0.09110243320712205\n",
      "Epoch: 2 - Batch: 1042, Training Loss: 0.09118257910260316\n",
      "Epoch: 2 - Batch: 1043, Training Loss: 0.09129112729089177\n",
      "Epoch: 2 - Batch: 1044, Training Loss: 0.09139294992162418\n",
      "Epoch: 2 - Batch: 1045, Training Loss: 0.09147544156383124\n",
      "Epoch: 2 - Batch: 1046, Training Loss: 0.0915644186399074\n",
      "Epoch: 2 - Batch: 1047, Training Loss: 0.09164258260063667\n",
      "Epoch: 2 - Batch: 1048, Training Loss: 0.0917207744732425\n",
      "Epoch: 2 - Batch: 1049, Training Loss: 0.09181475282876843\n",
      "Epoch: 2 - Batch: 1050, Training Loss: 0.09190108554799166\n",
      "Epoch: 2 - Batch: 1051, Training Loss: 0.09198326611301397\n",
      "Epoch: 2 - Batch: 1052, Training Loss: 0.09206970014407069\n",
      "Epoch: 2 - Batch: 1053, Training Loss: 0.09215429149086203\n",
      "Epoch: 2 - Batch: 1054, Training Loss: 0.09224587477508864\n",
      "Epoch: 2 - Batch: 1055, Training Loss: 0.09233020739645309\n",
      "Epoch: 2 - Batch: 1056, Training Loss: 0.09241639399533445\n",
      "Epoch: 2 - Batch: 1057, Training Loss: 0.09250116503332582\n",
      "Epoch: 2 - Batch: 1058, Training Loss: 0.0925829065948181\n",
      "Epoch: 2 - Batch: 1059, Training Loss: 0.09267275396172285\n",
      "Epoch: 2 - Batch: 1060, Training Loss: 0.09276978887778214\n",
      "Epoch: 2 - Batch: 1061, Training Loss: 0.09285940644431667\n",
      "Epoch: 2 - Batch: 1062, Training Loss: 0.09295051282960581\n",
      "Epoch: 2 - Batch: 1063, Training Loss: 0.09303857271202762\n",
      "Epoch: 2 - Batch: 1064, Training Loss: 0.09313057873947901\n",
      "Epoch: 2 - Batch: 1065, Training Loss: 0.0932191766823208\n",
      "Epoch: 2 - Batch: 1066, Training Loss: 0.09330660026291908\n",
      "Epoch: 2 - Batch: 1067, Training Loss: 0.09339536067907687\n",
      "Epoch: 2 - Batch: 1068, Training Loss: 0.0934866268811732\n",
      "Epoch: 2 - Batch: 1069, Training Loss: 0.09356873239673193\n",
      "Epoch: 2 - Batch: 1070, Training Loss: 0.09365439245702813\n",
      "Epoch: 2 - Batch: 1071, Training Loss: 0.09373625037978538\n",
      "Epoch: 2 - Batch: 1072, Training Loss: 0.09383406778596724\n",
      "Epoch: 2 - Batch: 1073, Training Loss: 0.09392599566263543\n",
      "Epoch: 2 - Batch: 1074, Training Loss: 0.09401317061871833\n",
      "Epoch: 2 - Batch: 1075, Training Loss: 0.09409913677061177\n",
      "Epoch: 2 - Batch: 1076, Training Loss: 0.09418373037234665\n",
      "Epoch: 2 - Batch: 1077, Training Loss: 0.09427207778787139\n",
      "Epoch: 2 - Batch: 1078, Training Loss: 0.09435610599185697\n",
      "Epoch: 2 - Batch: 1079, Training Loss: 0.09443457726741311\n",
      "Epoch: 2 - Batch: 1080, Training Loss: 0.09451969987890416\n",
      "Epoch: 2 - Batch: 1081, Training Loss: 0.09461022004682824\n",
      "Epoch: 2 - Batch: 1082, Training Loss: 0.09470652246455451\n",
      "Epoch: 2 - Batch: 1083, Training Loss: 0.09479483355386538\n",
      "Epoch: 2 - Batch: 1084, Training Loss: 0.09487521159115123\n",
      "Epoch: 2 - Batch: 1085, Training Loss: 0.09495935396941542\n",
      "Epoch: 2 - Batch: 1086, Training Loss: 0.09504992614313343\n",
      "Epoch: 2 - Batch: 1087, Training Loss: 0.0951321816402387\n",
      "Epoch: 2 - Batch: 1088, Training Loss: 0.09521667882305868\n",
      "Epoch: 2 - Batch: 1089, Training Loss: 0.09530847297230763\n",
      "Epoch: 2 - Batch: 1090, Training Loss: 0.09539470646165892\n",
      "Epoch: 2 - Batch: 1091, Training Loss: 0.09547529339938615\n",
      "Epoch: 2 - Batch: 1092, Training Loss: 0.09555843028905578\n",
      "Epoch: 2 - Batch: 1093, Training Loss: 0.09563714873731433\n",
      "Epoch: 2 - Batch: 1094, Training Loss: 0.09571660331034937\n",
      "Epoch: 2 - Batch: 1095, Training Loss: 0.09580197902535324\n",
      "Epoch: 2 - Batch: 1096, Training Loss: 0.09588484446306529\n",
      "Epoch: 2 - Batch: 1097, Training Loss: 0.09597834298513817\n",
      "Epoch: 2 - Batch: 1098, Training Loss: 0.09605826953330246\n",
      "Epoch: 2 - Batch: 1099, Training Loss: 0.09614422378650747\n",
      "Epoch: 2 - Batch: 1100, Training Loss: 0.09624156934111866\n",
      "Epoch: 2 - Batch: 1101, Training Loss: 0.09632821410128331\n",
      "Epoch: 2 - Batch: 1102, Training Loss: 0.09641719311722871\n",
      "Epoch: 2 - Batch: 1103, Training Loss: 0.0965005250564262\n",
      "Epoch: 2 - Batch: 1104, Training Loss: 0.09658870456205869\n",
      "Epoch: 2 - Batch: 1105, Training Loss: 0.09667351646086272\n",
      "Epoch: 2 - Batch: 1106, Training Loss: 0.09675400282192982\n",
      "Epoch: 2 - Batch: 1107, Training Loss: 0.09684262667499964\n",
      "Epoch: 2 - Batch: 1108, Training Loss: 0.0969273181474624\n",
      "Epoch: 2 - Batch: 1109, Training Loss: 0.09701264939374393\n",
      "Epoch: 2 - Batch: 1110, Training Loss: 0.09710771249449668\n",
      "Epoch: 2 - Batch: 1111, Training Loss: 0.09719718065403192\n",
      "Epoch: 2 - Batch: 1112, Training Loss: 0.09728094268818795\n",
      "Epoch: 2 - Batch: 1113, Training Loss: 0.09737223754969007\n",
      "Epoch: 2 - Batch: 1114, Training Loss: 0.09745938573063508\n",
      "Epoch: 2 - Batch: 1115, Training Loss: 0.09754872362239049\n",
      "Epoch: 2 - Batch: 1116, Training Loss: 0.097631233211477\n",
      "Epoch: 2 - Batch: 1117, Training Loss: 0.0977136970097647\n",
      "Epoch: 2 - Batch: 1118, Training Loss: 0.09779569709281226\n",
      "Epoch: 2 - Batch: 1119, Training Loss: 0.09788824594386576\n",
      "Epoch: 2 - Batch: 1120, Training Loss: 0.09797750184216705\n",
      "Epoch: 2 - Batch: 1121, Training Loss: 0.09806522882696408\n",
      "Epoch: 2 - Batch: 1122, Training Loss: 0.09814817992781921\n",
      "Epoch: 2 - Batch: 1123, Training Loss: 0.09823248152980955\n",
      "Epoch: 2 - Batch: 1124, Training Loss: 0.0983194709812626\n",
      "Epoch: 2 - Batch: 1125, Training Loss: 0.09840811260205201\n",
      "Epoch: 2 - Batch: 1126, Training Loss: 0.09849633120160989\n",
      "Epoch: 2 - Batch: 1127, Training Loss: 0.0985803982400479\n",
      "Epoch: 2 - Batch: 1128, Training Loss: 0.09866418560988473\n",
      "Epoch: 2 - Batch: 1129, Training Loss: 0.09874469296972747\n",
      "Epoch: 2 - Batch: 1130, Training Loss: 0.098833114650119\n",
      "Epoch: 2 - Batch: 1131, Training Loss: 0.09891928951093806\n",
      "Epoch: 2 - Batch: 1132, Training Loss: 0.0990099559536522\n",
      "Epoch: 2 - Batch: 1133, Training Loss: 0.09909957672035319\n",
      "Epoch: 2 - Batch: 1134, Training Loss: 0.09917726795224606\n",
      "Epoch: 2 - Batch: 1135, Training Loss: 0.09926663092638723\n",
      "Epoch: 2 - Batch: 1136, Training Loss: 0.09935316586153424\n",
      "Epoch: 2 - Batch: 1137, Training Loss: 0.09944172102840583\n",
      "Epoch: 2 - Batch: 1138, Training Loss: 0.09953154370213425\n",
      "Epoch: 2 - Batch: 1139, Training Loss: 0.09961294787464846\n",
      "Epoch: 2 - Batch: 1140, Training Loss: 0.09969782980643892\n",
      "Epoch: 2 - Batch: 1141, Training Loss: 0.09977714881721025\n",
      "Epoch: 2 - Batch: 1142, Training Loss: 0.09986700424013248\n",
      "Epoch: 2 - Batch: 1143, Training Loss: 0.0999553074834754\n",
      "Epoch: 2 - Batch: 1144, Training Loss: 0.10004209452131099\n",
      "Epoch: 2 - Batch: 1145, Training Loss: 0.10012956177985688\n",
      "Epoch: 2 - Batch: 1146, Training Loss: 0.10021203640805153\n",
      "Epoch: 2 - Batch: 1147, Training Loss: 0.10031586065641288\n",
      "Epoch: 2 - Batch: 1148, Training Loss: 0.10041354372973861\n",
      "Epoch: 2 - Batch: 1149, Training Loss: 0.10050629598905593\n",
      "Epoch: 2 - Batch: 1150, Training Loss: 0.10059638191045418\n",
      "Epoch: 2 - Batch: 1151, Training Loss: 0.10068268947933444\n",
      "Epoch: 2 - Batch: 1152, Training Loss: 0.10077480638822908\n",
      "Epoch: 2 - Batch: 1153, Training Loss: 0.1008533978862549\n",
      "Epoch: 2 - Batch: 1154, Training Loss: 0.10094217217186\n",
      "Epoch: 2 - Batch: 1155, Training Loss: 0.10102695984701018\n",
      "Epoch: 2 - Batch: 1156, Training Loss: 0.10111146099605964\n",
      "Epoch: 2 - Batch: 1157, Training Loss: 0.10119929725603875\n",
      "Epoch: 2 - Batch: 1158, Training Loss: 0.1012969385418627\n",
      "Epoch: 2 - Batch: 1159, Training Loss: 0.10139045356518\n",
      "Epoch: 2 - Batch: 1160, Training Loss: 0.10147314807817118\n",
      "Epoch: 2 - Batch: 1161, Training Loss: 0.10155544696286148\n",
      "Epoch: 2 - Batch: 1162, Training Loss: 0.10165191703718496\n",
      "Epoch: 2 - Batch: 1163, Training Loss: 0.10173812685889588\n",
      "Epoch: 2 - Batch: 1164, Training Loss: 0.10182198635603658\n",
      "Epoch: 2 - Batch: 1165, Training Loss: 0.10191651045732435\n",
      "Epoch: 2 - Batch: 1166, Training Loss: 0.10200096968913552\n",
      "Epoch: 2 - Batch: 1167, Training Loss: 0.10208467749344373\n",
      "Epoch: 2 - Batch: 1168, Training Loss: 0.10218096134031986\n",
      "Epoch: 2 - Batch: 1169, Training Loss: 0.10226918062562769\n",
      "Epoch: 2 - Batch: 1170, Training Loss: 0.10235558596638898\n",
      "Epoch: 2 - Batch: 1171, Training Loss: 0.10244503916298374\n",
      "Epoch: 2 - Batch: 1172, Training Loss: 0.10252852690951346\n",
      "Epoch: 2 - Batch: 1173, Training Loss: 0.10261628322093246\n",
      "Epoch: 2 - Batch: 1174, Training Loss: 0.10269969873513353\n",
      "Epoch: 2 - Batch: 1175, Training Loss: 0.10278717929475738\n",
      "Epoch: 2 - Batch: 1176, Training Loss: 0.10287934675145505\n",
      "Epoch: 2 - Batch: 1177, Training Loss: 0.10296881209385533\n",
      "Epoch: 2 - Batch: 1178, Training Loss: 0.10305633268644956\n",
      "Epoch: 2 - Batch: 1179, Training Loss: 0.10314481611522672\n",
      "Epoch: 2 - Batch: 1180, Training Loss: 0.10323248086975977\n",
      "Epoch: 2 - Batch: 1181, Training Loss: 0.10331333381669043\n",
      "Epoch: 2 - Batch: 1182, Training Loss: 0.10339768084498187\n",
      "Epoch: 2 - Batch: 1183, Training Loss: 0.10348962850955193\n",
      "Epoch: 2 - Batch: 1184, Training Loss: 0.1035792994101348\n",
      "Epoch: 2 - Batch: 1185, Training Loss: 0.10366678537544524\n",
      "Epoch: 2 - Batch: 1186, Training Loss: 0.10375265323651173\n",
      "Epoch: 2 - Batch: 1187, Training Loss: 0.10384408723111967\n",
      "Epoch: 2 - Batch: 1188, Training Loss: 0.10392577819538551\n",
      "Epoch: 2 - Batch: 1189, Training Loss: 0.10402386904281763\n",
      "Epoch: 2 - Batch: 1190, Training Loss: 0.10411409722953095\n",
      "Epoch: 2 - Batch: 1191, Training Loss: 0.10419958538899374\n",
      "Epoch: 2 - Batch: 1192, Training Loss: 0.10428572053809466\n",
      "Epoch: 2 - Batch: 1193, Training Loss: 0.10436636581399152\n",
      "Epoch: 2 - Batch: 1194, Training Loss: 0.10445280633187215\n",
      "Epoch: 2 - Batch: 1195, Training Loss: 0.10453833596376241\n",
      "Epoch: 2 - Batch: 1196, Training Loss: 0.10462947358465313\n",
      "Epoch: 2 - Batch: 1197, Training Loss: 0.10471370126774061\n",
      "Epoch: 2 - Batch: 1198, Training Loss: 0.10480881760987279\n",
      "Epoch: 2 - Batch: 1199, Training Loss: 0.10488635724160209\n",
      "Epoch: 2 - Batch: 1200, Training Loss: 0.1049745926142332\n",
      "Epoch: 2 - Batch: 1201, Training Loss: 0.10506367034110461\n",
      "Epoch: 2 - Batch: 1202, Training Loss: 0.10515104545586144\n",
      "Epoch: 2 - Batch: 1203, Training Loss: 0.10523226273371213\n",
      "Epoch: 2 - Batch: 1204, Training Loss: 0.10531282861088441\n",
      "Epoch: 2 - Batch: 1205, Training Loss: 0.10540076162956445\n",
      "Epoch: 2 - Batch: 1206, Training Loss: 0.10548960610878211\n",
      "Epoch: 2 - Batch: 1207, Training Loss: 0.10558528227617293\n",
      "Epoch: 2 - Batch: 1208, Training Loss: 0.10567646410631303\n",
      "Epoch: 2 - Batch: 1209, Training Loss: 0.10577461704191679\n",
      "Epoch: 2 - Batch: 1210, Training Loss: 0.10586550207206266\n",
      "Epoch: 2 - Batch: 1211, Training Loss: 0.10595427541862278\n",
      "Epoch: 2 - Batch: 1212, Training Loss: 0.10604070272809435\n",
      "Epoch: 2 - Batch: 1213, Training Loss: 0.10612597441297661\n",
      "Epoch: 2 - Batch: 1214, Training Loss: 0.10621910901187269\n",
      "Epoch: 2 - Batch: 1215, Training Loss: 0.10630489249480501\n",
      "Epoch: 2 - Batch: 1216, Training Loss: 0.10640199433635321\n",
      "Epoch: 2 - Batch: 1217, Training Loss: 0.10648462855894965\n",
      "Epoch: 2 - Batch: 1218, Training Loss: 0.10657478378433889\n",
      "Epoch: 2 - Batch: 1219, Training Loss: 0.10666051388379946\n",
      "Epoch: 2 - Batch: 1220, Training Loss: 0.106745794409296\n",
      "Epoch: 2 - Batch: 1221, Training Loss: 0.1068301599651921\n",
      "Epoch: 2 - Batch: 1222, Training Loss: 0.10691634578565459\n",
      "Epoch: 2 - Batch: 1223, Training Loss: 0.1070023334058462\n",
      "Epoch: 2 - Batch: 1224, Training Loss: 0.1070953946665646\n",
      "Epoch: 2 - Batch: 1225, Training Loss: 0.10717811433607666\n",
      "Epoch: 2 - Batch: 1226, Training Loss: 0.10727118389992966\n",
      "Epoch: 2 - Batch: 1227, Training Loss: 0.10735593704664292\n",
      "Epoch: 2 - Batch: 1228, Training Loss: 0.10743917141165306\n",
      "Epoch: 2 - Batch: 1229, Training Loss: 0.10752273276818926\n",
      "Epoch: 2 - Batch: 1230, Training Loss: 0.10760689789976054\n",
      "Epoch: 2 - Batch: 1231, Training Loss: 0.1076905363012309\n",
      "Epoch: 2 - Batch: 1232, Training Loss: 0.10777778767827731\n",
      "Epoch: 2 - Batch: 1233, Training Loss: 0.1078581837242219\n",
      "Epoch: 2 - Batch: 1234, Training Loss: 0.10794398477718012\n",
      "Epoch: 2 - Batch: 1235, Training Loss: 0.10802914199742117\n",
      "Epoch: 2 - Batch: 1236, Training Loss: 0.10812578536261176\n",
      "Epoch: 2 - Batch: 1237, Training Loss: 0.10821910464447332\n",
      "Epoch: 2 - Batch: 1238, Training Loss: 0.10831167252354361\n",
      "Epoch: 2 - Batch: 1239, Training Loss: 0.10839508580133492\n",
      "Epoch: 2 - Batch: 1240, Training Loss: 0.10849392202807896\n",
      "Epoch: 2 - Batch: 1241, Training Loss: 0.10857404738195105\n",
      "Epoch: 2 - Batch: 1242, Training Loss: 0.1086609675047014\n",
      "Epoch: 2 - Batch: 1243, Training Loss: 0.108745949160598\n",
      "Epoch: 2 - Batch: 1244, Training Loss: 0.1088284641059477\n",
      "Epoch: 2 - Batch: 1245, Training Loss: 0.10891320754095887\n",
      "Epoch: 2 - Batch: 1246, Training Loss: 0.1089917971794285\n",
      "Epoch: 2 - Batch: 1247, Training Loss: 0.10907646120335926\n",
      "Epoch: 2 - Batch: 1248, Training Loss: 0.1091655981641998\n",
      "Epoch: 2 - Batch: 1249, Training Loss: 0.10925288070271265\n",
      "Epoch: 2 - Batch: 1250, Training Loss: 0.10935339303826218\n",
      "Epoch: 2 - Batch: 1251, Training Loss: 0.10944334381749579\n",
      "Epoch: 2 - Batch: 1252, Training Loss: 0.10953077361926708\n",
      "Epoch: 2 - Batch: 1253, Training Loss: 0.10961214094662153\n",
      "Epoch: 2 - Batch: 1254, Training Loss: 0.10969318642866355\n",
      "Epoch: 2 - Batch: 1255, Training Loss: 0.10978829333092245\n",
      "Epoch: 2 - Batch: 1256, Training Loss: 0.10987005211647669\n",
      "Epoch: 2 - Batch: 1257, Training Loss: 0.10995173288098417\n",
      "Epoch: 2 - Batch: 1258, Training Loss: 0.11004143156370713\n",
      "Epoch: 2 - Batch: 1259, Training Loss: 0.11012208082436724\n",
      "Epoch: 2 - Batch: 1260, Training Loss: 0.11021223752010323\n",
      "Epoch: 2 - Batch: 1261, Training Loss: 0.11029178403923365\n",
      "Epoch: 2 - Batch: 1262, Training Loss: 0.11037415201168749\n",
      "Epoch: 2 - Batch: 1263, Training Loss: 0.11045896172350517\n",
      "Epoch: 2 - Batch: 1264, Training Loss: 0.11054695676239965\n",
      "Epoch: 2 - Batch: 1265, Training Loss: 0.11063217216636213\n",
      "Epoch: 2 - Batch: 1266, Training Loss: 0.11073198716834212\n",
      "Epoch: 2 - Batch: 1267, Training Loss: 0.11082527712975963\n",
      "Epoch: 2 - Batch: 1268, Training Loss: 0.11091378958861824\n",
      "Epoch: 2 - Batch: 1269, Training Loss: 0.11100293285094485\n",
      "Epoch: 2 - Batch: 1270, Training Loss: 0.11109057670554909\n",
      "Epoch: 2 - Batch: 1271, Training Loss: 0.11117452515866824\n",
      "Epoch: 2 - Batch: 1272, Training Loss: 0.11125654515935414\n",
      "Epoch: 2 - Batch: 1273, Training Loss: 0.11134522914837051\n",
      "Epoch: 2 - Batch: 1274, Training Loss: 0.11143157858506561\n",
      "Epoch: 2 - Batch: 1275, Training Loss: 0.11151965137317801\n",
      "Epoch: 2 - Batch: 1276, Training Loss: 0.11161049589465309\n",
      "Epoch: 2 - Batch: 1277, Training Loss: 0.11169940844227623\n",
      "Epoch: 2 - Batch: 1278, Training Loss: 0.11178351484646845\n",
      "Epoch: 2 - Batch: 1279, Training Loss: 0.11187551222482131\n",
      "Epoch: 2 - Batch: 1280, Training Loss: 0.11196027700457209\n",
      "Epoch: 2 - Batch: 1281, Training Loss: 0.11204570749925935\n",
      "Epoch: 2 - Batch: 1282, Training Loss: 0.11212972460150916\n",
      "Epoch: 2 - Batch: 1283, Training Loss: 0.1122151601003177\n",
      "Epoch: 2 - Batch: 1284, Training Loss: 0.11229147640354993\n",
      "Epoch: 2 - Batch: 1285, Training Loss: 0.1123732046083629\n",
      "Epoch: 2 - Batch: 1286, Training Loss: 0.11247286007772038\n",
      "Epoch: 2 - Batch: 1287, Training Loss: 0.11255760036198852\n",
      "Epoch: 2 - Batch: 1288, Training Loss: 0.11264003873192652\n",
      "Epoch: 2 - Batch: 1289, Training Loss: 0.11271917546294617\n",
      "Epoch: 2 - Batch: 1290, Training Loss: 0.11280493521028095\n",
      "Epoch: 2 - Batch: 1291, Training Loss: 0.11288935698531753\n",
      "Epoch: 2 - Batch: 1292, Training Loss: 0.11297321706318342\n",
      "Epoch: 2 - Batch: 1293, Training Loss: 0.11305551930248836\n",
      "Epoch: 2 - Batch: 1294, Training Loss: 0.11315565196114591\n",
      "Epoch: 2 - Batch: 1295, Training Loss: 0.11324572576762827\n",
      "Epoch: 2 - Batch: 1296, Training Loss: 0.1133338854853589\n",
      "Epoch: 2 - Batch: 1297, Training Loss: 0.11341979909397872\n",
      "Epoch: 2 - Batch: 1298, Training Loss: 0.11350963823806191\n",
      "Epoch: 2 - Batch: 1299, Training Loss: 0.11359951700737227\n",
      "Epoch: 2 - Batch: 1300, Training Loss: 0.11368862664323937\n",
      "Epoch: 2 - Batch: 1301, Training Loss: 0.11377731438225774\n",
      "Epoch: 2 - Batch: 1302, Training Loss: 0.11386153484334795\n",
      "Epoch: 2 - Batch: 1303, Training Loss: 0.1139494642294066\n",
      "Epoch: 2 - Batch: 1304, Training Loss: 0.11403856352342302\n",
      "Epoch: 2 - Batch: 1305, Training Loss: 0.11412972830248314\n",
      "Epoch: 2 - Batch: 1306, Training Loss: 0.1142156119110869\n",
      "Epoch: 2 - Batch: 1307, Training Loss: 0.1143037956856673\n",
      "Epoch: 2 - Batch: 1308, Training Loss: 0.11438958364141917\n",
      "Epoch: 2 - Batch: 1309, Training Loss: 0.11447098991442874\n",
      "Epoch: 2 - Batch: 1310, Training Loss: 0.11456152970962856\n",
      "Epoch: 2 - Batch: 1311, Training Loss: 0.11465193554882584\n",
      "Epoch: 2 - Batch: 1312, Training Loss: 0.11474471831474929\n",
      "Epoch: 2 - Batch: 1313, Training Loss: 0.11482607243987261\n",
      "Epoch: 2 - Batch: 1314, Training Loss: 0.11492324086094574\n",
      "Epoch: 2 - Batch: 1315, Training Loss: 0.11501243515295373\n",
      "Epoch: 2 - Batch: 1316, Training Loss: 0.11510093271065114\n",
      "Epoch: 2 - Batch: 1317, Training Loss: 0.11519132683649783\n",
      "Epoch: 2 - Batch: 1318, Training Loss: 0.11527953500153611\n",
      "Epoch: 2 - Batch: 1319, Training Loss: 0.11537058125705664\n",
      "Epoch: 2 - Batch: 1320, Training Loss: 0.11545976098151152\n",
      "Epoch: 2 - Batch: 1321, Training Loss: 0.11554570589369012\n",
      "Epoch: 2 - Batch: 1322, Training Loss: 0.1156367229399693\n",
      "Epoch: 2 - Batch: 1323, Training Loss: 0.11573003061350504\n",
      "Epoch: 2 - Batch: 1324, Training Loss: 0.1158135532282577\n",
      "Epoch: 2 - Batch: 1325, Training Loss: 0.11589445072422376\n",
      "Epoch: 2 - Batch: 1326, Training Loss: 0.11598740920893984\n",
      "Epoch: 2 - Batch: 1327, Training Loss: 0.11606887257217768\n",
      "Epoch: 2 - Batch: 1328, Training Loss: 0.1161537913923734\n",
      "Epoch: 2 - Batch: 1329, Training Loss: 0.11623779377879986\n",
      "Epoch: 2 - Batch: 1330, Training Loss: 0.11632503240460385\n",
      "Epoch: 2 - Batch: 1331, Training Loss: 0.11640866585137634\n",
      "Epoch: 2 - Batch: 1332, Training Loss: 0.11649993176311008\n",
      "Epoch: 2 - Batch: 1333, Training Loss: 0.11659853713923624\n",
      "Epoch: 2 - Batch: 1334, Training Loss: 0.11668815555832476\n",
      "Epoch: 2 - Batch: 1335, Training Loss: 0.11677130550195526\n",
      "Epoch: 2 - Batch: 1336, Training Loss: 0.11685540411578087\n",
      "Epoch: 2 - Batch: 1337, Training Loss: 0.11694246332839156\n",
      "Epoch: 2 - Batch: 1338, Training Loss: 0.11703703666776172\n",
      "Epoch: 2 - Batch: 1339, Training Loss: 0.11711712547919248\n",
      "Epoch: 2 - Batch: 1340, Training Loss: 0.1171995881160298\n",
      "Epoch: 2 - Batch: 1341, Training Loss: 0.11728942340508622\n",
      "Epoch: 2 - Batch: 1342, Training Loss: 0.11737986749183281\n",
      "Epoch: 2 - Batch: 1343, Training Loss: 0.11746738009636674\n",
      "Epoch: 2 - Batch: 1344, Training Loss: 0.11755042600987563\n",
      "Epoch: 2 - Batch: 1345, Training Loss: 0.1176341541210316\n",
      "Epoch: 2 - Batch: 1346, Training Loss: 0.11771920655126596\n",
      "Epoch: 2 - Batch: 1347, Training Loss: 0.11781294003227852\n",
      "Epoch: 2 - Batch: 1348, Training Loss: 0.11790110114622077\n",
      "Epoch: 2 - Batch: 1349, Training Loss: 0.1179991605109342\n",
      "Epoch: 2 - Batch: 1350, Training Loss: 0.11808046628141877\n",
      "Epoch: 2 - Batch: 1351, Training Loss: 0.11816461664774328\n",
      "Epoch: 2 - Batch: 1352, Training Loss: 0.11824778303701684\n",
      "Epoch: 2 - Batch: 1353, Training Loss: 0.1183350089548239\n",
      "Epoch: 2 - Batch: 1354, Training Loss: 0.11842341042103657\n",
      "Epoch: 2 - Batch: 1355, Training Loss: 0.11851600212837334\n",
      "Epoch: 2 - Batch: 1356, Training Loss: 0.11858659259863753\n",
      "Epoch: 2 - Batch: 1357, Training Loss: 0.1186742192230019\n",
      "Epoch: 2 - Batch: 1358, Training Loss: 0.11876079926319778\n",
      "Epoch: 2 - Batch: 1359, Training Loss: 0.11884774661207476\n",
      "Epoch: 2 - Batch: 1360, Training Loss: 0.11893599490893025\n",
      "Epoch: 2 - Batch: 1361, Training Loss: 0.11902506789733126\n",
      "Epoch: 2 - Batch: 1362, Training Loss: 0.11911102601791891\n",
      "Epoch: 2 - Batch: 1363, Training Loss: 0.11920019804467609\n",
      "Epoch: 2 - Batch: 1364, Training Loss: 0.11928976159759026\n",
      "Epoch: 2 - Batch: 1365, Training Loss: 0.11936918909894689\n",
      "Epoch: 2 - Batch: 1366, Training Loss: 0.11945748679451089\n",
      "Epoch: 2 - Batch: 1367, Training Loss: 0.1195516269870263\n",
      "Epoch: 2 - Batch: 1368, Training Loss: 0.11963577153374307\n",
      "Epoch: 2 - Batch: 1369, Training Loss: 0.11972812013963166\n",
      "Epoch: 2 - Batch: 1370, Training Loss: 0.11981222235518901\n",
      "Epoch: 2 - Batch: 1371, Training Loss: 0.11989429221792798\n",
      "Epoch: 2 - Batch: 1372, Training Loss: 0.11998215929958753\n",
      "Epoch: 2 - Batch: 1373, Training Loss: 0.12007664368915716\n",
      "Epoch: 2 - Batch: 1374, Training Loss: 0.12015980047793136\n",
      "Epoch: 2 - Batch: 1375, Training Loss: 0.12024901098082116\n",
      "Epoch: 2 - Batch: 1376, Training Loss: 0.1203336058057857\n",
      "Epoch: 2 - Batch: 1377, Training Loss: 0.12041750949981991\n",
      "Epoch: 2 - Batch: 1378, Training Loss: 0.12050622251891774\n",
      "Epoch: 2 - Batch: 1379, Training Loss: 0.12058718028674474\n",
      "Epoch: 2 - Batch: 1380, Training Loss: 0.12067885032760761\n",
      "Epoch: 2 - Batch: 1381, Training Loss: 0.1207649784894725\n",
      "Epoch: 2 - Batch: 1382, Training Loss: 0.12084933232584008\n",
      "Epoch: 2 - Batch: 1383, Training Loss: 0.12093661660652849\n",
      "Epoch: 2 - Batch: 1384, Training Loss: 0.1210260434666182\n",
      "Epoch: 2 - Batch: 1385, Training Loss: 0.12111590371450184\n",
      "Epoch: 2 - Batch: 1386, Training Loss: 0.12120977945465156\n",
      "Epoch: 2 - Batch: 1387, Training Loss: 0.12130119649084845\n",
      "Epoch: 2 - Batch: 1388, Training Loss: 0.12138549164926037\n",
      "Epoch: 2 - Batch: 1389, Training Loss: 0.12147194100775528\n",
      "Epoch: 2 - Batch: 1390, Training Loss: 0.12155616137617659\n",
      "Epoch: 2 - Batch: 1391, Training Loss: 0.12164587204407894\n",
      "Epoch: 2 - Batch: 1392, Training Loss: 0.1217377628346482\n",
      "Epoch: 2 - Batch: 1393, Training Loss: 0.12182698765179609\n",
      "Epoch: 2 - Batch: 1394, Training Loss: 0.12190439456310834\n",
      "Epoch: 2 - Batch: 1395, Training Loss: 0.1219888241655791\n",
      "Epoch: 2 - Batch: 1396, Training Loss: 0.12206381522922176\n",
      "Epoch: 2 - Batch: 1397, Training Loss: 0.12214257757783331\n",
      "Epoch: 2 - Batch: 1398, Training Loss: 0.12223293330786042\n",
      "Epoch: 2 - Batch: 1399, Training Loss: 0.12231808376327083\n",
      "Epoch: 2 - Batch: 1400, Training Loss: 0.12241117058207542\n",
      "Epoch: 2 - Batch: 1401, Training Loss: 0.12250168689497272\n",
      "Epoch: 2 - Batch: 1402, Training Loss: 0.12258051759617443\n",
      "Epoch: 2 - Batch: 1403, Training Loss: 0.12266895493143432\n",
      "Epoch: 2 - Batch: 1404, Training Loss: 0.12275777886904295\n",
      "Epoch: 2 - Batch: 1405, Training Loss: 0.12284831936234858\n",
      "Epoch: 2 - Batch: 1406, Training Loss: 0.12292700627228711\n",
      "Epoch: 2 - Batch: 1407, Training Loss: 0.12301681332821475\n",
      "Epoch: 2 - Batch: 1408, Training Loss: 0.12310701187940973\n",
      "Epoch: 2 - Batch: 1409, Training Loss: 0.12319511820749066\n",
      "Epoch: 2 - Batch: 1410, Training Loss: 0.12327581416092702\n",
      "Epoch: 2 - Batch: 1411, Training Loss: 0.1233651570753375\n",
      "Epoch: 2 - Batch: 1412, Training Loss: 0.12345855329093056\n",
      "Epoch: 2 - Batch: 1413, Training Loss: 0.12353882154644426\n",
      "Epoch: 2 - Batch: 1414, Training Loss: 0.12362383445029829\n",
      "Epoch: 2 - Batch: 1415, Training Loss: 0.12371022569920688\n",
      "Epoch: 2 - Batch: 1416, Training Loss: 0.12379310451213202\n",
      "Epoch: 2 - Batch: 1417, Training Loss: 0.12387855621564448\n",
      "Epoch: 2 - Batch: 1418, Training Loss: 0.12396520883413294\n",
      "Epoch: 2 - Batch: 1419, Training Loss: 0.12405107746744037\n",
      "Epoch: 2 - Batch: 1420, Training Loss: 0.12413354248277979\n",
      "Epoch: 2 - Batch: 1421, Training Loss: 0.1242198782538992\n",
      "Epoch: 2 - Batch: 1422, Training Loss: 0.12430921575026725\n",
      "Epoch: 2 - Batch: 1423, Training Loss: 0.12440614963249978\n",
      "Epoch: 2 - Batch: 1424, Training Loss: 0.12449384303408279\n",
      "Epoch: 2 - Batch: 1425, Training Loss: 0.12457954922496382\n",
      "Epoch: 2 - Batch: 1426, Training Loss: 0.12466890751623594\n",
      "Epoch: 2 - Batch: 1427, Training Loss: 0.12476022621218245\n",
      "Epoch: 2 - Batch: 1428, Training Loss: 0.1248545091518913\n",
      "Epoch: 2 - Batch: 1429, Training Loss: 0.12493792137943492\n",
      "Epoch: 2 - Batch: 1430, Training Loss: 0.12503038124979826\n",
      "Epoch: 2 - Batch: 1431, Training Loss: 0.12511377259594686\n",
      "Epoch: 2 - Batch: 1432, Training Loss: 0.1252000132084486\n",
      "Epoch: 2 - Batch: 1433, Training Loss: 0.12528929643172332\n",
      "Epoch: 2 - Batch: 1434, Training Loss: 0.12538241292311383\n",
      "Epoch: 2 - Batch: 1435, Training Loss: 0.12546827004694228\n",
      "Epoch: 2 - Batch: 1436, Training Loss: 0.12555418948134775\n",
      "Epoch: 2 - Batch: 1437, Training Loss: 0.12564353239575823\n",
      "Epoch: 2 - Batch: 1438, Training Loss: 0.12573732250365452\n",
      "Epoch: 2 - Batch: 1439, Training Loss: 0.12582464538403412\n",
      "Epoch: 2 - Batch: 1440, Training Loss: 0.12591255791052855\n",
      "Epoch: 2 - Batch: 1441, Training Loss: 0.12599804120178443\n",
      "Epoch: 2 - Batch: 1442, Training Loss: 0.126096405984701\n",
      "Epoch: 2 - Batch: 1443, Training Loss: 0.12618759491945775\n",
      "Epoch: 2 - Batch: 1444, Training Loss: 0.12627053543752303\n",
      "Epoch: 2 - Batch: 1445, Training Loss: 0.1263629861521978\n",
      "Epoch: 2 - Batch: 1446, Training Loss: 0.12645320953248348\n",
      "Epoch: 2 - Batch: 1447, Training Loss: 0.12653951312277842\n",
      "Epoch: 2 - Batch: 1448, Training Loss: 0.12662377941791295\n",
      "Epoch: 2 - Batch: 1449, Training Loss: 0.12670518989191323\n",
      "Epoch: 2 - Batch: 1450, Training Loss: 0.12679152838132077\n",
      "Epoch: 2 - Batch: 1451, Training Loss: 0.12687342164182347\n",
      "Epoch: 2 - Batch: 1452, Training Loss: 0.12697138751595966\n",
      "Epoch: 2 - Batch: 1453, Training Loss: 0.1270588500545689\n",
      "Epoch: 2 - Batch: 1454, Training Loss: 0.12714018735101765\n",
      "Epoch: 2 - Batch: 1455, Training Loss: 0.1272309491871107\n",
      "Epoch: 2 - Batch: 1456, Training Loss: 0.1273177972505144\n",
      "Epoch: 2 - Batch: 1457, Training Loss: 0.1274041229639678\n",
      "Epoch: 2 - Batch: 1458, Training Loss: 0.12749725954597863\n",
      "Epoch: 2 - Batch: 1459, Training Loss: 0.127588067129625\n",
      "Epoch: 2 - Batch: 1460, Training Loss: 0.12768065498811293\n",
      "Epoch: 2 - Batch: 1461, Training Loss: 0.1277656049018179\n",
      "Epoch: 2 - Batch: 1462, Training Loss: 0.1278520148945586\n",
      "Epoch: 2 - Batch: 1463, Training Loss: 0.12793935302824128\n",
      "Epoch: 2 - Batch: 1464, Training Loss: 0.12802308668717619\n",
      "Epoch: 2 - Batch: 1465, Training Loss: 0.1281039118210771\n",
      "Epoch: 2 - Batch: 1466, Training Loss: 0.12818817604660593\n",
      "Epoch: 2 - Batch: 1467, Training Loss: 0.12828089865187112\n",
      "Epoch: 2 - Batch: 1468, Training Loss: 0.1283681835565302\n",
      "Epoch: 2 - Batch: 1469, Training Loss: 0.12845884065821792\n",
      "Epoch: 2 - Batch: 1470, Training Loss: 0.12854465377049065\n",
      "Epoch: 2 - Batch: 1471, Training Loss: 0.12862079686316882\n",
      "Epoch: 2 - Batch: 1472, Training Loss: 0.12869882911666117\n",
      "Epoch: 2 - Batch: 1473, Training Loss: 0.12878267167392457\n",
      "Epoch: 2 - Batch: 1474, Training Loss: 0.12888363982932288\n",
      "Epoch: 2 - Batch: 1475, Training Loss: 0.1289765882687288\n",
      "Epoch: 2 - Batch: 1476, Training Loss: 0.12905756645460628\n",
      "Epoch: 2 - Batch: 1477, Training Loss: 0.1291423807133786\n",
      "Epoch: 2 - Batch: 1478, Training Loss: 0.12922859437776046\n",
      "Epoch: 2 - Batch: 1479, Training Loss: 0.12931586231165265\n",
      "Epoch: 2 - Batch: 1480, Training Loss: 0.1294003048815933\n",
      "Epoch: 2 - Batch: 1481, Training Loss: 0.12948724293886726\n",
      "Epoch: 2 - Batch: 1482, Training Loss: 0.12957599693615837\n",
      "Epoch: 2 - Batch: 1483, Training Loss: 0.12966808718729572\n",
      "Epoch: 2 - Batch: 1484, Training Loss: 0.12975292867219468\n",
      "Epoch: 2 - Batch: 1485, Training Loss: 0.12984870599697082\n",
      "Epoch: 2 - Batch: 1486, Training Loss: 0.12993685014014617\n",
      "Epoch: 2 - Batch: 1487, Training Loss: 0.13001782731284353\n",
      "Epoch: 2 - Batch: 1488, Training Loss: 0.1301033920909635\n",
      "Epoch: 2 - Batch: 1489, Training Loss: 0.1301887785864509\n",
      "Epoch: 2 - Batch: 1490, Training Loss: 0.13027122477764513\n",
      "Epoch: 2 - Batch: 1491, Training Loss: 0.13035756106153254\n",
      "Epoch: 2 - Batch: 1492, Training Loss: 0.1304459380013729\n",
      "Epoch: 2 - Batch: 1493, Training Loss: 0.13053107894895286\n",
      "Epoch: 2 - Batch: 1494, Training Loss: 0.13061914087009074\n",
      "Epoch: 2 - Batch: 1495, Training Loss: 0.1307060008692504\n",
      "Epoch: 2 - Batch: 1496, Training Loss: 0.13079264055733658\n",
      "Epoch: 2 - Batch: 1497, Training Loss: 0.1308821370364916\n",
      "Epoch: 2 - Batch: 1498, Training Loss: 0.13097086664332483\n",
      "Epoch: 2 - Batch: 1499, Training Loss: 0.13105219495706694\n",
      "Epoch: 2 - Batch: 1500, Training Loss: 0.1311326034330017\n",
      "Epoch: 2 - Batch: 1501, Training Loss: 0.1312183239566746\n",
      "Epoch: 2 - Batch: 1502, Training Loss: 0.13130117305376834\n",
      "Epoch: 2 - Batch: 1503, Training Loss: 0.13138769345225187\n",
      "Epoch: 2 - Batch: 1504, Training Loss: 0.13147908135334255\n",
      "Epoch: 2 - Batch: 1505, Training Loss: 0.13156777282382917\n",
      "Epoch: 2 - Batch: 1506, Training Loss: 0.13165342351838724\n",
      "Epoch: 2 - Batch: 1507, Training Loss: 0.1317495871242599\n",
      "Epoch: 2 - Batch: 1508, Training Loss: 0.13182936195488\n",
      "Epoch: 2 - Batch: 1509, Training Loss: 0.13191660979815187\n",
      "Epoch: 2 - Batch: 1510, Training Loss: 0.1320015622386292\n",
      "Epoch: 2 - Batch: 1511, Training Loss: 0.13208793620169657\n",
      "Epoch: 2 - Batch: 1512, Training Loss: 0.13216826621795175\n",
      "Epoch: 2 - Batch: 1513, Training Loss: 0.13225786355177363\n",
      "Epoch: 2 - Batch: 1514, Training Loss: 0.1323441092425318\n",
      "Epoch: 2 - Batch: 1515, Training Loss: 0.1324380041466127\n",
      "Epoch: 2 - Batch: 1516, Training Loss: 0.13253407193282943\n",
      "Epoch: 2 - Batch: 1517, Training Loss: 0.13262822160228568\n",
      "Epoch: 2 - Batch: 1518, Training Loss: 0.13272815043875827\n",
      "Epoch: 2 - Batch: 1519, Training Loss: 0.13280961152234086\n",
      "Epoch: 2 - Batch: 1520, Training Loss: 0.13289492455609206\n",
      "Epoch: 2 - Batch: 1521, Training Loss: 0.1329839702442313\n",
      "Epoch: 2 - Batch: 1522, Training Loss: 0.13307813415998843\n",
      "Epoch: 2 - Batch: 1523, Training Loss: 0.1331638174863597\n",
      "Epoch: 2 - Batch: 1524, Training Loss: 0.1332564675071544\n",
      "Epoch: 2 - Batch: 1525, Training Loss: 0.1333340583786146\n",
      "Epoch: 2 - Batch: 1526, Training Loss: 0.1334222120543321\n",
      "Epoch: 2 - Batch: 1527, Training Loss: 0.13350343765385114\n",
      "Epoch: 2 - Batch: 1528, Training Loss: 0.13358665178614865\n",
      "Epoch: 2 - Batch: 1529, Training Loss: 0.1336756489034217\n",
      "Epoch: 2 - Batch: 1530, Training Loss: 0.13376263540800334\n",
      "Epoch: 2 - Batch: 1531, Training Loss: 0.13384760640079701\n",
      "Epoch: 2 - Batch: 1532, Training Loss: 0.13393745825287715\n",
      "Epoch: 2 - Batch: 1533, Training Loss: 0.1340211795312453\n",
      "Epoch: 2 - Batch: 1534, Training Loss: 0.13411217965000305\n",
      "Epoch: 2 - Batch: 1535, Training Loss: 0.13418859425024013\n",
      "Epoch: 2 - Batch: 1536, Training Loss: 0.13428645386402285\n",
      "Epoch: 2 - Batch: 1537, Training Loss: 0.13436537848825675\n",
      "Epoch: 2 - Batch: 1538, Training Loss: 0.13445299760643917\n",
      "Epoch: 2 - Batch: 1539, Training Loss: 0.13453556317372703\n",
      "Epoch: 2 - Batch: 1540, Training Loss: 0.13462239018893163\n",
      "Epoch: 2 - Batch: 1541, Training Loss: 0.13471847383388832\n",
      "Epoch: 2 - Batch: 1542, Training Loss: 0.1347967817712186\n",
      "Epoch: 2 - Batch: 1543, Training Loss: 0.13487858840482153\n",
      "Epoch: 2 - Batch: 1544, Training Loss: 0.13495994607607523\n",
      "Epoch: 2 - Batch: 1545, Training Loss: 0.13504616114284665\n",
      "Epoch: 2 - Batch: 1546, Training Loss: 0.13512827531368182\n",
      "Epoch: 2 - Batch: 1547, Training Loss: 0.13521591267528424\n",
      "Epoch: 2 - Batch: 1548, Training Loss: 0.13531345171022968\n",
      "Epoch: 2 - Batch: 1549, Training Loss: 0.13539488523407758\n",
      "Epoch: 2 - Batch: 1550, Training Loss: 0.1354818222040363\n",
      "Epoch: 2 - Batch: 1551, Training Loss: 0.1355645652261144\n",
      "Epoch: 2 - Batch: 1552, Training Loss: 0.13565978879853466\n",
      "Epoch: 2 - Batch: 1553, Training Loss: 0.13575149097697653\n",
      "Epoch: 2 - Batch: 1554, Training Loss: 0.13583671014329687\n",
      "Epoch: 2 - Batch: 1555, Training Loss: 0.1359251704913368\n",
      "Epoch: 2 - Batch: 1556, Training Loss: 0.13601128113877714\n",
      "Epoch: 2 - Batch: 1557, Training Loss: 0.13610018375847074\n",
      "Epoch: 2 - Batch: 1558, Training Loss: 0.13619269032161035\n",
      "Epoch: 2 - Batch: 1559, Training Loss: 0.13627472449446199\n",
      "Epoch: 2 - Batch: 1560, Training Loss: 0.13635698811554198\n",
      "Epoch: 2 - Batch: 1561, Training Loss: 0.1364439352729032\n",
      "Epoch: 2 - Batch: 1562, Training Loss: 0.13651987381440095\n",
      "Epoch: 2 - Batch: 1563, Training Loss: 0.1366050282731084\n",
      "Epoch: 2 - Batch: 1564, Training Loss: 0.1366969882070425\n",
      "Epoch: 2 - Batch: 1565, Training Loss: 0.13678597224564298\n",
      "Epoch: 2 - Batch: 1566, Training Loss: 0.13686868394562854\n",
      "Epoch: 2 - Batch: 1567, Training Loss: 0.13695374986227868\n",
      "Epoch: 2 - Batch: 1568, Training Loss: 0.13704071305753976\n",
      "Epoch: 2 - Batch: 1569, Training Loss: 0.13713786926485017\n",
      "Epoch: 2 - Batch: 1570, Training Loss: 0.13722174305771517\n",
      "Epoch: 2 - Batch: 1571, Training Loss: 0.13731519061188596\n",
      "Epoch: 2 - Batch: 1572, Training Loss: 0.1374069084513741\n",
      "Epoch: 2 - Batch: 1573, Training Loss: 0.1374894175462264\n",
      "Epoch: 2 - Batch: 1574, Training Loss: 0.13757392411949623\n",
      "Epoch: 2 - Batch: 1575, Training Loss: 0.13765565976871186\n",
      "Epoch: 2 - Batch: 1576, Training Loss: 0.13773931150832777\n",
      "Epoch: 2 - Batch: 1577, Training Loss: 0.13782593289121467\n",
      "Epoch: 2 - Batch: 1578, Training Loss: 0.13790880791707022\n",
      "Epoch: 2 - Batch: 1579, Training Loss: 0.13800023005005732\n",
      "Epoch: 2 - Batch: 1580, Training Loss: 0.13809587703909645\n",
      "Epoch: 2 - Batch: 1581, Training Loss: 0.13817824747654336\n",
      "Epoch: 2 - Batch: 1582, Training Loss: 0.13826500001683165\n",
      "Epoch: 2 - Batch: 1583, Training Loss: 0.13835334123589507\n",
      "Epoch: 2 - Batch: 1584, Training Loss: 0.13843951709136046\n",
      "Epoch: 2 - Batch: 1585, Training Loss: 0.1385279007157184\n",
      "Epoch: 2 - Batch: 1586, Training Loss: 0.13860876084757878\n",
      "Epoch: 2 - Batch: 1587, Training Loss: 0.13869564292665146\n",
      "Epoch: 2 - Batch: 1588, Training Loss: 0.1387887646816362\n",
      "Epoch: 2 - Batch: 1589, Training Loss: 0.13887928189651091\n",
      "Epoch: 2 - Batch: 1590, Training Loss: 0.13897016359675385\n",
      "Epoch: 2 - Batch: 1591, Training Loss: 0.13906051705948155\n",
      "Epoch: 2 - Batch: 1592, Training Loss: 0.13914112295726835\n",
      "Epoch: 2 - Batch: 1593, Training Loss: 0.13923392543695853\n",
      "Epoch: 2 - Batch: 1594, Training Loss: 0.13932092765818782\n",
      "Epoch: 2 - Batch: 1595, Training Loss: 0.13939908518769453\n",
      "Epoch: 2 - Batch: 1596, Training Loss: 0.13948407093337914\n",
      "Epoch: 2 - Batch: 1597, Training Loss: 0.13957000182166227\n",
      "Epoch: 2 - Batch: 1598, Training Loss: 0.13965931019527994\n",
      "Epoch: 2 - Batch: 1599, Training Loss: 0.1397443167791141\n",
      "Epoch: 2 - Batch: 1600, Training Loss: 0.13983090343587037\n",
      "Epoch: 2 - Batch: 1601, Training Loss: 0.1399130111260596\n",
      "Epoch: 2 - Batch: 1602, Training Loss: 0.13998717899939314\n",
      "Epoch: 2 - Batch: 1603, Training Loss: 0.14007219870514537\n",
      "Epoch: 2 - Batch: 1604, Training Loss: 0.14015957043439792\n",
      "Epoch: 2 - Batch: 1605, Training Loss: 0.14024259753611748\n",
      "Epoch: 2 - Batch: 1606, Training Loss: 0.14032964217499713\n",
      "Epoch: 2 - Batch: 1607, Training Loss: 0.1404115211521314\n",
      "Epoch: 2 - Batch: 1608, Training Loss: 0.14050086899035014\n",
      "Epoch: 2 - Batch: 1609, Training Loss: 0.1405997389115109\n",
      "Epoch: 2 - Batch: 1610, Training Loss: 0.14068216103967743\n",
      "Epoch: 2 - Batch: 1611, Training Loss: 0.14076557777710816\n",
      "Epoch: 2 - Batch: 1612, Training Loss: 0.14085624855054948\n",
      "Epoch: 2 - Batch: 1613, Training Loss: 0.14094484470179228\n",
      "Epoch: 2 - Batch: 1614, Training Loss: 0.1410346297148745\n",
      "Epoch: 2 - Batch: 1615, Training Loss: 0.14111881969407028\n",
      "Epoch: 2 - Batch: 1616, Training Loss: 0.14120941565280926\n",
      "Epoch: 2 - Batch: 1617, Training Loss: 0.141300297624881\n",
      "Epoch: 2 - Batch: 1618, Training Loss: 0.1413889289903107\n",
      "Epoch: 2 - Batch: 1619, Training Loss: 0.14147327382543787\n",
      "Epoch: 2 - Batch: 1620, Training Loss: 0.1415565604124694\n",
      "Epoch: 2 - Batch: 1621, Training Loss: 0.14164394063637228\n",
      "Epoch: 2 - Batch: 1622, Training Loss: 0.14173009700047634\n",
      "Epoch: 2 - Batch: 1623, Training Loss: 0.14182500240345103\n",
      "Epoch: 2 - Batch: 1624, Training Loss: 0.14191078930895523\n",
      "Epoch: 2 - Batch: 1625, Training Loss: 0.1419946822842852\n",
      "Epoch: 2 - Batch: 1626, Training Loss: 0.14208441298102858\n",
      "Epoch: 2 - Batch: 1627, Training Loss: 0.1421733310208293\n",
      "Epoch: 2 - Batch: 1628, Training Loss: 0.14226407208055208\n",
      "Epoch: 2 - Batch: 1629, Training Loss: 0.1423525249347659\n",
      "Epoch: 2 - Batch: 1630, Training Loss: 0.1424444615902691\n",
      "Epoch: 2 - Batch: 1631, Training Loss: 0.14252229753393636\n",
      "Epoch: 2 - Batch: 1632, Training Loss: 0.1426100642180957\n",
      "Epoch: 2 - Batch: 1633, Training Loss: 0.14269937057153106\n",
      "Epoch: 2 - Batch: 1634, Training Loss: 0.14278322168522412\n",
      "Epoch: 2 - Batch: 1635, Training Loss: 0.142867616326803\n",
      "Epoch: 2 - Batch: 1636, Training Loss: 0.14295575896256402\n",
      "Epoch: 2 - Batch: 1637, Training Loss: 0.14304967764548795\n",
      "Epoch: 2 - Batch: 1638, Training Loss: 0.14313501996261563\n",
      "Epoch: 2 - Batch: 1639, Training Loss: 0.14321788299711388\n",
      "Epoch: 2 - Batch: 1640, Training Loss: 0.14330426554132258\n",
      "Epoch: 2 - Batch: 1641, Training Loss: 0.14338912479394111\n",
      "Epoch: 2 - Batch: 1642, Training Loss: 0.1434818909597733\n",
      "Epoch: 2 - Batch: 1643, Training Loss: 0.1435718987921676\n",
      "Epoch: 2 - Batch: 1644, Training Loss: 0.14366080763303424\n",
      "Epoch: 2 - Batch: 1645, Training Loss: 0.14374170739012176\n",
      "Epoch: 2 - Batch: 1646, Training Loss: 0.14383502147016833\n",
      "Epoch: 2 - Batch: 1647, Training Loss: 0.14392362502874625\n",
      "Epoch: 2 - Batch: 1648, Training Loss: 0.1439983297578039\n",
      "Epoch: 2 - Batch: 1649, Training Loss: 0.1440883679300003\n",
      "Epoch: 2 - Batch: 1650, Training Loss: 0.14417369700782928\n",
      "Epoch: 2 - Batch: 1651, Training Loss: 0.1442523206928575\n",
      "Epoch: 2 - Batch: 1652, Training Loss: 0.14433592447939994\n",
      "Epoch: 2 - Batch: 1653, Training Loss: 0.14442484884539844\n",
      "Epoch: 2 - Batch: 1654, Training Loss: 0.14450884814286113\n",
      "Epoch: 2 - Batch: 1655, Training Loss: 0.1445955208952154\n",
      "Epoch: 2 - Batch: 1656, Training Loss: 0.14468138477151865\n",
      "Epoch: 2 - Batch: 1657, Training Loss: 0.1447766731442207\n",
      "Epoch: 2 - Batch: 1658, Training Loss: 0.14485936795993032\n",
      "Epoch: 2 - Batch: 1659, Training Loss: 0.14495065285025743\n",
      "Epoch: 2 - Batch: 1660, Training Loss: 0.1450345840051795\n",
      "Epoch: 2 - Batch: 1661, Training Loss: 0.14512245645545807\n",
      "Epoch: 2 - Batch: 1662, Training Loss: 0.14521735985060633\n",
      "Epoch: 2 - Batch: 1663, Training Loss: 0.14530072941956038\n",
      "Epoch: 2 - Batch: 1664, Training Loss: 0.14538614403079597\n",
      "Epoch: 2 - Batch: 1665, Training Loss: 0.14546672896069673\n",
      "Epoch: 2 - Batch: 1666, Training Loss: 0.14554913285779914\n",
      "Epoch: 2 - Batch: 1667, Training Loss: 0.14563147063254323\n",
      "Epoch: 2 - Batch: 1668, Training Loss: 0.14572590759416323\n",
      "Epoch: 2 - Batch: 1669, Training Loss: 0.14581370096696947\n",
      "Epoch: 2 - Batch: 1670, Training Loss: 0.1458944490312245\n",
      "Epoch: 2 - Batch: 1671, Training Loss: 0.14597876273577487\n",
      "Epoch: 2 - Batch: 1672, Training Loss: 0.14606588482733193\n",
      "Epoch: 2 - Batch: 1673, Training Loss: 0.14615648010032095\n",
      "Epoch: 2 - Batch: 1674, Training Loss: 0.14623569135568035\n",
      "Epoch: 2 - Batch: 1675, Training Loss: 0.14631827291144464\n",
      "Epoch: 2 - Batch: 1676, Training Loss: 0.14640609121218842\n",
      "Epoch: 2 - Batch: 1677, Training Loss: 0.14649325973704877\n",
      "Epoch: 2 - Batch: 1678, Training Loss: 0.1465736919918266\n",
      "Epoch: 2 - Batch: 1679, Training Loss: 0.14667277400741727\n",
      "Epoch: 2 - Batch: 1680, Training Loss: 0.14676359114213963\n",
      "Epoch: 2 - Batch: 1681, Training Loss: 0.14685279838926163\n",
      "Epoch: 2 - Batch: 1682, Training Loss: 0.146942628569172\n",
      "Epoch: 2 - Batch: 1683, Training Loss: 0.14702970682833325\n",
      "Epoch: 2 - Batch: 1684, Training Loss: 0.1471211168093369\n",
      "Epoch: 2 - Batch: 1685, Training Loss: 0.1472110290074131\n",
      "Epoch: 2 - Batch: 1686, Training Loss: 0.14729991287709665\n",
      "Epoch: 2 - Batch: 1687, Training Loss: 0.1473817223896138\n",
      "Epoch: 2 - Batch: 1688, Training Loss: 0.14746577628759996\n",
      "Epoch: 2 - Batch: 1689, Training Loss: 0.1475480212698726\n",
      "Epoch: 2 - Batch: 1690, Training Loss: 0.14763659325306294\n",
      "Epoch: 2 - Batch: 1691, Training Loss: 0.14771824920083554\n",
      "Epoch: 2 - Batch: 1692, Training Loss: 0.14779683990808665\n",
      "Epoch: 2 - Batch: 1693, Training Loss: 0.1478847793023879\n",
      "Epoch: 2 - Batch: 1694, Training Loss: 0.14797735700436296\n",
      "Epoch: 2 - Batch: 1695, Training Loss: 0.14805429738947803\n",
      "Epoch: 2 - Batch: 1696, Training Loss: 0.14814351227251846\n",
      "Epoch: 2 - Batch: 1697, Training Loss: 0.14823940967767196\n",
      "Epoch: 2 - Batch: 1698, Training Loss: 0.14831847975503154\n",
      "Epoch: 2 - Batch: 1699, Training Loss: 0.14840488895699752\n",
      "Epoch: 2 - Batch: 1700, Training Loss: 0.1484942842725893\n",
      "Epoch: 2 - Batch: 1701, Training Loss: 0.14858093509947878\n",
      "Epoch: 2 - Batch: 1702, Training Loss: 0.1486622768625691\n",
      "Epoch: 2 - Batch: 1703, Training Loss: 0.14874496399911483\n",
      "Epoch: 2 - Batch: 1704, Training Loss: 0.1488432518482999\n",
      "Epoch: 2 - Batch: 1705, Training Loss: 0.1489199478469579\n",
      "Epoch: 2 - Batch: 1706, Training Loss: 0.1490050928225466\n",
      "Epoch: 2 - Batch: 1707, Training Loss: 0.14909468326797928\n",
      "Epoch: 2 - Batch: 1708, Training Loss: 0.1491839406984066\n",
      "Epoch: 2 - Batch: 1709, Training Loss: 0.14926373817113106\n",
      "Epoch: 2 - Batch: 1710, Training Loss: 0.1493524772487272\n",
      "Epoch: 2 - Batch: 1711, Training Loss: 0.14944301336806015\n",
      "Epoch: 2 - Batch: 1712, Training Loss: 0.14952546493405133\n",
      "Epoch: 2 - Batch: 1713, Training Loss: 0.14962545461075422\n",
      "Epoch: 2 - Batch: 1714, Training Loss: 0.14971406379124616\n",
      "Epoch: 2 - Batch: 1715, Training Loss: 0.14981110883292867\n",
      "Epoch: 2 - Batch: 1716, Training Loss: 0.14989785129083924\n",
      "Epoch: 2 - Batch: 1717, Training Loss: 0.1499881488371449\n",
      "Epoch: 2 - Batch: 1718, Training Loss: 0.1500764837238326\n",
      "Epoch: 2 - Batch: 1719, Training Loss: 0.15016542986702563\n",
      "Epoch: 2 - Batch: 1720, Training Loss: 0.15025589113458868\n",
      "Epoch: 2 - Batch: 1721, Training Loss: 0.15034019056812645\n",
      "Epoch: 2 - Batch: 1722, Training Loss: 0.15042561851132963\n",
      "Epoch: 2 - Batch: 1723, Training Loss: 0.1505147099779119\n",
      "Epoch: 2 - Batch: 1724, Training Loss: 0.1506009805039387\n",
      "Epoch: 2 - Batch: 1725, Training Loss: 0.15069051522446508\n",
      "Epoch: 2 - Batch: 1726, Training Loss: 0.1507768838992265\n",
      "Epoch: 2 - Batch: 1727, Training Loss: 0.15085999258665697\n",
      "Epoch: 2 - Batch: 1728, Training Loss: 0.15094678415909138\n",
      "Epoch: 2 - Batch: 1729, Training Loss: 0.15103092242285585\n",
      "Epoch: 2 - Batch: 1730, Training Loss: 0.1511108340822148\n",
      "Epoch: 2 - Batch: 1731, Training Loss: 0.15120385185032342\n",
      "Epoch: 2 - Batch: 1732, Training Loss: 0.15128814917718791\n",
      "Epoch: 2 - Batch: 1733, Training Loss: 0.15137578986869324\n",
      "Epoch: 2 - Batch: 1734, Training Loss: 0.15145556645384475\n",
      "Epoch: 2 - Batch: 1735, Training Loss: 0.15154220884786315\n",
      "Epoch: 2 - Batch: 1736, Training Loss: 0.15162804848197878\n",
      "Epoch: 2 - Batch: 1737, Training Loss: 0.15171315352344394\n",
      "Epoch: 2 - Batch: 1738, Training Loss: 0.15180170128298043\n",
      "Epoch: 2 - Batch: 1739, Training Loss: 0.15188808071351367\n",
      "Epoch: 2 - Batch: 1740, Training Loss: 0.15197959170363237\n",
      "Epoch: 2 - Batch: 1741, Training Loss: 0.15207622753792932\n",
      "Epoch: 2 - Batch: 1742, Training Loss: 0.1521654290519346\n",
      "Epoch: 2 - Batch: 1743, Training Loss: 0.15224705971344984\n",
      "Epoch: 2 - Batch: 1744, Training Loss: 0.15233374640320863\n",
      "Epoch: 2 - Batch: 1745, Training Loss: 0.1524252915449107\n",
      "Epoch: 2 - Batch: 1746, Training Loss: 0.15251752909019614\n",
      "Epoch: 2 - Batch: 1747, Training Loss: 0.15260672662561608\n",
      "Epoch: 2 - Batch: 1748, Training Loss: 0.15269538837409335\n",
      "Epoch: 2 - Batch: 1749, Training Loss: 0.15278914649885883\n",
      "Epoch: 2 - Batch: 1750, Training Loss: 0.15287895173847577\n",
      "Epoch: 2 - Batch: 1751, Training Loss: 0.1529627593780928\n",
      "Epoch: 2 - Batch: 1752, Training Loss: 0.15304048237071108\n",
      "Epoch: 2 - Batch: 1753, Training Loss: 0.15312235286555084\n",
      "Epoch: 2 - Batch: 1754, Training Loss: 0.15320977537118974\n",
      "Epoch: 2 - Batch: 1755, Training Loss: 0.15330420598807817\n",
      "Epoch: 2 - Batch: 1756, Training Loss: 0.15339134186259154\n",
      "Epoch: 2 - Batch: 1757, Training Loss: 0.15347412252060413\n",
      "Epoch: 2 - Batch: 1758, Training Loss: 0.15356402568037236\n",
      "Epoch: 2 - Batch: 1759, Training Loss: 0.15365134768966418\n",
      "Epoch: 2 - Batch: 1760, Training Loss: 0.15374100679040548\n",
      "Epoch: 2 - Batch: 1761, Training Loss: 0.1538290305875526\n",
      "Epoch: 2 - Batch: 1762, Training Loss: 0.15391919698881273\n",
      "Epoch: 2 - Batch: 1763, Training Loss: 0.1540054565304844\n",
      "Epoch: 2 - Batch: 1764, Training Loss: 0.15409327266895356\n",
      "Epoch: 2 - Batch: 1765, Training Loss: 0.15418823251800357\n",
      "Epoch: 2 - Batch: 1766, Training Loss: 0.1542714180897421\n",
      "Epoch: 2 - Batch: 1767, Training Loss: 0.154351739178892\n",
      "Epoch: 2 - Batch: 1768, Training Loss: 0.15443747203988617\n",
      "Epoch: 2 - Batch: 1769, Training Loss: 0.1545276931837621\n",
      "Epoch: 2 - Batch: 1770, Training Loss: 0.1546158209801412\n",
      "Epoch: 2 - Batch: 1771, Training Loss: 0.15470052349256047\n",
      "Epoch: 2 - Batch: 1772, Training Loss: 0.1547944879786391\n",
      "Epoch: 2 - Batch: 1773, Training Loss: 0.15487927540667218\n",
      "Epoch: 2 - Batch: 1774, Training Loss: 0.15496793535119463\n",
      "Epoch: 2 - Batch: 1775, Training Loss: 0.15506923619416815\n",
      "Epoch: 2 - Batch: 1776, Training Loss: 0.15516827653345974\n",
      "Epoch: 2 - Batch: 1777, Training Loss: 0.15525289873899908\n",
      "Epoch: 2 - Batch: 1778, Training Loss: 0.15534417175534945\n",
      "Epoch: 2 - Batch: 1779, Training Loss: 0.15543379032888618\n",
      "Epoch: 2 - Batch: 1780, Training Loss: 0.15551993421975455\n",
      "Epoch: 2 - Batch: 1781, Training Loss: 0.1556107601086002\n",
      "Epoch: 2 - Batch: 1782, Training Loss: 0.15570282699481566\n",
      "Epoch: 2 - Batch: 1783, Training Loss: 0.1557916783994901\n",
      "Epoch: 2 - Batch: 1784, Training Loss: 0.15587416195479\n",
      "Epoch: 2 - Batch: 1785, Training Loss: 0.15597406487485663\n",
      "Epoch: 2 - Batch: 1786, Training Loss: 0.1560580709927513\n",
      "Epoch: 2 - Batch: 1787, Training Loss: 0.15615118450020282\n",
      "Epoch: 2 - Batch: 1788, Training Loss: 0.1562446281252117\n",
      "Epoch: 2 - Batch: 1789, Training Loss: 0.1563296414059193\n",
      "Epoch: 2 - Batch: 1790, Training Loss: 0.15641384095472483\n",
      "Epoch: 2 - Batch: 1791, Training Loss: 0.15649835142008897\n",
      "Epoch: 2 - Batch: 1792, Training Loss: 0.15658208432531673\n",
      "Epoch: 2 - Batch: 1793, Training Loss: 0.156655258787904\n",
      "Epoch: 2 - Batch: 1794, Training Loss: 0.15673302983862053\n",
      "Epoch: 2 - Batch: 1795, Training Loss: 0.15681216606922807\n",
      "Epoch: 2 - Batch: 1796, Training Loss: 0.15689903361163723\n",
      "Epoch: 2 - Batch: 1797, Training Loss: 0.15698355685295554\n",
      "Epoch: 2 - Batch: 1798, Training Loss: 0.15706499971782983\n",
      "Epoch: 2 - Batch: 1799, Training Loss: 0.1571590435284979\n",
      "Epoch: 2 - Batch: 1800, Training Loss: 0.1572473431515753\n",
      "Epoch: 2 - Batch: 1801, Training Loss: 0.15733465003730052\n",
      "Epoch: 2 - Batch: 1802, Training Loss: 0.15741921402205084\n",
      "Epoch: 2 - Batch: 1803, Training Loss: 0.15750598541737393\n",
      "Epoch: 2 - Batch: 1804, Training Loss: 0.1575868351085368\n",
      "Epoch: 2 - Batch: 1805, Training Loss: 0.15768171967953987\n",
      "Epoch: 2 - Batch: 1806, Training Loss: 0.15776131094539936\n",
      "Epoch: 2 - Batch: 1807, Training Loss: 0.15784667222876453\n",
      "Epoch: 2 - Batch: 1808, Training Loss: 0.15793421236214353\n",
      "Epoch: 2 - Batch: 1809, Training Loss: 0.15802852669808007\n",
      "Epoch: 2 - Batch: 1810, Training Loss: 0.15811183936214368\n",
      "Epoch: 2 - Batch: 1811, Training Loss: 0.15819815456076444\n",
      "Epoch: 2 - Batch: 1812, Training Loss: 0.15828932880440952\n",
      "Epoch: 2 - Batch: 1813, Training Loss: 0.1583748328535019\n",
      "Epoch: 2 - Batch: 1814, Training Loss: 0.15845766421994364\n",
      "Epoch: 2 - Batch: 1815, Training Loss: 0.1585424038802411\n",
      "Epoch: 2 - Batch: 1816, Training Loss: 0.15863242353101079\n",
      "Epoch: 2 - Batch: 1817, Training Loss: 0.1587251526292778\n",
      "Epoch: 2 - Batch: 1818, Training Loss: 0.15881272718606898\n",
      "Epoch: 2 - Batch: 1819, Training Loss: 0.1589113119279172\n",
      "Epoch: 2 - Batch: 1820, Training Loss: 0.15899579440726966\n",
      "Epoch: 2 - Batch: 1821, Training Loss: 0.15908483738947665\n",
      "Epoch: 2 - Batch: 1822, Training Loss: 0.15917595213967967\n",
      "Epoch: 2 - Batch: 1823, Training Loss: 0.15926114494478327\n",
      "Epoch: 2 - Batch: 1824, Training Loss: 0.15935288842267065\n",
      "Epoch: 2 - Batch: 1825, Training Loss: 0.15944722077344387\n",
      "Epoch: 2 - Batch: 1826, Training Loss: 0.15953691466827297\n",
      "Epoch: 2 - Batch: 1827, Training Loss: 0.15962610392527002\n",
      "Epoch: 2 - Batch: 1828, Training Loss: 0.1597096047010489\n",
      "Epoch: 2 - Batch: 1829, Training Loss: 0.1597931426308839\n",
      "Epoch: 2 - Batch: 1830, Training Loss: 0.15987653502110225\n",
      "Epoch: 2 - Batch: 1831, Training Loss: 0.1599610408221311\n",
      "Epoch: 2 - Batch: 1832, Training Loss: 0.16004051374361092\n",
      "Epoch: 2 - Batch: 1833, Training Loss: 0.16013273139596973\n",
      "Epoch: 2 - Batch: 1834, Training Loss: 0.16021521926346308\n",
      "Epoch: 2 - Batch: 1835, Training Loss: 0.16029753127624938\n",
      "Epoch: 2 - Batch: 1836, Training Loss: 0.16038455533027451\n",
      "Epoch: 2 - Batch: 1837, Training Loss: 0.16046874094785346\n",
      "Epoch: 2 - Batch: 1838, Training Loss: 0.16055467017427408\n",
      "Epoch: 2 - Batch: 1839, Training Loss: 0.1606510915541728\n",
      "Epoch: 2 - Batch: 1840, Training Loss: 0.16073177024030172\n",
      "Epoch: 2 - Batch: 1841, Training Loss: 0.16081632939391272\n",
      "Epoch: 2 - Batch: 1842, Training Loss: 0.16090112631124248\n",
      "Epoch: 2 - Batch: 1843, Training Loss: 0.16098863993513446\n",
      "Epoch: 2 - Batch: 1844, Training Loss: 0.16107467216614071\n",
      "Epoch: 2 - Batch: 1845, Training Loss: 0.1611592815463025\n",
      "Epoch: 2 - Batch: 1846, Training Loss: 0.1612481523867369\n",
      "Epoch: 2 - Batch: 1847, Training Loss: 0.16132865162260496\n",
      "Epoch: 2 - Batch: 1848, Training Loss: 0.16141126304247091\n",
      "Epoch: 2 - Batch: 1849, Training Loss: 0.16150742478878738\n",
      "Epoch: 2 - Batch: 1850, Training Loss: 0.16158718706585876\n",
      "Epoch: 2 - Batch: 1851, Training Loss: 0.16167162904271834\n",
      "Epoch: 2 - Batch: 1852, Training Loss: 0.1617597097447559\n",
      "Epoch: 2 - Batch: 1853, Training Loss: 0.1618445378853314\n",
      "Epoch: 2 - Batch: 1854, Training Loss: 0.16193256959022573\n",
      "Epoch: 2 - Batch: 1855, Training Loss: 0.16201395842294591\n",
      "Epoch: 2 - Batch: 1856, Training Loss: 0.16209947682336392\n",
      "Epoch: 2 - Batch: 1857, Training Loss: 0.16218531128037628\n",
      "Epoch: 2 - Batch: 1858, Training Loss: 0.16227877979007724\n",
      "Epoch: 2 - Batch: 1859, Training Loss: 0.16236446220211523\n",
      "Epoch: 2 - Batch: 1860, Training Loss: 0.16245706979908162\n",
      "Epoch: 2 - Batch: 1861, Training Loss: 0.1625435823515972\n",
      "Epoch: 2 - Batch: 1862, Training Loss: 0.16263069683194753\n",
      "Epoch: 2 - Batch: 1863, Training Loss: 0.1627235814101166\n",
      "Epoch: 2 - Batch: 1864, Training Loss: 0.16280653598296702\n",
      "Epoch: 2 - Batch: 1865, Training Loss: 0.16289825812229272\n",
      "Epoch: 2 - Batch: 1866, Training Loss: 0.16299719839151425\n",
      "Epoch: 2 - Batch: 1867, Training Loss: 0.1630915337200485\n",
      "Epoch: 2 - Batch: 1868, Training Loss: 0.16317566463825714\n",
      "Epoch: 2 - Batch: 1869, Training Loss: 0.16325598815533257\n",
      "Epoch: 2 - Batch: 1870, Training Loss: 0.1633413186170173\n",
      "Epoch: 2 - Batch: 1871, Training Loss: 0.16343207587526606\n",
      "Epoch: 2 - Batch: 1872, Training Loss: 0.16351665208613497\n",
      "Epoch: 2 - Batch: 1873, Training Loss: 0.16360010666954972\n",
      "Epoch: 2 - Batch: 1874, Training Loss: 0.16369126130445283\n",
      "Epoch: 2 - Batch: 1875, Training Loss: 0.16377608617072675\n",
      "Epoch: 2 - Batch: 1876, Training Loss: 0.1638591319915667\n",
      "Epoch: 2 - Batch: 1877, Training Loss: 0.16394434247172096\n",
      "Epoch: 2 - Batch: 1878, Training Loss: 0.16403184807666302\n",
      "Epoch: 2 - Batch: 1879, Training Loss: 0.16411712440116885\n",
      "Epoch: 2 - Batch: 1880, Training Loss: 0.16419980969669215\n",
      "Epoch: 2 - Batch: 1881, Training Loss: 0.16428706803626286\n",
      "Epoch: 2 - Batch: 1882, Training Loss: 0.164371295663749\n",
      "Epoch: 2 - Batch: 1883, Training Loss: 0.16445683382117926\n",
      "Epoch: 2 - Batch: 1884, Training Loss: 0.16454216160164345\n",
      "Epoch: 2 - Batch: 1885, Training Loss: 0.16463333903927707\n",
      "Epoch: 2 - Batch: 1886, Training Loss: 0.16471462193907394\n",
      "Epoch: 2 - Batch: 1887, Training Loss: 0.16480653691005154\n",
      "Epoch: 2 - Batch: 1888, Training Loss: 0.1648965090710923\n",
      "Epoch: 2 - Batch: 1889, Training Loss: 0.16498637090052537\n",
      "Epoch: 2 - Batch: 1890, Training Loss: 0.1650742366786422\n",
      "Epoch: 2 - Batch: 1891, Training Loss: 0.1651648732387209\n",
      "Epoch: 2 - Batch: 1892, Training Loss: 0.16525326537008506\n",
      "Epoch: 2 - Batch: 1893, Training Loss: 0.165334600689597\n",
      "Epoch: 2 - Batch: 1894, Training Loss: 0.16542501087187733\n",
      "Epoch: 2 - Batch: 1895, Training Loss: 0.16551138563189735\n",
      "Epoch: 2 - Batch: 1896, Training Loss: 0.16560124340243204\n",
      "Epoch: 2 - Batch: 1897, Training Loss: 0.1656891857806723\n",
      "Epoch: 2 - Batch: 1898, Training Loss: 0.16577190142835352\n",
      "Epoch: 2 - Batch: 1899, Training Loss: 0.16585543031988056\n",
      "Epoch: 2 - Batch: 1900, Training Loss: 0.1659410392392927\n",
      "Epoch: 2 - Batch: 1901, Training Loss: 0.16603186698151662\n",
      "Epoch: 2 - Batch: 1902, Training Loss: 0.1661204436043305\n",
      "Epoch: 2 - Batch: 1903, Training Loss: 0.16620110053797663\n",
      "Epoch: 2 - Batch: 1904, Training Loss: 0.16629085977674518\n",
      "Epoch: 2 - Batch: 1905, Training Loss: 0.1663665330195012\n",
      "Epoch: 2 - Batch: 1906, Training Loss: 0.16644225056417547\n",
      "Epoch: 2 - Batch: 1907, Training Loss: 0.16652118483215422\n",
      "Epoch: 2 - Batch: 1908, Training Loss: 0.1666147137681643\n",
      "Epoch: 2 - Batch: 1909, Training Loss: 0.16670936194694852\n",
      "Epoch: 2 - Batch: 1910, Training Loss: 0.1667862461747024\n",
      "Epoch: 2 - Batch: 1911, Training Loss: 0.1668752947417559\n",
      "Epoch: 2 - Batch: 1912, Training Loss: 0.16695774634481464\n",
      "Epoch: 2 - Batch: 1913, Training Loss: 0.16704434188543069\n",
      "Epoch: 2 - Batch: 1914, Training Loss: 0.16712067855111204\n",
      "Epoch: 2 - Batch: 1915, Training Loss: 0.16721140744649554\n",
      "Epoch: 2 - Batch: 1916, Training Loss: 0.16729723492862772\n",
      "Epoch: 2 - Batch: 1917, Training Loss: 0.1673840894973891\n",
      "Epoch: 2 - Batch: 1918, Training Loss: 0.16747173230174564\n",
      "Epoch: 2 - Batch: 1919, Training Loss: 0.16755294862819547\n",
      "Epoch: 2 - Batch: 1920, Training Loss: 0.16764588831965602\n",
      "Epoch: 2 - Batch: 1921, Training Loss: 0.16773191161737908\n",
      "Epoch: 2 - Batch: 1922, Training Loss: 0.16781312771524562\n",
      "Epoch: 2 - Batch: 1923, Training Loss: 0.1678978173343301\n",
      "Epoch: 2 - Batch: 1924, Training Loss: 0.1679877592482377\n",
      "Epoch: 2 - Batch: 1925, Training Loss: 0.16807615095332487\n",
      "Epoch: 2 - Batch: 1926, Training Loss: 0.16816513092067112\n",
      "Epoch: 2 - Batch: 1927, Training Loss: 0.168247378695367\n",
      "Epoch: 2 - Batch: 1928, Training Loss: 0.1683308410382587\n",
      "Epoch: 2 - Batch: 1929, Training Loss: 0.1684146973723005\n",
      "Epoch: 2 - Batch: 1930, Training Loss: 0.16851256886623786\n",
      "Epoch: 2 - Batch: 1931, Training Loss: 0.16860473315983665\n",
      "Epoch: 2 - Batch: 1932, Training Loss: 0.16869125557850248\n",
      "Epoch: 2 - Batch: 1933, Training Loss: 0.16877943725258754\n",
      "Epoch: 2 - Batch: 1934, Training Loss: 0.168866331026477\n",
      "Epoch: 2 - Batch: 1935, Training Loss: 0.16895551616897433\n",
      "Epoch: 2 - Batch: 1936, Training Loss: 0.16904435748509308\n",
      "Epoch: 2 - Batch: 1937, Training Loss: 0.16912735907197196\n",
      "Epoch: 2 - Batch: 1938, Training Loss: 0.16921088817972646\n",
      "Epoch: 2 - Batch: 1939, Training Loss: 0.1692927506123708\n",
      "Epoch: 2 - Batch: 1940, Training Loss: 0.16937587166183427\n",
      "Epoch: 2 - Batch: 1941, Training Loss: 0.1694601334965051\n",
      "Epoch: 2 - Batch: 1942, Training Loss: 0.1695461373831799\n",
      "Epoch: 2 - Batch: 1943, Training Loss: 0.1696307210322735\n",
      "Epoch: 2 - Batch: 1944, Training Loss: 0.16971563116012522\n",
      "Epoch: 2 - Batch: 1945, Training Loss: 0.1697900837617826\n",
      "Epoch: 2 - Batch: 1946, Training Loss: 0.16987404191424796\n",
      "Epoch: 2 - Batch: 1947, Training Loss: 0.16996106775369416\n",
      "Epoch: 2 - Batch: 1948, Training Loss: 0.17005030254201708\n",
      "Epoch: 2 - Batch: 1949, Training Loss: 0.17014913246727503\n",
      "Epoch: 2 - Batch: 1950, Training Loss: 0.1702406810006593\n",
      "Epoch: 2 - Batch: 1951, Training Loss: 0.1703145918598519\n",
      "Epoch: 2 - Batch: 1952, Training Loss: 0.1703933538130761\n",
      "Epoch: 2 - Batch: 1953, Training Loss: 0.1704749462393979\n",
      "Epoch: 2 - Batch: 1954, Training Loss: 0.1705638274896402\n",
      "Epoch: 2 - Batch: 1955, Training Loss: 0.17065652936795853\n",
      "Epoch: 2 - Batch: 1956, Training Loss: 0.17074085951197404\n",
      "Epoch: 2 - Batch: 1957, Training Loss: 0.17081968506078418\n",
      "Epoch: 2 - Batch: 1958, Training Loss: 0.170896838664712\n",
      "Epoch: 2 - Batch: 1959, Training Loss: 0.17097913391678093\n",
      "Epoch: 2 - Batch: 1960, Training Loss: 0.17107431098794068\n",
      "Epoch: 2 - Batch: 1961, Training Loss: 0.17116561009985692\n",
      "Epoch: 2 - Batch: 1962, Training Loss: 0.17125002435635572\n",
      "Epoch: 2 - Batch: 1963, Training Loss: 0.1713376069753423\n",
      "Epoch: 2 - Batch: 1964, Training Loss: 0.1714108338020433\n",
      "Epoch: 2 - Batch: 1965, Training Loss: 0.171498098282474\n",
      "Epoch: 2 - Batch: 1966, Training Loss: 0.17157626496149136\n",
      "Epoch: 2 - Batch: 1967, Training Loss: 0.17166122810213919\n",
      "Epoch: 2 - Batch: 1968, Training Loss: 0.17175786574656887\n",
      "Epoch: 2 - Batch: 1969, Training Loss: 0.17183954243982213\n",
      "Epoch: 2 - Batch: 1970, Training Loss: 0.17192374231605784\n",
      "Epoch: 2 - Batch: 1971, Training Loss: 0.17200745804664705\n",
      "Epoch: 2 - Batch: 1972, Training Loss: 0.17209199056721247\n",
      "Epoch: 2 - Batch: 1973, Training Loss: 0.17217503847619195\n",
      "Epoch: 2 - Batch: 1974, Training Loss: 0.1722539989571172\n",
      "Epoch: 2 - Batch: 1975, Training Loss: 0.172337416454433\n",
      "Epoch: 2 - Batch: 1976, Training Loss: 0.1724223286457125\n",
      "Epoch: 2 - Batch: 1977, Training Loss: 0.17250374048503478\n",
      "Epoch: 2 - Batch: 1978, Training Loss: 0.1725888843424186\n",
      "Epoch: 2 - Batch: 1979, Training Loss: 0.17266779308937874\n",
      "Epoch: 2 - Batch: 1980, Training Loss: 0.17275356648993334\n",
      "Epoch: 2 - Batch: 1981, Training Loss: 0.1728375158017844\n",
      "Epoch: 2 - Batch: 1982, Training Loss: 0.17293486102278752\n",
      "Epoch: 2 - Batch: 1983, Training Loss: 0.17301036668308142\n",
      "Epoch: 2 - Batch: 1984, Training Loss: 0.17309879772303313\n",
      "Epoch: 2 - Batch: 1985, Training Loss: 0.1731859071807284\n",
      "Epoch: 2 - Batch: 1986, Training Loss: 0.17326594339328422\n",
      "Epoch: 2 - Batch: 1987, Training Loss: 0.17336162099395422\n",
      "Epoch: 2 - Batch: 1988, Training Loss: 0.1734409454474797\n",
      "Epoch: 2 - Batch: 1989, Training Loss: 0.17352686810676335\n",
      "Epoch: 2 - Batch: 1990, Training Loss: 0.1736067079909603\n",
      "Epoch: 2 - Batch: 1991, Training Loss: 0.17369151092559149\n",
      "Epoch: 2 - Batch: 1992, Training Loss: 0.17378717791875994\n",
      "Epoch: 2 - Batch: 1993, Training Loss: 0.1738804835288679\n",
      "Epoch: 2 - Batch: 1994, Training Loss: 0.17396836270690952\n",
      "Epoch: 2 - Batch: 1995, Training Loss: 0.17404654797531083\n",
      "Epoch: 2 - Batch: 1996, Training Loss: 0.17412757620857922\n",
      "Epoch: 2 - Batch: 1997, Training Loss: 0.17420711844022793\n",
      "Epoch: 2 - Batch: 1998, Training Loss: 0.17428928267080984\n",
      "Epoch: 2 - Batch: 1999, Training Loss: 0.1743756084583984\n",
      "Epoch: 2 - Batch: 2000, Training Loss: 0.1744599861353883\n",
      "Epoch: 2 - Batch: 2001, Training Loss: 0.17453853082325724\n",
      "Epoch: 2 - Batch: 2002, Training Loss: 0.17462574353883317\n",
      "Epoch: 2 - Batch: 2003, Training Loss: 0.17470903108344346\n",
      "Epoch: 2 - Batch: 2004, Training Loss: 0.1747939168887945\n",
      "Epoch: 2 - Batch: 2005, Training Loss: 0.1748838381746021\n",
      "Epoch: 2 - Batch: 2006, Training Loss: 0.17496581269956346\n",
      "Epoch: 2 - Batch: 2007, Training Loss: 0.17505876647669877\n",
      "Epoch: 2 - Batch: 2008, Training Loss: 0.17514160347123248\n",
      "Epoch: 2 - Batch: 2009, Training Loss: 0.17523190516910545\n",
      "Epoch: 2 - Batch: 2010, Training Loss: 0.17532030362048948\n",
      "Epoch: 2 - Batch: 2011, Training Loss: 0.17540059386324724\n",
      "Epoch: 2 - Batch: 2012, Training Loss: 0.17548404544418922\n",
      "Epoch: 2 - Batch: 2013, Training Loss: 0.1755718024722478\n",
      "Epoch: 2 - Batch: 2014, Training Loss: 0.17566572435533823\n",
      "Epoch: 2 - Batch: 2015, Training Loss: 0.17574837807547394\n",
      "Epoch: 2 - Batch: 2016, Training Loss: 0.17582755894160784\n",
      "Epoch: 2 - Batch: 2017, Training Loss: 0.17591428392216144\n",
      "Epoch: 2 - Batch: 2018, Training Loss: 0.1760073593797573\n",
      "Epoch: 2 - Batch: 2019, Training Loss: 0.17609258203660672\n",
      "Epoch: 2 - Batch: 2020, Training Loss: 0.17618049823896803\n",
      "Epoch: 2 - Batch: 2021, Training Loss: 0.17626462832933437\n",
      "Epoch: 2 - Batch: 2022, Training Loss: 0.17635423140862885\n",
      "Epoch: 2 - Batch: 2023, Training Loss: 0.17643759745616422\n",
      "Epoch: 2 - Batch: 2024, Training Loss: 0.17651521213117918\n",
      "Epoch: 2 - Batch: 2025, Training Loss: 0.17659480565198224\n",
      "Epoch: 2 - Batch: 2026, Training Loss: 0.17668826874364074\n",
      "Epoch: 2 - Batch: 2027, Training Loss: 0.1767763293118125\n",
      "Epoch: 2 - Batch: 2028, Training Loss: 0.1768580167670155\n",
      "Epoch: 2 - Batch: 2029, Training Loss: 0.17694556045888074\n",
      "Epoch: 2 - Batch: 2030, Training Loss: 0.17703661136638071\n",
      "Epoch: 2 - Batch: 2031, Training Loss: 0.1771285322619908\n",
      "Epoch: 2 - Batch: 2032, Training Loss: 0.17722474472291433\n",
      "Epoch: 2 - Batch: 2033, Training Loss: 0.17731188415591395\n",
      "Epoch: 2 - Batch: 2034, Training Loss: 0.1773954803559614\n",
      "Epoch: 2 - Batch: 2035, Training Loss: 0.17748013409982075\n",
      "Epoch: 2 - Batch: 2036, Training Loss: 0.17756802627622192\n",
      "Epoch: 2 - Batch: 2037, Training Loss: 0.1776511969159096\n",
      "Epoch: 2 - Batch: 2038, Training Loss: 0.17773256401468074\n",
      "Epoch: 2 - Batch: 2039, Training Loss: 0.1778158555872998\n",
      "Epoch: 2 - Batch: 2040, Training Loss: 0.17789782873458332\n",
      "Epoch: 2 - Batch: 2041, Training Loss: 0.17798422860787877\n",
      "Epoch: 2 - Batch: 2042, Training Loss: 0.17806593111251323\n",
      "Epoch: 2 - Batch: 2043, Training Loss: 0.17816085299819856\n",
      "Epoch: 2 - Batch: 2044, Training Loss: 0.17825306648663422\n",
      "Epoch: 2 - Batch: 2045, Training Loss: 0.17833907829341208\n",
      "Epoch: 2 - Batch: 2046, Training Loss: 0.17842760654305345\n",
      "Epoch: 2 - Batch: 2047, Training Loss: 0.17851301211598106\n",
      "Epoch: 2 - Batch: 2048, Training Loss: 0.17859901378477983\n",
      "Epoch: 2 - Batch: 2049, Training Loss: 0.17867880326944402\n",
      "Epoch: 2 - Batch: 2050, Training Loss: 0.17875576868134352\n",
      "Epoch: 2 - Batch: 2051, Training Loss: 0.17884308041056393\n",
      "Epoch: 2 - Batch: 2052, Training Loss: 0.17893128211349002\n",
      "Epoch: 2 - Batch: 2053, Training Loss: 0.1790178043159284\n",
      "Epoch: 2 - Batch: 2054, Training Loss: 0.17911111986632172\n",
      "Epoch: 2 - Batch: 2055, Training Loss: 0.17919805107227407\n",
      "Epoch: 2 - Batch: 2056, Training Loss: 0.179283181622402\n",
      "Epoch: 2 - Batch: 2057, Training Loss: 0.17936644328767387\n",
      "Epoch: 2 - Batch: 2058, Training Loss: 0.17945132706666467\n",
      "Epoch: 2 - Batch: 2059, Training Loss: 0.17954158560553593\n",
      "Epoch: 2 - Batch: 2060, Training Loss: 0.17962338011186713\n",
      "Epoch: 2 - Batch: 2061, Training Loss: 0.17971811395379442\n",
      "Epoch: 2 - Batch: 2062, Training Loss: 0.17980082574644887\n",
      "Epoch: 2 - Batch: 2063, Training Loss: 0.17988983992715776\n",
      "Epoch: 2 - Batch: 2064, Training Loss: 0.1799748859199916\n",
      "Epoch: 2 - Batch: 2065, Training Loss: 0.18005669663720464\n",
      "Epoch: 2 - Batch: 2066, Training Loss: 0.18015116948640564\n",
      "Epoch: 2 - Batch: 2067, Training Loss: 0.18023779890059832\n",
      "Epoch: 2 - Batch: 2068, Training Loss: 0.1803262807848936\n",
      "Epoch: 2 - Batch: 2069, Training Loss: 0.1804124194986587\n",
      "Epoch: 2 - Batch: 2070, Training Loss: 0.18049446138774183\n",
      "Epoch: 2 - Batch: 2071, Training Loss: 0.18058507389097073\n",
      "Epoch: 2 - Batch: 2072, Training Loss: 0.18067290552368212\n",
      "Epoch: 2 - Batch: 2073, Training Loss: 0.18076028802724026\n",
      "Epoch: 2 - Batch: 2074, Training Loss: 0.18084408768497495\n",
      "Epoch: 2 - Batch: 2075, Training Loss: 0.18093206607435472\n",
      "Epoch: 2 - Batch: 2076, Training Loss: 0.18101825576219985\n",
      "Epoch: 2 - Batch: 2077, Training Loss: 0.18111272617729743\n",
      "Epoch: 2 - Batch: 2078, Training Loss: 0.18120012072781425\n",
      "Epoch: 2 - Batch: 2079, Training Loss: 0.18128160214295633\n",
      "Epoch: 2 - Batch: 2080, Training Loss: 0.18136444110207098\n",
      "Epoch: 2 - Batch: 2081, Training Loss: 0.1814577171013723\n",
      "Epoch: 2 - Batch: 2082, Training Loss: 0.18154995554245723\n",
      "Epoch: 2 - Batch: 2083, Training Loss: 0.1816318513482661\n",
      "Epoch: 2 - Batch: 2084, Training Loss: 0.1817220272388229\n",
      "Epoch: 2 - Batch: 2085, Training Loss: 0.18181592210583625\n",
      "Epoch: 2 - Batch: 2086, Training Loss: 0.18190157162658807\n",
      "Epoch: 2 - Batch: 2087, Training Loss: 0.18199161008914708\n",
      "Epoch: 2 - Batch: 2088, Training Loss: 0.18209017522843124\n",
      "Epoch: 2 - Batch: 2089, Training Loss: 0.18217834636297195\n",
      "Epoch: 2 - Batch: 2090, Training Loss: 0.18226913697585143\n",
      "Epoch: 2 - Batch: 2091, Training Loss: 0.1823510015028132\n",
      "Epoch: 2 - Batch: 2092, Training Loss: 0.1824373579205664\n",
      "Epoch: 2 - Batch: 2093, Training Loss: 0.18251468000893373\n",
      "Epoch: 2 - Batch: 2094, Training Loss: 0.1825982738799025\n",
      "Epoch: 2 - Batch: 2095, Training Loss: 0.18267737373487272\n",
      "Epoch: 2 - Batch: 2096, Training Loss: 0.18276862399791605\n",
      "Epoch: 2 - Batch: 2097, Training Loss: 0.1828613609792186\n",
      "Epoch: 2 - Batch: 2098, Training Loss: 0.18294843824991144\n",
      "Epoch: 2 - Batch: 2099, Training Loss: 0.18303675411476028\n",
      "Epoch: 2 - Batch: 2100, Training Loss: 0.1831238312186491\n",
      "Epoch: 2 - Batch: 2101, Training Loss: 0.1832222588990458\n",
      "Epoch: 2 - Batch: 2102, Training Loss: 0.18331250141237307\n",
      "Epoch: 2 - Batch: 2103, Training Loss: 0.1833907209064929\n",
      "Epoch: 2 - Batch: 2104, Training Loss: 0.18348392537157135\n",
      "Epoch: 2 - Batch: 2105, Training Loss: 0.18357753407698169\n",
      "Epoch: 2 - Batch: 2106, Training Loss: 0.18366601259430645\n",
      "Epoch: 2 - Batch: 2107, Training Loss: 0.18374733224659417\n",
      "Epoch: 2 - Batch: 2108, Training Loss: 0.1838308802835186\n",
      "Epoch: 2 - Batch: 2109, Training Loss: 0.1839165058648013\n",
      "Epoch: 2 - Batch: 2110, Training Loss: 0.1840019110732312\n",
      "Epoch: 2 - Batch: 2111, Training Loss: 0.1840869411208341\n",
      "Epoch: 2 - Batch: 2112, Training Loss: 0.18417685830449781\n",
      "Epoch: 2 - Batch: 2113, Training Loss: 0.18425772388529027\n",
      "Epoch: 2 - Batch: 2114, Training Loss: 0.18435452643713943\n",
      "Epoch: 2 - Batch: 2115, Training Loss: 0.18445295158458586\n",
      "Epoch: 2 - Batch: 2116, Training Loss: 0.1845401604574909\n",
      "Epoch: 2 - Batch: 2117, Training Loss: 0.18462404678537084\n",
      "Epoch: 2 - Batch: 2118, Training Loss: 0.1847190310248787\n",
      "Epoch: 2 - Batch: 2119, Training Loss: 0.18480190529084917\n",
      "Epoch: 2 - Batch: 2120, Training Loss: 0.18488485665735518\n",
      "Epoch: 2 - Batch: 2121, Training Loss: 0.18497441343308285\n",
      "Epoch: 2 - Batch: 2122, Training Loss: 0.18506280381609352\n",
      "Epoch: 2 - Batch: 2123, Training Loss: 0.1851554034929568\n",
      "Epoch: 2 - Batch: 2124, Training Loss: 0.18524260565663253\n",
      "Epoch: 2 - Batch: 2125, Training Loss: 0.18532317962688988\n",
      "Epoch: 2 - Batch: 2126, Training Loss: 0.18540990776436445\n",
      "Epoch: 2 - Batch: 2127, Training Loss: 0.18549354707399016\n",
      "Epoch: 2 - Batch: 2128, Training Loss: 0.1855803655945444\n",
      "Epoch: 2 - Batch: 2129, Training Loss: 0.1856710162032303\n",
      "Epoch: 2 - Batch: 2130, Training Loss: 0.1857553798995702\n",
      "Epoch: 2 - Batch: 2131, Training Loss: 0.18584312149516583\n",
      "Epoch: 2 - Batch: 2132, Training Loss: 0.18592666185910428\n",
      "Epoch: 2 - Batch: 2133, Training Loss: 0.18601176045699103\n",
      "Epoch: 2 - Batch: 2134, Training Loss: 0.1860867478880123\n",
      "Epoch: 2 - Batch: 2135, Training Loss: 0.18617116013050672\n",
      "Epoch: 2 - Batch: 2136, Training Loss: 0.18624794465886618\n",
      "Epoch: 2 - Batch: 2137, Training Loss: 0.18633541559945688\n",
      "Epoch: 2 - Batch: 2138, Training Loss: 0.1864299570668989\n",
      "Epoch: 2 - Batch: 2139, Training Loss: 0.18653959727801295\n",
      "Epoch: 2 - Batch: 2140, Training Loss: 0.18663441756768012\n",
      "Epoch: 2 - Batch: 2141, Training Loss: 0.18671798696170003\n",
      "Epoch: 2 - Batch: 2142, Training Loss: 0.18680916424488547\n",
      "Epoch: 2 - Batch: 2143, Training Loss: 0.18689704224912088\n",
      "Epoch: 2 - Batch: 2144, Training Loss: 0.186984676737987\n",
      "Epoch: 2 - Batch: 2145, Training Loss: 0.18707089226192503\n",
      "Epoch: 2 - Batch: 2146, Training Loss: 0.1871540049650084\n",
      "Epoch: 2 - Batch: 2147, Training Loss: 0.18723318595470084\n",
      "Epoch: 2 - Batch: 2148, Training Loss: 0.1873171798341211\n",
      "Epoch: 2 - Batch: 2149, Training Loss: 0.18740500858174033\n",
      "Epoch: 2 - Batch: 2150, Training Loss: 0.18749460325287548\n",
      "Epoch: 2 - Batch: 2151, Training Loss: 0.18758579334290468\n",
      "Epoch: 2 - Batch: 2152, Training Loss: 0.18766823698879276\n",
      "Epoch: 2 - Batch: 2153, Training Loss: 0.1877545174180374\n",
      "Epoch: 2 - Batch: 2154, Training Loss: 0.18784555818138035\n",
      "Epoch: 2 - Batch: 2155, Training Loss: 0.1879344333401861\n",
      "Epoch: 2 - Batch: 2156, Training Loss: 0.1880087734356053\n",
      "Epoch: 2 - Batch: 2157, Training Loss: 0.18809858825718784\n",
      "Epoch: 2 - Batch: 2158, Training Loss: 0.18817258110983454\n",
      "Epoch: 2 - Batch: 2159, Training Loss: 0.18825780970985023\n",
      "Epoch: 2 - Batch: 2160, Training Loss: 0.18834519132378683\n",
      "Epoch: 2 - Batch: 2161, Training Loss: 0.18843774041577951\n",
      "Epoch: 2 - Batch: 2162, Training Loss: 0.18851910238687078\n",
      "Epoch: 2 - Batch: 2163, Training Loss: 0.18860899180075028\n",
      "Epoch: 2 - Batch: 2164, Training Loss: 0.18869511722579327\n",
      "Epoch: 2 - Batch: 2165, Training Loss: 0.1887830074685031\n",
      "Epoch: 2 - Batch: 2166, Training Loss: 0.1888650109246991\n",
      "Epoch: 2 - Batch: 2167, Training Loss: 0.18895518776665676\n",
      "Epoch: 2 - Batch: 2168, Training Loss: 0.18904411254657638\n",
      "Epoch: 2 - Batch: 2169, Training Loss: 0.18913282335397616\n",
      "Epoch: 2 - Batch: 2170, Training Loss: 0.18923249033281261\n",
      "Epoch: 2 - Batch: 2171, Training Loss: 0.1893296350615337\n",
      "Epoch: 2 - Batch: 2172, Training Loss: 0.1894122804723569\n",
      "Epoch: 2 - Batch: 2173, Training Loss: 0.18949771043574237\n",
      "Epoch: 2 - Batch: 2174, Training Loss: 0.18958729215176345\n",
      "Epoch: 2 - Batch: 2175, Training Loss: 0.1896718664845424\n",
      "Epoch: 2 - Batch: 2176, Training Loss: 0.1897578322472264\n",
      "Epoch: 2 - Batch: 2177, Training Loss: 0.18984553917229274\n",
      "Epoch: 2 - Batch: 2178, Training Loss: 0.18992218514827156\n",
      "Epoch: 2 - Batch: 2179, Training Loss: 0.19001388845109624\n",
      "Epoch: 2 - Batch: 2180, Training Loss: 0.19010485239191038\n",
      "Epoch: 2 - Batch: 2181, Training Loss: 0.1901898678719602\n",
      "Epoch: 2 - Batch: 2182, Training Loss: 0.19027121398431152\n",
      "Epoch: 2 - Batch: 2183, Training Loss: 0.19035959825117393\n",
      "Epoch: 2 - Batch: 2184, Training Loss: 0.19044740887275383\n",
      "Epoch: 2 - Batch: 2185, Training Loss: 0.19054232656362638\n",
      "Epoch: 2 - Batch: 2186, Training Loss: 0.19063226441245767\n",
      "Epoch: 2 - Batch: 2187, Training Loss: 0.1907145114334464\n",
      "Epoch: 2 - Batch: 2188, Training Loss: 0.190792371557522\n",
      "Epoch: 2 - Batch: 2189, Training Loss: 0.19087903369001882\n",
      "Epoch: 2 - Batch: 2190, Training Loss: 0.19096588817228924\n",
      "Epoch: 2 - Batch: 2191, Training Loss: 0.19105399344394455\n",
      "Epoch: 2 - Batch: 2192, Training Loss: 0.19113831732971948\n",
      "Epoch: 2 - Batch: 2193, Training Loss: 0.19121976738570143\n",
      "Epoch: 2 - Batch: 2194, Training Loss: 0.1913049737142884\n",
      "Epoch: 2 - Batch: 2195, Training Loss: 0.19138557219238422\n",
      "Epoch: 2 - Batch: 2196, Training Loss: 0.1914687775396688\n",
      "Epoch: 2 - Batch: 2197, Training Loss: 0.19156233161984393\n",
      "Epoch: 2 - Batch: 2198, Training Loss: 0.19165763737105612\n",
      "Epoch: 2 - Batch: 2199, Training Loss: 0.1917544200423345\n",
      "Epoch: 2 - Batch: 2200, Training Loss: 0.19185099236753647\n",
      "Epoch: 2 - Batch: 2201, Training Loss: 0.1919385436150071\n",
      "Epoch: 2 - Batch: 2202, Training Loss: 0.19202432960000007\n",
      "Epoch: 2 - Batch: 2203, Training Loss: 0.19210652021033253\n",
      "Epoch: 2 - Batch: 2204, Training Loss: 0.19218966012100872\n",
      "Epoch: 2 - Batch: 2205, Training Loss: 0.1922722445247976\n",
      "Epoch: 2 - Batch: 2206, Training Loss: 0.19236119619105785\n",
      "Epoch: 2 - Batch: 2207, Training Loss: 0.19244160140504687\n",
      "Epoch: 2 - Batch: 2208, Training Loss: 0.19253013102618813\n",
      "Epoch: 2 - Batch: 2209, Training Loss: 0.19261207542757489\n",
      "Epoch: 2 - Batch: 2210, Training Loss: 0.19269998993100615\n",
      "Epoch: 2 - Batch: 2211, Training Loss: 0.1927873114151741\n",
      "Epoch: 2 - Batch: 2212, Training Loss: 0.19287407969682174\n",
      "Epoch: 2 - Batch: 2213, Training Loss: 0.1929712854943564\n",
      "Epoch: 2 - Batch: 2214, Training Loss: 0.1930590132328606\n",
      "Epoch: 2 - Batch: 2215, Training Loss: 0.19313513376992536\n",
      "Epoch: 2 - Batch: 2216, Training Loss: 0.19323203299398445\n",
      "Epoch: 2 - Batch: 2217, Training Loss: 0.19331810724725376\n",
      "Epoch: 2 - Batch: 2218, Training Loss: 0.19340291979474017\n",
      "Epoch: 2 - Batch: 2219, Training Loss: 0.1934819955187255\n",
      "Epoch: 2 - Batch: 2220, Training Loss: 0.1935770019802387\n",
      "Epoch: 2 - Batch: 2221, Training Loss: 0.19366797212369208\n",
      "Epoch: 2 - Batch: 2222, Training Loss: 0.19374970481985837\n",
      "Epoch: 2 - Batch: 2223, Training Loss: 0.19383019022952463\n",
      "Epoch: 2 - Batch: 2224, Training Loss: 0.19392099620073194\n",
      "Epoch: 2 - Batch: 2225, Training Loss: 0.19400137997113454\n",
      "Epoch: 2 - Batch: 2226, Training Loss: 0.19410230542469775\n",
      "Epoch: 2 - Batch: 2227, Training Loss: 0.19418526554532708\n",
      "Epoch: 2 - Batch: 2228, Training Loss: 0.1942728668216548\n",
      "Epoch: 2 - Batch: 2229, Training Loss: 0.19436325548621355\n",
      "Epoch: 2 - Batch: 2230, Training Loss: 0.19445376919820337\n",
      "Epoch: 2 - Batch: 2231, Training Loss: 0.1945475124341635\n",
      "Epoch: 2 - Batch: 2232, Training Loss: 0.194634937188568\n",
      "Epoch: 2 - Batch: 2233, Training Loss: 0.19472450454090762\n",
      "Epoch: 2 - Batch: 2234, Training Loss: 0.1948109875567517\n",
      "Epoch: 2 - Batch: 2235, Training Loss: 0.19489954476233937\n",
      "Epoch: 2 - Batch: 2236, Training Loss: 0.19499356503510357\n",
      "Epoch: 2 - Batch: 2237, Training Loss: 0.19508659464629927\n",
      "Epoch: 2 - Batch: 2238, Training Loss: 0.19516524404386185\n",
      "Epoch: 2 - Batch: 2239, Training Loss: 0.19526386222683179\n",
      "Epoch: 2 - Batch: 2240, Training Loss: 0.19535242114370538\n",
      "Epoch: 2 - Batch: 2241, Training Loss: 0.19543869186742585\n",
      "Epoch: 2 - Batch: 2242, Training Loss: 0.1955206096666567\n",
      "Epoch: 2 - Batch: 2243, Training Loss: 0.19560262014097834\n",
      "Epoch: 2 - Batch: 2244, Training Loss: 0.1956894231302228\n",
      "Epoch: 2 - Batch: 2245, Training Loss: 0.1957715871198656\n",
      "Epoch: 2 - Batch: 2246, Training Loss: 0.19586554081310484\n",
      "Epoch: 2 - Batch: 2247, Training Loss: 0.19595282731784713\n",
      "Epoch: 2 - Batch: 2248, Training Loss: 0.19603329791902113\n",
      "Epoch: 2 - Batch: 2249, Training Loss: 0.19613066740692353\n",
      "Epoch: 2 - Batch: 2250, Training Loss: 0.19621690641934203\n",
      "Epoch: 2 - Batch: 2251, Training Loss: 0.19630305349802102\n",
      "Epoch: 2 - Batch: 2252, Training Loss: 0.19639315437000388\n",
      "Epoch: 2 - Batch: 2253, Training Loss: 0.19648717126344173\n",
      "Epoch: 2 - Batch: 2254, Training Loss: 0.1965723302691038\n",
      "Epoch: 2 - Batch: 2255, Training Loss: 0.19666039244971464\n",
      "Epoch: 2 - Batch: 2256, Training Loss: 0.19674502701110902\n",
      "Epoch: 2 - Batch: 2257, Training Loss: 0.19682834768794463\n",
      "Epoch: 2 - Batch: 2258, Training Loss: 0.19691163911229342\n",
      "Epoch: 2 - Batch: 2259, Training Loss: 0.1969962896992318\n",
      "Epoch: 2 - Batch: 2260, Training Loss: 0.19708896525340097\n",
      "Epoch: 2 - Batch: 2261, Training Loss: 0.19718033513965497\n",
      "Epoch: 2 - Batch: 2262, Training Loss: 0.19726757062707176\n",
      "Epoch: 2 - Batch: 2263, Training Loss: 0.19735597445325273\n",
      "Epoch: 2 - Batch: 2264, Training Loss: 0.19744452233016985\n",
      "Epoch: 2 - Batch: 2265, Training Loss: 0.1975306959121579\n",
      "Epoch: 2 - Batch: 2266, Training Loss: 0.19761890605413301\n",
      "Epoch: 2 - Batch: 2267, Training Loss: 0.19769956548983975\n",
      "Epoch: 2 - Batch: 2268, Training Loss: 0.19778688032037978\n",
      "Epoch: 2 - Batch: 2269, Training Loss: 0.19786629106101902\n",
      "Epoch: 2 - Batch: 2270, Training Loss: 0.1979491460024322\n",
      "Epoch: 2 - Batch: 2271, Training Loss: 0.19803514566958247\n",
      "Epoch: 2 - Batch: 2272, Training Loss: 0.19811550790990762\n",
      "Epoch: 2 - Batch: 2273, Training Loss: 0.1981986985351909\n",
      "Epoch: 2 - Batch: 2274, Training Loss: 0.1982800258851763\n",
      "Epoch: 2 - Batch: 2275, Training Loss: 0.19837137695346305\n",
      "Epoch: 2 - Batch: 2276, Training Loss: 0.19845273408723707\n",
      "Epoch: 2 - Batch: 2277, Training Loss: 0.19853628061056927\n",
      "Epoch: 2 - Batch: 2278, Training Loss: 0.19861966508068454\n",
      "Epoch: 2 - Batch: 2279, Training Loss: 0.19869690334693116\n",
      "Epoch: 2 - Batch: 2280, Training Loss: 0.198777595253825\n",
      "Epoch: 2 - Batch: 2281, Training Loss: 0.19886306121599417\n",
      "Epoch: 2 - Batch: 2282, Training Loss: 0.1989498488654446\n",
      "Epoch: 2 - Batch: 2283, Training Loss: 0.19903939517576302\n",
      "Epoch: 2 - Batch: 2284, Training Loss: 0.19912053812696764\n",
      "Epoch: 2 - Batch: 2285, Training Loss: 0.19921702097724525\n",
      "Epoch: 2 - Batch: 2286, Training Loss: 0.19929753092563962\n",
      "Epoch: 2 - Batch: 2287, Training Loss: 0.19938150058140605\n",
      "Epoch: 2 - Batch: 2288, Training Loss: 0.19946704447195304\n",
      "Epoch: 2 - Batch: 2289, Training Loss: 0.1995492996725178\n",
      "Epoch: 2 - Batch: 2290, Training Loss: 0.1996404308004067\n",
      "Epoch: 2 - Batch: 2291, Training Loss: 0.19973085167667956\n",
      "Epoch: 2 - Batch: 2292, Training Loss: 0.1998107346704548\n",
      "Epoch: 2 - Batch: 2293, Training Loss: 0.19989621855479173\n",
      "Epoch: 2 - Batch: 2294, Training Loss: 0.19999055144304462\n",
      "Epoch: 2 - Batch: 2295, Training Loss: 0.20007820521322253\n",
      "Epoch: 2 - Batch: 2296, Training Loss: 0.20017282484380364\n",
      "Epoch: 2 - Batch: 2297, Training Loss: 0.200254973011774\n",
      "Epoch: 2 - Batch: 2298, Training Loss: 0.2003435049063926\n",
      "Epoch: 2 - Batch: 2299, Training Loss: 0.20043378721175106\n",
      "Epoch: 2 - Batch: 2300, Training Loss: 0.20051515675698148\n",
      "Epoch: 2 - Batch: 2301, Training Loss: 0.20061135718948014\n",
      "Epoch: 2 - Batch: 2302, Training Loss: 0.20069459346964783\n",
      "Epoch: 2 - Batch: 2303, Training Loss: 0.20077921893713288\n",
      "Epoch: 2 - Batch: 2304, Training Loss: 0.20086414226103777\n",
      "Epoch: 2 - Batch: 2305, Training Loss: 0.20095914358251526\n",
      "Epoch: 2 - Batch: 2306, Training Loss: 0.20103671196014133\n",
      "Epoch: 2 - Batch: 2307, Training Loss: 0.20112646487271213\n",
      "Epoch: 2 - Batch: 2308, Training Loss: 0.20121100114945747\n",
      "Epoch: 2 - Batch: 2309, Training Loss: 0.20130848913000987\n",
      "Epoch: 2 - Batch: 2310, Training Loss: 0.20138856230510604\n",
      "Epoch: 2 - Batch: 2311, Training Loss: 0.20148334684310662\n",
      "Epoch: 2 - Batch: 2312, Training Loss: 0.20158244421702515\n",
      "Epoch: 2 - Batch: 2313, Training Loss: 0.20166427516695082\n",
      "Epoch: 2 - Batch: 2314, Training Loss: 0.2017521106176214\n",
      "Epoch: 2 - Batch: 2315, Training Loss: 0.20183433800515646\n",
      "Epoch: 2 - Batch: 2316, Training Loss: 0.20191855466064332\n",
      "Epoch: 2 - Batch: 2317, Training Loss: 0.20200481191648179\n",
      "Epoch: 2 - Batch: 2318, Training Loss: 0.20209169221631132\n",
      "Epoch: 2 - Batch: 2319, Training Loss: 0.20217717964060072\n",
      "Epoch: 2 - Batch: 2320, Training Loss: 0.2022674290237834\n",
      "Epoch: 2 - Batch: 2321, Training Loss: 0.20234918673437824\n",
      "Epoch: 2 - Batch: 2322, Training Loss: 0.20244137184141484\n",
      "Epoch: 2 - Batch: 2323, Training Loss: 0.20252523288921534\n",
      "Epoch: 2 - Batch: 2324, Training Loss: 0.20261388842269754\n",
      "Epoch: 2 - Batch: 2325, Training Loss: 0.2026993323605551\n",
      "Epoch: 2 - Batch: 2326, Training Loss: 0.20279332174368164\n",
      "Epoch: 2 - Batch: 2327, Training Loss: 0.2028746755351969\n",
      "Epoch: 2 - Batch: 2328, Training Loss: 0.20296141141361462\n",
      "Epoch: 2 - Batch: 2329, Training Loss: 0.2030496458163111\n",
      "Epoch: 2 - Batch: 2330, Training Loss: 0.20314348212892736\n",
      "Epoch: 2 - Batch: 2331, Training Loss: 0.20322798622484825\n",
      "Epoch: 2 - Batch: 2332, Training Loss: 0.20331127051369072\n",
      "Epoch: 2 - Batch: 2333, Training Loss: 0.20339468602171387\n",
      "Epoch: 2 - Batch: 2334, Training Loss: 0.20348309120086097\n",
      "Epoch: 2 - Batch: 2335, Training Loss: 0.20357018071002825\n",
      "Epoch: 2 - Batch: 2336, Training Loss: 0.20365903486388043\n",
      "Epoch: 2 - Batch: 2337, Training Loss: 0.20374271827155283\n",
      "Epoch: 2 - Batch: 2338, Training Loss: 0.20383284413943045\n",
      "Epoch: 2 - Batch: 2339, Training Loss: 0.20392630513677154\n",
      "Epoch: 2 - Batch: 2340, Training Loss: 0.20402167013430872\n",
      "Epoch: 2 - Batch: 2341, Training Loss: 0.20410259186628446\n",
      "Epoch: 2 - Batch: 2342, Training Loss: 0.20418601288501895\n",
      "Epoch: 2 - Batch: 2343, Training Loss: 0.2042929681194066\n",
      "Epoch: 2 - Batch: 2344, Training Loss: 0.2043835431226154\n",
      "Epoch: 2 - Batch: 2345, Training Loss: 0.2044733971060805\n",
      "Epoch: 2 - Batch: 2346, Training Loss: 0.204569631393622\n",
      "Epoch: 2 - Batch: 2347, Training Loss: 0.20464932059198865\n",
      "Epoch: 2 - Batch: 2348, Training Loss: 0.2047398640259878\n",
      "Epoch: 2 - Batch: 2349, Training Loss: 0.2048265984587705\n",
      "Epoch: 2 - Batch: 2350, Training Loss: 0.20492359720924205\n",
      "Epoch: 2 - Batch: 2351, Training Loss: 0.20500661656384048\n",
      "Epoch: 2 - Batch: 2352, Training Loss: 0.20508766917521087\n",
      "Epoch: 2 - Batch: 2353, Training Loss: 0.20516756901619446\n",
      "Epoch: 2 - Batch: 2354, Training Loss: 0.20525120555810866\n",
      "Epoch: 2 - Batch: 2355, Training Loss: 0.20534151761018815\n",
      "Epoch: 2 - Batch: 2356, Training Loss: 0.20542849568807664\n",
      "Epoch: 2 - Batch: 2357, Training Loss: 0.20551158104147485\n",
      "Epoch: 2 - Batch: 2358, Training Loss: 0.2055968476789903\n",
      "Epoch: 2 - Batch: 2359, Training Loss: 0.20568211780703483\n",
      "Epoch: 2 - Batch: 2360, Training Loss: 0.20576859694931834\n",
      "Epoch: 2 - Batch: 2361, Training Loss: 0.205851518427011\n",
      "Epoch: 2 - Batch: 2362, Training Loss: 0.20593420883168034\n",
      "Epoch: 2 - Batch: 2363, Training Loss: 0.2060207269875762\n",
      "Epoch: 2 - Batch: 2364, Training Loss: 0.20610856750414738\n",
      "Epoch: 2 - Batch: 2365, Training Loss: 0.20618977432276678\n",
      "Epoch: 2 - Batch: 2366, Training Loss: 0.206285588721286\n",
      "Epoch: 2 - Batch: 2367, Training Loss: 0.20637164151243506\n",
      "Epoch: 2 - Batch: 2368, Training Loss: 0.2064587310772037\n",
      "Epoch: 2 - Batch: 2369, Training Loss: 0.20654453509556714\n",
      "Epoch: 2 - Batch: 2370, Training Loss: 0.20662492960320777\n",
      "Epoch: 2 - Batch: 2371, Training Loss: 0.20671579495665446\n",
      "Epoch: 2 - Batch: 2372, Training Loss: 0.20679830018412415\n",
      "Epoch: 2 - Batch: 2373, Training Loss: 0.20688284607990268\n",
      "Epoch: 2 - Batch: 2374, Training Loss: 0.2069673580217915\n",
      "Epoch: 2 - Batch: 2375, Training Loss: 0.20705436319607012\n",
      "Epoch: 2 - Batch: 2376, Training Loss: 0.2071501186695166\n",
      "Epoch: 2 - Batch: 2377, Training Loss: 0.20724165535981381\n",
      "Epoch: 2 - Batch: 2378, Training Loss: 0.2073270025945718\n",
      "Epoch: 2 - Batch: 2379, Training Loss: 0.2074093944138259\n",
      "Epoch: 2 - Batch: 2380, Training Loss: 0.2074972809497792\n",
      "Epoch: 2 - Batch: 2381, Training Loss: 0.20757755201624994\n",
      "Epoch: 2 - Batch: 2382, Training Loss: 0.20766349745355236\n",
      "Epoch: 2 - Batch: 2383, Training Loss: 0.20774664321472594\n",
      "Epoch: 2 - Batch: 2384, Training Loss: 0.20783727858864254\n",
      "Epoch: 2 - Batch: 2385, Training Loss: 0.20792893424729011\n",
      "Epoch: 2 - Batch: 2386, Training Loss: 0.20801143851100312\n",
      "Epoch: 2 - Batch: 2387, Training Loss: 0.20810829928981922\n",
      "Epoch: 2 - Batch: 2388, Training Loss: 0.20819474211206682\n",
      "Epoch: 2 - Batch: 2389, Training Loss: 0.2082827991649978\n",
      "Epoch: 2 - Batch: 2390, Training Loss: 0.20837553554704139\n",
      "Epoch: 2 - Batch: 2391, Training Loss: 0.20845426539975415\n",
      "Epoch: 2 - Batch: 2392, Training Loss: 0.20854148471953463\n",
      "Epoch: 2 - Batch: 2393, Training Loss: 0.20862574337875073\n",
      "Epoch: 2 - Batch: 2394, Training Loss: 0.20870642696397618\n",
      "Epoch: 2 - Batch: 2395, Training Loss: 0.20878735945172372\n",
      "Epoch: 2 - Batch: 2396, Training Loss: 0.2088816471978602\n",
      "Epoch: 2 - Batch: 2397, Training Loss: 0.20896500489283754\n",
      "Epoch: 2 - Batch: 2398, Training Loss: 0.20904769770689866\n",
      "Epoch: 2 - Batch: 2399, Training Loss: 0.209129063934582\n",
      "Epoch: 2 - Batch: 2400, Training Loss: 0.20921166905296185\n",
      "Epoch: 2 - Batch: 2401, Training Loss: 0.20930020300483032\n",
      "Epoch: 2 - Batch: 2402, Training Loss: 0.20938775484538197\n",
      "Epoch: 2 - Batch: 2403, Training Loss: 0.20946491867377984\n",
      "Epoch: 2 - Batch: 2404, Training Loss: 0.20955261110542822\n",
      "Epoch: 2 - Batch: 2405, Training Loss: 0.20963984118715842\n",
      "Epoch: 2 - Batch: 2406, Training Loss: 0.20972542249123452\n",
      "Epoch: 2 - Batch: 2407, Training Loss: 0.20980721008446482\n",
      "Epoch: 2 - Batch: 2408, Training Loss: 0.2098976508660202\n",
      "Epoch: 2 - Batch: 2409, Training Loss: 0.2099861944122101\n",
      "Epoch: 2 - Batch: 2410, Training Loss: 0.21006956295562818\n",
      "Epoch: 2 - Batch: 2411, Training Loss: 0.2101552141505984\n",
      "Epoch: 2 - Batch: 2412, Training Loss: 0.2102390778734415\n",
      "Epoch 2 - Batch 2412, Training Loss: 0.2102390778734415, Validation Loss: 0.20842209747478144\n",
      "Validation loss decreased (0.212703 --> 0.208422). Saving model...\n",
      "Epoch: 3 - Batch: 1, Training Loss: 9.559650919330654e-05\n",
      "Epoch: 3 - Batch: 2, Training Loss: 0.00017881982806903214\n",
      "Epoch: 3 - Batch: 3, Training Loss: 0.0002643575283326518\n",
      "Epoch: 3 - Batch: 4, Training Loss: 0.0003555226410008584\n",
      "Epoch: 3 - Batch: 5, Training Loss: 0.00044167859118376203\n",
      "Epoch: 3 - Batch: 6, Training Loss: 0.0005204465802431502\n",
      "Epoch: 3 - Batch: 7, Training Loss: 0.0006110582185621878\n",
      "Epoch: 3 - Batch: 8, Training Loss: 0.0007016616587713979\n",
      "Epoch: 3 - Batch: 9, Training Loss: 0.0007947848902808295\n",
      "Epoch: 3 - Batch: 10, Training Loss: 0.0008783688642096955\n",
      "Epoch: 3 - Batch: 11, Training Loss: 0.0009680885642420989\n",
      "Epoch: 3 - Batch: 12, Training Loss: 0.0010589492348197286\n",
      "Epoch: 3 - Batch: 13, Training Loss: 0.0011535519440573445\n",
      "Epoch: 3 - Batch: 14, Training Loss: 0.0012431495929535348\n",
      "Epoch: 3 - Batch: 15, Training Loss: 0.0013288798097947343\n",
      "Epoch: 3 - Batch: 16, Training Loss: 0.0014185635048655136\n",
      "Epoch: 3 - Batch: 17, Training Loss: 0.0015001016828056987\n",
      "Epoch: 3 - Batch: 18, Training Loss: 0.0015851986806092175\n",
      "Epoch: 3 - Batch: 19, Training Loss: 0.0016711776269905602\n",
      "Epoch: 3 - Batch: 20, Training Loss: 0.0017602588999923782\n",
      "Epoch: 3 - Batch: 21, Training Loss: 0.0018447665914670744\n",
      "Epoch: 3 - Batch: 22, Training Loss: 0.0019265843700512527\n",
      "Epoch: 3 - Batch: 23, Training Loss: 0.0020145870695758615\n",
      "Epoch: 3 - Batch: 24, Training Loss: 0.0021023781194813413\n",
      "Epoch: 3 - Batch: 25, Training Loss: 0.0021914304118251324\n",
      "Epoch: 3 - Batch: 26, Training Loss: 0.002274354200062665\n",
      "Epoch: 3 - Batch: 27, Training Loss: 0.0023508404704369913\n",
      "Epoch: 3 - Batch: 28, Training Loss: 0.0024394500772059458\n",
      "Epoch: 3 - Batch: 29, Training Loss: 0.002525716377530327\n",
      "Epoch: 3 - Batch: 30, Training Loss: 0.0026085478428189038\n",
      "Epoch: 3 - Batch: 31, Training Loss: 0.002695549192960385\n",
      "Epoch: 3 - Batch: 32, Training Loss: 0.0027779375587529804\n",
      "Epoch: 3 - Batch: 33, Training Loss: 0.002859687114483484\n",
      "Epoch: 3 - Batch: 34, Training Loss: 0.0029459917179585292\n",
      "Epoch: 3 - Batch: 35, Training Loss: 0.003027902634978097\n",
      "Epoch: 3 - Batch: 36, Training Loss: 0.0031088352215724994\n",
      "Epoch: 3 - Batch: 37, Training Loss: 0.0031856020501695856\n",
      "Epoch: 3 - Batch: 38, Training Loss: 0.0032649928237470623\n",
      "Epoch: 3 - Batch: 39, Training Loss: 0.003345252226290616\n",
      "Epoch: 3 - Batch: 40, Training Loss: 0.0034220881673629406\n",
      "Epoch: 3 - Batch: 41, Training Loss: 0.003498953323854538\n",
      "Epoch: 3 - Batch: 42, Training Loss: 0.00358716214375314\n",
      "Epoch: 3 - Batch: 43, Training Loss: 0.0036732384913398656\n",
      "Epoch: 3 - Batch: 44, Training Loss: 0.0037615636786813562\n",
      "Epoch: 3 - Batch: 45, Training Loss: 0.003854288989878808\n",
      "Epoch: 3 - Batch: 46, Training Loss: 0.003941232156298845\n",
      "Epoch: 3 - Batch: 47, Training Loss: 0.00402122957764771\n",
      "Epoch: 3 - Batch: 48, Training Loss: 0.0041149371731439436\n",
      "Epoch: 3 - Batch: 49, Training Loss: 0.004208746197854306\n",
      "Epoch: 3 - Batch: 50, Training Loss: 0.0042997355484843845\n",
      "Epoch: 3 - Batch: 51, Training Loss: 0.00439450899710505\n",
      "Epoch: 3 - Batch: 52, Training Loss: 0.004488373579917658\n",
      "Epoch: 3 - Batch: 53, Training Loss: 0.004575585937173806\n",
      "Epoch: 3 - Batch: 54, Training Loss: 0.004666299430983972\n",
      "Epoch: 3 - Batch: 55, Training Loss: 0.004756563554701718\n",
      "Epoch: 3 - Batch: 56, Training Loss: 0.004845610855280068\n",
      "Epoch: 3 - Batch: 57, Training Loss: 0.004931393695707938\n",
      "Epoch: 3 - Batch: 58, Training Loss: 0.005018558890665348\n",
      "Epoch: 3 - Batch: 59, Training Loss: 0.005110909529092102\n",
      "Epoch: 3 - Batch: 60, Training Loss: 0.0052053949750873385\n",
      "Epoch: 3 - Batch: 61, Training Loss: 0.005295489032816136\n",
      "Epoch: 3 - Batch: 62, Training Loss: 0.005374888789100236\n",
      "Epoch: 3 - Batch: 63, Training Loss: 0.005463965892000973\n",
      "Epoch: 3 - Batch: 64, Training Loss: 0.0055540212141241795\n",
      "Epoch: 3 - Batch: 65, Training Loss: 0.005639798117563697\n",
      "Epoch: 3 - Batch: 66, Training Loss: 0.005722248003158601\n",
      "Epoch: 3 - Batch: 67, Training Loss: 0.0058079661853969785\n",
      "Epoch: 3 - Batch: 68, Training Loss: 0.005892089245281805\n",
      "Epoch: 3 - Batch: 69, Training Loss: 0.005978509376001595\n",
      "Epoch: 3 - Batch: 70, Training Loss: 0.006067398323941587\n",
      "Epoch: 3 - Batch: 71, Training Loss: 0.0061608425482094385\n",
      "Epoch: 3 - Batch: 72, Training Loss: 0.006242787320271851\n",
      "Epoch: 3 - Batch: 73, Training Loss: 0.006324348801354666\n",
      "Epoch: 3 - Batch: 74, Training Loss: 0.0064087677839680095\n",
      "Epoch: 3 - Batch: 75, Training Loss: 0.00649494919339025\n",
      "Epoch: 3 - Batch: 76, Training Loss: 0.006578159964895169\n",
      "Epoch: 3 - Batch: 77, Training Loss: 0.006671348750393585\n",
      "Epoch: 3 - Batch: 78, Training Loss: 0.006764906858577459\n",
      "Epoch: 3 - Batch: 79, Training Loss: 0.0068478854326762964\n",
      "Epoch: 3 - Batch: 80, Training Loss: 0.006930739768652576\n",
      "Epoch: 3 - Batch: 81, Training Loss: 0.007006965416976271\n",
      "Epoch: 3 - Batch: 82, Training Loss: 0.007101805259784062\n",
      "Epoch: 3 - Batch: 83, Training Loss: 0.00718802973553909\n",
      "Epoch: 3 - Batch: 84, Training Loss: 0.007268478133241533\n",
      "Epoch: 3 - Batch: 85, Training Loss: 0.007357083044785568\n",
      "Epoch: 3 - Batch: 86, Training Loss: 0.0074412569799035735\n",
      "Epoch: 3 - Batch: 87, Training Loss: 0.007531782936545747\n",
      "Epoch: 3 - Batch: 88, Training Loss: 0.007623999909332538\n",
      "Epoch: 3 - Batch: 89, Training Loss: 0.007710599477957335\n",
      "Epoch: 3 - Batch: 90, Training Loss: 0.007794141633494774\n",
      "Epoch: 3 - Batch: 91, Training Loss: 0.00788665126706435\n",
      "Epoch: 3 - Batch: 92, Training Loss: 0.007972604389708631\n",
      "Epoch: 3 - Batch: 93, Training Loss: 0.008046994464561516\n",
      "Epoch: 3 - Batch: 94, Training Loss: 0.008127192829181108\n",
      "Epoch: 3 - Batch: 95, Training Loss: 0.008217724537473808\n",
      "Epoch: 3 - Batch: 96, Training Loss: 0.008309176522206706\n",
      "Epoch: 3 - Batch: 97, Training Loss: 0.008397637308879475\n",
      "Epoch: 3 - Batch: 98, Training Loss: 0.00848764423311844\n",
      "Epoch: 3 - Batch: 99, Training Loss: 0.00856748871369346\n",
      "Epoch: 3 - Batch: 100, Training Loss: 0.00864996568332264\n",
      "Epoch: 3 - Batch: 101, Training Loss: 0.00873823922319001\n",
      "Epoch: 3 - Batch: 102, Training Loss: 0.008823256655464917\n",
      "Epoch: 3 - Batch: 103, Training Loss: 0.008910170211078318\n",
      "Epoch: 3 - Batch: 104, Training Loss: 0.009008982757826151\n",
      "Epoch: 3 - Batch: 105, Training Loss: 0.009094930585986543\n",
      "Epoch: 3 - Batch: 106, Training Loss: 0.009183639509118414\n",
      "Epoch: 3 - Batch: 107, Training Loss: 0.00927126946956364\n",
      "Epoch: 3 - Batch: 108, Training Loss: 0.009351933069193541\n",
      "Epoch: 3 - Batch: 109, Training Loss: 0.009445599334354623\n",
      "Epoch: 3 - Batch: 110, Training Loss: 0.009528697729357835\n",
      "Epoch: 3 - Batch: 111, Training Loss: 0.00961351193252883\n",
      "Epoch: 3 - Batch: 112, Training Loss: 0.009704166890475683\n",
      "Epoch: 3 - Batch: 113, Training Loss: 0.009788589295660876\n",
      "Epoch: 3 - Batch: 114, Training Loss: 0.00987194122045392\n",
      "Epoch: 3 - Batch: 115, Training Loss: 0.009957911147061075\n",
      "Epoch: 3 - Batch: 116, Training Loss: 0.01004394321126329\n",
      "Epoch: 3 - Batch: 117, Training Loss: 0.010129532769050566\n",
      "Epoch: 3 - Batch: 118, Training Loss: 0.010214347651793588\n",
      "Epoch: 3 - Batch: 119, Training Loss: 0.010298567853410842\n",
      "Epoch: 3 - Batch: 120, Training Loss: 0.01038566564100101\n",
      "Epoch: 3 - Batch: 121, Training Loss: 0.010476148383040135\n",
      "Epoch: 3 - Batch: 122, Training Loss: 0.010564826045858722\n",
      "Epoch: 3 - Batch: 123, Training Loss: 0.010647073721707756\n",
      "Epoch: 3 - Batch: 124, Training Loss: 0.010740058196964945\n",
      "Epoch: 3 - Batch: 125, Training Loss: 0.01082740253328684\n",
      "Epoch: 3 - Batch: 126, Training Loss: 0.010907538698532095\n",
      "Epoch: 3 - Batch: 127, Training Loss: 0.010994469583232209\n",
      "Epoch: 3 - Batch: 128, Training Loss: 0.011089565229307163\n",
      "Epoch: 3 - Batch: 129, Training Loss: 0.011177947704570606\n",
      "Epoch: 3 - Batch: 130, Training Loss: 0.011271632552937687\n",
      "Epoch: 3 - Batch: 131, Training Loss: 0.01135919383954053\n",
      "Epoch: 3 - Batch: 132, Training Loss: 0.011448512029885059\n",
      "Epoch: 3 - Batch: 133, Training Loss: 0.011533649289242269\n",
      "Epoch: 3 - Batch: 134, Training Loss: 0.011609606222430272\n",
      "Epoch: 3 - Batch: 135, Training Loss: 0.011689907579279658\n",
      "Epoch: 3 - Batch: 136, Training Loss: 0.011771705569961969\n",
      "Epoch: 3 - Batch: 137, Training Loss: 0.011862491388769688\n",
      "Epoch: 3 - Batch: 138, Training Loss: 0.011943370245928392\n",
      "Epoch: 3 - Batch: 139, Training Loss: 0.012032442560935297\n",
      "Epoch: 3 - Batch: 140, Training Loss: 0.012121572923749241\n",
      "Epoch: 3 - Batch: 141, Training Loss: 0.012203320144223139\n",
      "Epoch: 3 - Batch: 142, Training Loss: 0.012286846743739066\n",
      "Epoch: 3 - Batch: 143, Training Loss: 0.012375374851288092\n",
      "Epoch: 3 - Batch: 144, Training Loss: 0.01245213852914214\n",
      "Epoch: 3 - Batch: 145, Training Loss: 0.012554787914847853\n",
      "Epoch: 3 - Batch: 146, Training Loss: 0.012640927790063332\n",
      "Epoch: 3 - Batch: 147, Training Loss: 0.012717427231779146\n",
      "Epoch: 3 - Batch: 148, Training Loss: 0.012802599532341285\n",
      "Epoch: 3 - Batch: 149, Training Loss: 0.012887275146064078\n",
      "Epoch: 3 - Batch: 150, Training Loss: 0.01297318139059429\n",
      "Epoch: 3 - Batch: 151, Training Loss: 0.013053093297070334\n",
      "Epoch: 3 - Batch: 152, Training Loss: 0.013140051605590739\n",
      "Epoch: 3 - Batch: 153, Training Loss: 0.013224594264135234\n",
      "Epoch: 3 - Batch: 154, Training Loss: 0.013315583614765312\n",
      "Epoch: 3 - Batch: 155, Training Loss: 0.013404425801971856\n",
      "Epoch: 3 - Batch: 156, Training Loss: 0.013488775684465817\n",
      "Epoch: 3 - Batch: 157, Training Loss: 0.013570042311601575\n",
      "Epoch: 3 - Batch: 158, Training Loss: 0.013657538124528492\n",
      "Epoch: 3 - Batch: 159, Training Loss: 0.013742332447128707\n",
      "Epoch: 3 - Batch: 160, Training Loss: 0.013834094508222086\n",
      "Epoch: 3 - Batch: 161, Training Loss: 0.013915722408203738\n",
      "Epoch: 3 - Batch: 162, Training Loss: 0.013993136708317309\n",
      "Epoch: 3 - Batch: 163, Training Loss: 0.014079515656972208\n",
      "Epoch: 3 - Batch: 164, Training Loss: 0.014174482085515017\n",
      "Epoch: 3 - Batch: 165, Training Loss: 0.014255541467893974\n",
      "Epoch: 3 - Batch: 166, Training Loss: 0.014348517794464754\n",
      "Epoch: 3 - Batch: 167, Training Loss: 0.01443016986249889\n",
      "Epoch: 3 - Batch: 168, Training Loss: 0.014513657899391196\n",
      "Epoch: 3 - Batch: 169, Training Loss: 0.014597974402542731\n",
      "Epoch: 3 - Batch: 170, Training Loss: 0.014684650583646784\n",
      "Epoch: 3 - Batch: 171, Training Loss: 0.014775175249102103\n",
      "Epoch: 3 - Batch: 172, Training Loss: 0.014855395570076126\n",
      "Epoch: 3 - Batch: 173, Training Loss: 0.014935045155571468\n",
      "Epoch: 3 - Batch: 174, Training Loss: 0.015027113901343116\n",
      "Epoch: 3 - Batch: 175, Training Loss: 0.015112329385618664\n",
      "Epoch: 3 - Batch: 176, Training Loss: 0.015195718106148058\n",
      "Epoch: 3 - Batch: 177, Training Loss: 0.015279539226002954\n",
      "Epoch: 3 - Batch: 178, Training Loss: 0.015368280694457033\n",
      "Epoch: 3 - Batch: 179, Training Loss: 0.015447688895968063\n",
      "Epoch: 3 - Batch: 180, Training Loss: 0.015533517619863671\n",
      "Epoch: 3 - Batch: 181, Training Loss: 0.015620949052607835\n",
      "Epoch: 3 - Batch: 182, Training Loss: 0.015716369923321566\n",
      "Epoch: 3 - Batch: 183, Training Loss: 0.015796973380826994\n",
      "Epoch: 3 - Batch: 184, Training Loss: 0.0158758759683934\n",
      "Epoch: 3 - Batch: 185, Training Loss: 0.015966245425775474\n",
      "Epoch: 3 - Batch: 186, Training Loss: 0.016058862919781734\n",
      "Epoch: 3 - Batch: 187, Training Loss: 0.016144306876173065\n",
      "Epoch: 3 - Batch: 188, Training Loss: 0.016227054463739617\n",
      "Epoch: 3 - Batch: 189, Training Loss: 0.01632147299660181\n",
      "Epoch: 3 - Batch: 190, Training Loss: 0.016406687826017046\n",
      "Epoch: 3 - Batch: 191, Training Loss: 0.016499303114503178\n",
      "Epoch: 3 - Batch: 192, Training Loss: 0.016589315740969248\n",
      "Epoch: 3 - Batch: 193, Training Loss: 0.016681268928606514\n",
      "Epoch: 3 - Batch: 194, Training Loss: 0.016767132199472852\n",
      "Epoch: 3 - Batch: 195, Training Loss: 0.01685472804540228\n",
      "Epoch: 3 - Batch: 196, Training Loss: 0.01695106070408378\n",
      "Epoch: 3 - Batch: 197, Training Loss: 0.01703862660559849\n",
      "Epoch: 3 - Batch: 198, Training Loss: 0.017119799550641236\n",
      "Epoch: 3 - Batch: 199, Training Loss: 0.017207381823663884\n",
      "Epoch: 3 - Batch: 200, Training Loss: 0.017299239296669985\n",
      "Epoch: 3 - Batch: 201, Training Loss: 0.01738408364194938\n",
      "Epoch: 3 - Batch: 202, Training Loss: 0.01746842283662872\n",
      "Epoch: 3 - Batch: 203, Training Loss: 0.0175536198797906\n",
      "Epoch: 3 - Batch: 204, Training Loss: 0.017635648473973694\n",
      "Epoch: 3 - Batch: 205, Training Loss: 0.01771858768231833\n",
      "Epoch: 3 - Batch: 206, Training Loss: 0.017808260010002464\n",
      "Epoch: 3 - Batch: 207, Training Loss: 0.017892621377263693\n",
      "Epoch: 3 - Batch: 208, Training Loss: 0.01797452440506981\n",
      "Epoch: 3 - Batch: 209, Training Loss: 0.01805604012006551\n",
      "Epoch: 3 - Batch: 210, Training Loss: 0.018137501438409337\n",
      "Epoch: 3 - Batch: 211, Training Loss: 0.01823284869293866\n",
      "Epoch: 3 - Batch: 212, Training Loss: 0.01832086812229101\n",
      "Epoch: 3 - Batch: 213, Training Loss: 0.018406855106156066\n",
      "Epoch: 3 - Batch: 214, Training Loss: 0.018495686692038976\n",
      "Epoch: 3 - Batch: 215, Training Loss: 0.01858767132532735\n",
      "Epoch: 3 - Batch: 216, Training Loss: 0.018680127933744965\n",
      "Epoch: 3 - Batch: 217, Training Loss: 0.018777701250010263\n",
      "Epoch: 3 - Batch: 218, Training Loss: 0.0188630226424974\n",
      "Epoch: 3 - Batch: 219, Training Loss: 0.018950399221423057\n",
      "Epoch: 3 - Batch: 220, Training Loss: 0.019043550506901385\n",
      "Epoch: 3 - Batch: 221, Training Loss: 0.019139190589017537\n",
      "Epoch: 3 - Batch: 222, Training Loss: 0.019236965441634602\n",
      "Epoch: 3 - Batch: 223, Training Loss: 0.019320150976305576\n",
      "Epoch: 3 - Batch: 224, Training Loss: 0.019402742639159285\n",
      "Epoch: 3 - Batch: 225, Training Loss: 0.019492200172659176\n",
      "Epoch: 3 - Batch: 226, Training Loss: 0.01957862279926169\n",
      "Epoch: 3 - Batch: 227, Training Loss: 0.019665129235629023\n",
      "Epoch: 3 - Batch: 228, Training Loss: 0.019744927332324174\n",
      "Epoch: 3 - Batch: 229, Training Loss: 0.019831573550479726\n",
      "Epoch: 3 - Batch: 230, Training Loss: 0.019913913450430875\n",
      "Epoch: 3 - Batch: 231, Training Loss: 0.020005484292311453\n",
      "Epoch: 3 - Batch: 232, Training Loss: 0.020084835873116703\n",
      "Epoch: 3 - Batch: 233, Training Loss: 0.02016534413493688\n",
      "Epoch: 3 - Batch: 234, Training Loss: 0.02024943348804912\n",
      "Epoch: 3 - Batch: 235, Training Loss: 0.02033395141634973\n",
      "Epoch: 3 - Batch: 236, Training Loss: 0.020421995588301822\n",
      "Epoch: 3 - Batch: 237, Training Loss: 0.02050743358917102\n",
      "Epoch: 3 - Batch: 238, Training Loss: 0.020595785916148135\n",
      "Epoch: 3 - Batch: 239, Training Loss: 0.020682109745333643\n",
      "Epoch: 3 - Batch: 240, Training Loss: 0.020778544809341826\n",
      "Epoch: 3 - Batch: 241, Training Loss: 0.020863357103533216\n",
      "Epoch: 3 - Batch: 242, Training Loss: 0.020954771143435247\n",
      "Epoch: 3 - Batch: 243, Training Loss: 0.021036667814153937\n",
      "Epoch: 3 - Batch: 244, Training Loss: 0.021120661736819677\n",
      "Epoch: 3 - Batch: 245, Training Loss: 0.021207983702865998\n",
      "Epoch: 3 - Batch: 246, Training Loss: 0.021299148969982393\n",
      "Epoch: 3 - Batch: 247, Training Loss: 0.021376895401657714\n",
      "Epoch: 3 - Batch: 248, Training Loss: 0.021457575737293285\n",
      "Epoch: 3 - Batch: 249, Training Loss: 0.021545475790552042\n",
      "Epoch: 3 - Batch: 250, Training Loss: 0.02163340169225957\n",
      "Epoch: 3 - Batch: 251, Training Loss: 0.0217211273302684\n",
      "Epoch: 3 - Batch: 252, Training Loss: 0.021799082100193694\n",
      "Epoch: 3 - Batch: 253, Training Loss: 0.021882567832719035\n",
      "Epoch: 3 - Batch: 254, Training Loss: 0.021974578048805298\n",
      "Epoch: 3 - Batch: 255, Training Loss: 0.022057632568167215\n",
      "Epoch: 3 - Batch: 256, Training Loss: 0.022146436130970865\n",
      "Epoch: 3 - Batch: 257, Training Loss: 0.022224182062234057\n",
      "Epoch: 3 - Batch: 258, Training Loss: 0.022316163847497843\n",
      "Epoch: 3 - Batch: 259, Training Loss: 0.022403286068791377\n",
      "Epoch: 3 - Batch: 260, Training Loss: 0.022501963739025455\n",
      "Epoch: 3 - Batch: 261, Training Loss: 0.0225930775316497\n",
      "Epoch: 3 - Batch: 262, Training Loss: 0.02267696043695779\n",
      "Epoch: 3 - Batch: 263, Training Loss: 0.022765108354846835\n",
      "Epoch: 3 - Batch: 264, Training Loss: 0.022848618428406627\n",
      "Epoch: 3 - Batch: 265, Training Loss: 0.02294087341398149\n",
      "Epoch: 3 - Batch: 266, Training Loss: 0.02302918199505379\n",
      "Epoch: 3 - Batch: 267, Training Loss: 0.023117761478248126\n",
      "Epoch: 3 - Batch: 268, Training Loss: 0.023199122467048923\n",
      "Epoch: 3 - Batch: 269, Training Loss: 0.023287339534480773\n",
      "Epoch: 3 - Batch: 270, Training Loss: 0.023374400779629625\n",
      "Epoch: 3 - Batch: 271, Training Loss: 0.02346341839526621\n",
      "Epoch: 3 - Batch: 272, Training Loss: 0.023562268825065635\n",
      "Epoch: 3 - Batch: 273, Training Loss: 0.02365662214990279\n",
      "Epoch: 3 - Batch: 274, Training Loss: 0.0237497357932687\n",
      "Epoch: 3 - Batch: 275, Training Loss: 0.023829816208895956\n",
      "Epoch: 3 - Batch: 276, Training Loss: 0.023917986923337575\n",
      "Epoch: 3 - Batch: 277, Training Loss: 0.024004297334064496\n",
      "Epoch: 3 - Batch: 278, Training Loss: 0.024091934436193944\n",
      "Epoch: 3 - Batch: 279, Training Loss: 0.02417395645088422\n",
      "Epoch: 3 - Batch: 280, Training Loss: 0.024261047004121256\n",
      "Epoch: 3 - Batch: 281, Training Loss: 0.024345890255907478\n",
      "Epoch: 3 - Batch: 282, Training Loss: 0.02443228173957733\n",
      "Epoch: 3 - Batch: 283, Training Loss: 0.024518493457912016\n",
      "Epoch: 3 - Batch: 284, Training Loss: 0.024604939733621097\n",
      "Epoch: 3 - Batch: 285, Training Loss: 0.024695214396883203\n",
      "Epoch: 3 - Batch: 286, Training Loss: 0.024786816597923908\n",
      "Epoch: 3 - Batch: 287, Training Loss: 0.02486810891906025\n",
      "Epoch: 3 - Batch: 288, Training Loss: 0.02495506219256972\n",
      "Epoch: 3 - Batch: 289, Training Loss: 0.025041159192968166\n",
      "Epoch: 3 - Batch: 290, Training Loss: 0.025143917341087983\n",
      "Epoch: 3 - Batch: 291, Training Loss: 0.025232417079593807\n",
      "Epoch: 3 - Batch: 292, Training Loss: 0.025314670537983006\n",
      "Epoch: 3 - Batch: 293, Training Loss: 0.025393997987803337\n",
      "Epoch: 3 - Batch: 294, Training Loss: 0.025475133272199885\n",
      "Epoch: 3 - Batch: 295, Training Loss: 0.025556427934770757\n",
      "Epoch: 3 - Batch: 296, Training Loss: 0.025645440521574338\n",
      "Epoch: 3 - Batch: 297, Training Loss: 0.025729607586836935\n",
      "Epoch: 3 - Batch: 298, Training Loss: 0.025820672351428327\n",
      "Epoch: 3 - Batch: 299, Training Loss: 0.025907874014691926\n",
      "Epoch: 3 - Batch: 300, Training Loss: 0.02599848154468916\n",
      "Epoch: 3 - Batch: 301, Training Loss: 0.02608278494445641\n",
      "Epoch: 3 - Batch: 302, Training Loss: 0.026167716503538697\n",
      "Epoch: 3 - Batch: 303, Training Loss: 0.026250912163832886\n",
      "Epoch: 3 - Batch: 304, Training Loss: 0.026345040766556267\n",
      "Epoch: 3 - Batch: 305, Training Loss: 0.02642944738713663\n",
      "Epoch: 3 - Batch: 306, Training Loss: 0.026514001783743427\n",
      "Epoch: 3 - Batch: 307, Training Loss: 0.026602009456499695\n",
      "Epoch: 3 - Batch: 308, Training Loss: 0.02667623511198939\n",
      "Epoch: 3 - Batch: 309, Training Loss: 0.026761808687478156\n",
      "Epoch: 3 - Batch: 310, Training Loss: 0.02684456503534594\n",
      "Epoch: 3 - Batch: 311, Training Loss: 0.02692822607274277\n",
      "Epoch: 3 - Batch: 312, Training Loss: 0.027017079021899065\n",
      "Epoch: 3 - Batch: 313, Training Loss: 0.02710447359094968\n",
      "Epoch: 3 - Batch: 314, Training Loss: 0.027193534427367237\n",
      "Epoch: 3 - Batch: 315, Training Loss: 0.0272797010962544\n",
      "Epoch: 3 - Batch: 316, Training Loss: 0.027372605023692497\n",
      "Epoch: 3 - Batch: 317, Training Loss: 0.027459873000830165\n",
      "Epoch: 3 - Batch: 318, Training Loss: 0.027553063763265388\n",
      "Epoch: 3 - Batch: 319, Training Loss: 0.02764168369335124\n",
      "Epoch: 3 - Batch: 320, Training Loss: 0.027725157829906612\n",
      "Epoch: 3 - Batch: 321, Training Loss: 0.027824472320910117\n",
      "Epoch: 3 - Batch: 322, Training Loss: 0.02791087728481783\n",
      "Epoch: 3 - Batch: 323, Training Loss: 0.027992184680097336\n",
      "Epoch: 3 - Batch: 324, Training Loss: 0.028080919155137454\n",
      "Epoch: 3 - Batch: 325, Training Loss: 0.02817096271448665\n",
      "Epoch: 3 - Batch: 326, Training Loss: 0.02825572476359347\n",
      "Epoch: 3 - Batch: 327, Training Loss: 0.028332946306190286\n",
      "Epoch: 3 - Batch: 328, Training Loss: 0.028420709376261996\n",
      "Epoch: 3 - Batch: 329, Training Loss: 0.028514900320452046\n",
      "Epoch: 3 - Batch: 330, Training Loss: 0.028596135903501987\n",
      "Epoch: 3 - Batch: 331, Training Loss: 0.02867857207409778\n",
      "Epoch: 3 - Batch: 332, Training Loss: 0.028766013885760187\n",
      "Epoch: 3 - Batch: 333, Training Loss: 0.028845370489220517\n",
      "Epoch: 3 - Batch: 334, Training Loss: 0.028939387778045726\n",
      "Epoch: 3 - Batch: 335, Training Loss: 0.029035768680163283\n",
      "Epoch: 3 - Batch: 336, Training Loss: 0.02911621776361568\n",
      "Epoch: 3 - Batch: 337, Training Loss: 0.029203823679566976\n",
      "Epoch: 3 - Batch: 338, Training Loss: 0.029288302396561574\n",
      "Epoch: 3 - Batch: 339, Training Loss: 0.029377616175865852\n",
      "Epoch: 3 - Batch: 340, Training Loss: 0.02946621667636963\n",
      "Epoch: 3 - Batch: 341, Training Loss: 0.029557646111667057\n",
      "Epoch: 3 - Batch: 342, Training Loss: 0.02964050391346068\n",
      "Epoch: 3 - Batch: 343, Training Loss: 0.029725094518900708\n",
      "Epoch: 3 - Batch: 344, Training Loss: 0.029816264623334355\n",
      "Epoch: 3 - Batch: 345, Training Loss: 0.029903661107542504\n",
      "Epoch: 3 - Batch: 346, Training Loss: 0.029984783535671865\n",
      "Epoch: 3 - Batch: 347, Training Loss: 0.030074607729170453\n",
      "Epoch: 3 - Batch: 348, Training Loss: 0.0301597158657773\n",
      "Epoch: 3 - Batch: 349, Training Loss: 0.030243507300690434\n",
      "Epoch: 3 - Batch: 350, Training Loss: 0.0303310509740219\n",
      "Epoch: 3 - Batch: 351, Training Loss: 0.03042091589241874\n",
      "Epoch: 3 - Batch: 352, Training Loss: 0.0305079231857265\n",
      "Epoch: 3 - Batch: 353, Training Loss: 0.030593458624868647\n",
      "Epoch: 3 - Batch: 354, Training Loss: 0.030682511992171826\n",
      "Epoch: 3 - Batch: 355, Training Loss: 0.03076937915772743\n",
      "Epoch: 3 - Batch: 356, Training Loss: 0.030855509111191306\n",
      "Epoch: 3 - Batch: 357, Training Loss: 0.030942339950533054\n",
      "Epoch: 3 - Batch: 358, Training Loss: 0.03102695186982305\n",
      "Epoch: 3 - Batch: 359, Training Loss: 0.031112879792700953\n",
      "Epoch: 3 - Batch: 360, Training Loss: 0.03121235681879975\n",
      "Epoch: 3 - Batch: 361, Training Loss: 0.03129690557495871\n",
      "Epoch: 3 - Batch: 362, Training Loss: 0.031389345540039575\n",
      "Epoch: 3 - Batch: 363, Training Loss: 0.03147632545895047\n",
      "Epoch: 3 - Batch: 364, Training Loss: 0.03155192632110755\n",
      "Epoch: 3 - Batch: 365, Training Loss: 0.03163747892253237\n",
      "Epoch: 3 - Batch: 366, Training Loss: 0.03172833808569568\n",
      "Epoch: 3 - Batch: 367, Training Loss: 0.031825862188590306\n",
      "Epoch: 3 - Batch: 368, Training Loss: 0.03191830214131531\n",
      "Epoch: 3 - Batch: 369, Training Loss: 0.032001004580587494\n",
      "Epoch: 3 - Batch: 370, Training Loss: 0.0320955072568226\n",
      "Epoch: 3 - Batch: 371, Training Loss: 0.032177398027620506\n",
      "Epoch: 3 - Batch: 372, Training Loss: 0.03226784192902927\n",
      "Epoch: 3 - Batch: 373, Training Loss: 0.032354254689481526\n",
      "Epoch: 3 - Batch: 374, Training Loss: 0.03244320136397632\n",
      "Epoch: 3 - Batch: 375, Training Loss: 0.032532550437780555\n",
      "Epoch: 3 - Batch: 376, Training Loss: 0.03262234450770452\n",
      "Epoch: 3 - Batch: 377, Training Loss: 0.032705725635560984\n",
      "Epoch: 3 - Batch: 378, Training Loss: 0.03278519827903404\n",
      "Epoch: 3 - Batch: 379, Training Loss: 0.03287076004232538\n",
      "Epoch: 3 - Batch: 380, Training Loss: 0.032960936581564584\n",
      "Epoch: 3 - Batch: 381, Training Loss: 0.03304452474412831\n",
      "Epoch: 3 - Batch: 382, Training Loss: 0.03314089089335494\n",
      "Epoch: 3 - Batch: 383, Training Loss: 0.03322887983463495\n",
      "Epoch: 3 - Batch: 384, Training Loss: 0.03331418357678314\n",
      "Epoch: 3 - Batch: 385, Training Loss: 0.03340736743229539\n",
      "Epoch: 3 - Batch: 386, Training Loss: 0.03350015918710338\n",
      "Epoch: 3 - Batch: 387, Training Loss: 0.03359291634551723\n",
      "Epoch: 3 - Batch: 388, Training Loss: 0.0336771857234376\n",
      "Epoch: 3 - Batch: 389, Training Loss: 0.033769234792510074\n",
      "Epoch: 3 - Batch: 390, Training Loss: 0.03385905091145738\n",
      "Epoch: 3 - Batch: 391, Training Loss: 0.03393987854124104\n",
      "Epoch: 3 - Batch: 392, Training Loss: 0.034020544834447344\n",
      "Epoch: 3 - Batch: 393, Training Loss: 0.034110617918112186\n",
      "Epoch: 3 - Batch: 394, Training Loss: 0.034202607493742584\n",
      "Epoch: 3 - Batch: 395, Training Loss: 0.034285564859016224\n",
      "Epoch: 3 - Batch: 396, Training Loss: 0.034372618857456086\n",
      "Epoch: 3 - Batch: 397, Training Loss: 0.034466876955718345\n",
      "Epoch: 3 - Batch: 398, Training Loss: 0.03454712228494299\n",
      "Epoch: 3 - Batch: 399, Training Loss: 0.03463095836156043\n",
      "Epoch: 3 - Batch: 400, Training Loss: 0.03471520058131139\n",
      "Epoch: 3 - Batch: 401, Training Loss: 0.034804960462584425\n",
      "Epoch: 3 - Batch: 402, Training Loss: 0.034889964612315146\n",
      "Epoch: 3 - Batch: 403, Training Loss: 0.034980826326962526\n",
      "Epoch: 3 - Batch: 404, Training Loss: 0.03506608174536161\n",
      "Epoch: 3 - Batch: 405, Training Loss: 0.0351560401975812\n",
      "Epoch: 3 - Batch: 406, Training Loss: 0.035241102586634716\n",
      "Epoch: 3 - Batch: 407, Training Loss: 0.0353265647555564\n",
      "Epoch: 3 - Batch: 408, Training Loss: 0.03541120057107006\n",
      "Epoch: 3 - Batch: 409, Training Loss: 0.03550484376167184\n",
      "Epoch: 3 - Batch: 410, Training Loss: 0.03559196592736402\n",
      "Epoch: 3 - Batch: 411, Training Loss: 0.0356778001434372\n",
      "Epoch: 3 - Batch: 412, Training Loss: 0.03576114292489751\n",
      "Epoch: 3 - Batch: 413, Training Loss: 0.0358488509125674\n",
      "Epoch: 3 - Batch: 414, Training Loss: 0.035942798761487205\n",
      "Epoch: 3 - Batch: 415, Training Loss: 0.036023293845787965\n",
      "Epoch: 3 - Batch: 416, Training Loss: 0.03610391898781901\n",
      "Epoch: 3 - Batch: 417, Training Loss: 0.036185668840090036\n",
      "Epoch: 3 - Batch: 418, Training Loss: 0.03627759996017018\n",
      "Epoch: 3 - Batch: 419, Training Loss: 0.03636252287015393\n",
      "Epoch: 3 - Batch: 420, Training Loss: 0.036444583101503884\n",
      "Epoch: 3 - Batch: 421, Training Loss: 0.03652508276982687\n",
      "Epoch: 3 - Batch: 422, Training Loss: 0.0366097260050315\n",
      "Epoch: 3 - Batch: 423, Training Loss: 0.03669774091831882\n",
      "Epoch: 3 - Batch: 424, Training Loss: 0.03679243704399857\n",
      "Epoch: 3 - Batch: 425, Training Loss: 0.03688176677471172\n",
      "Epoch: 3 - Batch: 426, Training Loss: 0.03697344282052015\n",
      "Epoch: 3 - Batch: 427, Training Loss: 0.037060144850072974\n",
      "Epoch: 3 - Batch: 428, Training Loss: 0.037146011506443595\n",
      "Epoch: 3 - Batch: 429, Training Loss: 0.03723847167334746\n",
      "Epoch: 3 - Batch: 430, Training Loss: 0.037324552648201906\n",
      "Epoch: 3 - Batch: 431, Training Loss: 0.037405207487530574\n",
      "Epoch: 3 - Batch: 432, Training Loss: 0.037492732305827225\n",
      "Epoch: 3 - Batch: 433, Training Loss: 0.037582839096264656\n",
      "Epoch: 3 - Batch: 434, Training Loss: 0.037675155533684626\n",
      "Epoch: 3 - Batch: 435, Training Loss: 0.037766724429518034\n",
      "Epoch: 3 - Batch: 436, Training Loss: 0.03785574029680113\n",
      "Epoch: 3 - Batch: 437, Training Loss: 0.03793661480469886\n",
      "Epoch: 3 - Batch: 438, Training Loss: 0.0380267391713401\n",
      "Epoch: 3 - Batch: 439, Training Loss: 0.03811357096208269\n",
      "Epoch: 3 - Batch: 440, Training Loss: 0.038205407899657685\n",
      "Epoch: 3 - Batch: 441, Training Loss: 0.038290468633026625\n",
      "Epoch: 3 - Batch: 442, Training Loss: 0.03837569199415977\n",
      "Epoch: 3 - Batch: 443, Training Loss: 0.0384609523610788\n",
      "Epoch: 3 - Batch: 444, Training Loss: 0.038548097366569056\n",
      "Epoch: 3 - Batch: 445, Training Loss: 0.03863206014012421\n",
      "Epoch: 3 - Batch: 446, Training Loss: 0.03871277913723024\n",
      "Epoch: 3 - Batch: 447, Training Loss: 0.03880158638825662\n",
      "Epoch: 3 - Batch: 448, Training Loss: 0.038885547648219526\n",
      "Epoch: 3 - Batch: 449, Training Loss: 0.03898058959574842\n",
      "Epoch: 3 - Batch: 450, Training Loss: 0.039076218471104034\n",
      "Epoch: 3 - Batch: 451, Training Loss: 0.03916596254923846\n",
      "Epoch: 3 - Batch: 452, Training Loss: 0.03925283257517451\n",
      "Epoch: 3 - Batch: 453, Training Loss: 0.03934177786581354\n",
      "Epoch: 3 - Batch: 454, Training Loss: 0.039433234613728566\n",
      "Epoch: 3 - Batch: 455, Training Loss: 0.03951966887336861\n",
      "Epoch: 3 - Batch: 456, Training Loss: 0.0396127013573006\n",
      "Epoch: 3 - Batch: 457, Training Loss: 0.03970126433925052\n",
      "Epoch: 3 - Batch: 458, Training Loss: 0.03978423194752799\n",
      "Epoch: 3 - Batch: 459, Training Loss: 0.03986485393263808\n",
      "Epoch: 3 - Batch: 460, Training Loss: 0.03994485770489643\n",
      "Epoch: 3 - Batch: 461, Training Loss: 0.040032986242626835\n",
      "Epoch: 3 - Batch: 462, Training Loss: 0.040123322431869175\n",
      "Epoch: 3 - Batch: 463, Training Loss: 0.040206222132367284\n",
      "Epoch: 3 - Batch: 464, Training Loss: 0.04028199702056486\n",
      "Epoch: 3 - Batch: 465, Training Loss: 0.04036611208003354\n",
      "Epoch: 3 - Batch: 466, Training Loss: 0.04044654268530471\n",
      "Epoch: 3 - Batch: 467, Training Loss: 0.040523108150532\n",
      "Epoch: 3 - Batch: 468, Training Loss: 0.040614904653571336\n",
      "Epoch: 3 - Batch: 469, Training Loss: 0.04070170775401849\n",
      "Epoch: 3 - Batch: 470, Training Loss: 0.04080085348875368\n",
      "Epoch: 3 - Batch: 471, Training Loss: 0.04088814045066264\n",
      "Epoch: 3 - Batch: 472, Training Loss: 0.040974966187036256\n",
      "Epoch: 3 - Batch: 473, Training Loss: 0.041065739489362804\n",
      "Epoch: 3 - Batch: 474, Training Loss: 0.04115342977727033\n",
      "Epoch: 3 - Batch: 475, Training Loss: 0.04123938713797289\n",
      "Epoch: 3 - Batch: 476, Training Loss: 0.041324845785475885\n",
      "Epoch: 3 - Batch: 477, Training Loss: 0.04141048830045792\n",
      "Epoch: 3 - Batch: 478, Training Loss: 0.041498027575104984\n",
      "Epoch: 3 - Batch: 479, Training Loss: 0.04158614019231614\n",
      "Epoch: 3 - Batch: 480, Training Loss: 0.0416775754472213\n",
      "Epoch: 3 - Batch: 481, Training Loss: 0.0417591305959284\n",
      "Epoch: 3 - Batch: 482, Training Loss: 0.04184851282049174\n",
      "Epoch: 3 - Batch: 483, Training Loss: 0.041939681343375945\n",
      "Epoch: 3 - Batch: 484, Training Loss: 0.04203242213645978\n",
      "Epoch: 3 - Batch: 485, Training Loss: 0.04212251649690702\n",
      "Epoch: 3 - Batch: 486, Training Loss: 0.04221216339006353\n",
      "Epoch: 3 - Batch: 487, Training Loss: 0.0422999371788395\n",
      "Epoch: 3 - Batch: 488, Training Loss: 0.042382979119940975\n",
      "Epoch: 3 - Batch: 489, Training Loss: 0.04246991982754585\n",
      "Epoch: 3 - Batch: 490, Training Loss: 0.042557246464105386\n",
      "Epoch: 3 - Batch: 491, Training Loss: 0.042642003836521065\n",
      "Epoch: 3 - Batch: 492, Training Loss: 0.04273766021601003\n",
      "Epoch: 3 - Batch: 493, Training Loss: 0.04281934196898593\n",
      "Epoch: 3 - Batch: 494, Training Loss: 0.04289833461837982\n",
      "Epoch: 3 - Batch: 495, Training Loss: 0.04298510004582492\n",
      "Epoch: 3 - Batch: 496, Training Loss: 0.04307144190838088\n",
      "Epoch: 3 - Batch: 497, Training Loss: 0.043156929468584695\n",
      "Epoch: 3 - Batch: 498, Training Loss: 0.043244031858730866\n",
      "Epoch: 3 - Batch: 499, Training Loss: 0.043335619448972976\n",
      "Epoch: 3 - Batch: 500, Training Loss: 0.043419010090837824\n",
      "Epoch: 3 - Batch: 501, Training Loss: 0.04349921884055359\n",
      "Epoch: 3 - Batch: 502, Training Loss: 0.04358491034650091\n",
      "Epoch: 3 - Batch: 503, Training Loss: 0.04367765115811852\n",
      "Epoch: 3 - Batch: 504, Training Loss: 0.04376971709293315\n",
      "Epoch: 3 - Batch: 505, Training Loss: 0.04385823296200774\n",
      "Epoch: 3 - Batch: 506, Training Loss: 0.043947242398068284\n",
      "Epoch: 3 - Batch: 507, Training Loss: 0.04402868203188649\n",
      "Epoch: 3 - Batch: 508, Training Loss: 0.04411219007290813\n",
      "Epoch: 3 - Batch: 509, Training Loss: 0.0441950597116109\n",
      "Epoch: 3 - Batch: 510, Training Loss: 0.04428524953339428\n",
      "Epoch: 3 - Batch: 511, Training Loss: 0.04437538744823059\n",
      "Epoch: 3 - Batch: 512, Training Loss: 0.04446532254788413\n",
      "Epoch: 3 - Batch: 513, Training Loss: 0.04455741305231652\n",
      "Epoch: 3 - Batch: 514, Training Loss: 0.044646799317244475\n",
      "Epoch: 3 - Batch: 515, Training Loss: 0.04473256358064427\n",
      "Epoch: 3 - Batch: 516, Training Loss: 0.04482393069300881\n",
      "Epoch: 3 - Batch: 517, Training Loss: 0.044905137777279065\n",
      "Epoch: 3 - Batch: 518, Training Loss: 0.04498965605154362\n",
      "Epoch: 3 - Batch: 519, Training Loss: 0.045071355368367476\n",
      "Epoch: 3 - Batch: 520, Training Loss: 0.04515336751690749\n",
      "Epoch: 3 - Batch: 521, Training Loss: 0.04522817416323556\n",
      "Epoch: 3 - Batch: 522, Training Loss: 0.04531633152717579\n",
      "Epoch: 3 - Batch: 523, Training Loss: 0.04539610379395596\n",
      "Epoch: 3 - Batch: 524, Training Loss: 0.04548153717373536\n",
      "Epoch: 3 - Batch: 525, Training Loss: 0.045560554646042056\n",
      "Epoch: 3 - Batch: 526, Training Loss: 0.04564205180872139\n",
      "Epoch: 3 - Batch: 527, Training Loss: 0.04572642183743701\n",
      "Epoch: 3 - Batch: 528, Training Loss: 0.045814885682183906\n",
      "Epoch: 3 - Batch: 529, Training Loss: 0.04590504502825081\n",
      "Epoch: 3 - Batch: 530, Training Loss: 0.045994342213641744\n",
      "Epoch: 3 - Batch: 531, Training Loss: 0.04608406830165121\n",
      "Epoch: 3 - Batch: 532, Training Loss: 0.04616158674926703\n",
      "Epoch: 3 - Batch: 533, Training Loss: 0.04624566459290028\n",
      "Epoch: 3 - Batch: 534, Training Loss: 0.046328529585801546\n",
      "Epoch: 3 - Batch: 535, Training Loss: 0.0464202422420085\n",
      "Epoch: 3 - Batch: 536, Training Loss: 0.04651335003487704\n",
      "Epoch: 3 - Batch: 537, Training Loss: 0.046602186050333984\n",
      "Epoch: 3 - Batch: 538, Training Loss: 0.0466841676984458\n",
      "Epoch: 3 - Batch: 539, Training Loss: 0.046779114586203846\n",
      "Epoch: 3 - Batch: 540, Training Loss: 0.04687155104222187\n",
      "Epoch: 3 - Batch: 541, Training Loss: 0.046955250487794135\n",
      "Epoch: 3 - Batch: 542, Training Loss: 0.04704412454692879\n",
      "Epoch: 3 - Batch: 543, Training Loss: 0.047128634462457394\n",
      "Epoch: 3 - Batch: 544, Training Loss: 0.04722107240735595\n",
      "Epoch: 3 - Batch: 545, Training Loss: 0.047310775291069626\n",
      "Epoch: 3 - Batch: 546, Training Loss: 0.047393592472989764\n",
      "Epoch: 3 - Batch: 547, Training Loss: 0.04747315531420471\n",
      "Epoch: 3 - Batch: 548, Training Loss: 0.0475578548982863\n",
      "Epoch: 3 - Batch: 549, Training Loss: 0.04763903558427224\n",
      "Epoch: 3 - Batch: 550, Training Loss: 0.04772181846016082\n",
      "Epoch: 3 - Batch: 551, Training Loss: 0.04780242807705999\n",
      "Epoch: 3 - Batch: 552, Training Loss: 0.047886061517654566\n",
      "Epoch: 3 - Batch: 553, Training Loss: 0.047987488977252746\n",
      "Epoch: 3 - Batch: 554, Training Loss: 0.048063439819004204\n",
      "Epoch: 3 - Batch: 555, Training Loss: 0.04814929279744329\n",
      "Epoch: 3 - Batch: 556, Training Loss: 0.04823241928348296\n",
      "Epoch: 3 - Batch: 557, Training Loss: 0.04833142862811215\n",
      "Epoch: 3 - Batch: 558, Training Loss: 0.04841998458575847\n",
      "Epoch: 3 - Batch: 559, Training Loss: 0.04850911252682482\n",
      "Epoch: 3 - Batch: 560, Training Loss: 0.04859070853016665\n",
      "Epoch: 3 - Batch: 561, Training Loss: 0.04867519313472618\n",
      "Epoch: 3 - Batch: 562, Training Loss: 0.04875127106262479\n",
      "Epoch: 3 - Batch: 563, Training Loss: 0.04883722922645793\n",
      "Epoch: 3 - Batch: 564, Training Loss: 0.048919877664465615\n",
      "Epoch: 3 - Batch: 565, Training Loss: 0.049005898491017656\n",
      "Epoch: 3 - Batch: 566, Training Loss: 0.049093628150857306\n",
      "Epoch: 3 - Batch: 567, Training Loss: 0.0491823952762444\n",
      "Epoch: 3 - Batch: 568, Training Loss: 0.04927075769177717\n",
      "Epoch: 3 - Batch: 569, Training Loss: 0.049358558095064925\n",
      "Epoch: 3 - Batch: 570, Training Loss: 0.049449876618029465\n",
      "Epoch: 3 - Batch: 571, Training Loss: 0.049527319676395676\n",
      "Epoch: 3 - Batch: 572, Training Loss: 0.04962579533457756\n",
      "Epoch: 3 - Batch: 573, Training Loss: 0.04970340369575059\n",
      "Epoch: 3 - Batch: 574, Training Loss: 0.049790256584115684\n",
      "Epoch: 3 - Batch: 575, Training Loss: 0.04988485313395956\n",
      "Epoch: 3 - Batch: 576, Training Loss: 0.0499688881954447\n",
      "Epoch: 3 - Batch: 577, Training Loss: 0.05005019181601048\n",
      "Epoch: 3 - Batch: 578, Training Loss: 0.05013333446968649\n",
      "Epoch: 3 - Batch: 579, Training Loss: 0.050223039299447346\n",
      "Epoch: 3 - Batch: 580, Training Loss: 0.050313948324663724\n",
      "Epoch: 3 - Batch: 581, Training Loss: 0.0503981634603804\n",
      "Epoch: 3 - Batch: 582, Training Loss: 0.05048744515152911\n",
      "Epoch: 3 - Batch: 583, Training Loss: 0.0505719590147534\n",
      "Epoch: 3 - Batch: 584, Training Loss: 0.05065419642906482\n",
      "Epoch: 3 - Batch: 585, Training Loss: 0.05073209784023006\n",
      "Epoch: 3 - Batch: 586, Training Loss: 0.05082297897828159\n",
      "Epoch: 3 - Batch: 587, Training Loss: 0.050910922202897904\n",
      "Epoch: 3 - Batch: 588, Training Loss: 0.05099525378637053\n",
      "Epoch: 3 - Batch: 589, Training Loss: 0.05107835134735353\n",
      "Epoch: 3 - Batch: 590, Training Loss: 0.051166090521201565\n",
      "Epoch: 3 - Batch: 591, Training Loss: 0.05124483832697172\n",
      "Epoch: 3 - Batch: 592, Training Loss: 0.0513320860837526\n",
      "Epoch: 3 - Batch: 593, Training Loss: 0.05142374485607566\n",
      "Epoch: 3 - Batch: 594, Training Loss: 0.05150840577250887\n",
      "Epoch: 3 - Batch: 595, Training Loss: 0.051602130517931916\n",
      "Epoch: 3 - Batch: 596, Training Loss: 0.05169571318337771\n",
      "Epoch: 3 - Batch: 597, Training Loss: 0.05178715312732986\n",
      "Epoch: 3 - Batch: 598, Training Loss: 0.05187643431806643\n",
      "Epoch: 3 - Batch: 599, Training Loss: 0.05195874564510278\n",
      "Epoch: 3 - Batch: 600, Training Loss: 0.052037528090512576\n",
      "Epoch: 3 - Batch: 601, Training Loss: 0.05212132272559217\n",
      "Epoch: 3 - Batch: 602, Training Loss: 0.05220345457147801\n",
      "Epoch: 3 - Batch: 603, Training Loss: 0.05229925574305441\n",
      "Epoch: 3 - Batch: 604, Training Loss: 0.052382261419721306\n",
      "Epoch: 3 - Batch: 605, Training Loss: 0.05245899268198962\n",
      "Epoch: 3 - Batch: 606, Training Loss: 0.05255083236256444\n",
      "Epoch: 3 - Batch: 607, Training Loss: 0.052632025299380665\n",
      "Epoch: 3 - Batch: 608, Training Loss: 0.05272048625285748\n",
      "Epoch: 3 - Batch: 609, Training Loss: 0.05280911696342686\n",
      "Epoch: 3 - Batch: 610, Training Loss: 0.05289107522808299\n",
      "Epoch: 3 - Batch: 611, Training Loss: 0.05297198973188353\n",
      "Epoch: 3 - Batch: 612, Training Loss: 0.053066079531044114\n",
      "Epoch: 3 - Batch: 613, Training Loss: 0.0531438067431869\n",
      "Epoch: 3 - Batch: 614, Training Loss: 0.053225496657205065\n",
      "Epoch: 3 - Batch: 615, Training Loss: 0.053309540775531954\n",
      "Epoch: 3 - Batch: 616, Training Loss: 0.05339109062975517\n",
      "Epoch: 3 - Batch: 617, Training Loss: 0.053477608520000136\n",
      "Epoch: 3 - Batch: 618, Training Loss: 0.05356863073720466\n",
      "Epoch: 3 - Batch: 619, Training Loss: 0.053651363386542444\n",
      "Epoch: 3 - Batch: 620, Training Loss: 0.053744853142136166\n",
      "Epoch: 3 - Batch: 621, Training Loss: 0.053829511031384884\n",
      "Epoch: 3 - Batch: 622, Training Loss: 0.05390858330808666\n",
      "Epoch: 3 - Batch: 623, Training Loss: 0.05399433978729778\n",
      "Epoch: 3 - Batch: 624, Training Loss: 0.054088826234117274\n",
      "Epoch: 3 - Batch: 625, Training Loss: 0.0541744047828377\n",
      "Epoch: 3 - Batch: 626, Training Loss: 0.05425831773758528\n",
      "Epoch: 3 - Batch: 627, Training Loss: 0.0543490179117837\n",
      "Epoch: 3 - Batch: 628, Training Loss: 0.05444951917030918\n",
      "Epoch: 3 - Batch: 629, Training Loss: 0.05453749704074306\n",
      "Epoch: 3 - Batch: 630, Training Loss: 0.05462127600242052\n",
      "Epoch: 3 - Batch: 631, Training Loss: 0.05470164097956757\n",
      "Epoch: 3 - Batch: 632, Training Loss: 0.05478451391728363\n",
      "Epoch: 3 - Batch: 633, Training Loss: 0.0548671694228404\n",
      "Epoch: 3 - Batch: 634, Training Loss: 0.05495557922307729\n",
      "Epoch: 3 - Batch: 635, Training Loss: 0.05503582228500254\n",
      "Epoch: 3 - Batch: 636, Training Loss: 0.0551238069511567\n",
      "Epoch: 3 - Batch: 637, Training Loss: 0.055217084000705685\n",
      "Epoch: 3 - Batch: 638, Training Loss: 0.05531242854312483\n",
      "Epoch: 3 - Batch: 639, Training Loss: 0.055401065777585676\n",
      "Epoch: 3 - Batch: 640, Training Loss: 0.05549816057629649\n",
      "Epoch: 3 - Batch: 641, Training Loss: 0.055577578946676226\n",
      "Epoch: 3 - Batch: 642, Training Loss: 0.055666521970015856\n",
      "Epoch: 3 - Batch: 643, Training Loss: 0.055755063372464914\n",
      "Epoch: 3 - Batch: 644, Training Loss: 0.05583874236909707\n",
      "Epoch: 3 - Batch: 645, Training Loss: 0.05592481215572476\n",
      "Epoch: 3 - Batch: 646, Training Loss: 0.056012944665862555\n",
      "Epoch: 3 - Batch: 647, Training Loss: 0.05609885883667378\n",
      "Epoch: 3 - Batch: 648, Training Loss: 0.056187924646323\n",
      "Epoch: 3 - Batch: 649, Training Loss: 0.05627103004709603\n",
      "Epoch: 3 - Batch: 650, Training Loss: 0.05636085242428392\n",
      "Epoch: 3 - Batch: 651, Training Loss: 0.05644036144957218\n",
      "Epoch: 3 - Batch: 652, Training Loss: 0.056526762576986904\n",
      "Epoch: 3 - Batch: 653, Training Loss: 0.056615033781597664\n",
      "Epoch: 3 - Batch: 654, Training Loss: 0.056697810844056444\n",
      "Epoch: 3 - Batch: 655, Training Loss: 0.05678955617532208\n",
      "Epoch: 3 - Batch: 656, Training Loss: 0.056875490968055394\n",
      "Epoch: 3 - Batch: 657, Training Loss: 0.056955412166134436\n",
      "Epoch: 3 - Batch: 658, Training Loss: 0.057046707169728886\n",
      "Epoch: 3 - Batch: 659, Training Loss: 0.057131217184104334\n",
      "Epoch: 3 - Batch: 660, Training Loss: 0.05721446533824871\n",
      "Epoch: 3 - Batch: 661, Training Loss: 0.05730695599322493\n",
      "Epoch: 3 - Batch: 662, Training Loss: 0.05738940699702472\n",
      "Epoch: 3 - Batch: 663, Training Loss: 0.057468702976482225\n",
      "Epoch: 3 - Batch: 664, Training Loss: 0.057565071300339346\n",
      "Epoch: 3 - Batch: 665, Training Loss: 0.05764990822592778\n",
      "Epoch: 3 - Batch: 666, Training Loss: 0.05773472929277625\n",
      "Epoch: 3 - Batch: 667, Training Loss: 0.05781684582357976\n",
      "Epoch: 3 - Batch: 668, Training Loss: 0.057906583377822715\n",
      "Epoch: 3 - Batch: 669, Training Loss: 0.05797652504213809\n",
      "Epoch: 3 - Batch: 670, Training Loss: 0.05807153654484013\n",
      "Epoch: 3 - Batch: 671, Training Loss: 0.058164636022218226\n",
      "Epoch: 3 - Batch: 672, Training Loss: 0.05824830270624082\n",
      "Epoch: 3 - Batch: 673, Training Loss: 0.058346573788530594\n",
      "Epoch: 3 - Batch: 674, Training Loss: 0.05842607506669774\n",
      "Epoch: 3 - Batch: 675, Training Loss: 0.05851629721139794\n",
      "Epoch: 3 - Batch: 676, Training Loss: 0.05860825290109585\n",
      "Epoch: 3 - Batch: 677, Training Loss: 0.058698861308358795\n",
      "Epoch: 3 - Batch: 678, Training Loss: 0.05878227752066568\n",
      "Epoch: 3 - Batch: 679, Training Loss: 0.05886515621003227\n",
      "Epoch: 3 - Batch: 680, Training Loss: 0.05894669709390471\n",
      "Epoch: 3 - Batch: 681, Training Loss: 0.059028080527116215\n",
      "Epoch: 3 - Batch: 682, Training Loss: 0.05911201008195505\n",
      "Epoch: 3 - Batch: 683, Training Loss: 0.05920123377472014\n",
      "Epoch: 3 - Batch: 684, Training Loss: 0.05929308144953318\n",
      "Epoch: 3 - Batch: 685, Training Loss: 0.05938109919231132\n",
      "Epoch: 3 - Batch: 686, Training Loss: 0.05947307369379855\n",
      "Epoch: 3 - Batch: 687, Training Loss: 0.059559105405858896\n",
      "Epoch: 3 - Batch: 688, Training Loss: 0.05964099811652604\n",
      "Epoch: 3 - Batch: 689, Training Loss: 0.05972976326497633\n",
      "Epoch: 3 - Batch: 690, Training Loss: 0.05981270085470396\n",
      "Epoch: 3 - Batch: 691, Training Loss: 0.05990954934562221\n",
      "Epoch: 3 - Batch: 692, Training Loss: 0.05999116476001234\n",
      "Epoch: 3 - Batch: 693, Training Loss: 0.06007967948617034\n",
      "Epoch: 3 - Batch: 694, Training Loss: 0.060164991377004935\n",
      "Epoch: 3 - Batch: 695, Training Loss: 0.060248211137394404\n",
      "Epoch: 3 - Batch: 696, Training Loss: 0.06034079587602892\n",
      "Epoch: 3 - Batch: 697, Training Loss: 0.0604291830279835\n",
      "Epoch: 3 - Batch: 698, Training Loss: 0.06051611239291344\n",
      "Epoch: 3 - Batch: 699, Training Loss: 0.0606014529122642\n",
      "Epoch: 3 - Batch: 700, Training Loss: 0.06068478490706305\n",
      "Epoch: 3 - Batch: 701, Training Loss: 0.06077424682071355\n",
      "Epoch: 3 - Batch: 702, Training Loss: 0.060859169650384244\n",
      "Epoch: 3 - Batch: 703, Training Loss: 0.06094835045597644\n",
      "Epoch: 3 - Batch: 704, Training Loss: 0.061033930586246316\n",
      "Epoch: 3 - Batch: 705, Training Loss: 0.061126665238470185\n",
      "Epoch: 3 - Batch: 706, Training Loss: 0.06121143258823882\n",
      "Epoch: 3 - Batch: 707, Training Loss: 0.06130019829888051\n",
      "Epoch: 3 - Batch: 708, Training Loss: 0.061379704723261284\n",
      "Epoch: 3 - Batch: 709, Training Loss: 0.06145866036686929\n",
      "Epoch: 3 - Batch: 710, Training Loss: 0.06154594807630748\n",
      "Epoch: 3 - Batch: 711, Training Loss: 0.06163061028392754\n",
      "Epoch: 3 - Batch: 712, Training Loss: 0.0617203901013727\n",
      "Epoch: 3 - Batch: 713, Training Loss: 0.061808687259457006\n",
      "Epoch: 3 - Batch: 714, Training Loss: 0.061897215811816814\n",
      "Epoch: 3 - Batch: 715, Training Loss: 0.061987420794234346\n",
      "Epoch: 3 - Batch: 716, Training Loss: 0.062070744986310725\n",
      "Epoch: 3 - Batch: 717, Training Loss: 0.06215797158986775\n",
      "Epoch: 3 - Batch: 718, Training Loss: 0.06224580105412659\n",
      "Epoch: 3 - Batch: 719, Training Loss: 0.062331112747267506\n",
      "Epoch: 3 - Batch: 720, Training Loss: 0.062410371195815294\n",
      "Epoch: 3 - Batch: 721, Training Loss: 0.062498034461467816\n",
      "Epoch: 3 - Batch: 722, Training Loss: 0.06257657554142708\n",
      "Epoch: 3 - Batch: 723, Training Loss: 0.06266355158677742\n",
      "Epoch: 3 - Batch: 724, Training Loss: 0.06274894630508636\n",
      "Epoch: 3 - Batch: 725, Training Loss: 0.06283916512606155\n",
      "Epoch: 3 - Batch: 726, Training Loss: 0.0629316620269225\n",
      "Epoch: 3 - Batch: 727, Training Loss: 0.06301947510731754\n",
      "Epoch: 3 - Batch: 728, Training Loss: 0.06310503167497182\n",
      "Epoch: 3 - Batch: 729, Training Loss: 0.06318985882090099\n",
      "Epoch: 3 - Batch: 730, Training Loss: 0.06328698308214817\n",
      "Epoch: 3 - Batch: 731, Training Loss: 0.06337637464773793\n",
      "Epoch: 3 - Batch: 732, Training Loss: 0.06346748130485588\n",
      "Epoch: 3 - Batch: 733, Training Loss: 0.06355690271601352\n",
      "Epoch: 3 - Batch: 734, Training Loss: 0.06364628323546888\n",
      "Epoch: 3 - Batch: 735, Training Loss: 0.0637294024624437\n",
      "Epoch: 3 - Batch: 736, Training Loss: 0.06381112965407656\n",
      "Epoch: 3 - Batch: 737, Training Loss: 0.06389882515615491\n",
      "Epoch: 3 - Batch: 738, Training Loss: 0.0639904902855654\n",
      "Epoch: 3 - Batch: 739, Training Loss: 0.06407380711355218\n",
      "Epoch: 3 - Batch: 740, Training Loss: 0.06416166400780923\n",
      "Epoch: 3 - Batch: 741, Training Loss: 0.06424550140650316\n",
      "Epoch: 3 - Batch: 742, Training Loss: 0.06433162158030775\n",
      "Epoch: 3 - Batch: 743, Training Loss: 0.06441642210554721\n",
      "Epoch: 3 - Batch: 744, Training Loss: 0.06450909387264679\n",
      "Epoch: 3 - Batch: 745, Training Loss: 0.06459507469094017\n",
      "Epoch: 3 - Batch: 746, Training Loss: 0.06468286722972619\n",
      "Epoch: 3 - Batch: 747, Training Loss: 0.06476397625175281\n",
      "Epoch: 3 - Batch: 748, Training Loss: 0.0648526487065785\n",
      "Epoch: 3 - Batch: 749, Training Loss: 0.06494397108456981\n",
      "Epoch: 3 - Batch: 750, Training Loss: 0.06502457561085671\n",
      "Epoch: 3 - Batch: 751, Training Loss: 0.06510546395113417\n",
      "Epoch: 3 - Batch: 752, Training Loss: 0.06519241089226792\n",
      "Epoch: 3 - Batch: 753, Training Loss: 0.0652723357106125\n",
      "Epoch: 3 - Batch: 754, Training Loss: 0.06534934428151369\n",
      "Epoch: 3 - Batch: 755, Training Loss: 0.06543171508345834\n",
      "Epoch: 3 - Batch: 756, Training Loss: 0.0655177978375559\n",
      "Epoch: 3 - Batch: 757, Training Loss: 0.06560714855468885\n",
      "Epoch: 3 - Batch: 758, Training Loss: 0.06569990936425787\n",
      "Epoch: 3 - Batch: 759, Training Loss: 0.06579315354723242\n",
      "Epoch: 3 - Batch: 760, Training Loss: 0.06587970636130171\n",
      "Epoch: 3 - Batch: 761, Training Loss: 0.06596527660688753\n",
      "Epoch: 3 - Batch: 762, Training Loss: 0.06605768285901788\n",
      "Epoch: 3 - Batch: 763, Training Loss: 0.06614313138089765\n",
      "Epoch: 3 - Batch: 764, Training Loss: 0.06623087437888284\n",
      "Epoch: 3 - Batch: 765, Training Loss: 0.06632557062194319\n",
      "Epoch: 3 - Batch: 766, Training Loss: 0.0664082837243183\n",
      "Epoch: 3 - Batch: 767, Training Loss: 0.0665028430027254\n",
      "Epoch: 3 - Batch: 768, Training Loss: 0.06659682236525352\n",
      "Epoch: 3 - Batch: 769, Training Loss: 0.06668813506861034\n",
      "Epoch: 3 - Batch: 770, Training Loss: 0.06677618711742002\n",
      "Epoch: 3 - Batch: 771, Training Loss: 0.06686530768821884\n",
      "Epoch: 3 - Batch: 772, Training Loss: 0.06695131100034635\n",
      "Epoch: 3 - Batch: 773, Training Loss: 0.06704434901970141\n",
      "Epoch: 3 - Batch: 774, Training Loss: 0.0671266193314769\n",
      "Epoch: 3 - Batch: 775, Training Loss: 0.06721236205862134\n",
      "Epoch: 3 - Batch: 776, Training Loss: 0.06729553518183591\n",
      "Epoch: 3 - Batch: 777, Training Loss: 0.06737901242588883\n",
      "Epoch: 3 - Batch: 778, Training Loss: 0.06747103357072888\n",
      "Epoch: 3 - Batch: 779, Training Loss: 0.0675540168771202\n",
      "Epoch: 3 - Batch: 780, Training Loss: 0.06764712004272104\n",
      "Epoch: 3 - Batch: 781, Training Loss: 0.0677318606667752\n",
      "Epoch: 3 - Batch: 782, Training Loss: 0.06781956982207338\n",
      "Epoch: 3 - Batch: 783, Training Loss: 0.06790700449205156\n",
      "Epoch: 3 - Batch: 784, Training Loss: 0.0679905496994852\n",
      "Epoch: 3 - Batch: 785, Training Loss: 0.0680797010610748\n",
      "Epoch: 3 - Batch: 786, Training Loss: 0.06816767515679498\n",
      "Epoch: 3 - Batch: 787, Training Loss: 0.06826007304401145\n",
      "Epoch: 3 - Batch: 788, Training Loss: 0.06834229758969983\n",
      "Epoch: 3 - Batch: 789, Training Loss: 0.06843032565579485\n",
      "Epoch: 3 - Batch: 790, Training Loss: 0.06851208581902692\n",
      "Epoch: 3 - Batch: 791, Training Loss: 0.06859616679486943\n",
      "Epoch: 3 - Batch: 792, Training Loss: 0.06868375221245719\n",
      "Epoch: 3 - Batch: 793, Training Loss: 0.06877711788437656\n",
      "Epoch: 3 - Batch: 794, Training Loss: 0.06886935043789656\n",
      "Epoch: 3 - Batch: 795, Training Loss: 0.06895295946332156\n",
      "Epoch: 3 - Batch: 796, Training Loss: 0.06903883484910377\n",
      "Epoch: 3 - Batch: 797, Training Loss: 0.06912275613787558\n",
      "Epoch: 3 - Batch: 798, Training Loss: 0.06920982773723096\n",
      "Epoch: 3 - Batch: 799, Training Loss: 0.06929446927350552\n",
      "Epoch: 3 - Batch: 800, Training Loss: 0.06938837360510383\n",
      "Epoch: 3 - Batch: 801, Training Loss: 0.06947224049722377\n",
      "Epoch: 3 - Batch: 802, Training Loss: 0.06955331654147327\n",
      "Epoch: 3 - Batch: 803, Training Loss: 0.06963382616861542\n",
      "Epoch: 3 - Batch: 804, Training Loss: 0.06972536208049378\n",
      "Epoch: 3 - Batch: 805, Training Loss: 0.0698150658044056\n",
      "Epoch: 3 - Batch: 806, Training Loss: 0.06988758229917752\n",
      "Epoch: 3 - Batch: 807, Training Loss: 0.06998042156224821\n",
      "Epoch: 3 - Batch: 808, Training Loss: 0.0700631709784813\n",
      "Epoch: 3 - Batch: 809, Training Loss: 0.0701545227757635\n",
      "Epoch: 3 - Batch: 810, Training Loss: 0.07023294458465394\n",
      "Epoch: 3 - Batch: 811, Training Loss: 0.07031892274643849\n",
      "Epoch: 3 - Batch: 812, Training Loss: 0.07040134381200149\n",
      "Epoch: 3 - Batch: 813, Training Loss: 0.07048837287014792\n",
      "Epoch: 3 - Batch: 814, Training Loss: 0.07057071566754708\n",
      "Epoch: 3 - Batch: 815, Training Loss: 0.07065280031406662\n",
      "Epoch: 3 - Batch: 816, Training Loss: 0.07073591433304854\n",
      "Epoch: 3 - Batch: 817, Training Loss: 0.07081603384877912\n",
      "Epoch: 3 - Batch: 818, Training Loss: 0.07090089889011573\n",
      "Epoch: 3 - Batch: 819, Training Loss: 0.07099062586156883\n",
      "Epoch: 3 - Batch: 820, Training Loss: 0.07107840229449779\n",
      "Epoch: 3 - Batch: 821, Training Loss: 0.07116721330788203\n",
      "Epoch: 3 - Batch: 822, Training Loss: 0.07126091828393699\n",
      "Epoch: 3 - Batch: 823, Training Loss: 0.07134445017175887\n",
      "Epoch: 3 - Batch: 824, Training Loss: 0.07142078298859138\n",
      "Epoch: 3 - Batch: 825, Training Loss: 0.07150649120583265\n",
      "Epoch: 3 - Batch: 826, Training Loss: 0.0715931684186506\n",
      "Epoch: 3 - Batch: 827, Training Loss: 0.07168344735086063\n",
      "Epoch: 3 - Batch: 828, Training Loss: 0.07177954885002788\n",
      "Epoch: 3 - Batch: 829, Training Loss: 0.07186617599692115\n",
      "Epoch: 3 - Batch: 830, Training Loss: 0.07195519324806002\n",
      "Epoch: 3 - Batch: 831, Training Loss: 0.07203904244659552\n",
      "Epoch: 3 - Batch: 832, Training Loss: 0.07213015641837967\n",
      "Epoch: 3 - Batch: 833, Training Loss: 0.07221487427059295\n",
      "Epoch: 3 - Batch: 834, Training Loss: 0.07230020794480001\n",
      "Epoch: 3 - Batch: 835, Training Loss: 0.0723872239057401\n",
      "Epoch: 3 - Batch: 836, Training Loss: 0.0724759674005544\n",
      "Epoch: 3 - Batch: 837, Training Loss: 0.07257703232083154\n",
      "Epoch: 3 - Batch: 838, Training Loss: 0.072667243277305\n",
      "Epoch: 3 - Batch: 839, Training Loss: 0.07275245915696792\n",
      "Epoch: 3 - Batch: 840, Training Loss: 0.07283160637193058\n",
      "Epoch: 3 - Batch: 841, Training Loss: 0.07291804784121204\n",
      "Epoch: 3 - Batch: 842, Training Loss: 0.0730089555134623\n",
      "Epoch: 3 - Batch: 843, Training Loss: 0.07308839345551645\n",
      "Epoch: 3 - Batch: 844, Training Loss: 0.0731746666627638\n",
      "Epoch: 3 - Batch: 845, Training Loss: 0.07325914010380828\n",
      "Epoch: 3 - Batch: 846, Training Loss: 0.07333743073676356\n",
      "Epoch: 3 - Batch: 847, Training Loss: 0.07342805250070582\n",
      "Epoch: 3 - Batch: 848, Training Loss: 0.07351955125235009\n",
      "Epoch: 3 - Batch: 849, Training Loss: 0.07359427575077583\n",
      "Epoch: 3 - Batch: 850, Training Loss: 0.07368465146022056\n",
      "Epoch: 3 - Batch: 851, Training Loss: 0.07377702667444302\n",
      "Epoch: 3 - Batch: 852, Training Loss: 0.0738569793243511\n",
      "Epoch: 3 - Batch: 853, Training Loss: 0.07394681413152918\n",
      "Epoch: 3 - Batch: 854, Training Loss: 0.07402509618334312\n",
      "Epoch: 3 - Batch: 855, Training Loss: 0.07412369308953064\n",
      "Epoch: 3 - Batch: 856, Training Loss: 0.07421313506083109\n",
      "Epoch: 3 - Batch: 857, Training Loss: 0.07430156687920168\n",
      "Epoch: 3 - Batch: 858, Training Loss: 0.07439059053067347\n",
      "Epoch: 3 - Batch: 859, Training Loss: 0.0744758266801166\n",
      "Epoch: 3 - Batch: 860, Training Loss: 0.07457068027499106\n",
      "Epoch: 3 - Batch: 861, Training Loss: 0.0746557588749264\n",
      "Epoch: 3 - Batch: 862, Training Loss: 0.07474042579630516\n",
      "Epoch: 3 - Batch: 863, Training Loss: 0.07483488733372087\n",
      "Epoch: 3 - Batch: 864, Training Loss: 0.07491190933543651\n",
      "Epoch: 3 - Batch: 865, Training Loss: 0.07499555260511377\n",
      "Epoch: 3 - Batch: 866, Training Loss: 0.07508672258598888\n",
      "Epoch: 3 - Batch: 867, Training Loss: 0.07517222425040124\n",
      "Epoch: 3 - Batch: 868, Training Loss: 0.075255783938897\n",
      "Epoch: 3 - Batch: 869, Training Loss: 0.07534917701610286\n",
      "Epoch: 3 - Batch: 870, Training Loss: 0.0754270126199841\n",
      "Epoch: 3 - Batch: 871, Training Loss: 0.0755099829959731\n",
      "Epoch: 3 - Batch: 872, Training Loss: 0.07559706529549896\n",
      "Epoch: 3 - Batch: 873, Training Loss: 0.07567927191901959\n",
      "Epoch: 3 - Batch: 874, Training Loss: 0.07576458914758356\n",
      "Epoch: 3 - Batch: 875, Training Loss: 0.07584184579316458\n",
      "Epoch: 3 - Batch: 876, Training Loss: 0.07591733254282233\n",
      "Epoch: 3 - Batch: 877, Training Loss: 0.07599630973504155\n",
      "Epoch: 3 - Batch: 878, Training Loss: 0.07608738978382565\n",
      "Epoch: 3 - Batch: 879, Training Loss: 0.07618027865565435\n",
      "Epoch: 3 - Batch: 880, Training Loss: 0.07627163503695879\n",
      "Epoch: 3 - Batch: 881, Training Loss: 0.0763579358842539\n",
      "Epoch: 3 - Batch: 882, Training Loss: 0.07644657103675317\n",
      "Epoch: 3 - Batch: 883, Training Loss: 0.07653859297236795\n",
      "Epoch: 3 - Batch: 884, Training Loss: 0.07662527630133416\n",
      "Epoch: 3 - Batch: 885, Training Loss: 0.07671035791609813\n",
      "Epoch: 3 - Batch: 886, Training Loss: 0.07679617500695621\n",
      "Epoch: 3 - Batch: 887, Training Loss: 0.0768792918245393\n",
      "Epoch: 3 - Batch: 888, Training Loss: 0.07697017439587002\n",
      "Epoch: 3 - Batch: 889, Training Loss: 0.07705891574694347\n",
      "Epoch: 3 - Batch: 890, Training Loss: 0.0771366653231839\n",
      "Epoch: 3 - Batch: 891, Training Loss: 0.07721818061808053\n",
      "Epoch: 3 - Batch: 892, Training Loss: 0.07730215547566192\n",
      "Epoch: 3 - Batch: 893, Training Loss: 0.07738280961688478\n",
      "Epoch: 3 - Batch: 894, Training Loss: 0.07747192662301941\n",
      "Epoch: 3 - Batch: 895, Training Loss: 0.07755613043459493\n",
      "Epoch: 3 - Batch: 896, Training Loss: 0.07763591844273444\n",
      "Epoch: 3 - Batch: 897, Training Loss: 0.07773146524432287\n",
      "Epoch: 3 - Batch: 898, Training Loss: 0.07782071351906159\n",
      "Epoch: 3 - Batch: 899, Training Loss: 0.07790336537964111\n",
      "Epoch: 3 - Batch: 900, Training Loss: 0.07798327953488277\n",
      "Epoch: 3 - Batch: 901, Training Loss: 0.07806667014585798\n",
      "Epoch: 3 - Batch: 902, Training Loss: 0.07815880227098813\n",
      "Epoch: 3 - Batch: 903, Training Loss: 0.07824648617091859\n",
      "Epoch: 3 - Batch: 904, Training Loss: 0.07832836087910493\n",
      "Epoch: 3 - Batch: 905, Training Loss: 0.07842213392356537\n",
      "Epoch: 3 - Batch: 906, Training Loss: 0.07850766146731614\n",
      "Epoch: 3 - Batch: 907, Training Loss: 0.07858944768286859\n",
      "Epoch: 3 - Batch: 908, Training Loss: 0.07867210203933084\n",
      "Epoch: 3 - Batch: 909, Training Loss: 0.07875762914444874\n",
      "Epoch: 3 - Batch: 910, Training Loss: 0.07884185166896675\n",
      "Epoch: 3 - Batch: 911, Training Loss: 0.07892708692261038\n",
      "Epoch: 3 - Batch: 912, Training Loss: 0.07900265431275613\n",
      "Epoch: 3 - Batch: 913, Training Loss: 0.07908486014550203\n",
      "Epoch: 3 - Batch: 914, Training Loss: 0.07917578103233926\n",
      "Epoch: 3 - Batch: 915, Training Loss: 0.0792636531181201\n",
      "Epoch: 3 - Batch: 916, Training Loss: 0.07934541053835234\n",
      "Epoch: 3 - Batch: 917, Training Loss: 0.07943316592926014\n",
      "Epoch: 3 - Batch: 918, Training Loss: 0.07951275030673044\n",
      "Epoch: 3 - Batch: 919, Training Loss: 0.0796030070478248\n",
      "Epoch: 3 - Batch: 920, Training Loss: 0.07968660856706784\n",
      "Epoch: 3 - Batch: 921, Training Loss: 0.07977976895881134\n",
      "Epoch: 3 - Batch: 922, Training Loss: 0.07986269194391829\n",
      "Epoch: 3 - Batch: 923, Training Loss: 0.07994489396488291\n",
      "Epoch: 3 - Batch: 924, Training Loss: 0.08002661920429067\n",
      "Epoch: 3 - Batch: 925, Training Loss: 0.08011146406851598\n",
      "Epoch: 3 - Batch: 926, Training Loss: 0.08019899479908928\n",
      "Epoch: 3 - Batch: 927, Training Loss: 0.0802946718190341\n",
      "Epoch: 3 - Batch: 928, Training Loss: 0.08038152581942615\n",
      "Epoch: 3 - Batch: 929, Training Loss: 0.08047460521853979\n",
      "Epoch: 3 - Batch: 930, Training Loss: 0.0805615090377394\n",
      "Epoch: 3 - Batch: 931, Training Loss: 0.0806463120650395\n",
      "Epoch: 3 - Batch: 932, Training Loss: 0.08072934985457368\n",
      "Epoch: 3 - Batch: 933, Training Loss: 0.08081249170511912\n",
      "Epoch: 3 - Batch: 934, Training Loss: 0.08089980274241164\n",
      "Epoch: 3 - Batch: 935, Training Loss: 0.08099290165142041\n",
      "Epoch: 3 - Batch: 936, Training Loss: 0.08108330834317168\n",
      "Epoch: 3 - Batch: 937, Training Loss: 0.08116984953038135\n",
      "Epoch: 3 - Batch: 938, Training Loss: 0.08125552172412721\n",
      "Epoch: 3 - Batch: 939, Training Loss: 0.08133594854232881\n",
      "Epoch: 3 - Batch: 940, Training Loss: 0.08141989288094822\n",
      "Epoch: 3 - Batch: 941, Training Loss: 0.08150032849651861\n",
      "Epoch: 3 - Batch: 942, Training Loss: 0.08158364385415863\n",
      "Epoch: 3 - Batch: 943, Training Loss: 0.08167070020021096\n",
      "Epoch: 3 - Batch: 944, Training Loss: 0.08176006467558851\n",
      "Epoch: 3 - Batch: 945, Training Loss: 0.08185439933690661\n",
      "Epoch: 3 - Batch: 946, Training Loss: 0.08193617116406585\n",
      "Epoch: 3 - Batch: 947, Training Loss: 0.08202639558224338\n",
      "Epoch: 3 - Batch: 948, Training Loss: 0.08211474967610777\n",
      "Epoch: 3 - Batch: 949, Training Loss: 0.08219791173465414\n",
      "Epoch: 3 - Batch: 950, Training Loss: 0.08228792762924388\n",
      "Epoch: 3 - Batch: 951, Training Loss: 0.08236804795428294\n",
      "Epoch: 3 - Batch: 952, Training Loss: 0.08245472386355819\n",
      "Epoch: 3 - Batch: 953, Training Loss: 0.08254208940252736\n",
      "Epoch: 3 - Batch: 954, Training Loss: 0.08262886090905315\n",
      "Epoch: 3 - Batch: 955, Training Loss: 0.08271011325282046\n",
      "Epoch: 3 - Batch: 956, Training Loss: 0.08279491049140247\n",
      "Epoch: 3 - Batch: 957, Training Loss: 0.08288393788669833\n",
      "Epoch: 3 - Batch: 958, Training Loss: 0.08297312826807819\n",
      "Epoch: 3 - Batch: 959, Training Loss: 0.083055543724083\n",
      "Epoch: 3 - Batch: 960, Training Loss: 0.08314262402196033\n",
      "Epoch: 3 - Batch: 961, Training Loss: 0.08321970365756187\n",
      "Epoch: 3 - Batch: 962, Training Loss: 0.08330694811532945\n",
      "Epoch: 3 - Batch: 963, Training Loss: 0.08339888960197198\n",
      "Epoch: 3 - Batch: 964, Training Loss: 0.08348197475767649\n",
      "Epoch: 3 - Batch: 965, Training Loss: 0.08356166545727953\n",
      "Epoch: 3 - Batch: 966, Training Loss: 0.0836474373257081\n",
      "Epoch: 3 - Batch: 967, Training Loss: 0.08373513007860872\n",
      "Epoch: 3 - Batch: 968, Training Loss: 0.08382367764045152\n",
      "Epoch: 3 - Batch: 969, Training Loss: 0.08390349454893599\n",
      "Epoch: 3 - Batch: 970, Training Loss: 0.08398935134533428\n",
      "Epoch: 3 - Batch: 971, Training Loss: 0.08407293747535985\n",
      "Epoch: 3 - Batch: 972, Training Loss: 0.08416196243655227\n",
      "Epoch: 3 - Batch: 973, Training Loss: 0.08425367158987432\n",
      "Epoch: 3 - Batch: 974, Training Loss: 0.08434656433526358\n",
      "Epoch: 3 - Batch: 975, Training Loss: 0.08442895945353097\n",
      "Epoch: 3 - Batch: 976, Training Loss: 0.08452225193554291\n",
      "Epoch: 3 - Batch: 977, Training Loss: 0.0846051689342381\n",
      "Epoch: 3 - Batch: 978, Training Loss: 0.08468840033384302\n",
      "Epoch: 3 - Batch: 979, Training Loss: 0.08477504916526192\n",
      "Epoch: 3 - Batch: 980, Training Loss: 0.08485653004910222\n",
      "Epoch: 3 - Batch: 981, Training Loss: 0.08494736773778945\n",
      "Epoch: 3 - Batch: 982, Training Loss: 0.08503194716735858\n",
      "Epoch: 3 - Batch: 983, Training Loss: 0.08512010325246783\n",
      "Epoch: 3 - Batch: 984, Training Loss: 0.08521062610779039\n",
      "Epoch: 3 - Batch: 985, Training Loss: 0.08529709610674116\n",
      "Epoch: 3 - Batch: 986, Training Loss: 0.08538268563363881\n",
      "Epoch: 3 - Batch: 987, Training Loss: 0.08548306741104593\n",
      "Epoch: 3 - Batch: 988, Training Loss: 0.0855746639283933\n",
      "Epoch: 3 - Batch: 989, Training Loss: 0.08566758427676277\n",
      "Epoch: 3 - Batch: 990, Training Loss: 0.08576539124842504\n",
      "Epoch: 3 - Batch: 991, Training Loss: 0.0858408427406504\n",
      "Epoch: 3 - Batch: 992, Training Loss: 0.08592691014259807\n",
      "Epoch: 3 - Batch: 993, Training Loss: 0.08600986008200281\n",
      "Epoch: 3 - Batch: 994, Training Loss: 0.08609400634000551\n",
      "Epoch: 3 - Batch: 995, Training Loss: 0.08617886037227526\n",
      "Epoch: 3 - Batch: 996, Training Loss: 0.08626734719273463\n",
      "Epoch: 3 - Batch: 997, Training Loss: 0.08635258589983975\n",
      "Epoch: 3 - Batch: 998, Training Loss: 0.08644397430752047\n",
      "Epoch: 3 - Batch: 999, Training Loss: 0.0865280995667475\n",
      "Epoch: 3 - Batch: 1000, Training Loss: 0.08661934675318289\n",
      "Epoch: 3 - Batch: 1001, Training Loss: 0.08670206510061847\n",
      "Epoch: 3 - Batch: 1002, Training Loss: 0.08679077788495504\n",
      "Epoch: 3 - Batch: 1003, Training Loss: 0.08687337741848841\n",
      "Epoch: 3 - Batch: 1004, Training Loss: 0.08696971550756821\n",
      "Epoch: 3 - Batch: 1005, Training Loss: 0.08705309445461626\n",
      "Epoch: 3 - Batch: 1006, Training Loss: 0.08713857376728683\n",
      "Epoch: 3 - Batch: 1007, Training Loss: 0.08721916349125937\n",
      "Epoch: 3 - Batch: 1008, Training Loss: 0.08730775200658375\n",
      "Epoch: 3 - Batch: 1009, Training Loss: 0.08740386845015768\n",
      "Epoch: 3 - Batch: 1010, Training Loss: 0.08748984779810431\n",
      "Epoch: 3 - Batch: 1011, Training Loss: 0.08757550488063945\n",
      "Epoch: 3 - Batch: 1012, Training Loss: 0.08767168286872741\n",
      "Epoch: 3 - Batch: 1013, Training Loss: 0.08776446197113982\n",
      "Epoch: 3 - Batch: 1014, Training Loss: 0.08784955020246418\n",
      "Epoch: 3 - Batch: 1015, Training Loss: 0.08792867703644396\n",
      "Epoch: 3 - Batch: 1016, Training Loss: 0.08801471951663198\n",
      "Epoch: 3 - Batch: 1017, Training Loss: 0.0880973250303992\n",
      "Epoch: 3 - Batch: 1018, Training Loss: 0.08818517458527836\n",
      "Epoch: 3 - Batch: 1019, Training Loss: 0.08827562908801076\n",
      "Epoch: 3 - Batch: 1020, Training Loss: 0.08835854490054385\n",
      "Epoch: 3 - Batch: 1021, Training Loss: 0.08843554245620026\n",
      "Epoch: 3 - Batch: 1022, Training Loss: 0.0885241636836509\n",
      "Epoch: 3 - Batch: 1023, Training Loss: 0.08862375812126234\n",
      "Epoch: 3 - Batch: 1024, Training Loss: 0.08869965942839089\n",
      "Epoch: 3 - Batch: 1025, Training Loss: 0.08878379050104772\n",
      "Epoch: 3 - Batch: 1026, Training Loss: 0.08887247730102112\n",
      "Epoch: 3 - Batch: 1027, Training Loss: 0.08895315358393623\n",
      "Epoch: 3 - Batch: 1028, Training Loss: 0.08903032905772748\n",
      "Epoch: 3 - Batch: 1029, Training Loss: 0.08911229905426798\n",
      "Epoch: 3 - Batch: 1030, Training Loss: 0.08919365345739805\n",
      "Epoch: 3 - Batch: 1031, Training Loss: 0.08928360159865659\n",
      "Epoch: 3 - Batch: 1032, Training Loss: 0.08936130026259628\n",
      "Epoch: 3 - Batch: 1033, Training Loss: 0.08944389802306446\n",
      "Epoch: 3 - Batch: 1034, Training Loss: 0.08952639431725094\n",
      "Epoch: 3 - Batch: 1035, Training Loss: 0.08960121141663238\n",
      "Epoch: 3 - Batch: 1036, Training Loss: 0.0896950872000276\n",
      "Epoch: 3 - Batch: 1037, Training Loss: 0.0897818996476791\n",
      "Epoch: 3 - Batch: 1038, Training Loss: 0.08986015128555583\n",
      "Epoch: 3 - Batch: 1039, Training Loss: 0.08995260444051195\n",
      "Epoch: 3 - Batch: 1040, Training Loss: 0.0900448559911591\n",
      "Epoch: 3 - Batch: 1041, Training Loss: 0.09013316406564136\n",
      "Epoch: 3 - Batch: 1042, Training Loss: 0.09022254601837588\n",
      "Epoch: 3 - Batch: 1043, Training Loss: 0.09031450078138467\n",
      "Epoch: 3 - Batch: 1044, Training Loss: 0.09040536809323439\n",
      "Epoch: 3 - Batch: 1045, Training Loss: 0.09049302687371152\n",
      "Epoch: 3 - Batch: 1046, Training Loss: 0.09057750208782121\n",
      "Epoch: 3 - Batch: 1047, Training Loss: 0.09066495778746471\n",
      "Epoch: 3 - Batch: 1048, Training Loss: 0.09075341368121888\n",
      "Epoch: 3 - Batch: 1049, Training Loss: 0.09084764175475335\n",
      "Epoch: 3 - Batch: 1050, Training Loss: 0.09093506380322561\n",
      "Epoch: 3 - Batch: 1051, Training Loss: 0.0910278000987782\n",
      "Epoch: 3 - Batch: 1052, Training Loss: 0.09111330619894252\n",
      "Epoch: 3 - Batch: 1053, Training Loss: 0.09120557910050721\n",
      "Epoch: 3 - Batch: 1054, Training Loss: 0.09129290052289592\n",
      "Epoch: 3 - Batch: 1055, Training Loss: 0.09138180068995229\n",
      "Epoch: 3 - Batch: 1056, Training Loss: 0.09147492308126358\n",
      "Epoch: 3 - Batch: 1057, Training Loss: 0.09155996140111144\n",
      "Epoch: 3 - Batch: 1058, Training Loss: 0.09164877526869822\n",
      "Epoch: 3 - Batch: 1059, Training Loss: 0.09173192439684227\n",
      "Epoch: 3 - Batch: 1060, Training Loss: 0.09181088377809643\n",
      "Epoch: 3 - Batch: 1061, Training Loss: 0.09189520315398426\n",
      "Epoch: 3 - Batch: 1062, Training Loss: 0.09198061006134423\n",
      "Epoch: 3 - Batch: 1063, Training Loss: 0.09206904098391533\n",
      "Epoch: 3 - Batch: 1064, Training Loss: 0.0921532421575158\n",
      "Epoch: 3 - Batch: 1065, Training Loss: 0.0922470628152637\n",
      "Epoch: 3 - Batch: 1066, Training Loss: 0.09233256568437193\n",
      "Epoch: 3 - Batch: 1067, Training Loss: 0.09242612613399033\n",
      "Epoch: 3 - Batch: 1068, Training Loss: 0.09251199342050956\n",
      "Epoch: 3 - Batch: 1069, Training Loss: 0.09260292983247866\n",
      "Epoch: 3 - Batch: 1070, Training Loss: 0.09268062761915263\n",
      "Epoch: 3 - Batch: 1071, Training Loss: 0.09276353265738013\n",
      "Epoch: 3 - Batch: 1072, Training Loss: 0.09285154967116281\n",
      "Epoch: 3 - Batch: 1073, Training Loss: 0.09294286244865475\n",
      "Epoch: 3 - Batch: 1074, Training Loss: 0.09303211848287045\n",
      "Epoch: 3 - Batch: 1075, Training Loss: 0.09312490187276458\n",
      "Epoch: 3 - Batch: 1076, Training Loss: 0.09320402810130744\n",
      "Epoch: 3 - Batch: 1077, Training Loss: 0.09328984012626494\n",
      "Epoch: 3 - Batch: 1078, Training Loss: 0.09337695976518477\n",
      "Epoch: 3 - Batch: 1079, Training Loss: 0.09345582616245174\n",
      "Epoch: 3 - Batch: 1080, Training Loss: 0.09354917734713103\n",
      "Epoch: 3 - Batch: 1081, Training Loss: 0.09363601950443602\n",
      "Epoch: 3 - Batch: 1082, Training Loss: 0.09371726996393544\n",
      "Epoch: 3 - Batch: 1083, Training Loss: 0.09379939833782601\n",
      "Epoch: 3 - Batch: 1084, Training Loss: 0.09388435834008663\n",
      "Epoch: 3 - Batch: 1085, Training Loss: 0.09397416278892884\n",
      "Epoch: 3 - Batch: 1086, Training Loss: 0.09406735630054182\n",
      "Epoch: 3 - Batch: 1087, Training Loss: 0.09416099047136939\n",
      "Epoch: 3 - Batch: 1088, Training Loss: 0.09425484861678152\n",
      "Epoch: 3 - Batch: 1089, Training Loss: 0.09434621396843672\n",
      "Epoch: 3 - Batch: 1090, Training Loss: 0.09442968418818604\n",
      "Epoch: 3 - Batch: 1091, Training Loss: 0.09452591192094643\n",
      "Epoch: 3 - Batch: 1092, Training Loss: 0.09461896878298044\n",
      "Epoch: 3 - Batch: 1093, Training Loss: 0.09470525869534384\n",
      "Epoch: 3 - Batch: 1094, Training Loss: 0.09479490072029345\n",
      "Epoch: 3 - Batch: 1095, Training Loss: 0.09488395319179714\n",
      "Epoch: 3 - Batch: 1096, Training Loss: 0.09496432181392143\n",
      "Epoch: 3 - Batch: 1097, Training Loss: 0.09504241129973437\n",
      "Epoch: 3 - Batch: 1098, Training Loss: 0.09513182798477746\n",
      "Epoch: 3 - Batch: 1099, Training Loss: 0.09521292194501678\n",
      "Epoch: 3 - Batch: 1100, Training Loss: 0.09529864494603862\n",
      "Epoch: 3 - Batch: 1101, Training Loss: 0.09538468127918875\n",
      "Epoch: 3 - Batch: 1102, Training Loss: 0.09547007895672499\n",
      "Epoch: 3 - Batch: 1103, Training Loss: 0.09555541480556254\n",
      "Epoch: 3 - Batch: 1104, Training Loss: 0.09563992934218093\n",
      "Epoch: 3 - Batch: 1105, Training Loss: 0.09573825996115828\n",
      "Epoch: 3 - Batch: 1106, Training Loss: 0.09582220615315595\n",
      "Epoch: 3 - Batch: 1107, Training Loss: 0.09591443697685625\n",
      "Epoch: 3 - Batch: 1108, Training Loss: 0.09600759044932093\n",
      "Epoch: 3 - Batch: 1109, Training Loss: 0.09610636344497675\n",
      "Epoch: 3 - Batch: 1110, Training Loss: 0.09619230902437152\n",
      "Epoch: 3 - Batch: 1111, Training Loss: 0.0962899198839024\n",
      "Epoch: 3 - Batch: 1112, Training Loss: 0.09637671788755935\n",
      "Epoch: 3 - Batch: 1113, Training Loss: 0.0964628780449109\n",
      "Epoch: 3 - Batch: 1114, Training Loss: 0.09655437870553477\n",
      "Epoch: 3 - Batch: 1115, Training Loss: 0.09664616628146883\n",
      "Epoch: 3 - Batch: 1116, Training Loss: 0.09673304266449231\n",
      "Epoch: 3 - Batch: 1117, Training Loss: 0.09681139572540523\n",
      "Epoch: 3 - Batch: 1118, Training Loss: 0.09690175154192333\n",
      "Epoch: 3 - Batch: 1119, Training Loss: 0.09698654386287503\n",
      "Epoch: 3 - Batch: 1120, Training Loss: 0.09707342326690506\n",
      "Epoch: 3 - Batch: 1121, Training Loss: 0.09716767714564283\n",
      "Epoch: 3 - Batch: 1122, Training Loss: 0.09725915376795069\n",
      "Epoch: 3 - Batch: 1123, Training Loss: 0.09734454436558199\n",
      "Epoch: 3 - Batch: 1124, Training Loss: 0.09743952598564858\n",
      "Epoch: 3 - Batch: 1125, Training Loss: 0.0975277230489511\n",
      "Epoch: 3 - Batch: 1126, Training Loss: 0.09761180284480946\n",
      "Epoch: 3 - Batch: 1127, Training Loss: 0.09769750303074495\n",
      "Epoch: 3 - Batch: 1128, Training Loss: 0.09778128012051036\n",
      "Epoch: 3 - Batch: 1129, Training Loss: 0.09786305920055652\n",
      "Epoch: 3 - Batch: 1130, Training Loss: 0.09795809983218685\n",
      "Epoch: 3 - Batch: 1131, Training Loss: 0.09804721506525628\n",
      "Epoch: 3 - Batch: 1132, Training Loss: 0.09813013865580013\n",
      "Epoch: 3 - Batch: 1133, Training Loss: 0.09822355954603572\n",
      "Epoch: 3 - Batch: 1134, Training Loss: 0.09830960736395312\n",
      "Epoch: 3 - Batch: 1135, Training Loss: 0.09840016066410252\n",
      "Epoch: 3 - Batch: 1136, Training Loss: 0.09848378219983075\n",
      "Epoch: 3 - Batch: 1137, Training Loss: 0.0985726705546897\n",
      "Epoch: 3 - Batch: 1138, Training Loss: 0.09865853447423843\n",
      "Epoch: 3 - Batch: 1139, Training Loss: 0.0987481373249496\n",
      "Epoch: 3 - Batch: 1140, Training Loss: 0.09884568778288305\n",
      "Epoch: 3 - Batch: 1141, Training Loss: 0.09893202253464442\n",
      "Epoch: 3 - Batch: 1142, Training Loss: 0.09900859796486881\n",
      "Epoch: 3 - Batch: 1143, Training Loss: 0.09909685075307763\n",
      "Epoch: 3 - Batch: 1144, Training Loss: 0.09918183158730987\n",
      "Epoch: 3 - Batch: 1145, Training Loss: 0.09926614683016419\n",
      "Epoch: 3 - Batch: 1146, Training Loss: 0.0993458598135518\n",
      "Epoch: 3 - Batch: 1147, Training Loss: 0.0994297080915761\n",
      "Epoch: 3 - Batch: 1148, Training Loss: 0.09951489347698875\n",
      "Epoch: 3 - Batch: 1149, Training Loss: 0.09960243135542418\n",
      "Epoch: 3 - Batch: 1150, Training Loss: 0.09968706736863153\n",
      "Epoch: 3 - Batch: 1151, Training Loss: 0.0997758909973438\n",
      "Epoch: 3 - Batch: 1152, Training Loss: 0.09986377569227471\n",
      "Epoch: 3 - Batch: 1153, Training Loss: 0.09994764566718049\n",
      "Epoch: 3 - Batch: 1154, Training Loss: 0.10003076284926131\n",
      "Epoch: 3 - Batch: 1155, Training Loss: 0.10011752291426532\n",
      "Epoch: 3 - Batch: 1156, Training Loss: 0.10020186192360683\n",
      "Epoch: 3 - Batch: 1157, Training Loss: 0.1003017009144794\n",
      "Epoch: 3 - Batch: 1158, Training Loss: 0.10039378952228806\n",
      "Epoch: 3 - Batch: 1159, Training Loss: 0.10048003517597864\n",
      "Epoch: 3 - Batch: 1160, Training Loss: 0.10056209634019565\n",
      "Epoch: 3 - Batch: 1161, Training Loss: 0.10064710477123015\n",
      "Epoch: 3 - Batch: 1162, Training Loss: 0.1007278833606251\n",
      "Epoch: 3 - Batch: 1163, Training Loss: 0.1008186620130369\n",
      "Epoch: 3 - Batch: 1164, Training Loss: 0.1009068270684969\n",
      "Epoch: 3 - Batch: 1165, Training Loss: 0.10098960353787463\n",
      "Epoch: 3 - Batch: 1166, Training Loss: 0.10108476434872914\n",
      "Epoch: 3 - Batch: 1167, Training Loss: 0.10116003188698446\n",
      "Epoch: 3 - Batch: 1168, Training Loss: 0.10123931510902162\n",
      "Epoch: 3 - Batch: 1169, Training Loss: 0.10133009908680694\n",
      "Epoch: 3 - Batch: 1170, Training Loss: 0.10141487439992417\n",
      "Epoch: 3 - Batch: 1171, Training Loss: 0.10149914637257408\n",
      "Epoch: 3 - Batch: 1172, Training Loss: 0.10158745741246154\n",
      "Epoch: 3 - Batch: 1173, Training Loss: 0.10167106000666397\n",
      "Epoch: 3 - Batch: 1174, Training Loss: 0.10174912586190411\n",
      "Epoch: 3 - Batch: 1175, Training Loss: 0.10184039233582926\n",
      "Epoch: 3 - Batch: 1176, Training Loss: 0.10192158888673308\n",
      "Epoch: 3 - Batch: 1177, Training Loss: 0.10200605973304801\n",
      "Epoch: 3 - Batch: 1178, Training Loss: 0.10209865030982404\n",
      "Epoch: 3 - Batch: 1179, Training Loss: 0.102184803356381\n",
      "Epoch: 3 - Batch: 1180, Training Loss: 0.10226500024447592\n",
      "Epoch: 3 - Batch: 1181, Training Loss: 0.10234817998425087\n",
      "Epoch: 3 - Batch: 1182, Training Loss: 0.1024297095995835\n",
      "Epoch: 3 - Batch: 1183, Training Loss: 0.10251419545826232\n",
      "Epoch: 3 - Batch: 1184, Training Loss: 0.10259982883608953\n",
      "Epoch: 3 - Batch: 1185, Training Loss: 0.10269448213637566\n",
      "Epoch: 3 - Batch: 1186, Training Loss: 0.10278174294711741\n",
      "Epoch: 3 - Batch: 1187, Training Loss: 0.10286869951964017\n",
      "Epoch: 3 - Batch: 1188, Training Loss: 0.10294781329348116\n",
      "Epoch: 3 - Batch: 1189, Training Loss: 0.10303057090164615\n",
      "Epoch: 3 - Batch: 1190, Training Loss: 0.10312206558821412\n",
      "Epoch: 3 - Batch: 1191, Training Loss: 0.10321038948436875\n",
      "Epoch: 3 - Batch: 1192, Training Loss: 0.10330002507191788\n",
      "Epoch: 3 - Batch: 1193, Training Loss: 0.10338613802372519\n",
      "Epoch: 3 - Batch: 1194, Training Loss: 0.10346196780280885\n",
      "Epoch: 3 - Batch: 1195, Training Loss: 0.10354472696781158\n",
      "Epoch: 3 - Batch: 1196, Training Loss: 0.10364291800887233\n",
      "Epoch: 3 - Batch: 1197, Training Loss: 0.10373882929582896\n",
      "Epoch: 3 - Batch: 1198, Training Loss: 0.10382782590651196\n",
      "Epoch: 3 - Batch: 1199, Training Loss: 0.10391310347895914\n",
      "Epoch: 3 - Batch: 1200, Training Loss: 0.10400706595721135\n",
      "Epoch: 3 - Batch: 1201, Training Loss: 0.1040939076388159\n",
      "Epoch: 3 - Batch: 1202, Training Loss: 0.1041841130166208\n",
      "Epoch: 3 - Batch: 1203, Training Loss: 0.10426904841219607\n",
      "Epoch: 3 - Batch: 1204, Training Loss: 0.10435417422385358\n",
      "Epoch: 3 - Batch: 1205, Training Loss: 0.1044429561453771\n",
      "Epoch: 3 - Batch: 1206, Training Loss: 0.10452973530635509\n",
      "Epoch: 3 - Batch: 1207, Training Loss: 0.10461297480522301\n",
      "Epoch: 3 - Batch: 1208, Training Loss: 0.10469558277780538\n",
      "Epoch: 3 - Batch: 1209, Training Loss: 0.10477351570875688\n",
      "Epoch: 3 - Batch: 1210, Training Loss: 0.10486300253181118\n",
      "Epoch: 3 - Batch: 1211, Training Loss: 0.10494904341191597\n",
      "Epoch: 3 - Batch: 1212, Training Loss: 0.10503794268935078\n",
      "Epoch: 3 - Batch: 1213, Training Loss: 0.10513308783298109\n",
      "Epoch: 3 - Batch: 1214, Training Loss: 0.10521506059723311\n",
      "Epoch: 3 - Batch: 1215, Training Loss: 0.10530918402903115\n",
      "Epoch: 3 - Batch: 1216, Training Loss: 0.10539274484190972\n",
      "Epoch: 3 - Batch: 1217, Training Loss: 0.10547392438497907\n",
      "Epoch: 3 - Batch: 1218, Training Loss: 0.10555301186556049\n",
      "Epoch: 3 - Batch: 1219, Training Loss: 0.10564121387738296\n",
      "Epoch: 3 - Batch: 1220, Training Loss: 0.10572384832856271\n",
      "Epoch: 3 - Batch: 1221, Training Loss: 0.10581489432461029\n",
      "Epoch: 3 - Batch: 1222, Training Loss: 0.10589988840431915\n",
      "Epoch: 3 - Batch: 1223, Training Loss: 0.1059823672335045\n",
      "Epoch: 3 - Batch: 1224, Training Loss: 0.10607173551448543\n",
      "Epoch: 3 - Batch: 1225, Training Loss: 0.10615536286364345\n",
      "Epoch: 3 - Batch: 1226, Training Loss: 0.10624235617630122\n",
      "Epoch: 3 - Batch: 1227, Training Loss: 0.10633111120530622\n",
      "Epoch: 3 - Batch: 1228, Training Loss: 0.1064178214391468\n",
      "Epoch: 3 - Batch: 1229, Training Loss: 0.10649685344193903\n",
      "Epoch: 3 - Batch: 1230, Training Loss: 0.10658201693277651\n",
      "Epoch: 3 - Batch: 1231, Training Loss: 0.10667618655076075\n",
      "Epoch: 3 - Batch: 1232, Training Loss: 0.10676913413761267\n",
      "Epoch: 3 - Batch: 1233, Training Loss: 0.10685906845884734\n",
      "Epoch: 3 - Batch: 1234, Training Loss: 0.10694354020713968\n",
      "Epoch: 3 - Batch: 1235, Training Loss: 0.10702509334184242\n",
      "Epoch: 3 - Batch: 1236, Training Loss: 0.10711054836907988\n",
      "Epoch: 3 - Batch: 1237, Training Loss: 0.10720018522310415\n",
      "Epoch: 3 - Batch: 1238, Training Loss: 0.10728211223362492\n",
      "Epoch: 3 - Batch: 1239, Training Loss: 0.10737242869674468\n",
      "Epoch: 3 - Batch: 1240, Training Loss: 0.10745918882970588\n",
      "Epoch: 3 - Batch: 1241, Training Loss: 0.10755200356640428\n",
      "Epoch: 3 - Batch: 1242, Training Loss: 0.10763759545944816\n",
      "Epoch: 3 - Batch: 1243, Training Loss: 0.10772561569810903\n",
      "Epoch: 3 - Batch: 1244, Training Loss: 0.10781005297974369\n",
      "Epoch: 3 - Batch: 1245, Training Loss: 0.10789826411636512\n",
      "Epoch: 3 - Batch: 1246, Training Loss: 0.10798138255874316\n",
      "Epoch: 3 - Batch: 1247, Training Loss: 0.10806545638672353\n",
      "Epoch: 3 - Batch: 1248, Training Loss: 0.10815559459192242\n",
      "Epoch: 3 - Batch: 1249, Training Loss: 0.10823995608274221\n",
      "Epoch: 3 - Batch: 1250, Training Loss: 0.10832248830775519\n",
      "Epoch: 3 - Batch: 1251, Training Loss: 0.10840569675018143\n",
      "Epoch: 3 - Batch: 1252, Training Loss: 0.10849128832815101\n",
      "Epoch: 3 - Batch: 1253, Training Loss: 0.10857373005888157\n",
      "Epoch: 3 - Batch: 1254, Training Loss: 0.10866241976248091\n",
      "Epoch: 3 - Batch: 1255, Training Loss: 0.10875052869146934\n",
      "Epoch: 3 - Batch: 1256, Training Loss: 0.10884003529624757\n",
      "Epoch: 3 - Batch: 1257, Training Loss: 0.10892840720107702\n",
      "Epoch: 3 - Batch: 1258, Training Loss: 0.109027086669088\n",
      "Epoch: 3 - Batch: 1259, Training Loss: 0.10911778917854303\n",
      "Epoch: 3 - Batch: 1260, Training Loss: 0.10920945128076903\n",
      "Epoch: 3 - Batch: 1261, Training Loss: 0.109293972928182\n",
      "Epoch: 3 - Batch: 1262, Training Loss: 0.10937915655906323\n",
      "Epoch: 3 - Batch: 1263, Training Loss: 0.10946464637421059\n",
      "Epoch: 3 - Batch: 1264, Training Loss: 0.10955450200100443\n",
      "Epoch: 3 - Batch: 1265, Training Loss: 0.1096399063074569\n",
      "Epoch: 3 - Batch: 1266, Training Loss: 0.10972121110182892\n",
      "Epoch: 3 - Batch: 1267, Training Loss: 0.10980731841936633\n",
      "Epoch: 3 - Batch: 1268, Training Loss: 0.10989417498359831\n",
      "Epoch: 3 - Batch: 1269, Training Loss: 0.10998519788037485\n",
      "Epoch: 3 - Batch: 1270, Training Loss: 0.11006484412361141\n",
      "Epoch: 3 - Batch: 1271, Training Loss: 0.11014840241589555\n",
      "Epoch: 3 - Batch: 1272, Training Loss: 0.11023067315394804\n",
      "Epoch: 3 - Batch: 1273, Training Loss: 0.11031533530596675\n",
      "Epoch: 3 - Batch: 1274, Training Loss: 0.11041180468218441\n",
      "Epoch: 3 - Batch: 1275, Training Loss: 0.11050568583419865\n",
      "Epoch: 3 - Batch: 1276, Training Loss: 0.1105920779912626\n",
      "Epoch: 3 - Batch: 1277, Training Loss: 0.11067429247928497\n",
      "Epoch: 3 - Batch: 1278, Training Loss: 0.11077014614471156\n",
      "Epoch: 3 - Batch: 1279, Training Loss: 0.11085630124851839\n",
      "Epoch: 3 - Batch: 1280, Training Loss: 0.11093950166581677\n",
      "Epoch: 3 - Batch: 1281, Training Loss: 0.11102469781935709\n",
      "Epoch: 3 - Batch: 1282, Training Loss: 0.11111255318148812\n",
      "Epoch: 3 - Batch: 1283, Training Loss: 0.11120632079555028\n",
      "Epoch: 3 - Batch: 1284, Training Loss: 0.11129235344665561\n",
      "Epoch: 3 - Batch: 1285, Training Loss: 0.11137786964155351\n",
      "Epoch: 3 - Batch: 1286, Training Loss: 0.11145963369070198\n",
      "Epoch: 3 - Batch: 1287, Training Loss: 0.11154680740502146\n",
      "Epoch: 3 - Batch: 1288, Training Loss: 0.11162838881609848\n",
      "Epoch: 3 - Batch: 1289, Training Loss: 0.11172003531287954\n",
      "Epoch: 3 - Batch: 1290, Training Loss: 0.11181326727591344\n",
      "Epoch: 3 - Batch: 1291, Training Loss: 0.11189667558047309\n",
      "Epoch: 3 - Batch: 1292, Training Loss: 0.1119774993861196\n",
      "Epoch: 3 - Batch: 1293, Training Loss: 0.1120564988127\n",
      "Epoch: 3 - Batch: 1294, Training Loss: 0.11214271374355698\n",
      "Epoch: 3 - Batch: 1295, Training Loss: 0.11222072221202835\n",
      "Epoch: 3 - Batch: 1296, Training Loss: 0.11230933376484448\n",
      "Epoch: 3 - Batch: 1297, Training Loss: 0.11239355699982413\n",
      "Epoch: 3 - Batch: 1298, Training Loss: 0.11247860161498016\n",
      "Epoch: 3 - Batch: 1299, Training Loss: 0.11257215717167997\n",
      "Epoch: 3 - Batch: 1300, Training Loss: 0.11266256336919704\n",
      "Epoch: 3 - Batch: 1301, Training Loss: 0.1127576545733421\n",
      "Epoch: 3 - Batch: 1302, Training Loss: 0.11284415022304797\n",
      "Epoch: 3 - Batch: 1303, Training Loss: 0.11292675879488932\n",
      "Epoch: 3 - Batch: 1304, Training Loss: 0.11301669781134892\n",
      "Epoch: 3 - Batch: 1305, Training Loss: 0.11310928493466346\n",
      "Epoch: 3 - Batch: 1306, Training Loss: 0.11319378929005729\n",
      "Epoch: 3 - Batch: 1307, Training Loss: 0.11328079481029985\n",
      "Epoch: 3 - Batch: 1308, Training Loss: 0.11336585161450688\n",
      "Epoch: 3 - Batch: 1309, Training Loss: 0.11345033776972623\n",
      "Epoch: 3 - Batch: 1310, Training Loss: 0.1135418515584461\n",
      "Epoch: 3 - Batch: 1311, Training Loss: 0.11362668335635469\n",
      "Epoch: 3 - Batch: 1312, Training Loss: 0.11370662576943685\n",
      "Epoch: 3 - Batch: 1313, Training Loss: 0.11380428650337665\n",
      "Epoch: 3 - Batch: 1314, Training Loss: 0.11388793696416155\n",
      "Epoch: 3 - Batch: 1315, Training Loss: 0.11397774250031902\n",
      "Epoch: 3 - Batch: 1316, Training Loss: 0.11407046039800343\n",
      "Epoch: 3 - Batch: 1317, Training Loss: 0.11415676782479135\n",
      "Epoch: 3 - Batch: 1318, Training Loss: 0.11423725432177285\n",
      "Epoch: 3 - Batch: 1319, Training Loss: 0.11433901250436532\n",
      "Epoch: 3 - Batch: 1320, Training Loss: 0.11441822734292269\n",
      "Epoch: 3 - Batch: 1321, Training Loss: 0.11449764147524415\n",
      "Epoch: 3 - Batch: 1322, Training Loss: 0.11458384396993303\n",
      "Epoch: 3 - Batch: 1323, Training Loss: 0.11467253450755259\n",
      "Epoch: 3 - Batch: 1324, Training Loss: 0.11476741177624533\n",
      "Epoch: 3 - Batch: 1325, Training Loss: 0.11485713379300055\n",
      "Epoch: 3 - Batch: 1326, Training Loss: 0.11494142241516517\n",
      "Epoch: 3 - Batch: 1327, Training Loss: 0.11502949560484285\n",
      "Epoch: 3 - Batch: 1328, Training Loss: 0.11512164678887941\n",
      "Epoch: 3 - Batch: 1329, Training Loss: 0.11521204730270317\n",
      "Epoch: 3 - Batch: 1330, Training Loss: 0.11529501697440843\n",
      "Epoch: 3 - Batch: 1331, Training Loss: 0.11538139909234016\n",
      "Epoch: 3 - Batch: 1332, Training Loss: 0.11546120781507065\n",
      "Epoch: 3 - Batch: 1333, Training Loss: 0.11554652482122924\n",
      "Epoch: 3 - Batch: 1334, Training Loss: 0.11563148651006408\n",
      "Epoch: 3 - Batch: 1335, Training Loss: 0.11572823877358318\n",
      "Epoch: 3 - Batch: 1336, Training Loss: 0.11581466251221265\n",
      "Epoch: 3 - Batch: 1337, Training Loss: 0.11590694500809878\n",
      "Epoch: 3 - Batch: 1338, Training Loss: 0.11599205720360402\n",
      "Epoch: 3 - Batch: 1339, Training Loss: 0.11606718472530989\n",
      "Epoch: 3 - Batch: 1340, Training Loss: 0.11614887252647683\n",
      "Epoch: 3 - Batch: 1341, Training Loss: 0.11623492284027696\n",
      "Epoch: 3 - Batch: 1342, Training Loss: 0.11631898587541793\n",
      "Epoch: 3 - Batch: 1343, Training Loss: 0.11639826094876869\n",
      "Epoch: 3 - Batch: 1344, Training Loss: 0.11648248307789934\n",
      "Epoch: 3 - Batch: 1345, Training Loss: 0.11656901699986624\n",
      "Epoch: 3 - Batch: 1346, Training Loss: 0.11665067162556238\n",
      "Epoch: 3 - Batch: 1347, Training Loss: 0.11674658102825111\n",
      "Epoch: 3 - Batch: 1348, Training Loss: 0.11682783158041944\n",
      "Epoch: 3 - Batch: 1349, Training Loss: 0.11691560110642542\n",
      "Epoch: 3 - Batch: 1350, Training Loss: 0.11699699352852147\n",
      "Epoch: 3 - Batch: 1351, Training Loss: 0.11708371656302789\n",
      "Epoch: 3 - Batch: 1352, Training Loss: 0.11717159789716625\n",
      "Epoch: 3 - Batch: 1353, Training Loss: 0.11726650824866089\n",
      "Epoch: 3 - Batch: 1354, Training Loss: 0.11735071577934879\n",
      "Epoch: 3 - Batch: 1355, Training Loss: 0.11743796185573338\n",
      "Epoch: 3 - Batch: 1356, Training Loss: 0.11751878448757366\n",
      "Epoch: 3 - Batch: 1357, Training Loss: 0.11761025256580776\n",
      "Epoch: 3 - Batch: 1358, Training Loss: 0.11769134684729932\n",
      "Epoch: 3 - Batch: 1359, Training Loss: 0.11777488207738\n",
      "Epoch: 3 - Batch: 1360, Training Loss: 0.11786298606402641\n",
      "Epoch: 3 - Batch: 1361, Training Loss: 0.11793875953747858\n",
      "Epoch: 3 - Batch: 1362, Training Loss: 0.1180180932455395\n",
      "Epoch: 3 - Batch: 1363, Training Loss: 0.11810786131874443\n",
      "Epoch: 3 - Batch: 1364, Training Loss: 0.11819596518183229\n",
      "Epoch: 3 - Batch: 1365, Training Loss: 0.11828104810013898\n",
      "Epoch: 3 - Batch: 1366, Training Loss: 0.11837438643472903\n",
      "Epoch: 3 - Batch: 1367, Training Loss: 0.11845927917171474\n",
      "Epoch: 3 - Batch: 1368, Training Loss: 0.11854925946908607\n",
      "Epoch: 3 - Batch: 1369, Training Loss: 0.11863584826958318\n",
      "Epoch: 3 - Batch: 1370, Training Loss: 0.11872386202759806\n",
      "Epoch: 3 - Batch: 1371, Training Loss: 0.11880269675063058\n",
      "Epoch: 3 - Batch: 1372, Training Loss: 0.1188790352696902\n",
      "Epoch: 3 - Batch: 1373, Training Loss: 0.11896653346111921\n",
      "Epoch: 3 - Batch: 1374, Training Loss: 0.11905059832492673\n",
      "Epoch: 3 - Batch: 1375, Training Loss: 0.11912889507403025\n",
      "Epoch: 3 - Batch: 1376, Training Loss: 0.11920640876464188\n",
      "Epoch: 3 - Batch: 1377, Training Loss: 0.1192889239632866\n",
      "Epoch: 3 - Batch: 1378, Training Loss: 0.11937101500396112\n",
      "Epoch: 3 - Batch: 1379, Training Loss: 0.11945535628677996\n",
      "Epoch: 3 - Batch: 1380, Training Loss: 0.11954052560958103\n",
      "Epoch: 3 - Batch: 1381, Training Loss: 0.1196356970958943\n",
      "Epoch: 3 - Batch: 1382, Training Loss: 0.11972725278331865\n",
      "Epoch: 3 - Batch: 1383, Training Loss: 0.11981756387781939\n",
      "Epoch: 3 - Batch: 1384, Training Loss: 0.11989788922973929\n",
      "Epoch: 3 - Batch: 1385, Training Loss: 0.11998435848733877\n",
      "Epoch: 3 - Batch: 1386, Training Loss: 0.1200648922578217\n",
      "Epoch: 3 - Batch: 1387, Training Loss: 0.12015899438812562\n",
      "Epoch: 3 - Batch: 1388, Training Loss: 0.12024931389078572\n",
      "Epoch: 3 - Batch: 1389, Training Loss: 0.12033455884377557\n",
      "Epoch: 3 - Batch: 1390, Training Loss: 0.12042525766500786\n",
      "Epoch: 3 - Batch: 1391, Training Loss: 0.12050779031629784\n",
      "Epoch: 3 - Batch: 1392, Training Loss: 0.12059121714228421\n",
      "Epoch: 3 - Batch: 1393, Training Loss: 0.12068368301141519\n",
      "Epoch: 3 - Batch: 1394, Training Loss: 0.12076867198197798\n",
      "Epoch: 3 - Batch: 1395, Training Loss: 0.12085243379372862\n",
      "Epoch: 3 - Batch: 1396, Training Loss: 0.1209288202226162\n",
      "Epoch: 3 - Batch: 1397, Training Loss: 0.12100924980235139\n",
      "Epoch: 3 - Batch: 1398, Training Loss: 0.12108872198247989\n",
      "Epoch: 3 - Batch: 1399, Training Loss: 0.12117918267549567\n",
      "Epoch: 3 - Batch: 1400, Training Loss: 0.12126371572736286\n",
      "Epoch: 3 - Batch: 1401, Training Loss: 0.12134164849151031\n",
      "Epoch: 3 - Batch: 1402, Training Loss: 0.12142853755164107\n",
      "Epoch: 3 - Batch: 1403, Training Loss: 0.12151319909822289\n",
      "Epoch: 3 - Batch: 1404, Training Loss: 0.12160813622485543\n",
      "Epoch: 3 - Batch: 1405, Training Loss: 0.12169062194243\n",
      "Epoch: 3 - Batch: 1406, Training Loss: 0.12178881803703545\n",
      "Epoch: 3 - Batch: 1407, Training Loss: 0.12187599174517699\n",
      "Epoch: 3 - Batch: 1408, Training Loss: 0.12196403552174173\n",
      "Epoch: 3 - Batch: 1409, Training Loss: 0.12204646907907418\n",
      "Epoch: 3 - Batch: 1410, Training Loss: 0.12213780941630081\n",
      "Epoch: 3 - Batch: 1411, Training Loss: 0.12221864714699598\n",
      "Epoch: 3 - Batch: 1412, Training Loss: 0.12230937147071311\n",
      "Epoch: 3 - Batch: 1413, Training Loss: 0.12239423944038735\n",
      "Epoch: 3 - Batch: 1414, Training Loss: 0.12247596606365088\n",
      "Epoch: 3 - Batch: 1415, Training Loss: 0.1225593706120306\n",
      "Epoch: 3 - Batch: 1416, Training Loss: 0.1226359802494397\n",
      "Epoch: 3 - Batch: 1417, Training Loss: 0.12271905003446054\n",
      "Epoch: 3 - Batch: 1418, Training Loss: 0.12280549079328035\n",
      "Epoch: 3 - Batch: 1419, Training Loss: 0.1228858695966292\n",
      "Epoch: 3 - Batch: 1420, Training Loss: 0.1229652431399668\n",
      "Epoch: 3 - Batch: 1421, Training Loss: 0.12305322299969335\n",
      "Epoch: 3 - Batch: 1422, Training Loss: 0.12313767535547118\n",
      "Epoch: 3 - Batch: 1423, Training Loss: 0.123215683274107\n",
      "Epoch: 3 - Batch: 1424, Training Loss: 0.1233125470801076\n",
      "Epoch: 3 - Batch: 1425, Training Loss: 0.12339366411490622\n",
      "Epoch: 3 - Batch: 1426, Training Loss: 0.12348465051248694\n",
      "Epoch: 3 - Batch: 1427, Training Loss: 0.12356920250587994\n",
      "Epoch: 3 - Batch: 1428, Training Loss: 0.12366352547238123\n",
      "Epoch: 3 - Batch: 1429, Training Loss: 0.12374642705096929\n",
      "Epoch: 3 - Batch: 1430, Training Loss: 0.12383345347731861\n",
      "Epoch: 3 - Batch: 1431, Training Loss: 0.12391699743063296\n",
      "Epoch: 3 - Batch: 1432, Training Loss: 0.12400199176363684\n",
      "Epoch: 3 - Batch: 1433, Training Loss: 0.12408453161221239\n",
      "Epoch: 3 - Batch: 1434, Training Loss: 0.1241615482205973\n",
      "Epoch: 3 - Batch: 1435, Training Loss: 0.1242459973823866\n",
      "Epoch: 3 - Batch: 1436, Training Loss: 0.12433834996686052\n",
      "Epoch: 3 - Batch: 1437, Training Loss: 0.12443107039749524\n",
      "Epoch: 3 - Batch: 1438, Training Loss: 0.12451303909667095\n",
      "Epoch: 3 - Batch: 1439, Training Loss: 0.12459523060898085\n",
      "Epoch: 3 - Batch: 1440, Training Loss: 0.12467291447678411\n",
      "Epoch: 3 - Batch: 1441, Training Loss: 0.1247651499648197\n",
      "Epoch: 3 - Batch: 1442, Training Loss: 0.1248515554414954\n",
      "Epoch: 3 - Batch: 1443, Training Loss: 0.12494445299331229\n",
      "Epoch: 3 - Batch: 1444, Training Loss: 0.1250289400196194\n",
      "Epoch: 3 - Batch: 1445, Training Loss: 0.12511255553186829\n",
      "Epoch: 3 - Batch: 1446, Training Loss: 0.1252029269538313\n",
      "Epoch: 3 - Batch: 1447, Training Loss: 0.12529583524288626\n",
      "Epoch: 3 - Batch: 1448, Training Loss: 0.1253839696249361\n",
      "Epoch: 3 - Batch: 1449, Training Loss: 0.12546558363693666\n",
      "Epoch: 3 - Batch: 1450, Training Loss: 0.12555216722944088\n",
      "Epoch: 3 - Batch: 1451, Training Loss: 0.12563495759915555\n",
      "Epoch: 3 - Batch: 1452, Training Loss: 0.12571696770280155\n",
      "Epoch: 3 - Batch: 1453, Training Loss: 0.12580326253279525\n",
      "Epoch: 3 - Batch: 1454, Training Loss: 0.125901750052598\n",
      "Epoch: 3 - Batch: 1455, Training Loss: 0.12598100624620223\n",
      "Epoch: 3 - Batch: 1456, Training Loss: 0.1260575797180236\n",
      "Epoch: 3 - Batch: 1457, Training Loss: 0.1261484587432239\n",
      "Epoch: 3 - Batch: 1458, Training Loss: 0.12623293476046415\n",
      "Epoch: 3 - Batch: 1459, Training Loss: 0.126314464443754\n",
      "Epoch: 3 - Batch: 1460, Training Loss: 0.12641078296733732\n",
      "Epoch: 3 - Batch: 1461, Training Loss: 0.12650349734978097\n",
      "Epoch: 3 - Batch: 1462, Training Loss: 0.1265915267564863\n",
      "Epoch: 3 - Batch: 1463, Training Loss: 0.12667472803869453\n",
      "Epoch: 3 - Batch: 1464, Training Loss: 0.12676609965476823\n",
      "Epoch: 3 - Batch: 1465, Training Loss: 0.12684746737133212\n",
      "Epoch: 3 - Batch: 1466, Training Loss: 0.12693182741033893\n",
      "Epoch: 3 - Batch: 1467, Training Loss: 0.1270243240702607\n",
      "Epoch: 3 - Batch: 1468, Training Loss: 0.12711350697634824\n",
      "Epoch: 3 - Batch: 1469, Training Loss: 0.12720121847308097\n",
      "Epoch: 3 - Batch: 1470, Training Loss: 0.1272867212804099\n",
      "Epoch: 3 - Batch: 1471, Training Loss: 0.12736343774036388\n",
      "Epoch: 3 - Batch: 1472, Training Loss: 0.1274577043332765\n",
      "Epoch: 3 - Batch: 1473, Training Loss: 0.12753966041094628\n",
      "Epoch: 3 - Batch: 1474, Training Loss: 0.12762371188718485\n",
      "Epoch: 3 - Batch: 1475, Training Loss: 0.12770838779744817\n",
      "Epoch: 3 - Batch: 1476, Training Loss: 0.12780052780561385\n",
      "Epoch: 3 - Batch: 1477, Training Loss: 0.12788679664506644\n",
      "Epoch: 3 - Batch: 1478, Training Loss: 0.12797859518682186\n",
      "Epoch: 3 - Batch: 1479, Training Loss: 0.12805963841392032\n",
      "Epoch: 3 - Batch: 1480, Training Loss: 0.1281432698096209\n",
      "Epoch: 3 - Batch: 1481, Training Loss: 0.12822475972559125\n",
      "Epoch: 3 - Batch: 1482, Training Loss: 0.12830591649681972\n",
      "Epoch: 3 - Batch: 1483, Training Loss: 0.12838719770386445\n",
      "Epoch: 3 - Batch: 1484, Training Loss: 0.1284779311029571\n",
      "Epoch: 3 - Batch: 1485, Training Loss: 0.12856124693629753\n",
      "Epoch: 3 - Batch: 1486, Training Loss: 0.12864092962312856\n",
      "Epoch: 3 - Batch: 1487, Training Loss: 0.12872870350457344\n",
      "Epoch: 3 - Batch: 1488, Training Loss: 0.12881657243343334\n",
      "Epoch: 3 - Batch: 1489, Training Loss: 0.12890936313799364\n",
      "Epoch: 3 - Batch: 1490, Training Loss: 0.1289950376422844\n",
      "Epoch: 3 - Batch: 1491, Training Loss: 0.12908563520728455\n",
      "Epoch: 3 - Batch: 1492, Training Loss: 0.12917315660300935\n",
      "Epoch: 3 - Batch: 1493, Training Loss: 0.12925713891117133\n",
      "Epoch: 3 - Batch: 1494, Training Loss: 0.1293405560130998\n",
      "Epoch: 3 - Batch: 1495, Training Loss: 0.1294288865258148\n",
      "Epoch: 3 - Batch: 1496, Training Loss: 0.12952575335899988\n",
      "Epoch: 3 - Batch: 1497, Training Loss: 0.12960664328700472\n",
      "Epoch: 3 - Batch: 1498, Training Loss: 0.1296907489004222\n",
      "Epoch: 3 - Batch: 1499, Training Loss: 0.1297808402398629\n",
      "Epoch: 3 - Batch: 1500, Training Loss: 0.12987766320976254\n",
      "Epoch: 3 - Batch: 1501, Training Loss: 0.12996882022316777\n",
      "Epoch: 3 - Batch: 1502, Training Loss: 0.13005612597833224\n",
      "Epoch: 3 - Batch: 1503, Training Loss: 0.1301404992010364\n",
      "Epoch: 3 - Batch: 1504, Training Loss: 0.13022956438671496\n",
      "Epoch: 3 - Batch: 1505, Training Loss: 0.13031456363734914\n",
      "Epoch: 3 - Batch: 1506, Training Loss: 0.1304114828222921\n",
      "Epoch: 3 - Batch: 1507, Training Loss: 0.13049273402932074\n",
      "Epoch: 3 - Batch: 1508, Training Loss: 0.13058000848710438\n",
      "Epoch: 3 - Batch: 1509, Training Loss: 0.1306671375041182\n",
      "Epoch: 3 - Batch: 1510, Training Loss: 0.13075235992002843\n",
      "Epoch: 3 - Batch: 1511, Training Loss: 0.13083859280394283\n",
      "Epoch: 3 - Batch: 1512, Training Loss: 0.1309258340986115\n",
      "Epoch: 3 - Batch: 1513, Training Loss: 0.1310097773622715\n",
      "Epoch: 3 - Batch: 1514, Training Loss: 0.13109707585219324\n",
      "Epoch: 3 - Batch: 1515, Training Loss: 0.1311783170942248\n",
      "Epoch: 3 - Batch: 1516, Training Loss: 0.13126021629171586\n",
      "Epoch: 3 - Batch: 1517, Training Loss: 0.13134690566269516\n",
      "Epoch: 3 - Batch: 1518, Training Loss: 0.13141947175386928\n",
      "Epoch: 3 - Batch: 1519, Training Loss: 0.13150408647793838\n",
      "Epoch: 3 - Batch: 1520, Training Loss: 0.1315848656665923\n",
      "Epoch: 3 - Batch: 1521, Training Loss: 0.1316636213368642\n",
      "Epoch: 3 - Batch: 1522, Training Loss: 0.13174644080461753\n",
      "Epoch: 3 - Batch: 1523, Training Loss: 0.13183211187398058\n",
      "Epoch: 3 - Batch: 1524, Training Loss: 0.13191251193557807\n",
      "Epoch: 3 - Batch: 1525, Training Loss: 0.13199719171940194\n",
      "Epoch: 3 - Batch: 1526, Training Loss: 0.13208777312294365\n",
      "Epoch: 3 - Batch: 1527, Training Loss: 0.13217468201422178\n",
      "Epoch: 3 - Batch: 1528, Training Loss: 0.13225908367392633\n",
      "Epoch: 3 - Batch: 1529, Training Loss: 0.13234621662628\n",
      "Epoch: 3 - Batch: 1530, Training Loss: 0.1324416132733399\n",
      "Epoch: 3 - Batch: 1531, Training Loss: 0.13253196711292117\n",
      "Epoch: 3 - Batch: 1532, Training Loss: 0.13261923051839247\n",
      "Epoch: 3 - Batch: 1533, Training Loss: 0.13271437920815315\n",
      "Epoch: 3 - Batch: 1534, Training Loss: 0.1328163469028018\n",
      "Epoch: 3 - Batch: 1535, Training Loss: 0.13290280214886166\n",
      "Epoch: 3 - Batch: 1536, Training Loss: 0.13299054322551138\n",
      "Epoch: 3 - Batch: 1537, Training Loss: 0.13307806901374267\n",
      "Epoch: 3 - Batch: 1538, Training Loss: 0.13316926042584834\n",
      "Epoch: 3 - Batch: 1539, Training Loss: 0.13325794720728798\n",
      "Epoch: 3 - Batch: 1540, Training Loss: 0.13334354764440562\n",
      "Epoch: 3 - Batch: 1541, Training Loss: 0.13343653320080012\n",
      "Epoch: 3 - Batch: 1542, Training Loss: 0.13352402072912622\n",
      "Epoch: 3 - Batch: 1543, Training Loss: 0.13360996901445327\n",
      "Epoch: 3 - Batch: 1544, Training Loss: 0.13369400858582547\n",
      "Epoch: 3 - Batch: 1545, Training Loss: 0.13379168466573726\n",
      "Epoch: 3 - Batch: 1546, Training Loss: 0.13387931284076143\n",
      "Epoch: 3 - Batch: 1547, Training Loss: 0.13396203181216768\n",
      "Epoch: 3 - Batch: 1548, Training Loss: 0.13404333740967028\n",
      "Epoch: 3 - Batch: 1549, Training Loss: 0.13413084227326103\n",
      "Epoch: 3 - Batch: 1550, Training Loss: 0.13421827866070307\n",
      "Epoch: 3 - Batch: 1551, Training Loss: 0.1343016179022109\n",
      "Epoch: 3 - Batch: 1552, Training Loss: 0.13438590321918428\n",
      "Epoch: 3 - Batch: 1553, Training Loss: 0.13447227856610744\n",
      "Epoch: 3 - Batch: 1554, Training Loss: 0.13455787902793678\n",
      "Epoch: 3 - Batch: 1555, Training Loss: 0.13464507158493522\n",
      "Epoch: 3 - Batch: 1556, Training Loss: 0.13472872996582322\n",
      "Epoch: 3 - Batch: 1557, Training Loss: 0.13481434123902575\n",
      "Epoch: 3 - Batch: 1558, Training Loss: 0.134896734831345\n",
      "Epoch: 3 - Batch: 1559, Training Loss: 0.13497197314800313\n",
      "Epoch: 3 - Batch: 1560, Training Loss: 0.13505105531556688\n",
      "Epoch: 3 - Batch: 1561, Training Loss: 0.1351332065910348\n",
      "Epoch: 3 - Batch: 1562, Training Loss: 0.1352162016972677\n",
      "Epoch: 3 - Batch: 1563, Training Loss: 0.13530565553636692\n",
      "Epoch: 3 - Batch: 1564, Training Loss: 0.13538603403081942\n",
      "Epoch: 3 - Batch: 1565, Training Loss: 0.13546069332739805\n",
      "Epoch: 3 - Batch: 1566, Training Loss: 0.1355422253521224\n",
      "Epoch: 3 - Batch: 1567, Training Loss: 0.13564065444726453\n",
      "Epoch: 3 - Batch: 1568, Training Loss: 0.1357341707617687\n",
      "Epoch: 3 - Batch: 1569, Training Loss: 0.13582435750694416\n",
      "Epoch: 3 - Batch: 1570, Training Loss: 0.13590552734448938\n",
      "Epoch: 3 - Batch: 1571, Training Loss: 0.13598840133863105\n",
      "Epoch: 3 - Batch: 1572, Training Loss: 0.1360852945577446\n",
      "Epoch: 3 - Batch: 1573, Training Loss: 0.13617467656608048\n",
      "Epoch: 3 - Batch: 1574, Training Loss: 0.1362641918570248\n",
      "Epoch: 3 - Batch: 1575, Training Loss: 0.13635353317135207\n",
      "Epoch: 3 - Batch: 1576, Training Loss: 0.13643863063250014\n",
      "Epoch: 3 - Batch: 1577, Training Loss: 0.136523215968168\n",
      "Epoch: 3 - Batch: 1578, Training Loss: 0.13661010983472638\n",
      "Epoch: 3 - Batch: 1579, Training Loss: 0.13670186249301405\n",
      "Epoch: 3 - Batch: 1580, Training Loss: 0.13678350184687335\n",
      "Epoch: 3 - Batch: 1581, Training Loss: 0.13686789370838484\n",
      "Epoch: 3 - Batch: 1582, Training Loss: 0.13695583760639526\n",
      "Epoch: 3 - Batch: 1583, Training Loss: 0.13704145248750746\n",
      "Epoch: 3 - Batch: 1584, Training Loss: 0.13712550414908387\n",
      "Epoch: 3 - Batch: 1585, Training Loss: 0.13721346612988816\n",
      "Epoch: 3 - Batch: 1586, Training Loss: 0.13729479223811014\n",
      "Epoch: 3 - Batch: 1587, Training Loss: 0.13737658851750653\n",
      "Epoch: 3 - Batch: 1588, Training Loss: 0.13746530562639236\n",
      "Epoch: 3 - Batch: 1589, Training Loss: 0.1375626829293731\n",
      "Epoch: 3 - Batch: 1590, Training Loss: 0.1376496461308121\n",
      "Epoch: 3 - Batch: 1591, Training Loss: 0.13773633041735708\n",
      "Epoch: 3 - Batch: 1592, Training Loss: 0.1378220600781848\n",
      "Epoch: 3 - Batch: 1593, Training Loss: 0.13790182878647872\n",
      "Epoch: 3 - Batch: 1594, Training Loss: 0.13798824443202312\n",
      "Epoch: 3 - Batch: 1595, Training Loss: 0.1380759835935153\n",
      "Epoch: 3 - Batch: 1596, Training Loss: 0.13815688329500148\n",
      "Epoch: 3 - Batch: 1597, Training Loss: 0.13824424979154942\n",
      "Epoch: 3 - Batch: 1598, Training Loss: 0.13832712578116166\n",
      "Epoch: 3 - Batch: 1599, Training Loss: 0.13840572927054481\n",
      "Epoch: 3 - Batch: 1600, Training Loss: 0.13849813344071357\n",
      "Epoch: 3 - Batch: 1601, Training Loss: 0.13858529637454953\n",
      "Epoch: 3 - Batch: 1602, Training Loss: 0.13867039348355573\n",
      "Epoch: 3 - Batch: 1603, Training Loss: 0.13874424957542078\n",
      "Epoch: 3 - Batch: 1604, Training Loss: 0.1388260136801706\n",
      "Epoch: 3 - Batch: 1605, Training Loss: 0.13890212347999734\n",
      "Epoch: 3 - Batch: 1606, Training Loss: 0.1389891436295723\n",
      "Epoch: 3 - Batch: 1607, Training Loss: 0.1390703256747023\n",
      "Epoch: 3 - Batch: 1608, Training Loss: 0.13915695988914464\n",
      "Epoch: 3 - Batch: 1609, Training Loss: 0.1392438641840447\n",
      "Epoch: 3 - Batch: 1610, Training Loss: 0.13932294721034036\n",
      "Epoch: 3 - Batch: 1611, Training Loss: 0.13940695254981617\n",
      "Epoch: 3 - Batch: 1612, Training Loss: 0.1394931664551372\n",
      "Epoch: 3 - Batch: 1613, Training Loss: 0.13957503025310353\n",
      "Epoch: 3 - Batch: 1614, Training Loss: 0.13966761509676281\n",
      "Epoch: 3 - Batch: 1615, Training Loss: 0.13974900695048953\n",
      "Epoch: 3 - Batch: 1616, Training Loss: 0.13983460368702857\n",
      "Epoch: 3 - Batch: 1617, Training Loss: 0.139925558058233\n",
      "Epoch: 3 - Batch: 1618, Training Loss: 0.14001045962347716\n",
      "Epoch: 3 - Batch: 1619, Training Loss: 0.14009778512947596\n",
      "Epoch: 3 - Batch: 1620, Training Loss: 0.14018113778944236\n",
      "Epoch: 3 - Batch: 1621, Training Loss: 0.14026940775846763\n",
      "Epoch: 3 - Batch: 1622, Training Loss: 0.14034783893309621\n",
      "Epoch: 3 - Batch: 1623, Training Loss: 0.14043424179033062\n",
      "Epoch: 3 - Batch: 1624, Training Loss: 0.14052162410237304\n",
      "Epoch: 3 - Batch: 1625, Training Loss: 0.14061095290639705\n",
      "Epoch: 3 - Batch: 1626, Training Loss: 0.14070258958027335\n",
      "Epoch: 3 - Batch: 1627, Training Loss: 0.14078097019474306\n",
      "Epoch: 3 - Batch: 1628, Training Loss: 0.1408662987289144\n",
      "Epoch: 3 - Batch: 1629, Training Loss: 0.1409524179389623\n",
      "Epoch: 3 - Batch: 1630, Training Loss: 0.14103850929273498\n",
      "Epoch: 3 - Batch: 1631, Training Loss: 0.14113066045823777\n",
      "Epoch: 3 - Batch: 1632, Training Loss: 0.14121390836526507\n",
      "Epoch: 3 - Batch: 1633, Training Loss: 0.14129552163591433\n",
      "Epoch: 3 - Batch: 1634, Training Loss: 0.14138051612336638\n",
      "Epoch: 3 - Batch: 1635, Training Loss: 0.14147543737890314\n",
      "Epoch: 3 - Batch: 1636, Training Loss: 0.14155282470618513\n",
      "Epoch: 3 - Batch: 1637, Training Loss: 0.14164496929219508\n",
      "Epoch: 3 - Batch: 1638, Training Loss: 0.1417277373163459\n",
      "Epoch: 3 - Batch: 1639, Training Loss: 0.14181008009521126\n",
      "Epoch: 3 - Batch: 1640, Training Loss: 0.1418947575561344\n",
      "Epoch: 3 - Batch: 1641, Training Loss: 0.14198635626046813\n",
      "Epoch: 3 - Batch: 1642, Training Loss: 0.14207930518175238\n",
      "Epoch: 3 - Batch: 1643, Training Loss: 0.14216506914243374\n",
      "Epoch: 3 - Batch: 1644, Training Loss: 0.1422432212971435\n",
      "Epoch: 3 - Batch: 1645, Training Loss: 0.14232474204715015\n",
      "Epoch: 3 - Batch: 1646, Training Loss: 0.14240248041663003\n",
      "Epoch: 3 - Batch: 1647, Training Loss: 0.14250396800599682\n",
      "Epoch: 3 - Batch: 1648, Training Loss: 0.14259650152961215\n",
      "Epoch: 3 - Batch: 1649, Training Loss: 0.14269225856607431\n",
      "Epoch: 3 - Batch: 1650, Training Loss: 0.1427723330694249\n",
      "Epoch: 3 - Batch: 1651, Training Loss: 0.14285082990306724\n",
      "Epoch: 3 - Batch: 1652, Training Loss: 0.14293717969190423\n",
      "Epoch: 3 - Batch: 1653, Training Loss: 0.14301549295460803\n",
      "Epoch: 3 - Batch: 1654, Training Loss: 0.14309921186682992\n",
      "Epoch: 3 - Batch: 1655, Training Loss: 0.14319115326698148\n",
      "Epoch: 3 - Batch: 1656, Training Loss: 0.1432722286007693\n",
      "Epoch: 3 - Batch: 1657, Training Loss: 0.14336163792172277\n",
      "Epoch: 3 - Batch: 1658, Training Loss: 0.1434475076299896\n",
      "Epoch: 3 - Batch: 1659, Training Loss: 0.14353012857621977\n",
      "Epoch: 3 - Batch: 1660, Training Loss: 0.14361054310652352\n",
      "Epoch: 3 - Batch: 1661, Training Loss: 0.14368372484670944\n",
      "Epoch: 3 - Batch: 1662, Training Loss: 0.14377481763438008\n",
      "Epoch: 3 - Batch: 1663, Training Loss: 0.14385460181838244\n",
      "Epoch: 3 - Batch: 1664, Training Loss: 0.14394430284871787\n",
      "Epoch: 3 - Batch: 1665, Training Loss: 0.14403627752318707\n",
      "Epoch: 3 - Batch: 1666, Training Loss: 0.14412355744225863\n",
      "Epoch: 3 - Batch: 1667, Training Loss: 0.1442205920370657\n",
      "Epoch: 3 - Batch: 1668, Training Loss: 0.14429969705060544\n",
      "Epoch: 3 - Batch: 1669, Training Loss: 0.14438575233763723\n",
      "Epoch: 3 - Batch: 1670, Training Loss: 0.14446348666675252\n",
      "Epoch: 3 - Batch: 1671, Training Loss: 0.14455593046214846\n",
      "Epoch: 3 - Batch: 1672, Training Loss: 0.14463804647400605\n",
      "Epoch: 3 - Batch: 1673, Training Loss: 0.1447304334757142\n",
      "Epoch: 3 - Batch: 1674, Training Loss: 0.144807234258201\n",
      "Epoch: 3 - Batch: 1675, Training Loss: 0.14490104833645606\n",
      "Epoch: 3 - Batch: 1676, Training Loss: 0.14497994889148433\n",
      "Epoch: 3 - Batch: 1677, Training Loss: 0.14506264049467163\n",
      "Epoch: 3 - Batch: 1678, Training Loss: 0.14514383230710504\n",
      "Epoch: 3 - Batch: 1679, Training Loss: 0.14522216595079177\n",
      "Epoch: 3 - Batch: 1680, Training Loss: 0.1453118031816696\n",
      "Epoch: 3 - Batch: 1681, Training Loss: 0.14539111424762613\n",
      "Epoch: 3 - Batch: 1682, Training Loss: 0.14547567172701878\n",
      "Epoch: 3 - Batch: 1683, Training Loss: 0.14556539024088908\n",
      "Epoch: 3 - Batch: 1684, Training Loss: 0.1456373940974128\n",
      "Epoch: 3 - Batch: 1685, Training Loss: 0.14572436473089861\n",
      "Epoch: 3 - Batch: 1686, Training Loss: 0.14580473370516478\n",
      "Epoch: 3 - Batch: 1687, Training Loss: 0.14589874632965472\n",
      "Epoch: 3 - Batch: 1688, Training Loss: 0.14597850923687467\n",
      "Epoch: 3 - Batch: 1689, Training Loss: 0.14605431940721636\n",
      "Epoch: 3 - Batch: 1690, Training Loss: 0.14613993399796596\n",
      "Epoch: 3 - Batch: 1691, Training Loss: 0.14622494654967813\n",
      "Epoch: 3 - Batch: 1692, Training Loss: 0.14631466748529603\n",
      "Epoch: 3 - Batch: 1693, Training Loss: 0.14639339170373888\n",
      "Epoch: 3 - Batch: 1694, Training Loss: 0.14647851648986043\n",
      "Epoch: 3 - Batch: 1695, Training Loss: 0.14656189152628035\n",
      "Epoch: 3 - Batch: 1696, Training Loss: 0.14664560643520522\n",
      "Epoch: 3 - Batch: 1697, Training Loss: 0.14673460690091497\n",
      "Epoch: 3 - Batch: 1698, Training Loss: 0.1468230115425824\n",
      "Epoch: 3 - Batch: 1699, Training Loss: 0.14690806543080764\n",
      "Epoch: 3 - Batch: 1700, Training Loss: 0.14698967973317079\n",
      "Epoch: 3 - Batch: 1701, Training Loss: 0.14706907351539897\n",
      "Epoch: 3 - Batch: 1702, Training Loss: 0.14714989816124363\n",
      "Epoch: 3 - Batch: 1703, Training Loss: 0.14723968725793596\n",
      "Epoch: 3 - Batch: 1704, Training Loss: 0.14732593184284506\n",
      "Epoch: 3 - Batch: 1705, Training Loss: 0.14741389409547817\n",
      "Epoch: 3 - Batch: 1706, Training Loss: 0.14749485864666959\n",
      "Epoch: 3 - Batch: 1707, Training Loss: 0.14758782046128863\n",
      "Epoch: 3 - Batch: 1708, Training Loss: 0.1476788840582517\n",
      "Epoch: 3 - Batch: 1709, Training Loss: 0.14776399778588298\n",
      "Epoch: 3 - Batch: 1710, Training Loss: 0.14784419627406112\n",
      "Epoch: 3 - Batch: 1711, Training Loss: 0.14793293888543177\n",
      "Epoch: 3 - Batch: 1712, Training Loss: 0.14802161224223487\n",
      "Epoch: 3 - Batch: 1713, Training Loss: 0.1481154573707537\n",
      "Epoch: 3 - Batch: 1714, Training Loss: 0.1481918550928928\n",
      "Epoch: 3 - Batch: 1715, Training Loss: 0.14827925823690682\n",
      "Epoch: 3 - Batch: 1716, Training Loss: 0.1483590473879629\n",
      "Epoch: 3 - Batch: 1717, Training Loss: 0.1484490719316512\n",
      "Epoch: 3 - Batch: 1718, Training Loss: 0.1485324988379506\n",
      "Epoch: 3 - Batch: 1719, Training Loss: 0.14861028811355334\n",
      "Epoch: 3 - Batch: 1720, Training Loss: 0.1486994610113983\n",
      "Epoch: 3 - Batch: 1721, Training Loss: 0.14877933393515164\n",
      "Epoch: 3 - Batch: 1722, Training Loss: 0.1488653267942258\n",
      "Epoch: 3 - Batch: 1723, Training Loss: 0.14894531181029616\n",
      "Epoch: 3 - Batch: 1724, Training Loss: 0.14904470060323405\n",
      "Epoch: 3 - Batch: 1725, Training Loss: 0.14912684954962327\n",
      "Epoch: 3 - Batch: 1726, Training Loss: 0.14921093951435033\n",
      "Epoch: 3 - Batch: 1727, Training Loss: 0.1492862588174604\n",
      "Epoch: 3 - Batch: 1728, Training Loss: 0.14937575789752291\n",
      "Epoch: 3 - Batch: 1729, Training Loss: 0.1494652619137495\n",
      "Epoch: 3 - Batch: 1730, Training Loss: 0.14955753909044006\n",
      "Epoch: 3 - Batch: 1731, Training Loss: 0.1496406863590279\n",
      "Epoch: 3 - Batch: 1732, Training Loss: 0.14972802110102837\n",
      "Epoch: 3 - Batch: 1733, Training Loss: 0.14982007587232796\n",
      "Epoch: 3 - Batch: 1734, Training Loss: 0.14991174460964812\n",
      "Epoch: 3 - Batch: 1735, Training Loss: 0.15000478392019004\n",
      "Epoch: 3 - Batch: 1736, Training Loss: 0.15009248511213963\n",
      "Epoch: 3 - Batch: 1737, Training Loss: 0.1501843916131786\n",
      "Epoch: 3 - Batch: 1738, Training Loss: 0.15026717830084846\n",
      "Epoch: 3 - Batch: 1739, Training Loss: 0.15035398244536535\n",
      "Epoch: 3 - Batch: 1740, Training Loss: 0.15044241334940267\n",
      "Epoch: 3 - Batch: 1741, Training Loss: 0.15053390014057927\n",
      "Epoch: 3 - Batch: 1742, Training Loss: 0.15061285572240798\n",
      "Epoch: 3 - Batch: 1743, Training Loss: 0.15070203631177273\n",
      "Epoch: 3 - Batch: 1744, Training Loss: 0.15079166947139633\n",
      "Epoch: 3 - Batch: 1745, Training Loss: 0.150875602621789\n",
      "Epoch: 3 - Batch: 1746, Training Loss: 0.15095959548802915\n",
      "Epoch: 3 - Batch: 1747, Training Loss: 0.1510446917568372\n",
      "Epoch: 3 - Batch: 1748, Training Loss: 0.15112196977186954\n",
      "Epoch: 3 - Batch: 1749, Training Loss: 0.15121445116613239\n",
      "Epoch: 3 - Batch: 1750, Training Loss: 0.1512970452692675\n",
      "Epoch: 3 - Batch: 1751, Training Loss: 0.1513828307723821\n",
      "Epoch: 3 - Batch: 1752, Training Loss: 0.15146064437666343\n",
      "Epoch: 3 - Batch: 1753, Training Loss: 0.15155155455715225\n",
      "Epoch: 3 - Batch: 1754, Training Loss: 0.1516388262101568\n",
      "Epoch: 3 - Batch: 1755, Training Loss: 0.15172510101130945\n",
      "Epoch: 3 - Batch: 1756, Training Loss: 0.1518120205904022\n",
      "Epoch: 3 - Batch: 1757, Training Loss: 0.1518952398194899\n",
      "Epoch: 3 - Batch: 1758, Training Loss: 0.15197854871501773\n",
      "Epoch: 3 - Batch: 1759, Training Loss: 0.15206336130574963\n",
      "Epoch: 3 - Batch: 1760, Training Loss: 0.15214515393399086\n",
      "Epoch: 3 - Batch: 1761, Training Loss: 0.15223600250200844\n",
      "Epoch: 3 - Batch: 1762, Training Loss: 0.15231689766271791\n",
      "Epoch: 3 - Batch: 1763, Training Loss: 0.15240037172513815\n",
      "Epoch: 3 - Batch: 1764, Training Loss: 0.15249385475296878\n",
      "Epoch: 3 - Batch: 1765, Training Loss: 0.1525704059718952\n",
      "Epoch: 3 - Batch: 1766, Training Loss: 0.15265839880526955\n",
      "Epoch: 3 - Batch: 1767, Training Loss: 0.15273958540971005\n",
      "Epoch: 3 - Batch: 1768, Training Loss: 0.15282809749171508\n",
      "Epoch: 3 - Batch: 1769, Training Loss: 0.15291719670541845\n",
      "Epoch: 3 - Batch: 1770, Training Loss: 0.15299706883221914\n",
      "Epoch: 3 - Batch: 1771, Training Loss: 0.1530871947495202\n",
      "Epoch: 3 - Batch: 1772, Training Loss: 0.15316887516806374\n",
      "Epoch: 3 - Batch: 1773, Training Loss: 0.15325039344072144\n",
      "Epoch: 3 - Batch: 1774, Training Loss: 0.1533394611222827\n",
      "Epoch: 3 - Batch: 1775, Training Loss: 0.15342547271901102\n",
      "Epoch: 3 - Batch: 1776, Training Loss: 0.15350912281529822\n",
      "Epoch: 3 - Batch: 1777, Training Loss: 0.15359517618099452\n",
      "Epoch: 3 - Batch: 1778, Training Loss: 0.15369100183569773\n",
      "Epoch: 3 - Batch: 1779, Training Loss: 0.15378101371463457\n",
      "Epoch: 3 - Batch: 1780, Training Loss: 0.15386809940584265\n",
      "Epoch: 3 - Batch: 1781, Training Loss: 0.1539569175361994\n",
      "Epoch: 3 - Batch: 1782, Training Loss: 0.15403991376063716\n",
      "Epoch: 3 - Batch: 1783, Training Loss: 0.15413375110496733\n",
      "Epoch: 3 - Batch: 1784, Training Loss: 0.1542233717357539\n",
      "Epoch: 3 - Batch: 1785, Training Loss: 0.15430620346051543\n",
      "Epoch: 3 - Batch: 1786, Training Loss: 0.1543820023128939\n",
      "Epoch: 3 - Batch: 1787, Training Loss: 0.15446145789910906\n",
      "Epoch: 3 - Batch: 1788, Training Loss: 0.15454947178068248\n",
      "Epoch: 3 - Batch: 1789, Training Loss: 0.1546395796893248\n",
      "Epoch: 3 - Batch: 1790, Training Loss: 0.15472310696841274\n",
      "Epoch: 3 - Batch: 1791, Training Loss: 0.15480923886174586\n",
      "Epoch: 3 - Batch: 1792, Training Loss: 0.1548942420909654\n",
      "Epoch: 3 - Batch: 1793, Training Loss: 0.15497616128640784\n",
      "Epoch: 3 - Batch: 1794, Training Loss: 0.15507435780111237\n",
      "Epoch: 3 - Batch: 1795, Training Loss: 0.15515226739185367\n",
      "Epoch: 3 - Batch: 1796, Training Loss: 0.15525193553831843\n",
      "Epoch: 3 - Batch: 1797, Training Loss: 0.155340592912823\n",
      "Epoch: 3 - Batch: 1798, Training Loss: 0.15542586337447561\n",
      "Epoch: 3 - Batch: 1799, Training Loss: 0.15551419309641593\n",
      "Epoch: 3 - Batch: 1800, Training Loss: 0.1556014869281803\n",
      "Epoch: 3 - Batch: 1801, Training Loss: 0.15569117971716037\n",
      "Epoch: 3 - Batch: 1802, Training Loss: 0.15577497476616112\n",
      "Epoch: 3 - Batch: 1803, Training Loss: 0.15585983626754524\n",
      "Epoch: 3 - Batch: 1804, Training Loss: 0.15594458196367791\n",
      "Epoch: 3 - Batch: 1805, Training Loss: 0.1560314450209117\n",
      "Epoch: 3 - Batch: 1806, Training Loss: 0.1561158520121677\n",
      "Epoch: 3 - Batch: 1807, Training Loss: 0.15620963389724246\n",
      "Epoch: 3 - Batch: 1808, Training Loss: 0.15629650462746225\n",
      "Epoch: 3 - Batch: 1809, Training Loss: 0.15638076101320103\n",
      "Epoch: 3 - Batch: 1810, Training Loss: 0.1564674597560964\n",
      "Epoch: 3 - Batch: 1811, Training Loss: 0.15655771870271087\n",
      "Epoch: 3 - Batch: 1812, Training Loss: 0.15664268049039257\n",
      "Epoch: 3 - Batch: 1813, Training Loss: 0.15672832712605225\n",
      "Epoch: 3 - Batch: 1814, Training Loss: 0.15681348285741276\n",
      "Epoch: 3 - Batch: 1815, Training Loss: 0.15690534781188908\n",
      "Epoch: 3 - Batch: 1816, Training Loss: 0.1569901072600884\n",
      "Epoch: 3 - Batch: 1817, Training Loss: 0.1570787150752584\n",
      "Epoch: 3 - Batch: 1818, Training Loss: 0.15717698013406883\n",
      "Epoch: 3 - Batch: 1819, Training Loss: 0.15727389665014707\n",
      "Epoch: 3 - Batch: 1820, Training Loss: 0.1573551223794026\n",
      "Epoch: 3 - Batch: 1821, Training Loss: 0.1574408390851163\n",
      "Epoch: 3 - Batch: 1822, Training Loss: 0.15752694766912886\n",
      "Epoch: 3 - Batch: 1823, Training Loss: 0.15760780397685212\n",
      "Epoch: 3 - Batch: 1824, Training Loss: 0.15769341338432052\n",
      "Epoch: 3 - Batch: 1825, Training Loss: 0.15778180988965737\n",
      "Epoch: 3 - Batch: 1826, Training Loss: 0.15787671831981656\n",
      "Epoch: 3 - Batch: 1827, Training Loss: 0.15796092718493682\n",
      "Epoch: 3 - Batch: 1828, Training Loss: 0.15804106444367524\n",
      "Epoch: 3 - Batch: 1829, Training Loss: 0.1581266558115953\n",
      "Epoch: 3 - Batch: 1830, Training Loss: 0.15821916332613573\n",
      "Epoch: 3 - Batch: 1831, Training Loss: 0.15830821332029046\n",
      "Epoch: 3 - Batch: 1832, Training Loss: 0.15840018285472396\n",
      "Epoch: 3 - Batch: 1833, Training Loss: 0.15849143607081664\n",
      "Epoch: 3 - Batch: 1834, Training Loss: 0.1585795206155251\n",
      "Epoch: 3 - Batch: 1835, Training Loss: 0.15866505018563612\n",
      "Epoch: 3 - Batch: 1836, Training Loss: 0.1587492201668805\n",
      "Epoch: 3 - Batch: 1837, Training Loss: 0.15883596783278395\n",
      "Epoch: 3 - Batch: 1838, Training Loss: 0.15892020380047225\n",
      "Epoch: 3 - Batch: 1839, Training Loss: 0.15900983493373563\n",
      "Epoch: 3 - Batch: 1840, Training Loss: 0.15909005792357436\n",
      "Epoch: 3 - Batch: 1841, Training Loss: 0.1591755838919536\n",
      "Epoch: 3 - Batch: 1842, Training Loss: 0.15925583057414439\n",
      "Epoch: 3 - Batch: 1843, Training Loss: 0.15934655051153296\n",
      "Epoch: 3 - Batch: 1844, Training Loss: 0.15942647655310718\n",
      "Epoch: 3 - Batch: 1845, Training Loss: 0.15950975256352679\n",
      "Epoch: 3 - Batch: 1846, Training Loss: 0.1595979897771803\n",
      "Epoch: 3 - Batch: 1847, Training Loss: 0.15968118535716141\n",
      "Epoch: 3 - Batch: 1848, Training Loss: 0.15976952524797042\n",
      "Epoch: 3 - Batch: 1849, Training Loss: 0.1598594768488684\n",
      "Epoch: 3 - Batch: 1850, Training Loss: 0.1599421630649029\n",
      "Epoch: 3 - Batch: 1851, Training Loss: 0.1600348260655233\n",
      "Epoch: 3 - Batch: 1852, Training Loss: 0.1601181624713624\n",
      "Epoch: 3 - Batch: 1853, Training Loss: 0.16020493064180732\n",
      "Epoch: 3 - Batch: 1854, Training Loss: 0.16028395613924187\n",
      "Epoch: 3 - Batch: 1855, Training Loss: 0.1603731382483768\n",
      "Epoch: 3 - Batch: 1856, Training Loss: 0.16045921098805382\n",
      "Epoch: 3 - Batch: 1857, Training Loss: 0.1605512425983031\n",
      "Epoch: 3 - Batch: 1858, Training Loss: 0.16064111436184367\n",
      "Epoch: 3 - Batch: 1859, Training Loss: 0.16072350165267688\n",
      "Epoch: 3 - Batch: 1860, Training Loss: 0.16081318765829253\n",
      "Epoch: 3 - Batch: 1861, Training Loss: 0.16090140690035487\n",
      "Epoch: 3 - Batch: 1862, Training Loss: 0.16098396552365218\n",
      "Epoch: 3 - Batch: 1863, Training Loss: 0.16106551857804186\n",
      "Epoch: 3 - Batch: 1864, Training Loss: 0.16115024756288054\n",
      "Epoch: 3 - Batch: 1865, Training Loss: 0.16124269106173594\n",
      "Epoch: 3 - Batch: 1866, Training Loss: 0.16133970555356683\n",
      "Epoch: 3 - Batch: 1867, Training Loss: 0.16142991929010767\n",
      "Epoch: 3 - Batch: 1868, Training Loss: 0.1615190670684993\n",
      "Epoch: 3 - Batch: 1869, Training Loss: 0.16160928625397222\n",
      "Epoch: 3 - Batch: 1870, Training Loss: 0.16169177889082562\n",
      "Epoch: 3 - Batch: 1871, Training Loss: 0.1617746975946288\n",
      "Epoch: 3 - Batch: 1872, Training Loss: 0.16186275768710012\n",
      "Epoch: 3 - Batch: 1873, Training Loss: 0.16195001324660346\n",
      "Epoch: 3 - Batch: 1874, Training Loss: 0.16203538511893642\n",
      "Epoch: 3 - Batch: 1875, Training Loss: 0.1621294225539833\n",
      "Epoch: 3 - Batch: 1876, Training Loss: 0.16221992501596708\n",
      "Epoch: 3 - Batch: 1877, Training Loss: 0.16230719401864072\n",
      "Epoch: 3 - Batch: 1878, Training Loss: 0.16240255089838113\n",
      "Epoch: 3 - Batch: 1879, Training Loss: 0.16248873912205744\n",
      "Epoch: 3 - Batch: 1880, Training Loss: 0.1625839200976674\n",
      "Epoch: 3 - Batch: 1881, Training Loss: 0.16267106366987846\n",
      "Epoch: 3 - Batch: 1882, Training Loss: 0.16275013849188638\n",
      "Epoch: 3 - Batch: 1883, Training Loss: 0.1628388673573683\n",
      "Epoch: 3 - Batch: 1884, Training Loss: 0.16292343451139543\n",
      "Epoch: 3 - Batch: 1885, Training Loss: 0.1630185955199436\n",
      "Epoch: 3 - Batch: 1886, Training Loss: 0.163109399693374\n",
      "Epoch: 3 - Batch: 1887, Training Loss: 0.16318966757203412\n",
      "Epoch: 3 - Batch: 1888, Training Loss: 0.16327708286185366\n",
      "Epoch: 3 - Batch: 1889, Training Loss: 0.16335975202063027\n",
      "Epoch: 3 - Batch: 1890, Training Loss: 0.16344794369060206\n",
      "Epoch: 3 - Batch: 1891, Training Loss: 0.16352984388891736\n",
      "Epoch: 3 - Batch: 1892, Training Loss: 0.16361541021769715\n",
      "Epoch: 3 - Batch: 1893, Training Loss: 0.16371022317416434\n",
      "Epoch: 3 - Batch: 1894, Training Loss: 0.16379765887585643\n",
      "Epoch: 3 - Batch: 1895, Training Loss: 0.16388288486260877\n",
      "Epoch: 3 - Batch: 1896, Training Loss: 0.16397805193788179\n",
      "Epoch: 3 - Batch: 1897, Training Loss: 0.16406104372038968\n",
      "Epoch: 3 - Batch: 1898, Training Loss: 0.16414992807195158\n",
      "Epoch: 3 - Batch: 1899, Training Loss: 0.1642364336928325\n",
      "Epoch: 3 - Batch: 1900, Training Loss: 0.16432671467611445\n",
      "Epoch: 3 - Batch: 1901, Training Loss: 0.16441629109765166\n",
      "Epoch: 3 - Batch: 1902, Training Loss: 0.16449937077353446\n",
      "Epoch: 3 - Batch: 1903, Training Loss: 0.1645849250368218\n",
      "Epoch: 3 - Batch: 1904, Training Loss: 0.16466591396611524\n",
      "Epoch: 3 - Batch: 1905, Training Loss: 0.16474856090906445\n",
      "Epoch: 3 - Batch: 1906, Training Loss: 0.16484269175437552\n",
      "Epoch: 3 - Batch: 1907, Training Loss: 0.16492873144831824\n",
      "Epoch: 3 - Batch: 1908, Training Loss: 0.16501876569753066\n",
      "Epoch: 3 - Batch: 1909, Training Loss: 0.16510798196442686\n",
      "Epoch: 3 - Batch: 1910, Training Loss: 0.16518603372163637\n",
      "Epoch: 3 - Batch: 1911, Training Loss: 0.16526217647452854\n",
      "Epoch: 3 - Batch: 1912, Training Loss: 0.1653433126670804\n",
      "Epoch: 3 - Batch: 1913, Training Loss: 0.16541998509076697\n",
      "Epoch: 3 - Batch: 1914, Training Loss: 0.16551497642871357\n",
      "Epoch: 3 - Batch: 1915, Training Loss: 0.165600057067365\n",
      "Epoch: 3 - Batch: 1916, Training Loss: 0.16568653855726098\n",
      "Epoch: 3 - Batch: 1917, Training Loss: 0.16576877308648025\n",
      "Epoch: 3 - Batch: 1918, Training Loss: 0.1658603502298469\n",
      "Epoch: 3 - Batch: 1919, Training Loss: 0.1659374051290266\n",
      "Epoch: 3 - Batch: 1920, Training Loss: 0.16601995119259724\n",
      "Epoch: 3 - Batch: 1921, Training Loss: 0.16610813587533302\n",
      "Epoch: 3 - Batch: 1922, Training Loss: 0.16619751096439006\n",
      "Epoch: 3 - Batch: 1923, Training Loss: 0.1662764632719172\n",
      "Epoch: 3 - Batch: 1924, Training Loss: 0.1663581419544631\n",
      "Epoch: 3 - Batch: 1925, Training Loss: 0.16644119656649395\n",
      "Epoch: 3 - Batch: 1926, Training Loss: 0.16653100262777526\n",
      "Epoch: 3 - Batch: 1927, Training Loss: 0.1666237016086456\n",
      "Epoch: 3 - Batch: 1928, Training Loss: 0.1667232806261499\n",
      "Epoch: 3 - Batch: 1929, Training Loss: 0.16681345198210792\n",
      "Epoch: 3 - Batch: 1930, Training Loss: 0.16689868372668873\n",
      "Epoch: 3 - Batch: 1931, Training Loss: 0.16698608483198665\n",
      "Epoch: 3 - Batch: 1932, Training Loss: 0.16708551271267197\n",
      "Epoch: 3 - Batch: 1933, Training Loss: 0.16717007431274228\n",
      "Epoch: 3 - Batch: 1934, Training Loss: 0.1672621112730768\n",
      "Epoch: 3 - Batch: 1935, Training Loss: 0.16734793645701992\n",
      "Epoch: 3 - Batch: 1936, Training Loss: 0.1674409644248869\n",
      "Epoch: 3 - Batch: 1937, Training Loss: 0.16752334458639176\n",
      "Epoch: 3 - Batch: 1938, Training Loss: 0.1676090656351885\n",
      "Epoch: 3 - Batch: 1939, Training Loss: 0.16769432800375605\n",
      "Epoch: 3 - Batch: 1940, Training Loss: 0.1677838186571254\n",
      "Epoch: 3 - Batch: 1941, Training Loss: 0.1678701642573275\n",
      "Epoch: 3 - Batch: 1942, Training Loss: 0.16795895566814772\n",
      "Epoch: 3 - Batch: 1943, Training Loss: 0.16804526245860912\n",
      "Epoch: 3 - Batch: 1944, Training Loss: 0.1681317503920835\n",
      "Epoch: 3 - Batch: 1945, Training Loss: 0.1682193637586154\n",
      "Epoch: 3 - Batch: 1946, Training Loss: 0.16830384293895456\n",
      "Epoch: 3 - Batch: 1947, Training Loss: 0.16838718738227737\n",
      "Epoch: 3 - Batch: 1948, Training Loss: 0.16847482767839533\n",
      "Epoch: 3 - Batch: 1949, Training Loss: 0.1685570740568796\n",
      "Epoch: 3 - Batch: 1950, Training Loss: 0.16864343959943176\n",
      "Epoch: 3 - Batch: 1951, Training Loss: 0.16873500776626973\n",
      "Epoch: 3 - Batch: 1952, Training Loss: 0.16881583512422457\n",
      "Epoch: 3 - Batch: 1953, Training Loss: 0.16889196729432687\n",
      "Epoch: 3 - Batch: 1954, Training Loss: 0.1689776635634563\n",
      "Epoch: 3 - Batch: 1955, Training Loss: 0.16906762604986258\n",
      "Epoch: 3 - Batch: 1956, Training Loss: 0.16915941391615924\n",
      "Epoch: 3 - Batch: 1957, Training Loss: 0.16925073382915748\n",
      "Epoch: 3 - Batch: 1958, Training Loss: 0.16933589358852671\n",
      "Epoch: 3 - Batch: 1959, Training Loss: 0.1694205761771297\n",
      "Epoch: 3 - Batch: 1960, Training Loss: 0.1695051420646817\n",
      "Epoch: 3 - Batch: 1961, Training Loss: 0.16959720976020567\n",
      "Epoch: 3 - Batch: 1962, Training Loss: 0.1696777144017603\n",
      "Epoch: 3 - Batch: 1963, Training Loss: 0.16976919295158158\n",
      "Epoch: 3 - Batch: 1964, Training Loss: 0.16985439365825447\n",
      "Epoch: 3 - Batch: 1965, Training Loss: 0.1699383016397704\n",
      "Epoch: 3 - Batch: 1966, Training Loss: 0.17003899025630398\n",
      "Epoch: 3 - Batch: 1967, Training Loss: 0.17011950009967358\n",
      "Epoch: 3 - Batch: 1968, Training Loss: 0.17020340659230898\n",
      "Epoch: 3 - Batch: 1969, Training Loss: 0.17028624930760358\n",
      "Epoch: 3 - Batch: 1970, Training Loss: 0.17037806312119586\n",
      "Epoch: 3 - Batch: 1971, Training Loss: 0.17045768722274016\n",
      "Epoch: 3 - Batch: 1972, Training Loss: 0.17053604807401967\n",
      "Epoch: 3 - Batch: 1973, Training Loss: 0.17062588319627206\n",
      "Epoch: 3 - Batch: 1974, Training Loss: 0.17071725588730516\n",
      "Epoch: 3 - Batch: 1975, Training Loss: 0.1708070077990517\n",
      "Epoch: 3 - Batch: 1976, Training Loss: 0.17089898341256587\n",
      "Epoch: 3 - Batch: 1977, Training Loss: 0.17098349770206717\n",
      "Epoch: 3 - Batch: 1978, Training Loss: 0.17106962686475632\n",
      "Epoch: 3 - Batch: 1979, Training Loss: 0.17115623568173863\n",
      "Epoch: 3 - Batch: 1980, Training Loss: 0.17124604756880557\n",
      "Epoch: 3 - Batch: 1981, Training Loss: 0.17134259366252727\n",
      "Epoch: 3 - Batch: 1982, Training Loss: 0.17143083475591928\n",
      "Epoch: 3 - Batch: 1983, Training Loss: 0.1715157057280663\n",
      "Epoch: 3 - Batch: 1984, Training Loss: 0.17161198296455996\n",
      "Epoch: 3 - Batch: 1985, Training Loss: 0.1716948300661831\n",
      "Epoch: 3 - Batch: 1986, Training Loss: 0.17177459513567772\n",
      "Epoch: 3 - Batch: 1987, Training Loss: 0.17186097096447922\n",
      "Epoch: 3 - Batch: 1988, Training Loss: 0.17194768956323367\n",
      "Epoch: 3 - Batch: 1989, Training Loss: 0.17203357239341854\n",
      "Epoch: 3 - Batch: 1990, Training Loss: 0.17211947705022138\n",
      "Epoch: 3 - Batch: 1991, Training Loss: 0.17220479139987113\n",
      "Epoch: 3 - Batch: 1992, Training Loss: 0.1722885563129414\n",
      "Epoch: 3 - Batch: 1993, Training Loss: 0.1723787125391549\n",
      "Epoch: 3 - Batch: 1994, Training Loss: 0.17245873126199787\n",
      "Epoch: 3 - Batch: 1995, Training Loss: 0.17255609583844791\n",
      "Epoch: 3 - Batch: 1996, Training Loss: 0.17263881752814225\n",
      "Epoch: 3 - Batch: 1997, Training Loss: 0.1727309996759516\n",
      "Epoch: 3 - Batch: 1998, Training Loss: 0.17282250128179838\n",
      "Epoch: 3 - Batch: 1999, Training Loss: 0.1729161659345204\n",
      "Epoch: 3 - Batch: 2000, Training Loss: 0.17300327801783486\n",
      "Epoch: 3 - Batch: 2001, Training Loss: 0.1730939847097468\n",
      "Epoch: 3 - Batch: 2002, Training Loss: 0.1731762900133038\n",
      "Epoch: 3 - Batch: 2003, Training Loss: 0.1732760135139992\n",
      "Epoch: 3 - Batch: 2004, Training Loss: 0.17335101579675824\n",
      "Epoch: 3 - Batch: 2005, Training Loss: 0.17344512854074168\n",
      "Epoch: 3 - Batch: 2006, Training Loss: 0.17353451955649588\n",
      "Epoch: 3 - Batch: 2007, Training Loss: 0.17361806832241578\n",
      "Epoch: 3 - Batch: 2008, Training Loss: 0.17371549326957358\n",
      "Epoch: 3 - Batch: 2009, Training Loss: 0.17380569601558135\n",
      "Epoch: 3 - Batch: 2010, Training Loss: 0.17389133828962422\n",
      "Epoch: 3 - Batch: 2011, Training Loss: 0.1739808216283274\n",
      "Epoch: 3 - Batch: 2012, Training Loss: 0.17406950693942025\n",
      "Epoch: 3 - Batch: 2013, Training Loss: 0.17415522068590666\n",
      "Epoch: 3 - Batch: 2014, Training Loss: 0.1742453898239887\n",
      "Epoch: 3 - Batch: 2015, Training Loss: 0.17433139961058425\n",
      "Epoch: 3 - Batch: 2016, Training Loss: 0.17442252479530684\n",
      "Epoch: 3 - Batch: 2017, Training Loss: 0.17450173243040074\n",
      "Epoch: 3 - Batch: 2018, Training Loss: 0.17458731739798786\n",
      "Epoch: 3 - Batch: 2019, Training Loss: 0.1746770175325238\n",
      "Epoch: 3 - Batch: 2020, Training Loss: 0.1747689849788178\n",
      "Epoch: 3 - Batch: 2021, Training Loss: 0.1748611466097298\n",
      "Epoch: 3 - Batch: 2022, Training Loss: 0.17495032695815535\n",
      "Epoch: 3 - Batch: 2023, Training Loss: 0.1750312249483556\n",
      "Epoch: 3 - Batch: 2024, Training Loss: 0.17511753330801058\n",
      "Epoch: 3 - Batch: 2025, Training Loss: 0.1752024487612358\n",
      "Epoch: 3 - Batch: 2026, Training Loss: 0.1752911895748296\n",
      "Epoch: 3 - Batch: 2027, Training Loss: 0.17537042445458384\n",
      "Epoch: 3 - Batch: 2028, Training Loss: 0.17545362135664147\n",
      "Epoch: 3 - Batch: 2029, Training Loss: 0.17554553474606963\n",
      "Epoch: 3 - Batch: 2030, Training Loss: 0.1756368035428955\n",
      "Epoch: 3 - Batch: 2031, Training Loss: 0.17572884497604954\n",
      "Epoch: 3 - Batch: 2032, Training Loss: 0.17581470521973142\n",
      "Epoch: 3 - Batch: 2033, Training Loss: 0.17590181022314091\n",
      "Epoch: 3 - Batch: 2034, Training Loss: 0.17598890093700406\n",
      "Epoch: 3 - Batch: 2035, Training Loss: 0.17607574785749117\n",
      "Epoch: 3 - Batch: 2036, Training Loss: 0.17615914732143653\n",
      "Epoch: 3 - Batch: 2037, Training Loss: 0.17623567374216187\n",
      "Epoch: 3 - Batch: 2038, Training Loss: 0.17632299848827557\n",
      "Epoch: 3 - Batch: 2039, Training Loss: 0.17641444927449051\n",
      "Epoch: 3 - Batch: 2040, Training Loss: 0.17651167204897003\n",
      "Epoch: 3 - Batch: 2041, Training Loss: 0.17659370875477198\n",
      "Epoch: 3 - Batch: 2042, Training Loss: 0.17668574188479144\n",
      "Epoch: 3 - Batch: 2043, Training Loss: 0.17677035095605684\n",
      "Epoch: 3 - Batch: 2044, Training Loss: 0.17685680086438732\n",
      "Epoch: 3 - Batch: 2045, Training Loss: 0.17694654851336383\n",
      "Epoch: 3 - Batch: 2046, Training Loss: 0.17703614953540847\n",
      "Epoch: 3 - Batch: 2047, Training Loss: 0.17712806774362008\n",
      "Epoch: 3 - Batch: 2048, Training Loss: 0.17722033810605656\n",
      "Epoch: 3 - Batch: 2049, Training Loss: 0.17730690407911145\n",
      "Epoch: 3 - Batch: 2050, Training Loss: 0.1773955020095974\n",
      "Epoch: 3 - Batch: 2051, Training Loss: 0.1774794943754254\n",
      "Epoch: 3 - Batch: 2052, Training Loss: 0.1775649106299897\n",
      "Epoch: 3 - Batch: 2053, Training Loss: 0.1776566730741146\n",
      "Epoch: 3 - Batch: 2054, Training Loss: 0.177749533243292\n",
      "Epoch: 3 - Batch: 2055, Training Loss: 0.1778319653982349\n",
      "Epoch: 3 - Batch: 2056, Training Loss: 0.17791618351171265\n",
      "Epoch: 3 - Batch: 2057, Training Loss: 0.17799186770793415\n",
      "Epoch: 3 - Batch: 2058, Training Loss: 0.1780837294375323\n",
      "Epoch: 3 - Batch: 2059, Training Loss: 0.1781579808735136\n",
      "Epoch: 3 - Batch: 2060, Training Loss: 0.17825213179091118\n",
      "Epoch: 3 - Batch: 2061, Training Loss: 0.17833961194279183\n",
      "Epoch: 3 - Batch: 2062, Training Loss: 0.17842189148439103\n",
      "Epoch: 3 - Batch: 2063, Training Loss: 0.17851685668352626\n",
      "Epoch: 3 - Batch: 2064, Training Loss: 0.17860470515726812\n",
      "Epoch: 3 - Batch: 2065, Training Loss: 0.1786931089649153\n",
      "Epoch: 3 - Batch: 2066, Training Loss: 0.17877849858643405\n",
      "Epoch: 3 - Batch: 2067, Training Loss: 0.17885972088076188\n",
      "Epoch: 3 - Batch: 2068, Training Loss: 0.17894730939966924\n",
      "Epoch: 3 - Batch: 2069, Training Loss: 0.17903139064734058\n",
      "Epoch: 3 - Batch: 2070, Training Loss: 0.1791223903089317\n",
      "Epoch: 3 - Batch: 2071, Training Loss: 0.17921298584139367\n",
      "Epoch: 3 - Batch: 2072, Training Loss: 0.17930428512011395\n",
      "Epoch: 3 - Batch: 2073, Training Loss: 0.17939521627466676\n",
      "Epoch: 3 - Batch: 2074, Training Loss: 0.17947437888502482\n",
      "Epoch: 3 - Batch: 2075, Training Loss: 0.17956557034037599\n",
      "Epoch: 3 - Batch: 2076, Training Loss: 0.17965137796047118\n",
      "Epoch: 3 - Batch: 2077, Training Loss: 0.17973694835432727\n",
      "Epoch: 3 - Batch: 2078, Training Loss: 0.1798216148000056\n",
      "Epoch: 3 - Batch: 2079, Training Loss: 0.17991685020315706\n",
      "Epoch: 3 - Batch: 2080, Training Loss: 0.17999307966326206\n",
      "Epoch: 3 - Batch: 2081, Training Loss: 0.18006596885386786\n",
      "Epoch: 3 - Batch: 2082, Training Loss: 0.18015483923713563\n",
      "Epoch: 3 - Batch: 2083, Training Loss: 0.18023634999743346\n",
      "Epoch: 3 - Batch: 2084, Training Loss: 0.1803285647585222\n",
      "Epoch: 3 - Batch: 2085, Training Loss: 0.18041212950056268\n",
      "Epoch: 3 - Batch: 2086, Training Loss: 0.18049869894561285\n",
      "Epoch: 3 - Batch: 2087, Training Loss: 0.18059113795311493\n",
      "Epoch: 3 - Batch: 2088, Training Loss: 0.18067895700138797\n",
      "Epoch: 3 - Batch: 2089, Training Loss: 0.18076806244244228\n",
      "Epoch: 3 - Batch: 2090, Training Loss: 0.1808489091434882\n",
      "Epoch: 3 - Batch: 2091, Training Loss: 0.180937156563078\n",
      "Epoch: 3 - Batch: 2092, Training Loss: 0.18102795027727708\n",
      "Epoch: 3 - Batch: 2093, Training Loss: 0.1811151443540457\n",
      "Epoch: 3 - Batch: 2094, Training Loss: 0.18120001648764308\n",
      "Epoch: 3 - Batch: 2095, Training Loss: 0.18128297392705187\n",
      "Epoch: 3 - Batch: 2096, Training Loss: 0.18136289912842796\n",
      "Epoch: 3 - Batch: 2097, Training Loss: 0.1814512990007353\n",
      "Epoch: 3 - Batch: 2098, Training Loss: 0.18154515246327838\n",
      "Epoch: 3 - Batch: 2099, Training Loss: 0.1816417698008603\n",
      "Epoch: 3 - Batch: 2100, Training Loss: 0.18172694422045158\n",
      "Epoch: 3 - Batch: 2101, Training Loss: 0.18182225267759602\n",
      "Epoch: 3 - Batch: 2102, Training Loss: 0.18190830064996164\n",
      "Epoch: 3 - Batch: 2103, Training Loss: 0.18199965533851392\n",
      "Epoch: 3 - Batch: 2104, Training Loss: 0.18209008377657007\n",
      "Epoch: 3 - Batch: 2105, Training Loss: 0.1821791453419831\n",
      "Epoch: 3 - Batch: 2106, Training Loss: 0.18225980931846655\n",
      "Epoch: 3 - Batch: 2107, Training Loss: 0.1823446622263535\n",
      "Epoch: 3 - Batch: 2108, Training Loss: 0.1824222918519531\n",
      "Epoch: 3 - Batch: 2109, Training Loss: 0.18251189093412848\n",
      "Epoch: 3 - Batch: 2110, Training Loss: 0.18260844324902317\n",
      "Epoch: 3 - Batch: 2111, Training Loss: 0.1826931459591361\n",
      "Epoch: 3 - Batch: 2112, Training Loss: 0.18278109121011266\n",
      "Epoch: 3 - Batch: 2113, Training Loss: 0.18286373141912085\n",
      "Epoch: 3 - Batch: 2114, Training Loss: 0.18295369865017547\n",
      "Epoch: 3 - Batch: 2115, Training Loss: 0.18304156547853997\n",
      "Epoch: 3 - Batch: 2116, Training Loss: 0.18313120371642003\n",
      "Epoch: 3 - Batch: 2117, Training Loss: 0.18320962501254248\n",
      "Epoch: 3 - Batch: 2118, Training Loss: 0.18329671948258555\n",
      "Epoch: 3 - Batch: 2119, Training Loss: 0.18338089792141274\n",
      "Epoch: 3 - Batch: 2120, Training Loss: 0.1834723203632962\n",
      "Epoch: 3 - Batch: 2121, Training Loss: 0.18357195302374524\n",
      "Epoch: 3 - Batch: 2122, Training Loss: 0.18365445414289314\n",
      "Epoch: 3 - Batch: 2123, Training Loss: 0.18374129752342777\n",
      "Epoch: 3 - Batch: 2124, Training Loss: 0.18382207567418984\n",
      "Epoch: 3 - Batch: 2125, Training Loss: 0.18390099722181585\n",
      "Epoch: 3 - Batch: 2126, Training Loss: 0.18399070401615764\n",
      "Epoch: 3 - Batch: 2127, Training Loss: 0.18407413015639407\n",
      "Epoch: 3 - Batch: 2128, Training Loss: 0.1841564278116768\n",
      "Epoch: 3 - Batch: 2129, Training Loss: 0.1842462875159028\n",
      "Epoch: 3 - Batch: 2130, Training Loss: 0.18434333509671352\n",
      "Epoch: 3 - Batch: 2131, Training Loss: 0.18443022995791822\n",
      "Epoch: 3 - Batch: 2132, Training Loss: 0.18452093648920406\n",
      "Epoch: 3 - Batch: 2133, Training Loss: 0.1846086029179555\n",
      "Epoch: 3 - Batch: 2134, Training Loss: 0.1846928780351705\n",
      "Epoch: 3 - Batch: 2135, Training Loss: 0.1847795524493873\n",
      "Epoch: 3 - Batch: 2136, Training Loss: 0.18487736572871358\n",
      "Epoch: 3 - Batch: 2137, Training Loss: 0.1849639319735973\n",
      "Epoch: 3 - Batch: 2138, Training Loss: 0.18504948493334192\n",
      "Epoch: 3 - Batch: 2139, Training Loss: 0.18514329227147805\n",
      "Epoch: 3 - Batch: 2140, Training Loss: 0.18523145370667254\n",
      "Epoch: 3 - Batch: 2141, Training Loss: 0.18531457347730498\n",
      "Epoch: 3 - Batch: 2142, Training Loss: 0.18539629113021774\n",
      "Epoch: 3 - Batch: 2143, Training Loss: 0.18547751986194605\n",
      "Epoch: 3 - Batch: 2144, Training Loss: 0.18556557604378926\n",
      "Epoch: 3 - Batch: 2145, Training Loss: 0.1856470270388162\n",
      "Epoch: 3 - Batch: 2146, Training Loss: 0.18572907517378406\n",
      "Epoch: 3 - Batch: 2147, Training Loss: 0.185813909072188\n",
      "Epoch: 3 - Batch: 2148, Training Loss: 0.18590126884715077\n",
      "Epoch: 3 - Batch: 2149, Training Loss: 0.1859831092307718\n",
      "Epoch: 3 - Batch: 2150, Training Loss: 0.18607865222675685\n",
      "Epoch: 3 - Batch: 2151, Training Loss: 0.18616152367559238\n",
      "Epoch: 3 - Batch: 2152, Training Loss: 0.1862504979777791\n",
      "Epoch: 3 - Batch: 2153, Training Loss: 0.18634674327438744\n",
      "Epoch: 3 - Batch: 2154, Training Loss: 0.18643172229230898\n",
      "Epoch: 3 - Batch: 2155, Training Loss: 0.18652202285344327\n",
      "Epoch: 3 - Batch: 2156, Training Loss: 0.1866060366999253\n",
      "Epoch: 3 - Batch: 2157, Training Loss: 0.1866868202872636\n",
      "Epoch: 3 - Batch: 2158, Training Loss: 0.1867750659090765\n",
      "Epoch: 3 - Batch: 2159, Training Loss: 0.1868655840197507\n",
      "Epoch: 3 - Batch: 2160, Training Loss: 0.18695336392467493\n",
      "Epoch: 3 - Batch: 2161, Training Loss: 0.18704303715433648\n",
      "Epoch: 3 - Batch: 2162, Training Loss: 0.18713451811354948\n",
      "Epoch: 3 - Batch: 2163, Training Loss: 0.18721445119796107\n",
      "Epoch: 3 - Batch: 2164, Training Loss: 0.18729817879017116\n",
      "Epoch: 3 - Batch: 2165, Training Loss: 0.1873894733180652\n",
      "Epoch: 3 - Batch: 2166, Training Loss: 0.18747189938774947\n",
      "Epoch: 3 - Batch: 2167, Training Loss: 0.18755737197265696\n",
      "Epoch: 3 - Batch: 2168, Training Loss: 0.1876583071873457\n",
      "Epoch: 3 - Batch: 2169, Training Loss: 0.18774726191785798\n",
      "Epoch: 3 - Batch: 2170, Training Loss: 0.18782939004463145\n",
      "Epoch: 3 - Batch: 2171, Training Loss: 0.1879220701766449\n",
      "Epoch: 3 - Batch: 2172, Training Loss: 0.18800948434208162\n",
      "Epoch: 3 - Batch: 2173, Training Loss: 0.18808569044962056\n",
      "Epoch: 3 - Batch: 2174, Training Loss: 0.1881771710814034\n",
      "Epoch: 3 - Batch: 2175, Training Loss: 0.18825768000426182\n",
      "Epoch: 3 - Batch: 2176, Training Loss: 0.18834550058466087\n",
      "Epoch: 3 - Batch: 2177, Training Loss: 0.18842935899448632\n",
      "Epoch: 3 - Batch: 2178, Training Loss: 0.1885186313446086\n",
      "Epoch: 3 - Batch: 2179, Training Loss: 0.1886010029435059\n",
      "Epoch: 3 - Batch: 2180, Training Loss: 0.18869274956595838\n",
      "Epoch: 3 - Batch: 2181, Training Loss: 0.18877961034971485\n",
      "Epoch: 3 - Batch: 2182, Training Loss: 0.1888677500571382\n",
      "Epoch: 3 - Batch: 2183, Training Loss: 0.18895326225491701\n",
      "Epoch: 3 - Batch: 2184, Training Loss: 0.18904745955001656\n",
      "Epoch: 3 - Batch: 2185, Training Loss: 0.18913407807870092\n",
      "Epoch: 3 - Batch: 2186, Training Loss: 0.18922296041008055\n",
      "Epoch: 3 - Batch: 2187, Training Loss: 0.1893055703781335\n",
      "Epoch: 3 - Batch: 2188, Training Loss: 0.1893912640339996\n",
      "Epoch: 3 - Batch: 2189, Training Loss: 0.18948530031995195\n",
      "Epoch: 3 - Batch: 2190, Training Loss: 0.18956862238682126\n",
      "Epoch: 3 - Batch: 2191, Training Loss: 0.18965650049608146\n",
      "Epoch: 3 - Batch: 2192, Training Loss: 0.18975130188129039\n",
      "Epoch: 3 - Batch: 2193, Training Loss: 0.18983835752305897\n",
      "Epoch: 3 - Batch: 2194, Training Loss: 0.1899228163347711\n",
      "Epoch: 3 - Batch: 2195, Training Loss: 0.19001071843910178\n",
      "Epoch: 3 - Batch: 2196, Training Loss: 0.19010081270688012\n",
      "Epoch: 3 - Batch: 2197, Training Loss: 0.19017832703999618\n",
      "Epoch: 3 - Batch: 2198, Training Loss: 0.19025879717164768\n",
      "Epoch: 3 - Batch: 2199, Training Loss: 0.19033872210737288\n",
      "Epoch: 3 - Batch: 2200, Training Loss: 0.19042106203203574\n",
      "Epoch: 3 - Batch: 2201, Training Loss: 0.19050280802928\n",
      "Epoch: 3 - Batch: 2202, Training Loss: 0.1905950848661845\n",
      "Epoch: 3 - Batch: 2203, Training Loss: 0.19068823141937036\n",
      "Epoch: 3 - Batch: 2204, Training Loss: 0.1907782483271402\n",
      "Epoch: 3 - Batch: 2205, Training Loss: 0.19086379943350654\n",
      "Epoch: 3 - Batch: 2206, Training Loss: 0.1909597868943096\n",
      "Epoch: 3 - Batch: 2207, Training Loss: 0.19104180360833803\n",
      "Epoch: 3 - Batch: 2208, Training Loss: 0.19111954876118237\n",
      "Epoch: 3 - Batch: 2209, Training Loss: 0.19119445573087554\n",
      "Epoch: 3 - Batch: 2210, Training Loss: 0.1912812691361058\n",
      "Epoch: 3 - Batch: 2211, Training Loss: 0.19136741459616777\n",
      "Epoch: 3 - Batch: 2212, Training Loss: 0.1914578814300435\n",
      "Epoch: 3 - Batch: 2213, Training Loss: 0.19153878300961966\n",
      "Epoch: 3 - Batch: 2214, Training Loss: 0.19162327030775558\n",
      "Epoch: 3 - Batch: 2215, Training Loss: 0.1917146193682159\n",
      "Epoch: 3 - Batch: 2216, Training Loss: 0.19181603797484392\n",
      "Epoch: 3 - Batch: 2217, Training Loss: 0.1918990347800069\n",
      "Epoch: 3 - Batch: 2218, Training Loss: 0.1919863078106893\n",
      "Epoch: 3 - Batch: 2219, Training Loss: 0.19206841884931522\n",
      "Epoch: 3 - Batch: 2220, Training Loss: 0.19214650624698865\n",
      "Epoch: 3 - Batch: 2221, Training Loss: 0.19223691465002585\n",
      "Epoch: 3 - Batch: 2222, Training Loss: 0.19231514734637678\n",
      "Epoch: 3 - Batch: 2223, Training Loss: 0.19240479430131255\n",
      "Epoch: 3 - Batch: 2224, Training Loss: 0.19249009995244035\n",
      "Epoch: 3 - Batch: 2225, Training Loss: 0.19257579434347982\n",
      "Epoch: 3 - Batch: 2226, Training Loss: 0.19266388796974177\n",
      "Epoch: 3 - Batch: 2227, Training Loss: 0.19275121600869086\n",
      "Epoch: 3 - Batch: 2228, Training Loss: 0.1928346858948321\n",
      "Epoch: 3 - Batch: 2229, Training Loss: 0.19291568827347375\n",
      "Epoch: 3 - Batch: 2230, Training Loss: 0.19300919355419935\n",
      "Epoch: 3 - Batch: 2231, Training Loss: 0.19310073509416967\n",
      "Epoch: 3 - Batch: 2232, Training Loss: 0.1931900473784155\n",
      "Epoch: 3 - Batch: 2233, Training Loss: 0.1932725214135291\n",
      "Epoch: 3 - Batch: 2234, Training Loss: 0.1933625225552279\n",
      "Epoch: 3 - Batch: 2235, Training Loss: 0.19345191820442775\n",
      "Epoch: 3 - Batch: 2236, Training Loss: 0.19354190235682586\n",
      "Epoch: 3 - Batch: 2237, Training Loss: 0.19362562941155623\n",
      "Epoch: 3 - Batch: 2238, Training Loss: 0.1937051481794362\n",
      "Epoch: 3 - Batch: 2239, Training Loss: 0.19378984537265984\n",
      "Epoch: 3 - Batch: 2240, Training Loss: 0.19386868146101435\n",
      "Epoch: 3 - Batch: 2241, Training Loss: 0.1939538993423258\n",
      "Epoch: 3 - Batch: 2242, Training Loss: 0.19404110817198533\n",
      "Epoch: 3 - Batch: 2243, Training Loss: 0.19412076457791266\n",
      "Epoch: 3 - Batch: 2244, Training Loss: 0.19421008851372978\n",
      "Epoch: 3 - Batch: 2245, Training Loss: 0.19431311882426885\n",
      "Epoch: 3 - Batch: 2246, Training Loss: 0.19439833862073783\n",
      "Epoch: 3 - Batch: 2247, Training Loss: 0.19448676001076676\n",
      "Epoch: 3 - Batch: 2248, Training Loss: 0.19457364695186835\n",
      "Epoch: 3 - Batch: 2249, Training Loss: 0.1946559329802915\n",
      "Epoch: 3 - Batch: 2250, Training Loss: 0.19474894124056966\n",
      "Epoch: 3 - Batch: 2251, Training Loss: 0.19483501097777392\n",
      "Epoch: 3 - Batch: 2252, Training Loss: 0.19491952852304303\n",
      "Epoch: 3 - Batch: 2253, Training Loss: 0.19501059706234813\n",
      "Epoch: 3 - Batch: 2254, Training Loss: 0.19509191641809534\n",
      "Epoch: 3 - Batch: 2255, Training Loss: 0.19518928644347744\n",
      "Epoch: 3 - Batch: 2256, Training Loss: 0.19527777834837115\n",
      "Epoch: 3 - Batch: 2257, Training Loss: 0.19536479209143526\n",
      "Epoch: 3 - Batch: 2258, Training Loss: 0.19544704379529304\n",
      "Epoch: 3 - Batch: 2259, Training Loss: 0.1955325747430819\n",
      "Epoch: 3 - Batch: 2260, Training Loss: 0.1956183333104325\n",
      "Epoch: 3 - Batch: 2261, Training Loss: 0.19570645256273783\n",
      "Epoch: 3 - Batch: 2262, Training Loss: 0.1957974685093459\n",
      "Epoch: 3 - Batch: 2263, Training Loss: 0.19588470402147443\n",
      "Epoch: 3 - Batch: 2264, Training Loss: 0.19596743683761625\n",
      "Epoch: 3 - Batch: 2265, Training Loss: 0.19604292134468632\n",
      "Epoch: 3 - Batch: 2266, Training Loss: 0.19613593792663284\n",
      "Epoch: 3 - Batch: 2267, Training Loss: 0.19622209190605688\n",
      "Epoch: 3 - Batch: 2268, Training Loss: 0.19630347707526602\n",
      "Epoch: 3 - Batch: 2269, Training Loss: 0.19638882841216787\n",
      "Epoch: 3 - Batch: 2270, Training Loss: 0.19646850755904643\n",
      "Epoch: 3 - Batch: 2271, Training Loss: 0.19655107018564075\n",
      "Epoch: 3 - Batch: 2272, Training Loss: 0.19664038966717215\n",
      "Epoch: 3 - Batch: 2273, Training Loss: 0.1967423797135626\n",
      "Epoch: 3 - Batch: 2274, Training Loss: 0.19683195870511766\n",
      "Epoch: 3 - Batch: 2275, Training Loss: 0.19691387876546995\n",
      "Epoch: 3 - Batch: 2276, Training Loss: 0.19701148981033867\n",
      "Epoch: 3 - Batch: 2277, Training Loss: 0.19710188708075047\n",
      "Epoch: 3 - Batch: 2278, Training Loss: 0.19718515780286408\n",
      "Epoch: 3 - Batch: 2279, Training Loss: 0.19728180183527086\n",
      "Epoch: 3 - Batch: 2280, Training Loss: 0.19736620831993681\n",
      "Epoch: 3 - Batch: 2281, Training Loss: 0.1974476264422114\n",
      "Epoch: 3 - Batch: 2282, Training Loss: 0.19753458894554457\n",
      "Epoch: 3 - Batch: 2283, Training Loss: 0.19761926110580588\n",
      "Epoch: 3 - Batch: 2284, Training Loss: 0.1977009888905198\n",
      "Epoch: 3 - Batch: 2285, Training Loss: 0.19778776485133132\n",
      "Epoch: 3 - Batch: 2286, Training Loss: 0.19788334550178466\n",
      "Epoch: 3 - Batch: 2287, Training Loss: 0.1979685504588717\n",
      "Epoch: 3 - Batch: 2288, Training Loss: 0.1980616460997568\n",
      "Epoch: 3 - Batch: 2289, Training Loss: 0.19815095022296036\n",
      "Epoch: 3 - Batch: 2290, Training Loss: 0.1982315912731548\n",
      "Epoch: 3 - Batch: 2291, Training Loss: 0.19832086275836722\n",
      "Epoch: 3 - Batch: 2292, Training Loss: 0.19840394615954032\n",
      "Epoch: 3 - Batch: 2293, Training Loss: 0.19848763716606357\n",
      "Epoch: 3 - Batch: 2294, Training Loss: 0.1985799929136364\n",
      "Epoch: 3 - Batch: 2295, Training Loss: 0.198660694303649\n",
      "Epoch: 3 - Batch: 2296, Training Loss: 0.19874337978451012\n",
      "Epoch: 3 - Batch: 2297, Training Loss: 0.198826249497348\n",
      "Epoch: 3 - Batch: 2298, Training Loss: 0.19892162470082145\n",
      "Epoch: 3 - Batch: 2299, Training Loss: 0.19901028685086403\n",
      "Epoch: 3 - Batch: 2300, Training Loss: 0.19910853999863018\n",
      "Epoch: 3 - Batch: 2301, Training Loss: 0.19920362593300306\n",
      "Epoch: 3 - Batch: 2302, Training Loss: 0.19928997749490523\n",
      "Epoch: 3 - Batch: 2303, Training Loss: 0.1993836171393766\n",
      "Epoch: 3 - Batch: 2304, Training Loss: 0.19946426460225983\n",
      "Epoch: 3 - Batch: 2305, Training Loss: 0.19956385145337624\n",
      "Epoch: 3 - Batch: 2306, Training Loss: 0.19965115221059737\n",
      "Epoch: 3 - Batch: 2307, Training Loss: 0.19973742592443477\n",
      "Epoch: 3 - Batch: 2308, Training Loss: 0.19982245544691388\n",
      "Epoch: 3 - Batch: 2309, Training Loss: 0.19990375954318007\n",
      "Epoch: 3 - Batch: 2310, Training Loss: 0.19998301910375482\n",
      "Epoch: 3 - Batch: 2311, Training Loss: 0.2000674209982206\n",
      "Epoch: 3 - Batch: 2312, Training Loss: 0.20014651634948172\n",
      "Epoch: 3 - Batch: 2313, Training Loss: 0.20023868056276742\n",
      "Epoch: 3 - Batch: 2314, Training Loss: 0.20032188075766041\n",
      "Epoch: 3 - Batch: 2315, Training Loss: 0.20041096061591682\n",
      "Epoch: 3 - Batch: 2316, Training Loss: 0.20050651731454516\n",
      "Epoch: 3 - Batch: 2317, Training Loss: 0.20058932093591436\n",
      "Epoch: 3 - Batch: 2318, Training Loss: 0.20066993062694868\n",
      "Epoch: 3 - Batch: 2319, Training Loss: 0.20075409590679022\n",
      "Epoch: 3 - Batch: 2320, Training Loss: 0.20083608534526864\n",
      "Epoch: 3 - Batch: 2321, Training Loss: 0.20091397130543714\n",
      "Epoch: 3 - Batch: 2322, Training Loss: 0.20100482023177454\n",
      "Epoch: 3 - Batch: 2323, Training Loss: 0.20108843116617914\n",
      "Epoch: 3 - Batch: 2324, Training Loss: 0.2011676638898367\n",
      "Epoch: 3 - Batch: 2325, Training Loss: 0.20125859651473624\n",
      "Epoch: 3 - Batch: 2326, Training Loss: 0.20135002497392113\n",
      "Epoch: 3 - Batch: 2327, Training Loss: 0.2014385966358592\n",
      "Epoch: 3 - Batch: 2328, Training Loss: 0.20152642024344275\n",
      "Epoch: 3 - Batch: 2329, Training Loss: 0.20161211191001618\n",
      "Epoch: 3 - Batch: 2330, Training Loss: 0.20169509194828383\n",
      "Epoch: 3 - Batch: 2331, Training Loss: 0.2017924913188217\n",
      "Epoch: 3 - Batch: 2332, Training Loss: 0.2018806511848226\n",
      "Epoch: 3 - Batch: 2333, Training Loss: 0.20196713370025454\n",
      "Epoch: 3 - Batch: 2334, Training Loss: 0.20205744005010692\n",
      "Epoch: 3 - Batch: 2335, Training Loss: 0.20214689778130052\n",
      "Epoch: 3 - Batch: 2336, Training Loss: 0.2022332909330108\n",
      "Epoch: 3 - Batch: 2337, Training Loss: 0.20231086796591333\n",
      "Epoch: 3 - Batch: 2338, Training Loss: 0.20239534037524393\n",
      "Epoch: 3 - Batch: 2339, Training Loss: 0.2024817065911902\n",
      "Epoch: 3 - Batch: 2340, Training Loss: 0.20256874857356102\n",
      "Epoch: 3 - Batch: 2341, Training Loss: 0.2026548630204268\n",
      "Epoch: 3 - Batch: 2342, Training Loss: 0.2027512610106919\n",
      "Epoch: 3 - Batch: 2343, Training Loss: 0.20283834616442029\n",
      "Epoch: 3 - Batch: 2344, Training Loss: 0.2029302727931471\n",
      "Epoch: 3 - Batch: 2345, Training Loss: 0.20301237815425763\n",
      "Epoch: 3 - Batch: 2346, Training Loss: 0.20309969947162157\n",
      "Epoch: 3 - Batch: 2347, Training Loss: 0.20318095950073073\n",
      "Epoch: 3 - Batch: 2348, Training Loss: 0.20326223793388598\n",
      "Epoch: 3 - Batch: 2349, Training Loss: 0.2033452485961404\n",
      "Epoch: 3 - Batch: 2350, Training Loss: 0.20342782264572273\n",
      "Epoch: 3 - Batch: 2351, Training Loss: 0.20350636662930793\n",
      "Epoch: 3 - Batch: 2352, Training Loss: 0.20359926883928217\n",
      "Epoch: 3 - Batch: 2353, Training Loss: 0.2036889726064395\n",
      "Epoch: 3 - Batch: 2354, Training Loss: 0.20379437383258125\n",
      "Epoch: 3 - Batch: 2355, Training Loss: 0.2038802322352407\n",
      "Epoch: 3 - Batch: 2356, Training Loss: 0.20397036960024145\n",
      "Epoch: 3 - Batch: 2357, Training Loss: 0.2040575079829934\n",
      "Epoch: 3 - Batch: 2358, Training Loss: 0.20413996193366463\n",
      "Epoch: 3 - Batch: 2359, Training Loss: 0.20423225221373945\n",
      "Epoch: 3 - Batch: 2360, Training Loss: 0.20431081121586647\n",
      "Epoch: 3 - Batch: 2361, Training Loss: 0.20439976833723672\n",
      "Epoch: 3 - Batch: 2362, Training Loss: 0.2044893031195424\n",
      "Epoch: 3 - Batch: 2363, Training Loss: 0.20457538515082244\n",
      "Epoch: 3 - Batch: 2364, Training Loss: 0.20466171348989504\n",
      "Epoch: 3 - Batch: 2365, Training Loss: 0.20473902463097476\n",
      "Epoch: 3 - Batch: 2366, Training Loss: 0.2048200220858082\n",
      "Epoch: 3 - Batch: 2367, Training Loss: 0.20490195473059888\n",
      "Epoch: 3 - Batch: 2368, Training Loss: 0.2050001109775718\n",
      "Epoch: 3 - Batch: 2369, Training Loss: 0.20508966192751382\n",
      "Epoch: 3 - Batch: 2370, Training Loss: 0.2051725056498106\n",
      "Epoch: 3 - Batch: 2371, Training Loss: 0.2052542634407185\n",
      "Epoch: 3 - Batch: 2372, Training Loss: 0.20534876821127104\n",
      "Epoch: 3 - Batch: 2373, Training Loss: 0.20543644682907347\n",
      "Epoch: 3 - Batch: 2374, Training Loss: 0.2055307022955386\n",
      "Epoch: 3 - Batch: 2375, Training Loss: 0.20561407520057354\n",
      "Epoch: 3 - Batch: 2376, Training Loss: 0.20569637281261074\n",
      "Epoch: 3 - Batch: 2377, Training Loss: 0.2057839970090496\n",
      "Epoch: 3 - Batch: 2378, Training Loss: 0.20586603058264227\n",
      "Epoch: 3 - Batch: 2379, Training Loss: 0.2059536874294281\n",
      "Epoch: 3 - Batch: 2380, Training Loss: 0.20604654824111\n",
      "Epoch: 3 - Batch: 2381, Training Loss: 0.20613966656734498\n",
      "Epoch: 3 - Batch: 2382, Training Loss: 0.20622535373020923\n",
      "Epoch: 3 - Batch: 2383, Training Loss: 0.20631035644666076\n",
      "Epoch: 3 - Batch: 2384, Training Loss: 0.20639142530598056\n",
      "Epoch: 3 - Batch: 2385, Training Loss: 0.2064764216962343\n",
      "Epoch: 3 - Batch: 2386, Training Loss: 0.20656233607709507\n",
      "Epoch: 3 - Batch: 2387, Training Loss: 0.20664450054243824\n",
      "Epoch: 3 - Batch: 2388, Training Loss: 0.2067371206805975\n",
      "Epoch: 3 - Batch: 2389, Training Loss: 0.20681540595111167\n",
      "Epoch: 3 - Batch: 2390, Training Loss: 0.20690508312229097\n",
      "Epoch: 3 - Batch: 2391, Training Loss: 0.20699421531377146\n",
      "Epoch: 3 - Batch: 2392, Training Loss: 0.2070728037166556\n",
      "Epoch: 3 - Batch: 2393, Training Loss: 0.20715464342070455\n",
      "Epoch: 3 - Batch: 2394, Training Loss: 0.20724120294400314\n",
      "Epoch: 3 - Batch: 2395, Training Loss: 0.2073279950909848\n",
      "Epoch: 3 - Batch: 2396, Training Loss: 0.20741210336708904\n",
      "Epoch: 3 - Batch: 2397, Training Loss: 0.2075020505693026\n",
      "Epoch: 3 - Batch: 2398, Training Loss: 0.20759199116731164\n",
      "Epoch: 3 - Batch: 2399, Training Loss: 0.20768351201122476\n",
      "Epoch: 3 - Batch: 2400, Training Loss: 0.20776425796880651\n",
      "Epoch: 3 - Batch: 2401, Training Loss: 0.2078542064745628\n",
      "Epoch: 3 - Batch: 2402, Training Loss: 0.20794303948754694\n",
      "Epoch: 3 - Batch: 2403, Training Loss: 0.20802835742039466\n",
      "Epoch: 3 - Batch: 2404, Training Loss: 0.2081160013676678\n",
      "Epoch: 3 - Batch: 2405, Training Loss: 0.20820141639282455\n",
      "Epoch: 3 - Batch: 2406, Training Loss: 0.20828423881362723\n",
      "Epoch: 3 - Batch: 2407, Training Loss: 0.2083799497318604\n",
      "Epoch: 3 - Batch: 2408, Training Loss: 0.20846662062465848\n",
      "Epoch: 3 - Batch: 2409, Training Loss: 0.20856046567286424\n",
      "Epoch: 3 - Batch: 2410, Training Loss: 0.2086445045523086\n",
      "Epoch: 3 - Batch: 2411, Training Loss: 0.20872916254658208\n",
      "Epoch: 3 - Batch: 2412, Training Loss: 0.2088435373607263\n",
      "Epoch 3 - Batch 2412, Training Loss: 0.2088435373607263, Validation Loss: 0.20802246125578683\n",
      "Validation loss decreased (0.208422 --> 0.208022). Saving model...\n",
      "Epoch: 4 - Batch: 1, Training Loss: 8.925337152892281e-05\n",
      "Epoch: 4 - Batch: 2, Training Loss: 0.0001805488137562279\n",
      "Epoch: 4 - Batch: 3, Training Loss: 0.0002712364096942035\n",
      "Epoch: 4 - Batch: 4, Training Loss: 0.00036266108180950726\n",
      "Epoch: 4 - Batch: 5, Training Loss: 0.0004541607230753448\n",
      "Epoch: 4 - Batch: 6, Training Loss: 0.0005414117974033008\n",
      "Epoch: 4 - Batch: 7, Training Loss: 0.0006262550368336698\n",
      "Epoch: 4 - Batch: 8, Training Loss: 0.0007138842621055211\n",
      "Epoch: 4 - Batch: 9, Training Loss: 0.0007995079961878744\n",
      "Epoch: 4 - Batch: 10, Training Loss: 0.0008857818212279831\n",
      "Epoch: 4 - Batch: 11, Training Loss: 0.0009760330145434163\n",
      "Epoch: 4 - Batch: 12, Training Loss: 0.001066254232554491\n",
      "Epoch: 4 - Batch: 13, Training Loss: 0.001149363964054715\n",
      "Epoch: 4 - Batch: 14, Training Loss: 0.0012422561966760044\n",
      "Epoch: 4 - Batch: 15, Training Loss: 0.0013314546093616518\n",
      "Epoch: 4 - Batch: 16, Training Loss: 0.0014113365034657726\n",
      "Epoch: 4 - Batch: 17, Training Loss: 0.001501362406298098\n",
      "Epoch: 4 - Batch: 18, Training Loss: 0.0015928498894025634\n",
      "Epoch: 4 - Batch: 19, Training Loss: 0.001681807863326808\n",
      "Epoch: 4 - Batch: 20, Training Loss: 0.0017690743948294352\n",
      "Epoch: 4 - Batch: 21, Training Loss: 0.0018559988668407769\n",
      "Epoch: 4 - Batch: 22, Training Loss: 0.0019429041440312937\n",
      "Epoch: 4 - Batch: 23, Training Loss: 0.002028849698714356\n",
      "Epoch: 4 - Batch: 24, Training Loss: 0.0021158818828921214\n",
      "Epoch: 4 - Batch: 25, Training Loss: 0.002201610011593817\n",
      "Epoch: 4 - Batch: 26, Training Loss: 0.0022856926184091995\n",
      "Epoch: 4 - Batch: 27, Training Loss: 0.0023656404248221003\n",
      "Epoch: 4 - Batch: 28, Training Loss: 0.002463767351350974\n",
      "Epoch: 4 - Batch: 29, Training Loss: 0.0025438847542303316\n",
      "Epoch: 4 - Batch: 30, Training Loss: 0.0026286170504381803\n",
      "Epoch: 4 - Batch: 31, Training Loss: 0.002711325068378923\n",
      "Epoch: 4 - Batch: 32, Training Loss: 0.0027999568477297698\n",
      "Epoch: 4 - Batch: 33, Training Loss: 0.0028934274826377974\n",
      "Epoch: 4 - Batch: 34, Training Loss: 0.002972994061498895\n",
      "Epoch: 4 - Batch: 35, Training Loss: 0.0030593740912911115\n",
      "Epoch: 4 - Batch: 36, Training Loss: 0.003143024502652597\n",
      "Epoch: 4 - Batch: 37, Training Loss: 0.0032355225278963497\n",
      "Epoch: 4 - Batch: 38, Training Loss: 0.003325706369445889\n",
      "Epoch: 4 - Batch: 39, Training Loss: 0.003410202576153314\n",
      "Epoch: 4 - Batch: 40, Training Loss: 0.003496454142120545\n",
      "Epoch: 4 - Batch: 41, Training Loss: 0.0035824406193955423\n",
      "Epoch: 4 - Batch: 42, Training Loss: 0.003664429532750131\n",
      "Epoch: 4 - Batch: 43, Training Loss: 0.0037423557050489074\n",
      "Epoch: 4 - Batch: 44, Training Loss: 0.003830228130615766\n",
      "Epoch: 4 - Batch: 45, Training Loss: 0.003916789365200261\n",
      "Epoch: 4 - Batch: 46, Training Loss: 0.0040015449089493916\n",
      "Epoch: 4 - Batch: 47, Training Loss: 0.004092848363948699\n",
      "Epoch: 4 - Batch: 48, Training Loss: 0.004180898757073812\n",
      "Epoch: 4 - Batch: 49, Training Loss: 0.004267659612852542\n",
      "Epoch: 4 - Batch: 50, Training Loss: 0.004348339843463344\n",
      "Epoch: 4 - Batch: 51, Training Loss: 0.004431190145252949\n",
      "Epoch: 4 - Batch: 52, Training Loss: 0.004521030030687452\n",
      "Epoch: 4 - Batch: 53, Training Loss: 0.004604869171556944\n",
      "Epoch: 4 - Batch: 54, Training Loss: 0.004692999859206119\n",
      "Epoch: 4 - Batch: 55, Training Loss: 0.004785314758322132\n",
      "Epoch: 4 - Batch: 56, Training Loss: 0.0048643954493612\n",
      "Epoch: 4 - Batch: 57, Training Loss: 0.004946488522573886\n",
      "Epoch: 4 - Batch: 58, Training Loss: 0.005024805269628813\n",
      "Epoch: 4 - Batch: 59, Training Loss: 0.005114938229767245\n",
      "Epoch: 4 - Batch: 60, Training Loss: 0.0052016717049711775\n",
      "Epoch: 4 - Batch: 61, Training Loss: 0.005288809772648819\n",
      "Epoch: 4 - Batch: 62, Training Loss: 0.0053780828270548415\n",
      "Epoch: 4 - Batch: 63, Training Loss: 0.005471398303313042\n",
      "Epoch: 4 - Batch: 64, Training Loss: 0.005559299406819478\n",
      "Epoch: 4 - Batch: 65, Training Loss: 0.005648330632430404\n",
      "Epoch: 4 - Batch: 66, Training Loss: 0.005748814586580887\n",
      "Epoch: 4 - Batch: 67, Training Loss: 0.0058297473399793335\n",
      "Epoch: 4 - Batch: 68, Training Loss: 0.00591821457640844\n",
      "Epoch: 4 - Batch: 69, Training Loss: 0.006002072584668597\n",
      "Epoch: 4 - Batch: 70, Training Loss: 0.006086820060044379\n",
      "Epoch: 4 - Batch: 71, Training Loss: 0.006171346686866944\n",
      "Epoch: 4 - Batch: 72, Training Loss: 0.006250478735017539\n",
      "Epoch: 4 - Batch: 73, Training Loss: 0.0063425884280928334\n",
      "Epoch: 4 - Batch: 74, Training Loss: 0.0064228226679375715\n",
      "Epoch: 4 - Batch: 75, Training Loss: 0.006512037433597381\n",
      "Epoch: 4 - Batch: 76, Training Loss: 0.006601370648661656\n",
      "Epoch: 4 - Batch: 77, Training Loss: 0.0066807749333666335\n",
      "Epoch: 4 - Batch: 78, Training Loss: 0.0067681295930054255\n",
      "Epoch: 4 - Batch: 79, Training Loss: 0.006849779968287419\n",
      "Epoch: 4 - Batch: 80, Training Loss: 0.006926892896740393\n",
      "Epoch: 4 - Batch: 81, Training Loss: 0.007014911473538745\n",
      "Epoch: 4 - Batch: 82, Training Loss: 0.007090764135665957\n",
      "Epoch: 4 - Batch: 83, Training Loss: 0.0071782465424901415\n",
      "Epoch: 4 - Batch: 84, Training Loss: 0.0072742118430671404\n",
      "Epoch: 4 - Batch: 85, Training Loss: 0.00735537819008329\n",
      "Epoch: 4 - Batch: 86, Training Loss: 0.007437368128973849\n",
      "Epoch: 4 - Batch: 87, Training Loss: 0.007518898436234365\n",
      "Epoch: 4 - Batch: 88, Training Loss: 0.0076090398477776525\n",
      "Epoch: 4 - Batch: 89, Training Loss: 0.007697823474409173\n",
      "Epoch: 4 - Batch: 90, Training Loss: 0.007786277267667983\n",
      "Epoch: 4 - Batch: 91, Training Loss: 0.007877699703373521\n",
      "Epoch: 4 - Batch: 92, Training Loss: 0.007965661844803918\n",
      "Epoch: 4 - Batch: 93, Training Loss: 0.00805184025793131\n",
      "Epoch: 4 - Batch: 94, Training Loss: 0.008140753367745857\n",
      "Epoch: 4 - Batch: 95, Training Loss: 0.008225436530896087\n",
      "Epoch: 4 - Batch: 96, Training Loss: 0.008309519526920904\n",
      "Epoch: 4 - Batch: 97, Training Loss: 0.008395124239164402\n",
      "Epoch: 4 - Batch: 98, Training Loss: 0.00847837997362586\n",
      "Epoch: 4 - Batch: 99, Training Loss: 0.008564133876641195\n",
      "Epoch: 4 - Batch: 100, Training Loss: 0.008644433454247454\n",
      "Epoch: 4 - Batch: 101, Training Loss: 0.008732015418373727\n",
      "Epoch: 4 - Batch: 102, Training Loss: 0.008816530183575443\n",
      "Epoch: 4 - Batch: 103, Training Loss: 0.00889968288873954\n",
      "Epoch: 4 - Batch: 104, Training Loss: 0.008990106656282498\n",
      "Epoch: 4 - Batch: 105, Training Loss: 0.00907774537085103\n",
      "Epoch: 4 - Batch: 106, Training Loss: 0.009164151403540205\n",
      "Epoch: 4 - Batch: 107, Training Loss: 0.009260091479639113\n",
      "Epoch: 4 - Batch: 108, Training Loss: 0.009346522539112698\n",
      "Epoch: 4 - Batch: 109, Training Loss: 0.009434894635457897\n",
      "Epoch: 4 - Batch: 110, Training Loss: 0.00952678698904281\n",
      "Epoch: 4 - Batch: 111, Training Loss: 0.00961488841802722\n",
      "Epoch: 4 - Batch: 112, Training Loss: 0.009697137971966223\n",
      "Epoch: 4 - Batch: 113, Training Loss: 0.009782602890065653\n",
      "Epoch: 4 - Batch: 114, Training Loss: 0.009862469889186508\n",
      "Epoch: 4 - Batch: 115, Training Loss: 0.009948662974891774\n",
      "Epoch: 4 - Batch: 116, Training Loss: 0.010025803209784414\n",
      "Epoch: 4 - Batch: 117, Training Loss: 0.010119317701799952\n",
      "Epoch: 4 - Batch: 118, Training Loss: 0.010204164203176056\n",
      "Epoch: 4 - Batch: 119, Training Loss: 0.010284086179673968\n",
      "Epoch: 4 - Batch: 120, Training Loss: 0.010364348973899734\n",
      "Epoch: 4 - Batch: 121, Training Loss: 0.010444865334982895\n",
      "Epoch: 4 - Batch: 122, Training Loss: 0.010535006394384315\n",
      "Epoch: 4 - Batch: 123, Training Loss: 0.010619593608142132\n",
      "Epoch: 4 - Batch: 124, Training Loss: 0.01070785089388218\n",
      "Epoch: 4 - Batch: 125, Training Loss: 0.01079176129096776\n",
      "Epoch: 4 - Batch: 126, Training Loss: 0.010872602963180685\n",
      "Epoch: 4 - Batch: 127, Training Loss: 0.01095667234923116\n",
      "Epoch: 4 - Batch: 128, Training Loss: 0.01103824895388056\n",
      "Epoch: 4 - Batch: 129, Training Loss: 0.011132516813268314\n",
      "Epoch: 4 - Batch: 130, Training Loss: 0.011214649493174371\n",
      "Epoch: 4 - Batch: 131, Training Loss: 0.011306865743143641\n",
      "Epoch: 4 - Batch: 132, Training Loss: 0.011387656929332819\n",
      "Epoch: 4 - Batch: 133, Training Loss: 0.011472702940700462\n",
      "Epoch: 4 - Batch: 134, Training Loss: 0.011555895876528612\n",
      "Epoch: 4 - Batch: 135, Training Loss: 0.011635488711581697\n",
      "Epoch: 4 - Batch: 136, Training Loss: 0.011714941042029046\n",
      "Epoch: 4 - Batch: 137, Training Loss: 0.01180572248068615\n",
      "Epoch: 4 - Batch: 138, Training Loss: 0.011884606102015052\n",
      "Epoch: 4 - Batch: 139, Training Loss: 0.011962344941017443\n",
      "Epoch: 4 - Batch: 140, Training Loss: 0.012047255748441168\n",
      "Epoch: 4 - Batch: 141, Training Loss: 0.01213830231285807\n",
      "Epoch: 4 - Batch: 142, Training Loss: 0.012214751200592932\n",
      "Epoch: 4 - Batch: 143, Training Loss: 0.012303896754930663\n",
      "Epoch: 4 - Batch: 144, Training Loss: 0.012388694475391019\n",
      "Epoch: 4 - Batch: 145, Training Loss: 0.012482512933796713\n",
      "Epoch: 4 - Batch: 146, Training Loss: 0.012569364129409663\n",
      "Epoch: 4 - Batch: 147, Training Loss: 0.012658438069211507\n",
      "Epoch: 4 - Batch: 148, Training Loss: 0.012748705776127219\n",
      "Epoch: 4 - Batch: 149, Training Loss: 0.01283614248482149\n",
      "Epoch: 4 - Batch: 150, Training Loss: 0.012917453827796686\n",
      "Epoch: 4 - Batch: 151, Training Loss: 0.01300619573488362\n",
      "Epoch: 4 - Batch: 152, Training Loss: 0.013097349838484974\n",
      "Epoch: 4 - Batch: 153, Training Loss: 0.01318948964895696\n",
      "Epoch: 4 - Batch: 154, Training Loss: 0.013279918463866707\n",
      "Epoch: 4 - Batch: 155, Training Loss: 0.013360129616796279\n",
      "Epoch: 4 - Batch: 156, Training Loss: 0.01345293760101989\n",
      "Epoch: 4 - Batch: 157, Training Loss: 0.013537413000467406\n",
      "Epoch: 4 - Batch: 158, Training Loss: 0.013617789406780382\n",
      "Epoch: 4 - Batch: 159, Training Loss: 0.013708531201676548\n",
      "Epoch: 4 - Batch: 160, Training Loss: 0.013793028452453724\n",
      "Epoch: 4 - Batch: 161, Training Loss: 0.013883478618281002\n",
      "Epoch: 4 - Batch: 162, Training Loss: 0.01397369535729462\n",
      "Epoch: 4 - Batch: 163, Training Loss: 0.01405626141881666\n",
      "Epoch: 4 - Batch: 164, Training Loss: 0.014146957286999594\n",
      "Epoch: 4 - Batch: 165, Training Loss: 0.014241005039185434\n",
      "Epoch: 4 - Batch: 166, Training Loss: 0.014331182115904333\n",
      "Epoch: 4 - Batch: 167, Training Loss: 0.014409132097935795\n",
      "Epoch: 4 - Batch: 168, Training Loss: 0.014494007549080288\n",
      "Epoch: 4 - Batch: 169, Training Loss: 0.014584066367910474\n",
      "Epoch: 4 - Batch: 170, Training Loss: 0.014673443118830027\n",
      "Epoch: 4 - Batch: 171, Training Loss: 0.014758254029165651\n",
      "Epoch: 4 - Batch: 172, Training Loss: 0.014844209993656595\n",
      "Epoch: 4 - Batch: 173, Training Loss: 0.01492721786966569\n",
      "Epoch: 4 - Batch: 174, Training Loss: 0.015014711390581494\n",
      "Epoch: 4 - Batch: 175, Training Loss: 0.015104439548196682\n",
      "Epoch: 4 - Batch: 176, Training Loss: 0.015189937209312002\n",
      "Epoch: 4 - Batch: 177, Training Loss: 0.015270799756742037\n",
      "Epoch: 4 - Batch: 178, Training Loss: 0.015351115304290952\n",
      "Epoch: 4 - Batch: 179, Training Loss: 0.015431462427227453\n",
      "Epoch: 4 - Batch: 180, Training Loss: 0.01551208375129929\n",
      "Epoch: 4 - Batch: 181, Training Loss: 0.015599527348382753\n",
      "Epoch: 4 - Batch: 182, Training Loss: 0.015693776408337044\n",
      "Epoch: 4 - Batch: 183, Training Loss: 0.01578202443336373\n",
      "Epoch: 4 - Batch: 184, Training Loss: 0.015872235161253863\n",
      "Epoch: 4 - Batch: 185, Training Loss: 0.015960853392409647\n",
      "Epoch: 4 - Batch: 186, Training Loss: 0.016051841692792043\n",
      "Epoch: 4 - Batch: 187, Training Loss: 0.016132774291742303\n",
      "Epoch: 4 - Batch: 188, Training Loss: 0.016219491166854973\n",
      "Epoch: 4 - Batch: 189, Training Loss: 0.016303314368671447\n",
      "Epoch: 4 - Batch: 190, Training Loss: 0.016393409600206472\n",
      "Epoch: 4 - Batch: 191, Training Loss: 0.01647493885104138\n",
      "Epoch: 4 - Batch: 192, Training Loss: 0.01655932231057145\n",
      "Epoch: 4 - Batch: 193, Training Loss: 0.016646715094201008\n",
      "Epoch: 4 - Batch: 194, Training Loss: 0.01673891455874119\n",
      "Epoch: 4 - Batch: 195, Training Loss: 0.016831573691978977\n",
      "Epoch: 4 - Batch: 196, Training Loss: 0.016920893593609432\n",
      "Epoch: 4 - Batch: 197, Training Loss: 0.01700315452111301\n",
      "Epoch: 4 - Batch: 198, Training Loss: 0.017081382757000267\n",
      "Epoch: 4 - Batch: 199, Training Loss: 0.017168907346713603\n",
      "Epoch: 4 - Batch: 200, Training Loss: 0.01725084833788437\n",
      "Epoch: 4 - Batch: 201, Training Loss: 0.01734818875245985\n",
      "Epoch: 4 - Batch: 202, Training Loss: 0.01743593063841807\n",
      "Epoch: 4 - Batch: 203, Training Loss: 0.017524148595471484\n",
      "Epoch: 4 - Batch: 204, Training Loss: 0.01760602250670517\n",
      "Epoch: 4 - Batch: 205, Training Loss: 0.017700013075993823\n",
      "Epoch: 4 - Batch: 206, Training Loss: 0.01778560765643618\n",
      "Epoch: 4 - Batch: 207, Training Loss: 0.01786558031665152\n",
      "Epoch: 4 - Batch: 208, Training Loss: 0.01795253244106647\n",
      "Epoch: 4 - Batch: 209, Training Loss: 0.018038878659061335\n",
      "Epoch: 4 - Batch: 210, Training Loss: 0.018123271539930878\n",
      "Epoch: 4 - Batch: 211, Training Loss: 0.018208568869390297\n",
      "Epoch: 4 - Batch: 212, Training Loss: 0.018297616126723155\n",
      "Epoch: 4 - Batch: 213, Training Loss: 0.018374915121997373\n",
      "Epoch: 4 - Batch: 214, Training Loss: 0.018464533485484557\n",
      "Epoch: 4 - Batch: 215, Training Loss: 0.018547820010736807\n",
      "Epoch: 4 - Batch: 216, Training Loss: 0.018641759241447717\n",
      "Epoch: 4 - Batch: 217, Training Loss: 0.018735155030763763\n",
      "Epoch: 4 - Batch: 218, Training Loss: 0.018813383297540655\n",
      "Epoch: 4 - Batch: 219, Training Loss: 0.018897668873986993\n",
      "Epoch: 4 - Batch: 220, Training Loss: 0.018983969801595158\n",
      "Epoch: 4 - Batch: 221, Training Loss: 0.019070654953049982\n",
      "Epoch: 4 - Batch: 222, Training Loss: 0.01915875668475284\n",
      "Epoch: 4 - Batch: 223, Training Loss: 0.019242633134126663\n",
      "Epoch: 4 - Batch: 224, Training Loss: 0.01933370614007338\n",
      "Epoch: 4 - Batch: 225, Training Loss: 0.01942352202425944\n",
      "Epoch: 4 - Batch: 226, Training Loss: 0.01951654632861539\n",
      "Epoch: 4 - Batch: 227, Training Loss: 0.019602406362247703\n",
      "Epoch: 4 - Batch: 228, Training Loss: 0.01969427482578687\n",
      "Epoch: 4 - Batch: 229, Training Loss: 0.019788201311433294\n",
      "Epoch: 4 - Batch: 230, Training Loss: 0.019873741902966997\n",
      "Epoch: 4 - Batch: 231, Training Loss: 0.01995892745518368\n",
      "Epoch: 4 - Batch: 232, Training Loss: 0.020043495121978804\n",
      "Epoch: 4 - Batch: 233, Training Loss: 0.020135894047087106\n",
      "Epoch: 4 - Batch: 234, Training Loss: 0.020225242101533297\n",
      "Epoch: 4 - Batch: 235, Training Loss: 0.02030896677776158\n",
      "Epoch: 4 - Batch: 236, Training Loss: 0.020393188425013874\n",
      "Epoch: 4 - Batch: 237, Training Loss: 0.02047790163760359\n",
      "Epoch: 4 - Batch: 238, Training Loss: 0.020567880483161948\n",
      "Epoch: 4 - Batch: 239, Training Loss: 0.020650651880461186\n",
      "Epoch: 4 - Batch: 240, Training Loss: 0.020737875395054447\n",
      "Epoch: 4 - Batch: 241, Training Loss: 0.020815994306336193\n",
      "Epoch: 4 - Batch: 242, Training Loss: 0.020903431391884043\n",
      "Epoch: 4 - Batch: 243, Training Loss: 0.020991736074684072\n",
      "Epoch: 4 - Batch: 244, Training Loss: 0.021080716752491978\n",
      "Epoch: 4 - Batch: 245, Training Loss: 0.021173955437111026\n",
      "Epoch: 4 - Batch: 246, Training Loss: 0.021255304440732422\n",
      "Epoch: 4 - Batch: 247, Training Loss: 0.021342984417678904\n",
      "Epoch: 4 - Batch: 248, Training Loss: 0.021432119951418187\n",
      "Epoch: 4 - Batch: 249, Training Loss: 0.02151227237696869\n",
      "Epoch: 4 - Batch: 250, Training Loss: 0.02159858551216165\n",
      "Epoch: 4 - Batch: 251, Training Loss: 0.021693685204779132\n",
      "Epoch: 4 - Batch: 252, Training Loss: 0.02178705601673419\n",
      "Epoch: 4 - Batch: 253, Training Loss: 0.02187145904793866\n",
      "Epoch: 4 - Batch: 254, Training Loss: 0.02196252703740822\n",
      "Epoch: 4 - Batch: 255, Training Loss: 0.02205220896559173\n",
      "Epoch: 4 - Batch: 256, Training Loss: 0.022139798639771554\n",
      "Epoch: 4 - Batch: 257, Training Loss: 0.022229314462017657\n",
      "Epoch: 4 - Batch: 258, Training Loss: 0.022315861153760753\n",
      "Epoch: 4 - Batch: 259, Training Loss: 0.022406935525029453\n",
      "Epoch: 4 - Batch: 260, Training Loss: 0.02249418689589793\n",
      "Epoch: 4 - Batch: 261, Training Loss: 0.022579136173276365\n",
      "Epoch: 4 - Batch: 262, Training Loss: 0.022665367117321512\n",
      "Epoch: 4 - Batch: 263, Training Loss: 0.022758591475326623\n",
      "Epoch: 4 - Batch: 264, Training Loss: 0.022838933118441408\n",
      "Epoch: 4 - Batch: 265, Training Loss: 0.022919892850849364\n",
      "Epoch: 4 - Batch: 266, Training Loss: 0.023014089460198953\n",
      "Epoch: 4 - Batch: 267, Training Loss: 0.02309714693261023\n",
      "Epoch: 4 - Batch: 268, Training Loss: 0.02318148682539538\n",
      "Epoch: 4 - Batch: 269, Training Loss: 0.023271274927441358\n",
      "Epoch: 4 - Batch: 270, Training Loss: 0.02335855193670907\n",
      "Epoch: 4 - Batch: 271, Training Loss: 0.023444945940973353\n",
      "Epoch: 4 - Batch: 272, Training Loss: 0.023529701033733773\n",
      "Epoch: 4 - Batch: 273, Training Loss: 0.02361824954079949\n",
      "Epoch: 4 - Batch: 274, Training Loss: 0.02369997907386688\n",
      "Epoch: 4 - Batch: 275, Training Loss: 0.0237869492810757\n",
      "Epoch: 4 - Batch: 276, Training Loss: 0.023860920480086437\n",
      "Epoch: 4 - Batch: 277, Training Loss: 0.023951034418386013\n",
      "Epoch: 4 - Batch: 278, Training Loss: 0.024043448226121724\n",
      "Epoch: 4 - Batch: 279, Training Loss: 0.024123006248553198\n",
      "Epoch: 4 - Batch: 280, Training Loss: 0.02420000924078584\n",
      "Epoch: 4 - Batch: 281, Training Loss: 0.024286320065433905\n",
      "Epoch: 4 - Batch: 282, Training Loss: 0.024367579038117457\n",
      "Epoch: 4 - Batch: 283, Training Loss: 0.02445554750574564\n",
      "Epoch: 4 - Batch: 284, Training Loss: 0.02453471649172492\n",
      "Epoch: 4 - Batch: 285, Training Loss: 0.024617114803980832\n",
      "Epoch: 4 - Batch: 286, Training Loss: 0.024701540205460875\n",
      "Epoch: 4 - Batch: 287, Training Loss: 0.024793304669768063\n",
      "Epoch: 4 - Batch: 288, Training Loss: 0.02487773358648887\n",
      "Epoch: 4 - Batch: 289, Training Loss: 0.02496437313248269\n",
      "Epoch: 4 - Batch: 290, Training Loss: 0.02504521161688501\n",
      "Epoch: 4 - Batch: 291, Training Loss: 0.025133630924952365\n",
      "Epoch: 4 - Batch: 292, Training Loss: 0.02522819527543797\n",
      "Epoch: 4 - Batch: 293, Training Loss: 0.025325823655571313\n",
      "Epoch: 4 - Batch: 294, Training Loss: 0.025408576265792942\n",
      "Epoch: 4 - Batch: 295, Training Loss: 0.0254980164887399\n",
      "Epoch: 4 - Batch: 296, Training Loss: 0.025583563795680827\n",
      "Epoch: 4 - Batch: 297, Training Loss: 0.025672387900093498\n",
      "Epoch: 4 - Batch: 298, Training Loss: 0.025764488882290982\n",
      "Epoch: 4 - Batch: 299, Training Loss: 0.02585203863470313\n",
      "Epoch: 4 - Batch: 300, Training Loss: 0.025944132512283957\n",
      "Epoch: 4 - Batch: 301, Training Loss: 0.026023651465501754\n",
      "Epoch: 4 - Batch: 302, Training Loss: 0.026107386903679787\n",
      "Epoch: 4 - Batch: 303, Training Loss: 0.026196138757230036\n",
      "Epoch: 4 - Batch: 304, Training Loss: 0.02627596832428801\n",
      "Epoch: 4 - Batch: 305, Training Loss: 0.026373674626621244\n",
      "Epoch: 4 - Batch: 306, Training Loss: 0.02645923655671662\n",
      "Epoch: 4 - Batch: 307, Training Loss: 0.026544832916402104\n",
      "Epoch: 4 - Batch: 308, Training Loss: 0.02663057893020399\n",
      "Epoch: 4 - Batch: 309, Training Loss: 0.026711777513645972\n",
      "Epoch: 4 - Batch: 310, Training Loss: 0.026792438524724238\n",
      "Epoch: 4 - Batch: 311, Training Loss: 0.02687615493488549\n",
      "Epoch: 4 - Batch: 312, Training Loss: 0.026967685663482642\n",
      "Epoch: 4 - Batch: 313, Training Loss: 0.027051821147179723\n",
      "Epoch: 4 - Batch: 314, Training Loss: 0.027134318584282797\n",
      "Epoch: 4 - Batch: 315, Training Loss: 0.027210993108464712\n",
      "Epoch: 4 - Batch: 316, Training Loss: 0.027295866978059757\n",
      "Epoch: 4 - Batch: 317, Training Loss: 0.027379786913865438\n",
      "Epoch: 4 - Batch: 318, Training Loss: 0.027469147206786063\n",
      "Epoch: 4 - Batch: 319, Training Loss: 0.027562901563015742\n",
      "Epoch: 4 - Batch: 320, Training Loss: 0.0276452885511305\n",
      "Epoch: 4 - Batch: 321, Training Loss: 0.027742226764090223\n",
      "Epoch: 4 - Batch: 322, Training Loss: 0.027829844448993456\n",
      "Epoch: 4 - Batch: 323, Training Loss: 0.027922752114077704\n",
      "Epoch: 4 - Batch: 324, Training Loss: 0.028008382384407383\n",
      "Epoch: 4 - Batch: 325, Training Loss: 0.02809190381299797\n",
      "Epoch: 4 - Batch: 326, Training Loss: 0.028189398088858494\n",
      "Epoch: 4 - Batch: 327, Training Loss: 0.02827241585572956\n",
      "Epoch: 4 - Batch: 328, Training Loss: 0.02836220122713157\n",
      "Epoch: 4 - Batch: 329, Training Loss: 0.02845179288343806\n",
      "Epoch: 4 - Batch: 330, Training Loss: 0.028542668801140824\n",
      "Epoch: 4 - Batch: 331, Training Loss: 0.028625935995657843\n",
      "Epoch: 4 - Batch: 332, Training Loss: 0.028714105733986913\n",
      "Epoch: 4 - Batch: 333, Training Loss: 0.028799910679039472\n",
      "Epoch: 4 - Batch: 334, Training Loss: 0.028887202447376047\n",
      "Epoch: 4 - Batch: 335, Training Loss: 0.028980726156859454\n",
      "Epoch: 4 - Batch: 336, Training Loss: 0.029065602058992654\n",
      "Epoch: 4 - Batch: 337, Training Loss: 0.029154601925443457\n",
      "Epoch: 4 - Batch: 338, Training Loss: 0.029242759425298095\n",
      "Epoch: 4 - Batch: 339, Training Loss: 0.029329382383556507\n",
      "Epoch: 4 - Batch: 340, Training Loss: 0.029421066343290097\n",
      "Epoch: 4 - Batch: 341, Training Loss: 0.029507510561749314\n",
      "Epoch: 4 - Batch: 342, Training Loss: 0.029601025053764853\n",
      "Epoch: 4 - Batch: 343, Training Loss: 0.029685610006401193\n",
      "Epoch: 4 - Batch: 344, Training Loss: 0.029768402748440035\n",
      "Epoch: 4 - Batch: 345, Training Loss: 0.02985091550062545\n",
      "Epoch: 4 - Batch: 346, Training Loss: 0.02993382917559562\n",
      "Epoch: 4 - Batch: 347, Training Loss: 0.030016684024339886\n",
      "Epoch: 4 - Batch: 348, Training Loss: 0.03010001274483714\n",
      "Epoch: 4 - Batch: 349, Training Loss: 0.03018503381591136\n",
      "Epoch: 4 - Batch: 350, Training Loss: 0.030274030784914148\n",
      "Epoch: 4 - Batch: 351, Training Loss: 0.030366393562968493\n",
      "Epoch: 4 - Batch: 352, Training Loss: 0.030462534810921447\n",
      "Epoch: 4 - Batch: 353, Training Loss: 0.030546926239978022\n",
      "Epoch: 4 - Batch: 354, Training Loss: 0.030637369727465644\n",
      "Epoch: 4 - Batch: 355, Training Loss: 0.030720615700801607\n",
      "Epoch: 4 - Batch: 356, Training Loss: 0.030806743189272397\n",
      "Epoch: 4 - Batch: 357, Training Loss: 0.030895073535183372\n",
      "Epoch: 4 - Batch: 358, Training Loss: 0.030989149841750243\n",
      "Epoch: 4 - Batch: 359, Training Loss: 0.0310690626934492\n",
      "Epoch: 4 - Batch: 360, Training Loss: 0.03115494760559566\n",
      "Epoch: 4 - Batch: 361, Training Loss: 0.03123895011558066\n",
      "Epoch: 4 - Batch: 362, Training Loss: 0.03132033581609157\n",
      "Epoch: 4 - Batch: 363, Training Loss: 0.031405219706285056\n",
      "Epoch: 4 - Batch: 364, Training Loss: 0.03149120827787749\n",
      "Epoch: 4 - Batch: 365, Training Loss: 0.03157486606568444\n",
      "Epoch: 4 - Batch: 366, Training Loss: 0.03165608299139325\n",
      "Epoch: 4 - Batch: 367, Training Loss: 0.03173756396172454\n",
      "Epoch: 4 - Batch: 368, Training Loss: 0.03183290199260807\n",
      "Epoch: 4 - Batch: 369, Training Loss: 0.031920346769675687\n",
      "Epoch: 4 - Batch: 370, Training Loss: 0.032001731778258706\n",
      "Epoch: 4 - Batch: 371, Training Loss: 0.03208772403189594\n",
      "Epoch: 4 - Batch: 372, Training Loss: 0.0321679383232127\n",
      "Epoch: 4 - Batch: 373, Training Loss: 0.03225109582862649\n",
      "Epoch: 4 - Batch: 374, Training Loss: 0.03233468119876697\n",
      "Epoch: 4 - Batch: 375, Training Loss: 0.03242734339007888\n",
      "Epoch: 4 - Batch: 376, Training Loss: 0.03251824906068062\n",
      "Epoch: 4 - Batch: 377, Training Loss: 0.032599949057076506\n",
      "Epoch: 4 - Batch: 378, Training Loss: 0.032690251928755694\n",
      "Epoch: 4 - Batch: 379, Training Loss: 0.032776031476348194\n",
      "Epoch: 4 - Batch: 380, Training Loss: 0.03286669563322914\n",
      "Epoch: 4 - Batch: 381, Training Loss: 0.032953691707420504\n",
      "Epoch: 4 - Batch: 382, Training Loss: 0.0330442853803263\n",
      "Epoch: 4 - Batch: 383, Training Loss: 0.033132150565362097\n",
      "Epoch: 4 - Batch: 384, Training Loss: 0.03321623996789776\n",
      "Epoch: 4 - Batch: 385, Training Loss: 0.033295173395678376\n",
      "Epoch: 4 - Batch: 386, Training Loss: 0.03339252334279603\n",
      "Epoch: 4 - Batch: 387, Training Loss: 0.03347183795488296\n",
      "Epoch: 4 - Batch: 388, Training Loss: 0.03355961519094249\n",
      "Epoch: 4 - Batch: 389, Training Loss: 0.033665180020961\n",
      "Epoch: 4 - Batch: 390, Training Loss: 0.033749510492406674\n",
      "Epoch: 4 - Batch: 391, Training Loss: 0.03383583328370036\n",
      "Epoch: 4 - Batch: 392, Training Loss: 0.03392834846504885\n",
      "Epoch: 4 - Batch: 393, Training Loss: 0.03402095832313669\n",
      "Epoch: 4 - Batch: 394, Training Loss: 0.03410386431276502\n",
      "Epoch: 4 - Batch: 395, Training Loss: 0.03418744984970955\n",
      "Epoch: 4 - Batch: 396, Training Loss: 0.03427176610574398\n",
      "Epoch: 4 - Batch: 397, Training Loss: 0.03435089491048263\n",
      "Epoch: 4 - Batch: 398, Training Loss: 0.03444194878956572\n",
      "Epoch: 4 - Batch: 399, Training Loss: 0.034526097413714645\n",
      "Epoch: 4 - Batch: 400, Training Loss: 0.034604783310473064\n",
      "Epoch: 4 - Batch: 401, Training Loss: 0.0346936352588051\n",
      "Epoch: 4 - Batch: 402, Training Loss: 0.03477736487119747\n",
      "Epoch: 4 - Batch: 403, Training Loss: 0.03486348639179027\n",
      "Epoch: 4 - Batch: 404, Training Loss: 0.03494672112129814\n",
      "Epoch: 4 - Batch: 405, Training Loss: 0.03503260070807107\n",
      "Epoch: 4 - Batch: 406, Training Loss: 0.03511710694155488\n",
      "Epoch: 4 - Batch: 407, Training Loss: 0.035197948341938984\n",
      "Epoch: 4 - Batch: 408, Training Loss: 0.035278868696236886\n",
      "Epoch: 4 - Batch: 409, Training Loss: 0.035370811887987415\n",
      "Epoch: 4 - Batch: 410, Training Loss: 0.03546041211925731\n",
      "Epoch: 4 - Batch: 411, Training Loss: 0.03555439208722233\n",
      "Epoch: 4 - Batch: 412, Training Loss: 0.0356471013110965\n",
      "Epoch: 4 - Batch: 413, Training Loss: 0.03573176394499356\n",
      "Epoch: 4 - Batch: 414, Training Loss: 0.03581630178475459\n",
      "Epoch: 4 - Batch: 415, Training Loss: 0.035902354773597336\n",
      "Epoch: 4 - Batch: 416, Training Loss: 0.03598464339057209\n",
      "Epoch: 4 - Batch: 417, Training Loss: 0.03606949766378102\n",
      "Epoch: 4 - Batch: 418, Training Loss: 0.036158838020529514\n",
      "Epoch: 4 - Batch: 419, Training Loss: 0.03625069845894083\n",
      "Epoch: 4 - Batch: 420, Training Loss: 0.03633469848539896\n",
      "Epoch: 4 - Batch: 421, Training Loss: 0.036425743011099784\n",
      "Epoch: 4 - Batch: 422, Training Loss: 0.03651605480370632\n",
      "Epoch: 4 - Batch: 423, Training Loss: 0.03660070352491059\n",
      "Epoch: 4 - Batch: 424, Training Loss: 0.03668667138808995\n",
      "Epoch: 4 - Batch: 425, Training Loss: 0.03677580994901372\n",
      "Epoch: 4 - Batch: 426, Training Loss: 0.03686281367147937\n",
      "Epoch: 4 - Batch: 427, Training Loss: 0.03695756171646205\n",
      "Epoch: 4 - Batch: 428, Training Loss: 0.03704218686887281\n",
      "Epoch: 4 - Batch: 429, Training Loss: 0.0371320413403942\n",
      "Epoch: 4 - Batch: 430, Training Loss: 0.037214114310630715\n",
      "Epoch: 4 - Batch: 431, Training Loss: 0.0372928177959488\n",
      "Epoch: 4 - Batch: 432, Training Loss: 0.03737649492066891\n",
      "Epoch: 4 - Batch: 433, Training Loss: 0.03745798379256951\n",
      "Epoch: 4 - Batch: 434, Training Loss: 0.03754358566914427\n",
      "Epoch: 4 - Batch: 435, Training Loss: 0.0376264299165649\n",
      "Epoch: 4 - Batch: 436, Training Loss: 0.03770994491393293\n",
      "Epoch: 4 - Batch: 437, Training Loss: 0.03779955667939352\n",
      "Epoch: 4 - Batch: 438, Training Loss: 0.03788402077941159\n",
      "Epoch: 4 - Batch: 439, Training Loss: 0.03796911857416776\n",
      "Epoch: 4 - Batch: 440, Training Loss: 0.03805450817097479\n",
      "Epoch: 4 - Batch: 441, Training Loss: 0.03814456761377565\n",
      "Epoch: 4 - Batch: 442, Training Loss: 0.03822953777254913\n",
      "Epoch: 4 - Batch: 443, Training Loss: 0.038329019691566525\n",
      "Epoch: 4 - Batch: 444, Training Loss: 0.038414762702895636\n",
      "Epoch: 4 - Batch: 445, Training Loss: 0.03849660390200307\n",
      "Epoch: 4 - Batch: 446, Training Loss: 0.03858627037918983\n",
      "Epoch: 4 - Batch: 447, Training Loss: 0.03868297677777498\n",
      "Epoch: 4 - Batch: 448, Training Loss: 0.03875757148412132\n",
      "Epoch: 4 - Batch: 449, Training Loss: 0.03884231613000629\n",
      "Epoch: 4 - Batch: 450, Training Loss: 0.03892352992350585\n",
      "Epoch: 4 - Batch: 451, Training Loss: 0.039004262635610985\n",
      "Epoch: 4 - Batch: 452, Training Loss: 0.039089365601292496\n",
      "Epoch: 4 - Batch: 453, Training Loss: 0.03916949909767306\n",
      "Epoch: 4 - Batch: 454, Training Loss: 0.0392500622256676\n",
      "Epoch: 4 - Batch: 455, Training Loss: 0.03933872021796494\n",
      "Epoch: 4 - Batch: 456, Training Loss: 0.03943421157265382\n",
      "Epoch: 4 - Batch: 457, Training Loss: 0.03952359542819003\n",
      "Epoch: 4 - Batch: 458, Training Loss: 0.03960205830794267\n",
      "Epoch: 4 - Batch: 459, Training Loss: 0.03968668779725854\n",
      "Epoch: 4 - Batch: 460, Training Loss: 0.039771055842859435\n",
      "Epoch: 4 - Batch: 461, Training Loss: 0.03985455109557109\n",
      "Epoch: 4 - Batch: 462, Training Loss: 0.03992826490976522\n",
      "Epoch: 4 - Batch: 463, Training Loss: 0.04001447782661784\n",
      "Epoch: 4 - Batch: 464, Training Loss: 0.040099049947698716\n",
      "Epoch: 4 - Batch: 465, Training Loss: 0.040177026185922164\n",
      "Epoch: 4 - Batch: 466, Training Loss: 0.04026634144175112\n",
      "Epoch: 4 - Batch: 467, Training Loss: 0.0403477188329495\n",
      "Epoch: 4 - Batch: 468, Training Loss: 0.040423681110044817\n",
      "Epoch: 4 - Batch: 469, Training Loss: 0.040518548024531024\n",
      "Epoch: 4 - Batch: 470, Training Loss: 0.04060217853678795\n",
      "Epoch: 4 - Batch: 471, Training Loss: 0.040696051923147285\n",
      "Epoch: 4 - Batch: 472, Training Loss: 0.04079400242042186\n",
      "Epoch: 4 - Batch: 473, Training Loss: 0.04087947792748907\n",
      "Epoch: 4 - Batch: 474, Training Loss: 0.04096847285159785\n",
      "Epoch: 4 - Batch: 475, Training Loss: 0.041055339393182774\n",
      "Epoch: 4 - Batch: 476, Training Loss: 0.041146898805897426\n",
      "Epoch: 4 - Batch: 477, Training Loss: 0.04123778612187648\n",
      "Epoch: 4 - Batch: 478, Training Loss: 0.041319577193279965\n",
      "Epoch: 4 - Batch: 479, Training Loss: 0.04140622596291958\n",
      "Epoch: 4 - Batch: 480, Training Loss: 0.041488919938431054\n",
      "Epoch: 4 - Batch: 481, Training Loss: 0.041582302846768206\n",
      "Epoch: 4 - Batch: 482, Training Loss: 0.04167044264068256\n",
      "Epoch: 4 - Batch: 483, Training Loss: 0.04176144557657527\n",
      "Epoch: 4 - Batch: 484, Training Loss: 0.04183854699554926\n",
      "Epoch: 4 - Batch: 485, Training Loss: 0.04192110635401993\n",
      "Epoch: 4 - Batch: 486, Training Loss: 0.04201299105933057\n",
      "Epoch: 4 - Batch: 487, Training Loss: 0.04209584343072589\n",
      "Epoch: 4 - Batch: 488, Training Loss: 0.04217408010566215\n",
      "Epoch: 4 - Batch: 489, Training Loss: 0.04226489209417087\n",
      "Epoch: 4 - Batch: 490, Training Loss: 0.0423500607126882\n",
      "Epoch: 4 - Batch: 491, Training Loss: 0.042440979789392666\n",
      "Epoch: 4 - Batch: 492, Training Loss: 0.042521608743205\n",
      "Epoch: 4 - Batch: 493, Training Loss: 0.04261336647357118\n",
      "Epoch: 4 - Batch: 494, Training Loss: 0.042702856262030686\n",
      "Epoch: 4 - Batch: 495, Training Loss: 0.04278576199836399\n",
      "Epoch: 4 - Batch: 496, Training Loss: 0.0428719951108617\n",
      "Epoch: 4 - Batch: 497, Training Loss: 0.04296010651102113\n",
      "Epoch: 4 - Batch: 498, Training Loss: 0.0430528625944756\n",
      "Epoch: 4 - Batch: 499, Training Loss: 0.04314242943404721\n",
      "Epoch: 4 - Batch: 500, Training Loss: 0.04323284230990394\n",
      "Epoch: 4 - Batch: 501, Training Loss: 0.04331334001364597\n",
      "Epoch: 4 - Batch: 502, Training Loss: 0.043397417431002235\n",
      "Epoch: 4 - Batch: 503, Training Loss: 0.043479743980452\n",
      "Epoch: 4 - Batch: 504, Training Loss: 0.043568826655843364\n",
      "Epoch: 4 - Batch: 505, Training Loss: 0.04365367662921474\n",
      "Epoch: 4 - Batch: 506, Training Loss: 0.043735240847119446\n",
      "Epoch: 4 - Batch: 507, Training Loss: 0.04381659260609652\n",
      "Epoch: 4 - Batch: 508, Training Loss: 0.04390655257808628\n",
      "Epoch: 4 - Batch: 509, Training Loss: 0.04397510902131375\n",
      "Epoch: 4 - Batch: 510, Training Loss: 0.04407121406043347\n",
      "Epoch: 4 - Batch: 511, Training Loss: 0.04415284527178427\n",
      "Epoch: 4 - Batch: 512, Training Loss: 0.044242749104946605\n",
      "Epoch: 4 - Batch: 513, Training Loss: 0.04434216162112617\n",
      "Epoch: 4 - Batch: 514, Training Loss: 0.044431408579966324\n",
      "Epoch: 4 - Batch: 515, Training Loss: 0.04451675347328977\n",
      "Epoch: 4 - Batch: 516, Training Loss: 0.044599938303677006\n",
      "Epoch: 4 - Batch: 517, Training Loss: 0.044686795763708466\n",
      "Epoch: 4 - Batch: 518, Training Loss: 0.04476266128210286\n",
      "Epoch: 4 - Batch: 519, Training Loss: 0.04485042901650985\n",
      "Epoch: 4 - Batch: 520, Training Loss: 0.04493744397662568\n",
      "Epoch: 4 - Batch: 521, Training Loss: 0.04501826471184221\n",
      "Epoch: 4 - Batch: 522, Training Loss: 0.04509599159655484\n",
      "Epoch: 4 - Batch: 523, Training Loss: 0.0451768038989003\n",
      "Epoch: 4 - Batch: 524, Training Loss: 0.04525807525215062\n",
      "Epoch: 4 - Batch: 525, Training Loss: 0.045341693210858805\n",
      "Epoch: 4 - Batch: 526, Training Loss: 0.04542455248917711\n",
      "Epoch: 4 - Batch: 527, Training Loss: 0.04551088750658937\n",
      "Epoch: 4 - Batch: 528, Training Loss: 0.04559779636697785\n",
      "Epoch: 4 - Batch: 529, Training Loss: 0.04569129224489775\n",
      "Epoch: 4 - Batch: 530, Training Loss: 0.045772046104048815\n",
      "Epoch: 4 - Batch: 531, Training Loss: 0.045870831183730865\n",
      "Epoch: 4 - Batch: 532, Training Loss: 0.04594904598262575\n",
      "Epoch: 4 - Batch: 533, Training Loss: 0.04603467832873907\n",
      "Epoch: 4 - Batch: 534, Training Loss: 0.046117705646686095\n",
      "Epoch: 4 - Batch: 535, Training Loss: 0.04620287819436533\n",
      "Epoch: 4 - Batch: 536, Training Loss: 0.04628534948647912\n",
      "Epoch: 4 - Batch: 537, Training Loss: 0.04636777340624463\n",
      "Epoch: 4 - Batch: 538, Training Loss: 0.0464606717611921\n",
      "Epoch: 4 - Batch: 539, Training Loss: 0.0465380385097975\n",
      "Epoch: 4 - Batch: 540, Training Loss: 0.04662394431569485\n",
      "Epoch: 4 - Batch: 541, Training Loss: 0.04670957751436218\n",
      "Epoch: 4 - Batch: 542, Training Loss: 0.046793362283291506\n",
      "Epoch: 4 - Batch: 543, Training Loss: 0.04688175618772088\n",
      "Epoch: 4 - Batch: 544, Training Loss: 0.04696971526489922\n",
      "Epoch: 4 - Batch: 545, Training Loss: 0.0470527036124794\n",
      "Epoch: 4 - Batch: 546, Training Loss: 0.04713243225073538\n",
      "Epoch: 4 - Batch: 547, Training Loss: 0.047225011694886\n",
      "Epoch: 4 - Batch: 548, Training Loss: 0.047321241904995336\n",
      "Epoch: 4 - Batch: 549, Training Loss: 0.04741560702967407\n",
      "Epoch: 4 - Batch: 550, Training Loss: 0.04750508087908055\n",
      "Epoch: 4 - Batch: 551, Training Loss: 0.04758485123688111\n",
      "Epoch: 4 - Batch: 552, Training Loss: 0.047666359340670095\n",
      "Epoch: 4 - Batch: 553, Training Loss: 0.04774660434246459\n",
      "Epoch: 4 - Batch: 554, Training Loss: 0.04783294152215148\n",
      "Epoch: 4 - Batch: 555, Training Loss: 0.04791845286737627\n",
      "Epoch: 4 - Batch: 556, Training Loss: 0.04801005044732719\n",
      "Epoch: 4 - Batch: 557, Training Loss: 0.048102215000398914\n",
      "Epoch: 4 - Batch: 558, Training Loss: 0.0481902554903062\n",
      "Epoch: 4 - Batch: 559, Training Loss: 0.04827201845213944\n",
      "Epoch: 4 - Batch: 560, Training Loss: 0.04836032577291452\n",
      "Epoch: 4 - Batch: 561, Training Loss: 0.04845469787865136\n",
      "Epoch: 4 - Batch: 562, Training Loss: 0.048545272424693524\n",
      "Epoch: 4 - Batch: 563, Training Loss: 0.048629921949028376\n",
      "Epoch: 4 - Batch: 564, Training Loss: 0.04871311789968516\n",
      "Epoch: 4 - Batch: 565, Training Loss: 0.048813780507143856\n",
      "Epoch: 4 - Batch: 566, Training Loss: 0.04890503391475227\n",
      "Epoch: 4 - Batch: 567, Training Loss: 0.04898333045998418\n",
      "Epoch: 4 - Batch: 568, Training Loss: 0.04906679473656722\n",
      "Epoch: 4 - Batch: 569, Training Loss: 0.04914900784691175\n",
      "Epoch: 4 - Batch: 570, Training Loss: 0.04923958778628465\n",
      "Epoch: 4 - Batch: 571, Training Loss: 0.04931812079787057\n",
      "Epoch: 4 - Batch: 572, Training Loss: 0.04939899885189869\n",
      "Epoch: 4 - Batch: 573, Training Loss: 0.049487408510043254\n",
      "Epoch: 4 - Batch: 574, Training Loss: 0.04957645862775655\n",
      "Epoch: 4 - Batch: 575, Training Loss: 0.04965817415546224\n",
      "Epoch: 4 - Batch: 576, Training Loss: 0.04974820477205327\n",
      "Epoch: 4 - Batch: 577, Training Loss: 0.04984169490450057\n",
      "Epoch: 4 - Batch: 578, Training Loss: 0.04993183196678288\n",
      "Epoch: 4 - Batch: 579, Training Loss: 0.05002199254225736\n",
      "Epoch: 4 - Batch: 580, Training Loss: 0.05010500492815355\n",
      "Epoch: 4 - Batch: 581, Training Loss: 0.050198565328348536\n",
      "Epoch: 4 - Batch: 582, Training Loss: 0.05028844354782334\n",
      "Epoch: 4 - Batch: 583, Training Loss: 0.0503830530218916\n",
      "Epoch: 4 - Batch: 584, Training Loss: 0.050467340093396394\n",
      "Epoch: 4 - Batch: 585, Training Loss: 0.05055134635956133\n",
      "Epoch: 4 - Batch: 586, Training Loss: 0.05063576948756405\n",
      "Epoch: 4 - Batch: 587, Training Loss: 0.050717389751627276\n",
      "Epoch: 4 - Batch: 588, Training Loss: 0.05080619007101897\n",
      "Epoch: 4 - Batch: 589, Training Loss: 0.050886884659133345\n",
      "Epoch: 4 - Batch: 590, Training Loss: 0.05096580077018311\n",
      "Epoch: 4 - Batch: 591, Training Loss: 0.05105684155205984\n",
      "Epoch: 4 - Batch: 592, Training Loss: 0.0511526911462322\n",
      "Epoch: 4 - Batch: 593, Training Loss: 0.0512498368448879\n",
      "Epoch: 4 - Batch: 594, Training Loss: 0.051329969562849596\n",
      "Epoch: 4 - Batch: 595, Training Loss: 0.05141965644573098\n",
      "Epoch: 4 - Batch: 596, Training Loss: 0.051501366011736605\n",
      "Epoch: 4 - Batch: 597, Training Loss: 0.05158486956140493\n",
      "Epoch: 4 - Batch: 598, Training Loss: 0.051674682541965056\n",
      "Epoch: 4 - Batch: 599, Training Loss: 0.051756335357528424\n",
      "Epoch: 4 - Batch: 600, Training Loss: 0.05184130801218462\n",
      "Epoch: 4 - Batch: 601, Training Loss: 0.05192144550568429\n",
      "Epoch: 4 - Batch: 602, Training Loss: 0.05200642686504037\n",
      "Epoch: 4 - Batch: 603, Training Loss: 0.05208525656541781\n",
      "Epoch: 4 - Batch: 604, Training Loss: 0.052170909737324835\n",
      "Epoch: 4 - Batch: 605, Training Loss: 0.05225726405458268\n",
      "Epoch: 4 - Batch: 606, Training Loss: 0.0523381530373646\n",
      "Epoch: 4 - Batch: 607, Training Loss: 0.05243459507007504\n",
      "Epoch: 4 - Batch: 608, Training Loss: 0.05251252802573824\n",
      "Epoch: 4 - Batch: 609, Training Loss: 0.05260148782832903\n",
      "Epoch: 4 - Batch: 610, Training Loss: 0.05269918892266936\n",
      "Epoch: 4 - Batch: 611, Training Loss: 0.05278255797267751\n",
      "Epoch: 4 - Batch: 612, Training Loss: 0.05286846675015801\n",
      "Epoch: 4 - Batch: 613, Training Loss: 0.05295198059843152\n",
      "Epoch: 4 - Batch: 614, Training Loss: 0.05304388084740781\n",
      "Epoch: 4 - Batch: 615, Training Loss: 0.05312141096512872\n",
      "Epoch: 4 - Batch: 616, Training Loss: 0.05320758711095671\n",
      "Epoch: 4 - Batch: 617, Training Loss: 0.053293377877665596\n",
      "Epoch: 4 - Batch: 618, Training Loss: 0.05337969405857682\n",
      "Epoch: 4 - Batch: 619, Training Loss: 0.05346145872551806\n",
      "Epoch: 4 - Batch: 620, Training Loss: 0.053552010672701336\n",
      "Epoch: 4 - Batch: 621, Training Loss: 0.053631394187213965\n",
      "Epoch: 4 - Batch: 622, Training Loss: 0.05371305326101792\n",
      "Epoch: 4 - Batch: 623, Training Loss: 0.05380026365987104\n",
      "Epoch: 4 - Batch: 624, Training Loss: 0.05389073879688138\n",
      "Epoch: 4 - Batch: 625, Training Loss: 0.05397931672941591\n",
      "Epoch: 4 - Batch: 626, Training Loss: 0.0540627394285467\n",
      "Epoch: 4 - Batch: 627, Training Loss: 0.054148629907005856\n",
      "Epoch: 4 - Batch: 628, Training Loss: 0.054235670338716874\n",
      "Epoch: 4 - Batch: 629, Training Loss: 0.05431296279156584\n",
      "Epoch: 4 - Batch: 630, Training Loss: 0.054413240748900873\n",
      "Epoch: 4 - Batch: 631, Training Loss: 0.054487172236193475\n",
      "Epoch: 4 - Batch: 632, Training Loss: 0.054577471407294076\n",
      "Epoch: 4 - Batch: 633, Training Loss: 0.054664546299484834\n",
      "Epoch: 4 - Batch: 634, Training Loss: 0.054760281274567786\n",
      "Epoch: 4 - Batch: 635, Training Loss: 0.054844219237565994\n",
      "Epoch: 4 - Batch: 636, Training Loss: 0.05492627691125395\n",
      "Epoch: 4 - Batch: 637, Training Loss: 0.05500383167890569\n",
      "Epoch: 4 - Batch: 638, Training Loss: 0.05508515103465289\n",
      "Epoch: 4 - Batch: 639, Training Loss: 0.05517239111226985\n",
      "Epoch: 4 - Batch: 640, Training Loss: 0.05525872392416198\n",
      "Epoch: 4 - Batch: 641, Training Loss: 0.055340833961963654\n",
      "Epoch: 4 - Batch: 642, Training Loss: 0.05542196526159694\n",
      "Epoch: 4 - Batch: 643, Training Loss: 0.0555065876524741\n",
      "Epoch: 4 - Batch: 644, Training Loss: 0.05559680848127574\n",
      "Epoch: 4 - Batch: 645, Training Loss: 0.05567309999248479\n",
      "Epoch: 4 - Batch: 646, Training Loss: 0.05577010622442658\n",
      "Epoch: 4 - Batch: 647, Training Loss: 0.05585105639340272\n",
      "Epoch: 4 - Batch: 648, Training Loss: 0.05593403105687344\n",
      "Epoch: 4 - Batch: 649, Training Loss: 0.05601318215775253\n",
      "Epoch: 4 - Batch: 650, Training Loss: 0.05610341306275396\n",
      "Epoch: 4 - Batch: 651, Training Loss: 0.056191940904652106\n",
      "Epoch: 4 - Batch: 652, Training Loss: 0.05628212324496525\n",
      "Epoch: 4 - Batch: 653, Training Loss: 0.056367580341808436\n",
      "Epoch: 4 - Batch: 654, Training Loss: 0.05645377770263955\n",
      "Epoch: 4 - Batch: 655, Training Loss: 0.05654393187160318\n",
      "Epoch: 4 - Batch: 656, Training Loss: 0.056636129698992566\n",
      "Epoch: 4 - Batch: 657, Training Loss: 0.056720227769160546\n",
      "Epoch: 4 - Batch: 658, Training Loss: 0.05680616133248628\n",
      "Epoch: 4 - Batch: 659, Training Loss: 0.05689119936197156\n",
      "Epoch: 4 - Batch: 660, Training Loss: 0.05697584341884055\n",
      "Epoch: 4 - Batch: 661, Training Loss: 0.05705807248059395\n",
      "Epoch: 4 - Batch: 662, Training Loss: 0.05714581301358604\n",
      "Epoch: 4 - Batch: 663, Training Loss: 0.05723742262196185\n",
      "Epoch: 4 - Batch: 664, Training Loss: 0.057323465559316514\n",
      "Epoch: 4 - Batch: 665, Training Loss: 0.05741504996792594\n",
      "Epoch: 4 - Batch: 666, Training Loss: 0.0574958993687262\n",
      "Epoch: 4 - Batch: 667, Training Loss: 0.05758732895229389\n",
      "Epoch: 4 - Batch: 668, Training Loss: 0.05767916823748135\n",
      "Epoch: 4 - Batch: 669, Training Loss: 0.05776739129132497\n",
      "Epoch: 4 - Batch: 670, Training Loss: 0.057856295276340564\n",
      "Epoch: 4 - Batch: 671, Training Loss: 0.05793365836761286\n",
      "Epoch: 4 - Batch: 672, Training Loss: 0.05801928433810498\n",
      "Epoch: 4 - Batch: 673, Training Loss: 0.058106618894767606\n",
      "Epoch: 4 - Batch: 674, Training Loss: 0.058191174823500426\n",
      "Epoch: 4 - Batch: 675, Training Loss: 0.05827622008116091\n",
      "Epoch: 4 - Batch: 676, Training Loss: 0.05836570839309574\n",
      "Epoch: 4 - Batch: 677, Training Loss: 0.05844239375954046\n",
      "Epoch: 4 - Batch: 678, Training Loss: 0.05852997060216481\n",
      "Epoch: 4 - Batch: 679, Training Loss: 0.058614182506825395\n",
      "Epoch: 4 - Batch: 680, Training Loss: 0.058691028536453374\n",
      "Epoch: 4 - Batch: 681, Training Loss: 0.058781366869436566\n",
      "Epoch: 4 - Batch: 682, Training Loss: 0.058867676489388766\n",
      "Epoch: 4 - Batch: 683, Training Loss: 0.05895606351160687\n",
      "Epoch: 4 - Batch: 684, Training Loss: 0.05904807999211164\n",
      "Epoch: 4 - Batch: 685, Training Loss: 0.05912904959684778\n",
      "Epoch: 4 - Batch: 686, Training Loss: 0.05921596632791593\n",
      "Epoch: 4 - Batch: 687, Training Loss: 0.05930843476088683\n",
      "Epoch: 4 - Batch: 688, Training Loss: 0.059390353968685146\n",
      "Epoch: 4 - Batch: 689, Training Loss: 0.059476045301650494\n",
      "Epoch: 4 - Batch: 690, Training Loss: 0.059558600459130445\n",
      "Epoch: 4 - Batch: 691, Training Loss: 0.05964012280922031\n",
      "Epoch: 4 - Batch: 692, Training Loss: 0.05972705900076017\n",
      "Epoch: 4 - Batch: 693, Training Loss: 0.05981466557157178\n",
      "Epoch: 4 - Batch: 694, Training Loss: 0.059903840884986403\n",
      "Epoch: 4 - Batch: 695, Training Loss: 0.05999552032247705\n",
      "Epoch: 4 - Batch: 696, Training Loss: 0.06008390206256711\n",
      "Epoch: 4 - Batch: 697, Training Loss: 0.060167547370960466\n",
      "Epoch: 4 - Batch: 698, Training Loss: 0.06025735326543771\n",
      "Epoch: 4 - Batch: 699, Training Loss: 0.06034218320996805\n",
      "Epoch: 4 - Batch: 700, Training Loss: 0.060430393290163864\n",
      "Epoch: 4 - Batch: 701, Training Loss: 0.06052041295328939\n",
      "Epoch: 4 - Batch: 702, Training Loss: 0.06060648882517569\n",
      "Epoch: 4 - Batch: 703, Training Loss: 0.06069332453890226\n",
      "Epoch: 4 - Batch: 704, Training Loss: 0.06077512492044253\n",
      "Epoch: 4 - Batch: 705, Training Loss: 0.06085582037964468\n",
      "Epoch: 4 - Batch: 706, Training Loss: 0.060936266973392286\n",
      "Epoch: 4 - Batch: 707, Training Loss: 0.061024963305907856\n",
      "Epoch: 4 - Batch: 708, Training Loss: 0.06111021058179846\n",
      "Epoch: 4 - Batch: 709, Training Loss: 0.06119537059446275\n",
      "Epoch: 4 - Batch: 710, Training Loss: 0.06128824310096145\n",
      "Epoch: 4 - Batch: 711, Training Loss: 0.06137034114740579\n",
      "Epoch: 4 - Batch: 712, Training Loss: 0.06145895423852587\n",
      "Epoch: 4 - Batch: 713, Training Loss: 0.0615336010556909\n",
      "Epoch: 4 - Batch: 714, Training Loss: 0.06163036216600222\n",
      "Epoch: 4 - Batch: 715, Training Loss: 0.0617215936605017\n",
      "Epoch: 4 - Batch: 716, Training Loss: 0.06180945086365513\n",
      "Epoch: 4 - Batch: 717, Training Loss: 0.06189737030942839\n",
      "Epoch: 4 - Batch: 718, Training Loss: 0.061984658562524206\n",
      "Epoch: 4 - Batch: 719, Training Loss: 0.0620799292787094\n",
      "Epoch: 4 - Batch: 720, Training Loss: 0.06216488090987822\n",
      "Epoch: 4 - Batch: 721, Training Loss: 0.06226230437433344\n",
      "Epoch: 4 - Batch: 722, Training Loss: 0.062345935708254724\n",
      "Epoch: 4 - Batch: 723, Training Loss: 0.06243464869028498\n",
      "Epoch: 4 - Batch: 724, Training Loss: 0.06251697764294856\n",
      "Epoch: 4 - Batch: 725, Training Loss: 0.06259584945208002\n",
      "Epoch: 4 - Batch: 726, Training Loss: 0.06267671205511141\n",
      "Epoch: 4 - Batch: 727, Training Loss: 0.06276068349012096\n",
      "Epoch: 4 - Batch: 728, Training Loss: 0.06284990961081155\n",
      "Epoch: 4 - Batch: 729, Training Loss: 0.06293542844368451\n",
      "Epoch: 4 - Batch: 730, Training Loss: 0.06302148919818215\n",
      "Epoch: 4 - Batch: 731, Training Loss: 0.06310142431513191\n",
      "Epoch: 4 - Batch: 732, Training Loss: 0.06318864158384045\n",
      "Epoch: 4 - Batch: 733, Training Loss: 0.0632765452697206\n",
      "Epoch: 4 - Batch: 734, Training Loss: 0.06337147017023456\n",
      "Epoch: 4 - Batch: 735, Training Loss: 0.06345927682558498\n",
      "Epoch: 4 - Batch: 736, Training Loss: 0.06356218681480755\n",
      "Epoch: 4 - Batch: 737, Training Loss: 0.06364532156073632\n",
      "Epoch: 4 - Batch: 738, Training Loss: 0.06373778468686747\n",
      "Epoch: 4 - Batch: 739, Training Loss: 0.06382656254949261\n",
      "Epoch: 4 - Batch: 740, Training Loss: 0.06392168269722813\n",
      "Epoch: 4 - Batch: 741, Training Loss: 0.06400337681223701\n",
      "Epoch: 4 - Batch: 742, Training Loss: 0.06409141486804086\n",
      "Epoch: 4 - Batch: 743, Training Loss: 0.06417292641293548\n",
      "Epoch: 4 - Batch: 744, Training Loss: 0.06426201241205186\n",
      "Epoch: 4 - Batch: 745, Training Loss: 0.06434363488781314\n",
      "Epoch: 4 - Batch: 746, Training Loss: 0.06442219109751691\n",
      "Epoch: 4 - Batch: 747, Training Loss: 0.06450853755645096\n",
      "Epoch: 4 - Batch: 748, Training Loss: 0.0645941346265981\n",
      "Epoch: 4 - Batch: 749, Training Loss: 0.06468539271707559\n",
      "Epoch: 4 - Batch: 750, Training Loss: 0.06477910316677433\n",
      "Epoch: 4 - Batch: 751, Training Loss: 0.06486743257981825\n",
      "Epoch: 4 - Batch: 752, Training Loss: 0.06495856597500654\n",
      "Epoch: 4 - Batch: 753, Training Loss: 0.0650419198767364\n",
      "Epoch: 4 - Batch: 754, Training Loss: 0.06512685490781395\n",
      "Epoch: 4 - Batch: 755, Training Loss: 0.0652135608727066\n",
      "Epoch: 4 - Batch: 756, Training Loss: 0.06530098098748754\n",
      "Epoch: 4 - Batch: 757, Training Loss: 0.06539427047938257\n",
      "Epoch: 4 - Batch: 758, Training Loss: 0.0654735800626365\n",
      "Epoch: 4 - Batch: 759, Training Loss: 0.06556009917611111\n",
      "Epoch: 4 - Batch: 760, Training Loss: 0.06564646877138373\n",
      "Epoch: 4 - Batch: 761, Training Loss: 0.06573905589469828\n",
      "Epoch: 4 - Batch: 762, Training Loss: 0.06582258674462833\n",
      "Epoch: 4 - Batch: 763, Training Loss: 0.06591167333682575\n",
      "Epoch: 4 - Batch: 764, Training Loss: 0.06599280598942518\n",
      "Epoch: 4 - Batch: 765, Training Loss: 0.06607362125717585\n",
      "Epoch: 4 - Batch: 766, Training Loss: 0.06616370800023846\n",
      "Epoch: 4 - Batch: 767, Training Loss: 0.06624803748321573\n",
      "Epoch: 4 - Batch: 768, Training Loss: 0.06633329299428373\n",
      "Epoch: 4 - Batch: 769, Training Loss: 0.06641782580521174\n",
      "Epoch: 4 - Batch: 770, Training Loss: 0.06651658782269033\n",
      "Epoch: 4 - Batch: 771, Training Loss: 0.06660926041145428\n",
      "Epoch: 4 - Batch: 772, Training Loss: 0.0666940450408862\n",
      "Epoch: 4 - Batch: 773, Training Loss: 0.06677519841425454\n",
      "Epoch: 4 - Batch: 774, Training Loss: 0.0668606307499642\n",
      "Epoch: 4 - Batch: 775, Training Loss: 0.0669391287450569\n",
      "Epoch: 4 - Batch: 776, Training Loss: 0.06703126199456985\n",
      "Epoch: 4 - Batch: 777, Training Loss: 0.06711864834697685\n",
      "Epoch: 4 - Batch: 778, Training Loss: 0.06720273210288676\n",
      "Epoch: 4 - Batch: 779, Training Loss: 0.06729247741042876\n",
      "Epoch: 4 - Batch: 780, Training Loss: 0.06737945602579694\n",
      "Epoch: 4 - Batch: 781, Training Loss: 0.0674690628489155\n",
      "Epoch: 4 - Batch: 782, Training Loss: 0.06755424242707628\n",
      "Epoch: 4 - Batch: 783, Training Loss: 0.06763643804153004\n",
      "Epoch: 4 - Batch: 784, Training Loss: 0.0677283593325275\n",
      "Epoch: 4 - Batch: 785, Training Loss: 0.06781383260936286\n",
      "Epoch: 4 - Batch: 786, Training Loss: 0.06790994959041649\n",
      "Epoch: 4 - Batch: 787, Training Loss: 0.06799691391006038\n",
      "Epoch: 4 - Batch: 788, Training Loss: 0.06808524182186791\n",
      "Epoch: 4 - Batch: 789, Training Loss: 0.06817507837739947\n",
      "Epoch: 4 - Batch: 790, Training Loss: 0.06826405290816949\n",
      "Epoch: 4 - Batch: 791, Training Loss: 0.06834796176695113\n",
      "Epoch: 4 - Batch: 792, Training Loss: 0.06843637802930021\n",
      "Epoch: 4 - Batch: 793, Training Loss: 0.06852371093027824\n",
      "Epoch: 4 - Batch: 794, Training Loss: 0.06860907333802624\n",
      "Epoch: 4 - Batch: 795, Training Loss: 0.0686994480466467\n",
      "Epoch: 4 - Batch: 796, Training Loss: 0.06879129806907221\n",
      "Epoch: 4 - Batch: 797, Training Loss: 0.06887949753558853\n",
      "Epoch: 4 - Batch: 798, Training Loss: 0.06896650073293034\n",
      "Epoch: 4 - Batch: 799, Training Loss: 0.06905071705480911\n",
      "Epoch: 4 - Batch: 800, Training Loss: 0.06914583870378102\n",
      "Epoch: 4 - Batch: 801, Training Loss: 0.06923325363528077\n",
      "Epoch: 4 - Batch: 802, Training Loss: 0.06931864641372046\n",
      "Epoch: 4 - Batch: 803, Training Loss: 0.06940873184706244\n",
      "Epoch: 4 - Batch: 804, Training Loss: 0.06949073162739154\n",
      "Epoch: 4 - Batch: 805, Training Loss: 0.06958041424132501\n",
      "Epoch: 4 - Batch: 806, Training Loss: 0.06967171030752299\n",
      "Epoch: 4 - Batch: 807, Training Loss: 0.06975846158133613\n",
      "Epoch: 4 - Batch: 808, Training Loss: 0.06984338754321608\n",
      "Epoch: 4 - Batch: 809, Training Loss: 0.06993109545057291\n",
      "Epoch: 4 - Batch: 810, Training Loss: 0.07002471058720577\n",
      "Epoch: 4 - Batch: 811, Training Loss: 0.0701165638901108\n",
      "Epoch: 4 - Batch: 812, Training Loss: 0.07020236819265889\n",
      "Epoch: 4 - Batch: 813, Training Loss: 0.07029753039972501\n",
      "Epoch: 4 - Batch: 814, Training Loss: 0.07038377340926262\n",
      "Epoch: 4 - Batch: 815, Training Loss: 0.07047553243081565\n",
      "Epoch: 4 - Batch: 816, Training Loss: 0.07056513844462572\n",
      "Epoch: 4 - Batch: 817, Training Loss: 0.07065006612396951\n",
      "Epoch: 4 - Batch: 818, Training Loss: 0.07074593021773778\n",
      "Epoch: 4 - Batch: 819, Training Loss: 0.07083021373693425\n",
      "Epoch: 4 - Batch: 820, Training Loss: 0.07091997097405431\n",
      "Epoch: 4 - Batch: 821, Training Loss: 0.07100542341274013\n",
      "Epoch: 4 - Batch: 822, Training Loss: 0.07109163125751426\n",
      "Epoch: 4 - Batch: 823, Training Loss: 0.0711733826604451\n",
      "Epoch: 4 - Batch: 824, Training Loss: 0.07125797330913061\n",
      "Epoch: 4 - Batch: 825, Training Loss: 0.07134561050392897\n",
      "Epoch: 4 - Batch: 826, Training Loss: 0.07142982856180538\n",
      "Epoch: 4 - Batch: 827, Training Loss: 0.07151416833103197\n",
      "Epoch: 4 - Batch: 828, Training Loss: 0.07160650052834507\n",
      "Epoch: 4 - Batch: 829, Training Loss: 0.07168882500201118\n",
      "Epoch: 4 - Batch: 830, Training Loss: 0.07177212688559126\n",
      "Epoch: 4 - Batch: 831, Training Loss: 0.07185619383753829\n",
      "Epoch: 4 - Batch: 832, Training Loss: 0.07193768362377216\n",
      "Epoch: 4 - Batch: 833, Training Loss: 0.07202666975669007\n",
      "Epoch: 4 - Batch: 834, Training Loss: 0.07211667917063383\n",
      "Epoch: 4 - Batch: 835, Training Loss: 0.07220204057755755\n",
      "Epoch: 4 - Batch: 836, Training Loss: 0.07229846282236611\n",
      "Epoch: 4 - Batch: 837, Training Loss: 0.07239001197972107\n",
      "Epoch: 4 - Batch: 838, Training Loss: 0.07247586086837214\n",
      "Epoch: 4 - Batch: 839, Training Loss: 0.07256773418158441\n",
      "Epoch: 4 - Batch: 840, Training Loss: 0.07265463449789912\n",
      "Epoch: 4 - Batch: 841, Training Loss: 0.07273849129528548\n",
      "Epoch: 4 - Batch: 842, Training Loss: 0.07282974128032205\n",
      "Epoch: 4 - Batch: 843, Training Loss: 0.07292642010681664\n",
      "Epoch: 4 - Batch: 844, Training Loss: 0.0730082381942972\n",
      "Epoch: 4 - Batch: 845, Training Loss: 0.07309278438661625\n",
      "Epoch: 4 - Batch: 846, Training Loss: 0.07317419140097711\n",
      "Epoch: 4 - Batch: 847, Training Loss: 0.07325709207758777\n",
      "Epoch: 4 - Batch: 848, Training Loss: 0.0733391813019516\n",
      "Epoch: 4 - Batch: 849, Training Loss: 0.07342219823480245\n",
      "Epoch: 4 - Batch: 850, Training Loss: 0.07352572598588802\n",
      "Epoch: 4 - Batch: 851, Training Loss: 0.07361001085805062\n",
      "Epoch: 4 - Batch: 852, Training Loss: 0.07368710961433786\n",
      "Epoch: 4 - Batch: 853, Training Loss: 0.07377630588328266\n",
      "Epoch: 4 - Batch: 854, Training Loss: 0.0738593428140849\n",
      "Epoch: 4 - Batch: 855, Training Loss: 0.07394820367098844\n",
      "Epoch: 4 - Batch: 856, Training Loss: 0.07402617713532242\n",
      "Epoch: 4 - Batch: 857, Training Loss: 0.07411709606375663\n",
      "Epoch: 4 - Batch: 858, Training Loss: 0.07420112377350802\n",
      "Epoch: 4 - Batch: 859, Training Loss: 0.07428746318342674\n",
      "Epoch: 4 - Batch: 860, Training Loss: 0.07437300626466524\n",
      "Epoch: 4 - Batch: 861, Training Loss: 0.074463818419978\n",
      "Epoch: 4 - Batch: 862, Training Loss: 0.07455406058116339\n",
      "Epoch: 4 - Batch: 863, Training Loss: 0.07463947595846199\n",
      "Epoch: 4 - Batch: 864, Training Loss: 0.07472445167737023\n",
      "Epoch: 4 - Batch: 865, Training Loss: 0.07481519665389907\n",
      "Epoch: 4 - Batch: 866, Training Loss: 0.07489938273482259\n",
      "Epoch: 4 - Batch: 867, Training Loss: 0.0749883987380201\n",
      "Epoch: 4 - Batch: 868, Training Loss: 0.07507524302053215\n",
      "Epoch: 4 - Batch: 869, Training Loss: 0.07514648673867508\n",
      "Epoch: 4 - Batch: 870, Training Loss: 0.07523471595191245\n",
      "Epoch: 4 - Batch: 871, Training Loss: 0.07532135723390389\n",
      "Epoch: 4 - Batch: 872, Training Loss: 0.07540743967292717\n",
      "Epoch: 4 - Batch: 873, Training Loss: 0.07549786670859378\n",
      "Epoch: 4 - Batch: 874, Training Loss: 0.075585105229373\n",
      "Epoch: 4 - Batch: 875, Training Loss: 0.07567459228101062\n",
      "Epoch: 4 - Batch: 876, Training Loss: 0.07577060024017718\n",
      "Epoch: 4 - Batch: 877, Training Loss: 0.07586008957147006\n",
      "Epoch: 4 - Batch: 878, Training Loss: 0.07595205224222606\n",
      "Epoch: 4 - Batch: 879, Training Loss: 0.07603730904448092\n",
      "Epoch: 4 - Batch: 880, Training Loss: 0.07611374874563755\n",
      "Epoch: 4 - Batch: 881, Training Loss: 0.07619960349714183\n",
      "Epoch: 4 - Batch: 882, Training Loss: 0.0762861922667493\n",
      "Epoch: 4 - Batch: 883, Training Loss: 0.07636693410983134\n",
      "Epoch: 4 - Batch: 884, Training Loss: 0.07645634907123264\n",
      "Epoch: 4 - Batch: 885, Training Loss: 0.07653774107940754\n",
      "Epoch: 4 - Batch: 886, Training Loss: 0.07662491839545876\n",
      "Epoch: 4 - Batch: 887, Training Loss: 0.076731027019014\n",
      "Epoch: 4 - Batch: 888, Training Loss: 0.07681389684923252\n",
      "Epoch: 4 - Batch: 889, Training Loss: 0.0768956280379845\n",
      "Epoch: 4 - Batch: 890, Training Loss: 0.07698743157150535\n",
      "Epoch: 4 - Batch: 891, Training Loss: 0.0770785174893206\n",
      "Epoch: 4 - Batch: 892, Training Loss: 0.07716206521117075\n",
      "Epoch: 4 - Batch: 893, Training Loss: 0.0772454042549849\n",
      "Epoch: 4 - Batch: 894, Training Loss: 0.07732888093684641\n",
      "Epoch: 4 - Batch: 895, Training Loss: 0.07741366165565021\n",
      "Epoch: 4 - Batch: 896, Training Loss: 0.07750084917145977\n",
      "Epoch: 4 - Batch: 897, Training Loss: 0.07759380943541898\n",
      "Epoch: 4 - Batch: 898, Training Loss: 0.07767914785427438\n",
      "Epoch: 4 - Batch: 899, Training Loss: 0.07776314802900278\n",
      "Epoch: 4 - Batch: 900, Training Loss: 0.07785307798862062\n",
      "Epoch: 4 - Batch: 901, Training Loss: 0.07793338838995591\n",
      "Epoch: 4 - Batch: 902, Training Loss: 0.07802457514390423\n",
      "Epoch: 4 - Batch: 903, Training Loss: 0.07811549804474584\n",
      "Epoch: 4 - Batch: 904, Training Loss: 0.07820490271371988\n",
      "Epoch: 4 - Batch: 905, Training Loss: 0.07828466972926165\n",
      "Epoch: 4 - Batch: 906, Training Loss: 0.07837627991218472\n",
      "Epoch: 4 - Batch: 907, Training Loss: 0.07846421084890318\n",
      "Epoch: 4 - Batch: 908, Training Loss: 0.07854728102931138\n",
      "Epoch: 4 - Batch: 909, Training Loss: 0.07864165526025528\n",
      "Epoch: 4 - Batch: 910, Training Loss: 0.07871904733218561\n",
      "Epoch: 4 - Batch: 911, Training Loss: 0.07881166567874587\n",
      "Epoch: 4 - Batch: 912, Training Loss: 0.07889460229853888\n",
      "Epoch: 4 - Batch: 913, Training Loss: 0.07898785875705541\n",
      "Epoch: 4 - Batch: 914, Training Loss: 0.07907224867251975\n",
      "Epoch: 4 - Batch: 915, Training Loss: 0.07915591003734675\n",
      "Epoch: 4 - Batch: 916, Training Loss: 0.0792472391801687\n",
      "Epoch: 4 - Batch: 917, Training Loss: 0.07933809694094247\n",
      "Epoch: 4 - Batch: 918, Training Loss: 0.0794250590488883\n",
      "Epoch: 4 - Batch: 919, Training Loss: 0.07951572208756436\n",
      "Epoch: 4 - Batch: 920, Training Loss: 0.07959736620460577\n",
      "Epoch: 4 - Batch: 921, Training Loss: 0.0796810690542161\n",
      "Epoch: 4 - Batch: 922, Training Loss: 0.0797673767466549\n",
      "Epoch: 4 - Batch: 923, Training Loss: 0.07985725193894522\n",
      "Epoch: 4 - Batch: 924, Training Loss: 0.07994355371292947\n",
      "Epoch: 4 - Batch: 925, Training Loss: 0.08002306742726471\n",
      "Epoch: 4 - Batch: 926, Training Loss: 0.08010579963179172\n",
      "Epoch: 4 - Batch: 927, Training Loss: 0.0801942568290886\n",
      "Epoch: 4 - Batch: 928, Training Loss: 0.08028220169703364\n",
      "Epoch: 4 - Batch: 929, Training Loss: 0.0803670704080592\n",
      "Epoch: 4 - Batch: 930, Training Loss: 0.08044875123434597\n",
      "Epoch: 4 - Batch: 931, Training Loss: 0.08053329168737033\n",
      "Epoch: 4 - Batch: 932, Training Loss: 0.08061384567203213\n",
      "Epoch: 4 - Batch: 933, Training Loss: 0.08069428596429366\n",
      "Epoch: 4 - Batch: 934, Training Loss: 0.08077442265466275\n",
      "Epoch: 4 - Batch: 935, Training Loss: 0.08086250318371835\n",
      "Epoch: 4 - Batch: 936, Training Loss: 0.08094729672789969\n",
      "Epoch: 4 - Batch: 937, Training Loss: 0.0810345649150869\n",
      "Epoch: 4 - Batch: 938, Training Loss: 0.08112669847349623\n",
      "Epoch: 4 - Batch: 939, Training Loss: 0.08121726639062216\n",
      "Epoch: 4 - Batch: 940, Training Loss: 0.08130390860548067\n",
      "Epoch: 4 - Batch: 941, Training Loss: 0.0813931893822961\n",
      "Epoch: 4 - Batch: 942, Training Loss: 0.08148584695251822\n",
      "Epoch: 4 - Batch: 943, Training Loss: 0.0815739580499592\n",
      "Epoch: 4 - Batch: 944, Training Loss: 0.08165956777867986\n",
      "Epoch: 4 - Batch: 945, Training Loss: 0.08174605988843327\n",
      "Epoch: 4 - Batch: 946, Training Loss: 0.08183649638250692\n",
      "Epoch: 4 - Batch: 947, Training Loss: 0.08192504345629345\n",
      "Epoch: 4 - Batch: 948, Training Loss: 0.08202189792126763\n",
      "Epoch: 4 - Batch: 949, Training Loss: 0.08210849669293978\n",
      "Epoch: 4 - Batch: 950, Training Loss: 0.08219338175693951\n",
      "Epoch: 4 - Batch: 951, Training Loss: 0.0822782782192155\n",
      "Epoch: 4 - Batch: 952, Training Loss: 0.08236355204783861\n",
      "Epoch: 4 - Batch: 953, Training Loss: 0.08245279769695814\n",
      "Epoch: 4 - Batch: 954, Training Loss: 0.0825411540766557\n",
      "Epoch: 4 - Batch: 955, Training Loss: 0.08262917736721276\n",
      "Epoch: 4 - Batch: 956, Training Loss: 0.08270652015163728\n",
      "Epoch: 4 - Batch: 957, Training Loss: 0.08279391096450796\n",
      "Epoch: 4 - Batch: 958, Training Loss: 0.08287531917738677\n",
      "Epoch: 4 - Batch: 959, Training Loss: 0.0829623777227813\n",
      "Epoch: 4 - Batch: 960, Training Loss: 0.08304819399815294\n",
      "Epoch: 4 - Batch: 961, Training Loss: 0.08313234579775662\n",
      "Epoch: 4 - Batch: 962, Training Loss: 0.08322099245355695\n",
      "Epoch: 4 - Batch: 963, Training Loss: 0.08330538539002784\n",
      "Epoch: 4 - Batch: 964, Training Loss: 0.08338967482767888\n",
      "Epoch: 4 - Batch: 965, Training Loss: 0.08348332918179568\n",
      "Epoch: 4 - Batch: 966, Training Loss: 0.08357389315740386\n",
      "Epoch: 4 - Batch: 967, Training Loss: 0.08365359587307593\n",
      "Epoch: 4 - Batch: 968, Training Loss: 0.08373485556239908\n",
      "Epoch: 4 - Batch: 969, Training Loss: 0.08381982433731679\n",
      "Epoch: 4 - Batch: 970, Training Loss: 0.08390123361897706\n",
      "Epoch: 4 - Batch: 971, Training Loss: 0.08398816389206235\n",
      "Epoch: 4 - Batch: 972, Training Loss: 0.08407709903854438\n",
      "Epoch: 4 - Batch: 973, Training Loss: 0.08416290811045253\n",
      "Epoch: 4 - Batch: 974, Training Loss: 0.08424478357234602\n",
      "Epoch: 4 - Batch: 975, Training Loss: 0.08433687230989115\n",
      "Epoch: 4 - Batch: 976, Training Loss: 0.0844267820738639\n",
      "Epoch: 4 - Batch: 977, Training Loss: 0.08451481688007786\n",
      "Epoch: 4 - Batch: 978, Training Loss: 0.0846067835047075\n",
      "Epoch: 4 - Batch: 979, Training Loss: 0.08468980670197686\n",
      "Epoch: 4 - Batch: 980, Training Loss: 0.08477310097435023\n",
      "Epoch: 4 - Batch: 981, Training Loss: 0.08486467547356391\n",
      "Epoch: 4 - Batch: 982, Training Loss: 0.08496443120450721\n",
      "Epoch: 4 - Batch: 983, Training Loss: 0.08505385325199138\n",
      "Epoch: 4 - Batch: 984, Training Loss: 0.08514359094832667\n",
      "Epoch: 4 - Batch: 985, Training Loss: 0.08524096613845619\n",
      "Epoch: 4 - Batch: 986, Training Loss: 0.085325925868888\n",
      "Epoch: 4 - Batch: 987, Training Loss: 0.08542601307035481\n",
      "Epoch: 4 - Batch: 988, Training Loss: 0.08551037645162042\n",
      "Epoch: 4 - Batch: 989, Training Loss: 0.0855916269914329\n",
      "Epoch: 4 - Batch: 990, Training Loss: 0.08567989875823506\n",
      "Epoch: 4 - Batch: 991, Training Loss: 0.08576942642356823\n",
      "Epoch: 4 - Batch: 992, Training Loss: 0.08585241270772062\n",
      "Epoch: 4 - Batch: 993, Training Loss: 0.08594589905309835\n",
      "Epoch: 4 - Batch: 994, Training Loss: 0.08603356597608397\n",
      "Epoch: 4 - Batch: 995, Training Loss: 0.08612613320029394\n",
      "Epoch: 4 - Batch: 996, Training Loss: 0.08621293887693689\n",
      "Epoch: 4 - Batch: 997, Training Loss: 0.08629823019527282\n",
      "Epoch: 4 - Batch: 998, Training Loss: 0.08638214980364835\n",
      "Epoch: 4 - Batch: 999, Training Loss: 0.08646865878532183\n",
      "Epoch: 4 - Batch: 1000, Training Loss: 0.08655577626196702\n",
      "Epoch: 4 - Batch: 1001, Training Loss: 0.08665128166526309\n",
      "Epoch: 4 - Batch: 1002, Training Loss: 0.08674365870197416\n",
      "Epoch: 4 - Batch: 1003, Training Loss: 0.08683373880376467\n",
      "Epoch: 4 - Batch: 1004, Training Loss: 0.08692124967230097\n",
      "Epoch: 4 - Batch: 1005, Training Loss: 0.08701126857554141\n",
      "Epoch: 4 - Batch: 1006, Training Loss: 0.08709853384674683\n",
      "Epoch: 4 - Batch: 1007, Training Loss: 0.08718831093972595\n",
      "Epoch: 4 - Batch: 1008, Training Loss: 0.08727614764451586\n",
      "Epoch: 4 - Batch: 1009, Training Loss: 0.08735932661822779\n",
      "Epoch: 4 - Batch: 1010, Training Loss: 0.08744852091023578\n",
      "Epoch: 4 - Batch: 1011, Training Loss: 0.08753790425918193\n",
      "Epoch: 4 - Batch: 1012, Training Loss: 0.0876193470252094\n",
      "Epoch: 4 - Batch: 1013, Training Loss: 0.08769894866380921\n",
      "Epoch: 4 - Batch: 1014, Training Loss: 0.08779175163155567\n",
      "Epoch: 4 - Batch: 1015, Training Loss: 0.08787942520199131\n",
      "Epoch: 4 - Batch: 1016, Training Loss: 0.08796627459364942\n",
      "Epoch: 4 - Batch: 1017, Training Loss: 0.08804449866561352\n",
      "Epoch: 4 - Batch: 1018, Training Loss: 0.08812387638523013\n",
      "Epoch: 4 - Batch: 1019, Training Loss: 0.08820855424771855\n",
      "Epoch: 4 - Batch: 1020, Training Loss: 0.08829538655122912\n",
      "Epoch: 4 - Batch: 1021, Training Loss: 0.08838004707845289\n",
      "Epoch: 4 - Batch: 1022, Training Loss: 0.08848007672016893\n",
      "Epoch: 4 - Batch: 1023, Training Loss: 0.0885592786962812\n",
      "Epoch: 4 - Batch: 1024, Training Loss: 0.08863618795081948\n",
      "Epoch: 4 - Batch: 1025, Training Loss: 0.0887250019419648\n",
      "Epoch: 4 - Batch: 1026, Training Loss: 0.08880996445864192\n",
      "Epoch: 4 - Batch: 1027, Training Loss: 0.0888966701393499\n",
      "Epoch: 4 - Batch: 1028, Training Loss: 0.08898077379436438\n",
      "Epoch: 4 - Batch: 1029, Training Loss: 0.08906908411143431\n",
      "Epoch: 4 - Batch: 1030, Training Loss: 0.08916443945287077\n",
      "Epoch: 4 - Batch: 1031, Training Loss: 0.08925423868754215\n",
      "Epoch: 4 - Batch: 1032, Training Loss: 0.08933859513099514\n",
      "Epoch: 4 - Batch: 1033, Training Loss: 0.08942923723885274\n",
      "Epoch: 4 - Batch: 1034, Training Loss: 0.08951477594611855\n",
      "Epoch: 4 - Batch: 1035, Training Loss: 0.08959733927081868\n",
      "Epoch: 4 - Batch: 1036, Training Loss: 0.08968525252013064\n",
      "Epoch: 4 - Batch: 1037, Training Loss: 0.08977411447670527\n",
      "Epoch: 4 - Batch: 1038, Training Loss: 0.08986191242735579\n",
      "Epoch: 4 - Batch: 1039, Training Loss: 0.0899544531297229\n",
      "Epoch: 4 - Batch: 1040, Training Loss: 0.09004344583595571\n",
      "Epoch: 4 - Batch: 1041, Training Loss: 0.09013288135749982\n",
      "Epoch: 4 - Batch: 1042, Training Loss: 0.09021691825382942\n",
      "Epoch: 4 - Batch: 1043, Training Loss: 0.0903127629365494\n",
      "Epoch: 4 - Batch: 1044, Training Loss: 0.09039849650800524\n",
      "Epoch: 4 - Batch: 1045, Training Loss: 0.09047702353935733\n",
      "Epoch: 4 - Batch: 1046, Training Loss: 0.09056256552710264\n",
      "Epoch: 4 - Batch: 1047, Training Loss: 0.0906474936018338\n",
      "Epoch: 4 - Batch: 1048, Training Loss: 0.09072464947923894\n",
      "Epoch: 4 - Batch: 1049, Training Loss: 0.09081515514756712\n",
      "Epoch: 4 - Batch: 1050, Training Loss: 0.09090032936328679\n",
      "Epoch: 4 - Batch: 1051, Training Loss: 0.090981779141262\n",
      "Epoch: 4 - Batch: 1052, Training Loss: 0.09106706500448793\n",
      "Epoch: 4 - Batch: 1053, Training Loss: 0.09115558418493168\n",
      "Epoch: 4 - Batch: 1054, Training Loss: 0.09125011998103626\n",
      "Epoch: 4 - Batch: 1055, Training Loss: 0.09133738571558624\n",
      "Epoch: 4 - Batch: 1056, Training Loss: 0.09142131194670007\n",
      "Epoch: 4 - Batch: 1057, Training Loss: 0.09150406217224168\n",
      "Epoch: 4 - Batch: 1058, Training Loss: 0.09159325917635984\n",
      "Epoch: 4 - Batch: 1059, Training Loss: 0.0916786114090612\n",
      "Epoch: 4 - Batch: 1060, Training Loss: 0.09176854664462913\n",
      "Epoch: 4 - Batch: 1061, Training Loss: 0.09185078165572674\n",
      "Epoch: 4 - Batch: 1062, Training Loss: 0.0919452457199148\n",
      "Epoch: 4 - Batch: 1063, Training Loss: 0.09202915591930672\n",
      "Epoch: 4 - Batch: 1064, Training Loss: 0.09212094302571829\n",
      "Epoch: 4 - Batch: 1065, Training Loss: 0.09221556130333326\n",
      "Epoch: 4 - Batch: 1066, Training Loss: 0.09230312044001733\n",
      "Epoch: 4 - Batch: 1067, Training Loss: 0.09239250711886642\n",
      "Epoch: 4 - Batch: 1068, Training Loss: 0.09248934372963004\n",
      "Epoch: 4 - Batch: 1069, Training Loss: 0.09256779227659082\n",
      "Epoch: 4 - Batch: 1070, Training Loss: 0.09265650603703994\n",
      "Epoch: 4 - Batch: 1071, Training Loss: 0.09274544256737774\n",
      "Epoch: 4 - Batch: 1072, Training Loss: 0.09283044556801394\n",
      "Epoch: 4 - Batch: 1073, Training Loss: 0.09292733812608925\n",
      "Epoch: 4 - Batch: 1074, Training Loss: 0.093015857220042\n",
      "Epoch: 4 - Batch: 1075, Training Loss: 0.09310242344639194\n",
      "Epoch: 4 - Batch: 1076, Training Loss: 0.09319528174983525\n",
      "Epoch: 4 - Batch: 1077, Training Loss: 0.09328542894391871\n",
      "Epoch: 4 - Batch: 1078, Training Loss: 0.0933756984177217\n",
      "Epoch: 4 - Batch: 1079, Training Loss: 0.0934554021527518\n",
      "Epoch: 4 - Batch: 1080, Training Loss: 0.09354178944160887\n",
      "Epoch: 4 - Batch: 1081, Training Loss: 0.09363090739914434\n",
      "Epoch: 4 - Batch: 1082, Training Loss: 0.09372347580333848\n",
      "Epoch: 4 - Batch: 1083, Training Loss: 0.0938025093073671\n",
      "Epoch: 4 - Batch: 1084, Training Loss: 0.09388544247369861\n",
      "Epoch: 4 - Batch: 1085, Training Loss: 0.09396774642428948\n",
      "Epoch: 4 - Batch: 1086, Training Loss: 0.09405254437333316\n",
      "Epoch: 4 - Batch: 1087, Training Loss: 0.09413312319326361\n",
      "Epoch: 4 - Batch: 1088, Training Loss: 0.09421234116403025\n",
      "Epoch: 4 - Batch: 1089, Training Loss: 0.09430049770630612\n",
      "Epoch: 4 - Batch: 1090, Training Loss: 0.09437706978191586\n",
      "Epoch: 4 - Batch: 1091, Training Loss: 0.09446308055697981\n",
      "Epoch: 4 - Batch: 1092, Training Loss: 0.09455187973345493\n",
      "Epoch: 4 - Batch: 1093, Training Loss: 0.09463957972070866\n",
      "Epoch: 4 - Batch: 1094, Training Loss: 0.0947331011011134\n",
      "Epoch: 4 - Batch: 1095, Training Loss: 0.09481718168156855\n",
      "Epoch: 4 - Batch: 1096, Training Loss: 0.09490448030740467\n",
      "Epoch: 4 - Batch: 1097, Training Loss: 0.09499307297429635\n",
      "Epoch: 4 - Batch: 1098, Training Loss: 0.09508399585660417\n",
      "Epoch: 4 - Batch: 1099, Training Loss: 0.09516662231354571\n",
      "Epoch: 4 - Batch: 1100, Training Loss: 0.0952600890563594\n",
      "Epoch: 4 - Batch: 1101, Training Loss: 0.09534550004115153\n",
      "Epoch: 4 - Batch: 1102, Training Loss: 0.09543980363367209\n",
      "Epoch: 4 - Batch: 1103, Training Loss: 0.09553116471020144\n",
      "Epoch: 4 - Batch: 1104, Training Loss: 0.09561649419577363\n",
      "Epoch: 4 - Batch: 1105, Training Loss: 0.09569751402706056\n",
      "Epoch: 4 - Batch: 1106, Training Loss: 0.0957737532853586\n",
      "Epoch: 4 - Batch: 1107, Training Loss: 0.09585643825962967\n",
      "Epoch: 4 - Batch: 1108, Training Loss: 0.0959378612253994\n",
      "Epoch: 4 - Batch: 1109, Training Loss: 0.09602518647810318\n",
      "Epoch: 4 - Batch: 1110, Training Loss: 0.09610925180525527\n",
      "Epoch: 4 - Batch: 1111, Training Loss: 0.09619365127797348\n",
      "Epoch: 4 - Batch: 1112, Training Loss: 0.09628210466348908\n",
      "Epoch: 4 - Batch: 1113, Training Loss: 0.09637775321554386\n",
      "Epoch: 4 - Batch: 1114, Training Loss: 0.09647437303665266\n",
      "Epoch: 4 - Batch: 1115, Training Loss: 0.0965684973210047\n",
      "Epoch: 4 - Batch: 1116, Training Loss: 0.09665490489199782\n",
      "Epoch: 4 - Batch: 1117, Training Loss: 0.09673567597521082\n",
      "Epoch: 4 - Batch: 1118, Training Loss: 0.09681994833707018\n",
      "Epoch: 4 - Batch: 1119, Training Loss: 0.0969134423121884\n",
      "Epoch: 4 - Batch: 1120, Training Loss: 0.09699998217114367\n",
      "Epoch: 4 - Batch: 1121, Training Loss: 0.09708607734287557\n",
      "Epoch: 4 - Batch: 1122, Training Loss: 0.09716392897847873\n",
      "Epoch: 4 - Batch: 1123, Training Loss: 0.09725521757556234\n",
      "Epoch: 4 - Batch: 1124, Training Loss: 0.09734049965171869\n",
      "Epoch: 4 - Batch: 1125, Training Loss: 0.09743526410527688\n",
      "Epoch: 4 - Batch: 1126, Training Loss: 0.09752863087686733\n",
      "Epoch: 4 - Batch: 1127, Training Loss: 0.09761241804901047\n",
      "Epoch: 4 - Batch: 1128, Training Loss: 0.09770498382758541\n",
      "Epoch: 4 - Batch: 1129, Training Loss: 0.09778270916781616\n",
      "Epoch: 4 - Batch: 1130, Training Loss: 0.09787016232833143\n",
      "Epoch: 4 - Batch: 1131, Training Loss: 0.09795535341597117\n",
      "Epoch: 4 - Batch: 1132, Training Loss: 0.09804522752094624\n",
      "Epoch: 4 - Batch: 1133, Training Loss: 0.09813370189288165\n",
      "Epoch: 4 - Batch: 1134, Training Loss: 0.09821730870660858\n",
      "Epoch: 4 - Batch: 1135, Training Loss: 0.09830494848995858\n",
      "Epoch: 4 - Batch: 1136, Training Loss: 0.09839040526554953\n",
      "Epoch: 4 - Batch: 1137, Training Loss: 0.09847871267281559\n",
      "Epoch: 4 - Batch: 1138, Training Loss: 0.09855619207592943\n",
      "Epoch: 4 - Batch: 1139, Training Loss: 0.09863914923733147\n",
      "Epoch: 4 - Batch: 1140, Training Loss: 0.0987264892058586\n",
      "Epoch: 4 - Batch: 1141, Training Loss: 0.09881834538767785\n",
      "Epoch: 4 - Batch: 1142, Training Loss: 0.09890123771173048\n",
      "Epoch: 4 - Batch: 1143, Training Loss: 0.09898030413175697\n",
      "Epoch: 4 - Batch: 1144, Training Loss: 0.09906814955978054\n",
      "Epoch: 4 - Batch: 1145, Training Loss: 0.0991517752347202\n",
      "Epoch: 4 - Batch: 1146, Training Loss: 0.0992488289811026\n",
      "Epoch: 4 - Batch: 1147, Training Loss: 0.09934159341093715\n",
      "Epoch: 4 - Batch: 1148, Training Loss: 0.09942699160741929\n",
      "Epoch: 4 - Batch: 1149, Training Loss: 0.09953603174407684\n",
      "Epoch: 4 - Batch: 1150, Training Loss: 0.09961886717559489\n",
      "Epoch: 4 - Batch: 1151, Training Loss: 0.09970453220912276\n",
      "Epoch: 4 - Batch: 1152, Training Loss: 0.09979307754691165\n",
      "Epoch: 4 - Batch: 1153, Training Loss: 0.0998834002497382\n",
      "Epoch: 4 - Batch: 1154, Training Loss: 0.09997396345310543\n",
      "Epoch: 4 - Batch: 1155, Training Loss: 0.10005562371924939\n",
      "Epoch: 4 - Batch: 1156, Training Loss: 0.10015146071662752\n",
      "Epoch: 4 - Batch: 1157, Training Loss: 0.10023295639660426\n",
      "Epoch: 4 - Batch: 1158, Training Loss: 0.10032060776356837\n",
      "Epoch: 4 - Batch: 1159, Training Loss: 0.10041265155051281\n",
      "Epoch: 4 - Batch: 1160, Training Loss: 0.10049108923667699\n",
      "Epoch: 4 - Batch: 1161, Training Loss: 0.10057164943015595\n",
      "Epoch: 4 - Batch: 1162, Training Loss: 0.10064763010511944\n",
      "Epoch: 4 - Batch: 1163, Training Loss: 0.10073883340355769\n",
      "Epoch: 4 - Batch: 1164, Training Loss: 0.10082054793043911\n",
      "Epoch: 4 - Batch: 1165, Training Loss: 0.10090706500519765\n",
      "Epoch: 4 - Batch: 1166, Training Loss: 0.10099441991195354\n",
      "Epoch: 4 - Batch: 1167, Training Loss: 0.10108045894485801\n",
      "Epoch: 4 - Batch: 1168, Training Loss: 0.10116632130139107\n",
      "Epoch: 4 - Batch: 1169, Training Loss: 0.10125034371048064\n",
      "Epoch: 4 - Batch: 1170, Training Loss: 0.1013408362012894\n",
      "Epoch: 4 - Batch: 1171, Training Loss: 0.1014280435667801\n",
      "Epoch: 4 - Batch: 1172, Training Loss: 0.1015159568469817\n",
      "Epoch: 4 - Batch: 1173, Training Loss: 0.10160383048342235\n",
      "Epoch: 4 - Batch: 1174, Training Loss: 0.10169145025028715\n",
      "Epoch: 4 - Batch: 1175, Training Loss: 0.10178014258568362\n",
      "Epoch: 4 - Batch: 1176, Training Loss: 0.10185960294125883\n",
      "Epoch: 4 - Batch: 1177, Training Loss: 0.10194900135817021\n",
      "Epoch: 4 - Batch: 1178, Training Loss: 0.10203906961069574\n",
      "Epoch: 4 - Batch: 1179, Training Loss: 0.10213496312375488\n",
      "Epoch: 4 - Batch: 1180, Training Loss: 0.10222284110945651\n",
      "Epoch: 4 - Batch: 1181, Training Loss: 0.10230533549466339\n",
      "Epoch: 4 - Batch: 1182, Training Loss: 0.10238592739326642\n",
      "Epoch: 4 - Batch: 1183, Training Loss: 0.10247043839611027\n",
      "Epoch: 4 - Batch: 1184, Training Loss: 0.10255502781538821\n",
      "Epoch: 4 - Batch: 1185, Training Loss: 0.10264699991366164\n",
      "Epoch: 4 - Batch: 1186, Training Loss: 0.1027421665750135\n",
      "Epoch: 4 - Batch: 1187, Training Loss: 0.10282387787083884\n",
      "Epoch: 4 - Batch: 1188, Training Loss: 0.10291171190676404\n",
      "Epoch: 4 - Batch: 1189, Training Loss: 0.10300365753261802\n",
      "Epoch: 4 - Batch: 1190, Training Loss: 0.10309395837793699\n",
      "Epoch: 4 - Batch: 1191, Training Loss: 0.10318404896984844\n",
      "Epoch: 4 - Batch: 1192, Training Loss: 0.10326678045773585\n",
      "Epoch: 4 - Batch: 1193, Training Loss: 0.10335763795903666\n",
      "Epoch: 4 - Batch: 1194, Training Loss: 0.10344282912698946\n",
      "Epoch: 4 - Batch: 1195, Training Loss: 0.10353124826207485\n",
      "Epoch: 4 - Batch: 1196, Training Loss: 0.10361125666160093\n",
      "Epoch: 4 - Batch: 1197, Training Loss: 0.1036942535100094\n",
      "Epoch: 4 - Batch: 1198, Training Loss: 0.10378310689162061\n",
      "Epoch: 4 - Batch: 1199, Training Loss: 0.10387024279702362\n",
      "Epoch: 4 - Batch: 1200, Training Loss: 0.10395781899714351\n",
      "Epoch: 4 - Batch: 1201, Training Loss: 0.10404601247724807\n",
      "Epoch: 4 - Batch: 1202, Training Loss: 0.10413561849105812\n",
      "Epoch: 4 - Batch: 1203, Training Loss: 0.10423156872366988\n",
      "Epoch: 4 - Batch: 1204, Training Loss: 0.10431532414744348\n",
      "Epoch: 4 - Batch: 1205, Training Loss: 0.10440326384446316\n",
      "Epoch: 4 - Batch: 1206, Training Loss: 0.10449441220259192\n",
      "Epoch: 4 - Batch: 1207, Training Loss: 0.10457526070759268\n",
      "Epoch: 4 - Batch: 1208, Training Loss: 0.10465973174542337\n",
      "Epoch: 4 - Batch: 1209, Training Loss: 0.1047488981749782\n",
      "Epoch: 4 - Batch: 1210, Training Loss: 0.10482945526095963\n",
      "Epoch: 4 - Batch: 1211, Training Loss: 0.10491152833622092\n",
      "Epoch: 4 - Batch: 1212, Training Loss: 0.10499971549012768\n",
      "Epoch: 4 - Batch: 1213, Training Loss: 0.10508458204505654\n",
      "Epoch: 4 - Batch: 1214, Training Loss: 0.1051684644746642\n",
      "Epoch: 4 - Batch: 1215, Training Loss: 0.10525357597206363\n",
      "Epoch: 4 - Batch: 1216, Training Loss: 0.10534276492634223\n",
      "Epoch: 4 - Batch: 1217, Training Loss: 0.10542461709127102\n",
      "Epoch: 4 - Batch: 1218, Training Loss: 0.10551011520955297\n",
      "Epoch: 4 - Batch: 1219, Training Loss: 0.10559537113454212\n",
      "Epoch: 4 - Batch: 1220, Training Loss: 0.10568171971868322\n",
      "Epoch: 4 - Batch: 1221, Training Loss: 0.10576972543303646\n",
      "Epoch: 4 - Batch: 1222, Training Loss: 0.10585974424360799\n",
      "Epoch: 4 - Batch: 1223, Training Loss: 0.1059430915164216\n",
      "Epoch: 4 - Batch: 1224, Training Loss: 0.10602233476108974\n",
      "Epoch: 4 - Batch: 1225, Training Loss: 0.10610264754092713\n",
      "Epoch: 4 - Batch: 1226, Training Loss: 0.1061919540673445\n",
      "Epoch: 4 - Batch: 1227, Training Loss: 0.10627850728915698\n",
      "Epoch: 4 - Batch: 1228, Training Loss: 0.10636585050109607\n",
      "Epoch: 4 - Batch: 1229, Training Loss: 0.10645572283918386\n",
      "Epoch: 4 - Batch: 1230, Training Loss: 0.10653985515978207\n",
      "Epoch: 4 - Batch: 1231, Training Loss: 0.10663407583215936\n",
      "Epoch: 4 - Batch: 1232, Training Loss: 0.10672104598376683\n",
      "Epoch: 4 - Batch: 1233, Training Loss: 0.10681053704487942\n",
      "Epoch: 4 - Batch: 1234, Training Loss: 0.1068979700529951\n",
      "Epoch: 4 - Batch: 1235, Training Loss: 0.10698330188617976\n",
      "Epoch: 4 - Batch: 1236, Training Loss: 0.10707345145258737\n",
      "Epoch: 4 - Batch: 1237, Training Loss: 0.10716313692072335\n",
      "Epoch: 4 - Batch: 1238, Training Loss: 0.1072491716846799\n",
      "Epoch: 4 - Batch: 1239, Training Loss: 0.10733551731577165\n",
      "Epoch: 4 - Batch: 1240, Training Loss: 0.10742299583667941\n",
      "Epoch: 4 - Batch: 1241, Training Loss: 0.10751500551528599\n",
      "Epoch: 4 - Batch: 1242, Training Loss: 0.1076014481398399\n",
      "Epoch: 4 - Batch: 1243, Training Loss: 0.10768031619279143\n",
      "Epoch: 4 - Batch: 1244, Training Loss: 0.10776888165209028\n",
      "Epoch: 4 - Batch: 1245, Training Loss: 0.10785403344811097\n",
      "Epoch: 4 - Batch: 1246, Training Loss: 0.10793900639312975\n",
      "Epoch: 4 - Batch: 1247, Training Loss: 0.10803132716745484\n",
      "Epoch: 4 - Batch: 1248, Training Loss: 0.10811813906527079\n",
      "Epoch: 4 - Batch: 1249, Training Loss: 0.10819709750130205\n",
      "Epoch: 4 - Batch: 1250, Training Loss: 0.10827934250828639\n",
      "Epoch: 4 - Batch: 1251, Training Loss: 0.10836451492622914\n",
      "Epoch: 4 - Batch: 1252, Training Loss: 0.10846216605349164\n",
      "Epoch: 4 - Batch: 1253, Training Loss: 0.10854894725318572\n",
      "Epoch: 4 - Batch: 1254, Training Loss: 0.10863695448730913\n",
      "Epoch: 4 - Batch: 1255, Training Loss: 0.10872502920911284\n",
      "Epoch: 4 - Batch: 1256, Training Loss: 0.10881352953452178\n",
      "Epoch: 4 - Batch: 1257, Training Loss: 0.10889525571297452\n",
      "Epoch: 4 - Batch: 1258, Training Loss: 0.10898010698988861\n",
      "Epoch: 4 - Batch: 1259, Training Loss: 0.10906541577940358\n",
      "Epoch: 4 - Batch: 1260, Training Loss: 0.10915505291761253\n",
      "Epoch: 4 - Batch: 1261, Training Loss: 0.10923658744439754\n",
      "Epoch: 4 - Batch: 1262, Training Loss: 0.10931928996398278\n",
      "Epoch: 4 - Batch: 1263, Training Loss: 0.10941251935082091\n",
      "Epoch: 4 - Batch: 1264, Training Loss: 0.10949290491282249\n",
      "Epoch: 4 - Batch: 1265, Training Loss: 0.10958125112982928\n",
      "Epoch: 4 - Batch: 1266, Training Loss: 0.10967345854536217\n",
      "Epoch: 4 - Batch: 1267, Training Loss: 0.10976397261567179\n",
      "Epoch: 4 - Batch: 1268, Training Loss: 0.10985073917985555\n",
      "Epoch: 4 - Batch: 1269, Training Loss: 0.1099306233227253\n",
      "Epoch: 4 - Batch: 1270, Training Loss: 0.11001684865103432\n",
      "Epoch: 4 - Batch: 1271, Training Loss: 0.11010389197401542\n",
      "Epoch: 4 - Batch: 1272, Training Loss: 0.110192335301865\n",
      "Epoch: 4 - Batch: 1273, Training Loss: 0.11026980608676994\n",
      "Epoch: 4 - Batch: 1274, Training Loss: 0.11035026343011145\n",
      "Epoch: 4 - Batch: 1275, Training Loss: 0.11043982661852789\n",
      "Epoch: 4 - Batch: 1276, Training Loss: 0.11052326757343452\n",
      "Epoch: 4 - Batch: 1277, Training Loss: 0.1106103897823722\n",
      "Epoch: 4 - Batch: 1278, Training Loss: 0.11069112751713242\n",
      "Epoch: 4 - Batch: 1279, Training Loss: 0.1107787915364921\n",
      "Epoch: 4 - Batch: 1280, Training Loss: 0.11087259766993238\n",
      "Epoch: 4 - Batch: 1281, Training Loss: 0.11095262129187189\n",
      "Epoch: 4 - Batch: 1282, Training Loss: 0.11103810125322484\n",
      "Epoch: 4 - Batch: 1283, Training Loss: 0.11112196258521001\n",
      "Epoch: 4 - Batch: 1284, Training Loss: 0.1112160175099697\n",
      "Epoch: 4 - Batch: 1285, Training Loss: 0.11130773297713369\n",
      "Epoch: 4 - Batch: 1286, Training Loss: 0.11140219403884898\n",
      "Epoch: 4 - Batch: 1287, Training Loss: 0.1114949608410077\n",
      "Epoch: 4 - Batch: 1288, Training Loss: 0.1115798768070009\n",
      "Epoch: 4 - Batch: 1289, Training Loss: 0.11166814362528313\n",
      "Epoch: 4 - Batch: 1290, Training Loss: 0.11175230963412011\n",
      "Epoch: 4 - Batch: 1291, Training Loss: 0.111837873652355\n",
      "Epoch: 4 - Batch: 1292, Training Loss: 0.11192709060500115\n",
      "Epoch: 4 - Batch: 1293, Training Loss: 0.11201729728634875\n",
      "Epoch: 4 - Batch: 1294, Training Loss: 0.11210763672518098\n",
      "Epoch: 4 - Batch: 1295, Training Loss: 0.11219166204407441\n",
      "Epoch: 4 - Batch: 1296, Training Loss: 0.11227445496527315\n",
      "Epoch: 4 - Batch: 1297, Training Loss: 0.1123531972726878\n",
      "Epoch: 4 - Batch: 1298, Training Loss: 0.11243659213407715\n",
      "Epoch: 4 - Batch: 1299, Training Loss: 0.11252324739671861\n",
      "Epoch: 4 - Batch: 1300, Training Loss: 0.11260817692326273\n",
      "Epoch: 4 - Batch: 1301, Training Loss: 0.11269057340067418\n",
      "Epoch: 4 - Batch: 1302, Training Loss: 0.1127828193355852\n",
      "Epoch: 4 - Batch: 1303, Training Loss: 0.11286330698783915\n",
      "Epoch: 4 - Batch: 1304, Training Loss: 0.11294852958290928\n",
      "Epoch: 4 - Batch: 1305, Training Loss: 0.11304251341825694\n",
      "Epoch: 4 - Batch: 1306, Training Loss: 0.11313600047409633\n",
      "Epoch: 4 - Batch: 1307, Training Loss: 0.11322186567865399\n",
      "Epoch: 4 - Batch: 1308, Training Loss: 0.11330961841923087\n",
      "Epoch: 4 - Batch: 1309, Training Loss: 0.11339670651365276\n",
      "Epoch: 4 - Batch: 1310, Training Loss: 0.11347428207003062\n",
      "Epoch: 4 - Batch: 1311, Training Loss: 0.11356202155277503\n",
      "Epoch: 4 - Batch: 1312, Training Loss: 0.11364938455261599\n",
      "Epoch: 4 - Batch: 1313, Training Loss: 0.1137352703049606\n",
      "Epoch: 4 - Batch: 1314, Training Loss: 0.11381256036077368\n",
      "Epoch: 4 - Batch: 1315, Training Loss: 0.11390466855261258\n",
      "Epoch: 4 - Batch: 1316, Training Loss: 0.11398863226521272\n",
      "Epoch: 4 - Batch: 1317, Training Loss: 0.11407363614929256\n",
      "Epoch: 4 - Batch: 1318, Training Loss: 0.1141698145389458\n",
      "Epoch: 4 - Batch: 1319, Training Loss: 0.11425644043789772\n",
      "Epoch: 4 - Batch: 1320, Training Loss: 0.11434932185914584\n",
      "Epoch: 4 - Batch: 1321, Training Loss: 0.11443213833060431\n",
      "Epoch: 4 - Batch: 1322, Training Loss: 0.11452632488846581\n",
      "Epoch: 4 - Batch: 1323, Training Loss: 0.11460298790934667\n",
      "Epoch: 4 - Batch: 1324, Training Loss: 0.11468882305210898\n",
      "Epoch: 4 - Batch: 1325, Training Loss: 0.11477381868247764\n",
      "Epoch: 4 - Batch: 1326, Training Loss: 0.1148547872122544\n",
      "Epoch: 4 - Batch: 1327, Training Loss: 0.11493548012615042\n",
      "Epoch: 4 - Batch: 1328, Training Loss: 0.11502027294751424\n",
      "Epoch: 4 - Batch: 1329, Training Loss: 0.11509998837118322\n",
      "Epoch: 4 - Batch: 1330, Training Loss: 0.11518078719329082\n",
      "Epoch: 4 - Batch: 1331, Training Loss: 0.11525749828462577\n",
      "Epoch: 4 - Batch: 1332, Training Loss: 0.1153464133404875\n",
      "Epoch: 4 - Batch: 1333, Training Loss: 0.11542261045914187\n",
      "Epoch: 4 - Batch: 1334, Training Loss: 0.11551763455863813\n",
      "Epoch: 4 - Batch: 1335, Training Loss: 0.11560432760548434\n",
      "Epoch: 4 - Batch: 1336, Training Loss: 0.11569514599432597\n",
      "Epoch: 4 - Batch: 1337, Training Loss: 0.11578374158337737\n",
      "Epoch: 4 - Batch: 1338, Training Loss: 0.11586668727236799\n",
      "Epoch: 4 - Batch: 1339, Training Loss: 0.11595463569879927\n",
      "Epoch: 4 - Batch: 1340, Training Loss: 0.11604803959355624\n",
      "Epoch: 4 - Batch: 1341, Training Loss: 0.11613923758515474\n",
      "Epoch: 4 - Batch: 1342, Training Loss: 0.1162175241097882\n",
      "Epoch: 4 - Batch: 1343, Training Loss: 0.11630946263102553\n",
      "Epoch: 4 - Batch: 1344, Training Loss: 0.11639443505092047\n",
      "Epoch: 4 - Batch: 1345, Training Loss: 0.11648413626041579\n",
      "Epoch: 4 - Batch: 1346, Training Loss: 0.11656990528081977\n",
      "Epoch: 4 - Batch: 1347, Training Loss: 0.1166597700633022\n",
      "Epoch: 4 - Batch: 1348, Training Loss: 0.1167447369477741\n",
      "Epoch: 4 - Batch: 1349, Training Loss: 0.11682902758394308\n",
      "Epoch: 4 - Batch: 1350, Training Loss: 0.11691192832851094\n",
      "Epoch: 4 - Batch: 1351, Training Loss: 0.1169950142564564\n",
      "Epoch: 4 - Batch: 1352, Training Loss: 0.11708537033862537\n",
      "Epoch: 4 - Batch: 1353, Training Loss: 0.1171737967327756\n",
      "Epoch: 4 - Batch: 1354, Training Loss: 0.11726710098373949\n",
      "Epoch: 4 - Batch: 1355, Training Loss: 0.11735437880231571\n",
      "Epoch: 4 - Batch: 1356, Training Loss: 0.1174371804899936\n",
      "Epoch: 4 - Batch: 1357, Training Loss: 0.11751827941110873\n",
      "Epoch: 4 - Batch: 1358, Training Loss: 0.11760899178053609\n",
      "Epoch: 4 - Batch: 1359, Training Loss: 0.11769300940173183\n",
      "Epoch: 4 - Batch: 1360, Training Loss: 0.11778561649210813\n",
      "Epoch: 4 - Batch: 1361, Training Loss: 0.11787395923094172\n",
      "Epoch: 4 - Batch: 1362, Training Loss: 0.11796745090169297\n",
      "Epoch: 4 - Batch: 1363, Training Loss: 0.11805097425779695\n",
      "Epoch: 4 - Batch: 1364, Training Loss: 0.11814581322333903\n",
      "Epoch: 4 - Batch: 1365, Training Loss: 0.11822680626713221\n",
      "Epoch: 4 - Batch: 1366, Training Loss: 0.11831301961026183\n",
      "Epoch: 4 - Batch: 1367, Training Loss: 0.11840300159420737\n",
      "Epoch: 4 - Batch: 1368, Training Loss: 0.11847889220116546\n",
      "Epoch: 4 - Batch: 1369, Training Loss: 0.11856469250041651\n",
      "Epoch: 4 - Batch: 1370, Training Loss: 0.11864945981311759\n",
      "Epoch: 4 - Batch: 1371, Training Loss: 0.11872856061331075\n",
      "Epoch: 4 - Batch: 1372, Training Loss: 0.11881232818906778\n",
      "Epoch: 4 - Batch: 1373, Training Loss: 0.11889814698503386\n",
      "Epoch: 4 - Batch: 1374, Training Loss: 0.11898554484691984\n",
      "Epoch: 4 - Batch: 1375, Training Loss: 0.11907344826097117\n",
      "Epoch: 4 - Batch: 1376, Training Loss: 0.11915278892537848\n",
      "Epoch: 4 - Batch: 1377, Training Loss: 0.11923227191070221\n",
      "Epoch: 4 - Batch: 1378, Training Loss: 0.11931125242046851\n",
      "Epoch: 4 - Batch: 1379, Training Loss: 0.1194020783402038\n",
      "Epoch: 4 - Batch: 1380, Training Loss: 0.11948912680939854\n",
      "Epoch: 4 - Batch: 1381, Training Loss: 0.119571765035292\n",
      "Epoch: 4 - Batch: 1382, Training Loss: 0.11966242899447926\n",
      "Epoch: 4 - Batch: 1383, Training Loss: 0.11975485680757668\n",
      "Epoch: 4 - Batch: 1384, Training Loss: 0.11983867523385518\n",
      "Epoch: 4 - Batch: 1385, Training Loss: 0.11992624649163304\n",
      "Epoch: 4 - Batch: 1386, Training Loss: 0.12001224685688912\n",
      "Epoch: 4 - Batch: 1387, Training Loss: 0.12010384905792983\n",
      "Epoch: 4 - Batch: 1388, Training Loss: 0.12019261268660994\n",
      "Epoch: 4 - Batch: 1389, Training Loss: 0.12027597904304169\n",
      "Epoch: 4 - Batch: 1390, Training Loss: 0.12036460316176241\n",
      "Epoch: 4 - Batch: 1391, Training Loss: 0.12044695052465\n",
      "Epoch: 4 - Batch: 1392, Training Loss: 0.12053303206787379\n",
      "Epoch: 4 - Batch: 1393, Training Loss: 0.12061402050293303\n",
      "Epoch: 4 - Batch: 1394, Training Loss: 0.1207026947740694\n",
      "Epoch: 4 - Batch: 1395, Training Loss: 0.12079011446257333\n",
      "Epoch: 4 - Batch: 1396, Training Loss: 0.12087679479524469\n",
      "Epoch: 4 - Batch: 1397, Training Loss: 0.12095908398058877\n",
      "Epoch: 4 - Batch: 1398, Training Loss: 0.12103296906546772\n",
      "Epoch: 4 - Batch: 1399, Training Loss: 0.12112413083587713\n",
      "Epoch: 4 - Batch: 1400, Training Loss: 0.12120323090089692\n",
      "Epoch: 4 - Batch: 1401, Training Loss: 0.12129003956147885\n",
      "Epoch: 4 - Batch: 1402, Training Loss: 0.12138385409690057\n",
      "Epoch: 4 - Batch: 1403, Training Loss: 0.12147072711913147\n",
      "Epoch: 4 - Batch: 1404, Training Loss: 0.12154828822533685\n",
      "Epoch: 4 - Batch: 1405, Training Loss: 0.12162614154751424\n",
      "Epoch: 4 - Batch: 1406, Training Loss: 0.12171544660976277\n",
      "Epoch: 4 - Batch: 1407, Training Loss: 0.12181233436758838\n",
      "Epoch: 4 - Batch: 1408, Training Loss: 0.1218940574200098\n",
      "Epoch: 4 - Batch: 1409, Training Loss: 0.12197718779814382\n",
      "Epoch: 4 - Batch: 1410, Training Loss: 0.12206288906521663\n",
      "Epoch: 4 - Batch: 1411, Training Loss: 0.12214870607576164\n",
      "Epoch: 4 - Batch: 1412, Training Loss: 0.12223309293315185\n",
      "Epoch: 4 - Batch: 1413, Training Loss: 0.12231002298259419\n",
      "Epoch: 4 - Batch: 1414, Training Loss: 0.12240250425329849\n",
      "Epoch: 4 - Batch: 1415, Training Loss: 0.1224952725134481\n",
      "Epoch: 4 - Batch: 1416, Training Loss: 0.12259081667088355\n",
      "Epoch: 4 - Batch: 1417, Training Loss: 0.12267733012537656\n",
      "Epoch: 4 - Batch: 1418, Training Loss: 0.12275936243867203\n",
      "Epoch: 4 - Batch: 1419, Training Loss: 0.12284739655913603\n",
      "Epoch: 4 - Batch: 1420, Training Loss: 0.12293049130298407\n",
      "Epoch: 4 - Batch: 1421, Training Loss: 0.12301369558766509\n",
      "Epoch: 4 - Batch: 1422, Training Loss: 0.12310323199476571\n",
      "Epoch: 4 - Batch: 1423, Training Loss: 0.12318890656701367\n",
      "Epoch: 4 - Batch: 1424, Training Loss: 0.12327161556106697\n",
      "Epoch: 4 - Batch: 1425, Training Loss: 0.12336148778795207\n",
      "Epoch: 4 - Batch: 1426, Training Loss: 0.12344929071183426\n",
      "Epoch: 4 - Batch: 1427, Training Loss: 0.12353428896782212\n",
      "Epoch: 4 - Batch: 1428, Training Loss: 0.12361921117970007\n",
      "Epoch: 4 - Batch: 1429, Training Loss: 0.12369957703411283\n",
      "Epoch: 4 - Batch: 1430, Training Loss: 0.12378623222879707\n",
      "Epoch: 4 - Batch: 1431, Training Loss: 0.12387490068650364\n",
      "Epoch: 4 - Batch: 1432, Training Loss: 0.12396469300189619\n",
      "Epoch: 4 - Batch: 1433, Training Loss: 0.12405142542685245\n",
      "Epoch: 4 - Batch: 1434, Training Loss: 0.1241376853948011\n",
      "Epoch: 4 - Batch: 1435, Training Loss: 0.12422176936693848\n",
      "Epoch: 4 - Batch: 1436, Training Loss: 0.12430724203833696\n",
      "Epoch: 4 - Batch: 1437, Training Loss: 0.12439876574263051\n",
      "Epoch: 4 - Batch: 1438, Training Loss: 0.12448421439424676\n",
      "Epoch: 4 - Batch: 1439, Training Loss: 0.12456773497028335\n",
      "Epoch: 4 - Batch: 1440, Training Loss: 0.12465158272936173\n",
      "Epoch: 4 - Batch: 1441, Training Loss: 0.1247384098001677\n",
      "Epoch: 4 - Batch: 1442, Training Loss: 0.12482363796155053\n",
      "Epoch: 4 - Batch: 1443, Training Loss: 0.12491744544795694\n",
      "Epoch: 4 - Batch: 1444, Training Loss: 0.12501207369056902\n",
      "Epoch: 4 - Batch: 1445, Training Loss: 0.12509603571041703\n",
      "Epoch: 4 - Batch: 1446, Training Loss: 0.1251767362293418\n",
      "Epoch: 4 - Batch: 1447, Training Loss: 0.12526075440655102\n",
      "Epoch: 4 - Batch: 1448, Training Loss: 0.12534653542449029\n",
      "Epoch: 4 - Batch: 1449, Training Loss: 0.125438349201015\n",
      "Epoch: 4 - Batch: 1450, Training Loss: 0.12552761620105202\n",
      "Epoch: 4 - Batch: 1451, Training Loss: 0.12561533468502079\n",
      "Epoch: 4 - Batch: 1452, Training Loss: 0.1257045295701098\n",
      "Epoch: 4 - Batch: 1453, Training Loss: 0.12579491571407414\n",
      "Epoch: 4 - Batch: 1454, Training Loss: 0.12587631395577792\n",
      "Epoch: 4 - Batch: 1455, Training Loss: 0.12596278223726484\n",
      "Epoch: 4 - Batch: 1456, Training Loss: 0.12604396693890368\n",
      "Epoch: 4 - Batch: 1457, Training Loss: 0.1261330848902612\n",
      "Epoch: 4 - Batch: 1458, Training Loss: 0.12621802941063545\n",
      "Epoch: 4 - Batch: 1459, Training Loss: 0.12630284212492593\n",
      "Epoch: 4 - Batch: 1460, Training Loss: 0.12639015805513704\n",
      "Epoch: 4 - Batch: 1461, Training Loss: 0.12648575578756\n",
      "Epoch: 4 - Batch: 1462, Training Loss: 0.12657730061418776\n",
      "Epoch: 4 - Batch: 1463, Training Loss: 0.12666068219303295\n",
      "Epoch: 4 - Batch: 1464, Training Loss: 0.12674623679610628\n",
      "Epoch: 4 - Batch: 1465, Training Loss: 0.126839201959162\n",
      "Epoch: 4 - Batch: 1466, Training Loss: 0.12692412082878116\n",
      "Epoch: 4 - Batch: 1467, Training Loss: 0.12701145924664847\n",
      "Epoch: 4 - Batch: 1468, Training Loss: 0.12709822303076487\n",
      "Epoch: 4 - Batch: 1469, Training Loss: 0.12718151381260917\n",
      "Epoch: 4 - Batch: 1470, Training Loss: 0.12726512542040788\n",
      "Epoch: 4 - Batch: 1471, Training Loss: 0.1273493772159465\n",
      "Epoch: 4 - Batch: 1472, Training Loss: 0.1274333624895137\n",
      "Epoch: 4 - Batch: 1473, Training Loss: 0.1275210386361451\n",
      "Epoch: 4 - Batch: 1474, Training Loss: 0.1276070807518354\n",
      "Epoch: 4 - Batch: 1475, Training Loss: 0.12769273018609628\n",
      "Epoch: 4 - Batch: 1476, Training Loss: 0.1277865200098079\n",
      "Epoch: 4 - Batch: 1477, Training Loss: 0.12787542308666813\n",
      "Epoch: 4 - Batch: 1478, Training Loss: 0.12796464180002362\n",
      "Epoch: 4 - Batch: 1479, Training Loss: 0.12805705606492598\n",
      "Epoch: 4 - Batch: 1480, Training Loss: 0.12813697064644464\n",
      "Epoch: 4 - Batch: 1481, Training Loss: 0.12822211737038683\n",
      "Epoch: 4 - Batch: 1482, Training Loss: 0.1283115602436647\n",
      "Epoch: 4 - Batch: 1483, Training Loss: 0.1284018547936755\n",
      "Epoch: 4 - Batch: 1484, Training Loss: 0.1285007686213672\n",
      "Epoch: 4 - Batch: 1485, Training Loss: 0.12858660514798528\n",
      "Epoch: 4 - Batch: 1486, Training Loss: 0.1286718500206621\n",
      "Epoch: 4 - Batch: 1487, Training Loss: 0.12875808662368884\n",
      "Epoch: 4 - Batch: 1488, Training Loss: 0.12885400602844224\n",
      "Epoch: 4 - Batch: 1489, Training Loss: 0.12893792050296\n",
      "Epoch: 4 - Batch: 1490, Training Loss: 0.12902217902008375\n",
      "Epoch: 4 - Batch: 1491, Training Loss: 0.1291124071635516\n",
      "Epoch: 4 - Batch: 1492, Training Loss: 0.1292045627833401\n",
      "Epoch: 4 - Batch: 1493, Training Loss: 0.12929437336686436\n",
      "Epoch: 4 - Batch: 1494, Training Loss: 0.12938515746820822\n",
      "Epoch: 4 - Batch: 1495, Training Loss: 0.12946991460586266\n",
      "Epoch: 4 - Batch: 1496, Training Loss: 0.12955940665544363\n",
      "Epoch: 4 - Batch: 1497, Training Loss: 0.12964561403687322\n",
      "Epoch: 4 - Batch: 1498, Training Loss: 0.12973414713618767\n",
      "Epoch: 4 - Batch: 1499, Training Loss: 0.12982209836122013\n",
      "Epoch: 4 - Batch: 1500, Training Loss: 0.12990788607603282\n",
      "Epoch: 4 - Batch: 1501, Training Loss: 0.1299964749682108\n",
      "Epoch: 4 - Batch: 1502, Training Loss: 0.1300843913064865\n",
      "Epoch: 4 - Batch: 1503, Training Loss: 0.13017170340638257\n",
      "Epoch: 4 - Batch: 1504, Training Loss: 0.13025631144593405\n",
      "Epoch: 4 - Batch: 1505, Training Loss: 0.13034278205649968\n",
      "Epoch: 4 - Batch: 1506, Training Loss: 0.13043033147530375\n",
      "Epoch: 4 - Batch: 1507, Training Loss: 0.13051816022292298\n",
      "Epoch: 4 - Batch: 1508, Training Loss: 0.13060259041229685\n",
      "Epoch: 4 - Batch: 1509, Training Loss: 0.13068925810492849\n",
      "Epoch: 4 - Batch: 1510, Training Loss: 0.13077002287429956\n",
      "Epoch: 4 - Batch: 1511, Training Loss: 0.13085760082277295\n",
      "Epoch: 4 - Batch: 1512, Training Loss: 0.13094656179299205\n",
      "Epoch: 4 - Batch: 1513, Training Loss: 0.13102665047468634\n",
      "Epoch: 4 - Batch: 1514, Training Loss: 0.13111173442470692\n",
      "Epoch: 4 - Batch: 1515, Training Loss: 0.13119628289050328\n",
      "Epoch: 4 - Batch: 1516, Training Loss: 0.1312818624029804\n",
      "Epoch: 4 - Batch: 1517, Training Loss: 0.13136998115487358\n",
      "Epoch: 4 - Batch: 1518, Training Loss: 0.13145220964825766\n",
      "Epoch: 4 - Batch: 1519, Training Loss: 0.13153895019718861\n",
      "Epoch: 4 - Batch: 1520, Training Loss: 0.13162061758030508\n",
      "Epoch: 4 - Batch: 1521, Training Loss: 0.13170221890284253\n",
      "Epoch: 4 - Batch: 1522, Training Loss: 0.13178999359977384\n",
      "Epoch: 4 - Batch: 1523, Training Loss: 0.13187991060427764\n",
      "Epoch: 4 - Batch: 1524, Training Loss: 0.13196691381397532\n",
      "Epoch: 4 - Batch: 1525, Training Loss: 0.13205258165228229\n",
      "Epoch: 4 - Batch: 1526, Training Loss: 0.1321285359413173\n",
      "Epoch: 4 - Batch: 1527, Training Loss: 0.13221697855870523\n",
      "Epoch: 4 - Batch: 1528, Training Loss: 0.13230113131588767\n",
      "Epoch: 4 - Batch: 1529, Training Loss: 0.1323928549440939\n",
      "Epoch: 4 - Batch: 1530, Training Loss: 0.13247979659074377\n",
      "Epoch: 4 - Batch: 1531, Training Loss: 0.13256185173148144\n",
      "Epoch: 4 - Batch: 1532, Training Loss: 0.13264278108701974\n",
      "Epoch: 4 - Batch: 1533, Training Loss: 0.13271727085508914\n",
      "Epoch: 4 - Batch: 1534, Training Loss: 0.13280250502141752\n",
      "Epoch: 4 - Batch: 1535, Training Loss: 0.13288845802668117\n",
      "Epoch: 4 - Batch: 1536, Training Loss: 0.13296329721833144\n",
      "Epoch: 4 - Batch: 1537, Training Loss: 0.13303890209614144\n",
      "Epoch: 4 - Batch: 1538, Training Loss: 0.13312322040942573\n",
      "Epoch: 4 - Batch: 1539, Training Loss: 0.1332025572249842\n",
      "Epoch: 4 - Batch: 1540, Training Loss: 0.13330125993435854\n",
      "Epoch: 4 - Batch: 1541, Training Loss: 0.13339924336616474\n",
      "Epoch: 4 - Batch: 1542, Training Loss: 0.1334825254741988\n",
      "Epoch: 4 - Batch: 1543, Training Loss: 0.13356190473211937\n",
      "Epoch: 4 - Batch: 1544, Training Loss: 0.13364890623053113\n",
      "Epoch: 4 - Batch: 1545, Training Loss: 0.13373717362953855\n",
      "Epoch: 4 - Batch: 1546, Training Loss: 0.1338258092083148\n",
      "Epoch: 4 - Batch: 1547, Training Loss: 0.13390920370520643\n",
      "Epoch: 4 - Batch: 1548, Training Loss: 0.13399503757531567\n",
      "Epoch: 4 - Batch: 1549, Training Loss: 0.1340820606161607\n",
      "Epoch: 4 - Batch: 1550, Training Loss: 0.1341710508265108\n",
      "Epoch: 4 - Batch: 1551, Training Loss: 0.13425982272125794\n",
      "Epoch: 4 - Batch: 1552, Training Loss: 0.13435916335107279\n",
      "Epoch: 4 - Batch: 1553, Training Loss: 0.13444608929960883\n",
      "Epoch: 4 - Batch: 1554, Training Loss: 0.13453294519662462\n",
      "Epoch: 4 - Batch: 1555, Training Loss: 0.13462578849388196\n",
      "Epoch: 4 - Batch: 1556, Training Loss: 0.13470897773530946\n",
      "Epoch: 4 - Batch: 1557, Training Loss: 0.1347936398255489\n",
      "Epoch: 4 - Batch: 1558, Training Loss: 0.13487939665690188\n",
      "Epoch: 4 - Batch: 1559, Training Loss: 0.13496379363373737\n",
      "Epoch: 4 - Batch: 1560, Training Loss: 0.13504708122159315\n",
      "Epoch: 4 - Batch: 1561, Training Loss: 0.1351307149031269\n",
      "Epoch: 4 - Batch: 1562, Training Loss: 0.13521611078288623\n",
      "Epoch: 4 - Batch: 1563, Training Loss: 0.13530000815072266\n",
      "Epoch: 4 - Batch: 1564, Training Loss: 0.13538206172844466\n",
      "Epoch: 4 - Batch: 1565, Training Loss: 0.1354777754885245\n",
      "Epoch: 4 - Batch: 1566, Training Loss: 0.13557174175384626\n",
      "Epoch: 4 - Batch: 1567, Training Loss: 0.13565915429448805\n",
      "Epoch: 4 - Batch: 1568, Training Loss: 0.13574082026903705\n",
      "Epoch: 4 - Batch: 1569, Training Loss: 0.13583184422841713\n",
      "Epoch: 4 - Batch: 1570, Training Loss: 0.1359202435014655\n",
      "Epoch: 4 - Batch: 1571, Training Loss: 0.13600521710752256\n",
      "Epoch: 4 - Batch: 1572, Training Loss: 0.13609147186130038\n",
      "Epoch: 4 - Batch: 1573, Training Loss: 0.13618119810375803\n",
      "Epoch: 4 - Batch: 1574, Training Loss: 0.1362736425601922\n",
      "Epoch: 4 - Batch: 1575, Training Loss: 0.13635737093082115\n",
      "Epoch: 4 - Batch: 1576, Training Loss: 0.13644228666823105\n",
      "Epoch: 4 - Batch: 1577, Training Loss: 0.13653276978712375\n",
      "Epoch: 4 - Batch: 1578, Training Loss: 0.13662204481228865\n",
      "Epoch: 4 - Batch: 1579, Training Loss: 0.136710203955472\n",
      "Epoch: 4 - Batch: 1580, Training Loss: 0.136790745479254\n",
      "Epoch: 4 - Batch: 1581, Training Loss: 0.13687445852244473\n",
      "Epoch: 4 - Batch: 1582, Training Loss: 0.13697438889104335\n",
      "Epoch: 4 - Batch: 1583, Training Loss: 0.13705457751307124\n",
      "Epoch: 4 - Batch: 1584, Training Loss: 0.13714071750566734\n",
      "Epoch: 4 - Batch: 1585, Training Loss: 0.13722627928749245\n",
      "Epoch: 4 - Batch: 1586, Training Loss: 0.13731565196097983\n",
      "Epoch: 4 - Batch: 1587, Training Loss: 0.13739592845784887\n",
      "Epoch: 4 - Batch: 1588, Training Loss: 0.13748746601305592\n",
      "Epoch: 4 - Batch: 1589, Training Loss: 0.13757680538751396\n",
      "Epoch: 4 - Batch: 1590, Training Loss: 0.1376660831865683\n",
      "Epoch: 4 - Batch: 1591, Training Loss: 0.13775226095719123\n",
      "Epoch: 4 - Batch: 1592, Training Loss: 0.1378406735066059\n",
      "Epoch: 4 - Batch: 1593, Training Loss: 0.13793460204332425\n",
      "Epoch: 4 - Batch: 1594, Training Loss: 0.1380207986751599\n",
      "Epoch: 4 - Batch: 1595, Training Loss: 0.1381021660766495\n",
      "Epoch: 4 - Batch: 1596, Training Loss: 0.1381870877448698\n",
      "Epoch: 4 - Batch: 1597, Training Loss: 0.1382777050977718\n",
      "Epoch: 4 - Batch: 1598, Training Loss: 0.1383632475735734\n",
      "Epoch: 4 - Batch: 1599, Training Loss: 0.1384571739665509\n",
      "Epoch: 4 - Batch: 1600, Training Loss: 0.1385466424720501\n",
      "Epoch: 4 - Batch: 1601, Training Loss: 0.1386303810671491\n",
      "Epoch: 4 - Batch: 1602, Training Loss: 0.13870836170405693\n",
      "Epoch: 4 - Batch: 1603, Training Loss: 0.1387896073941963\n",
      "Epoch: 4 - Batch: 1604, Training Loss: 0.1388862765028109\n",
      "Epoch: 4 - Batch: 1605, Training Loss: 0.13896684589522396\n",
      "Epoch: 4 - Batch: 1606, Training Loss: 0.1390536035014128\n",
      "Epoch: 4 - Batch: 1607, Training Loss: 0.13913692311564488\n",
      "Epoch: 4 - Batch: 1608, Training Loss: 0.13921873137404275\n",
      "Epoch: 4 - Batch: 1609, Training Loss: 0.13930731672626823\n",
      "Epoch: 4 - Batch: 1610, Training Loss: 0.13939472799425695\n",
      "Epoch: 4 - Batch: 1611, Training Loss: 0.13947913350898827\n",
      "Epoch: 4 - Batch: 1612, Training Loss: 0.1395608096091605\n",
      "Epoch: 4 - Batch: 1613, Training Loss: 0.13965044536351368\n",
      "Epoch: 4 - Batch: 1614, Training Loss: 0.13973388455771096\n",
      "Epoch: 4 - Batch: 1615, Training Loss: 0.13982246674683754\n",
      "Epoch: 4 - Batch: 1616, Training Loss: 0.13990401962824525\n",
      "Epoch: 4 - Batch: 1617, Training Loss: 0.13999191338619585\n",
      "Epoch: 4 - Batch: 1618, Training Loss: 0.14008151414876752\n",
      "Epoch: 4 - Batch: 1619, Training Loss: 0.140164942500702\n",
      "Epoch: 4 - Batch: 1620, Training Loss: 0.14025169469502632\n",
      "Epoch: 4 - Batch: 1621, Training Loss: 0.1403435199995638\n",
      "Epoch: 4 - Batch: 1622, Training Loss: 0.14042921165378136\n",
      "Epoch: 4 - Batch: 1623, Training Loss: 0.14051270290319598\n",
      "Epoch: 4 - Batch: 1624, Training Loss: 0.1405947671872664\n",
      "Epoch: 4 - Batch: 1625, Training Loss: 0.14067555669305928\n",
      "Epoch: 4 - Batch: 1626, Training Loss: 0.14076026213999412\n",
      "Epoch: 4 - Batch: 1627, Training Loss: 0.1408551606731134\n",
      "Epoch: 4 - Batch: 1628, Training Loss: 0.1409425628410149\n",
      "Epoch: 4 - Batch: 1629, Training Loss: 0.14103422116234923\n",
      "Epoch: 4 - Batch: 1630, Training Loss: 0.14112472774501067\n",
      "Epoch: 4 - Batch: 1631, Training Loss: 0.14121246573269663\n",
      "Epoch: 4 - Batch: 1632, Training Loss: 0.14129525604680995\n",
      "Epoch: 4 - Batch: 1633, Training Loss: 0.14138357116412958\n",
      "Epoch: 4 - Batch: 1634, Training Loss: 0.1414690703326592\n",
      "Epoch: 4 - Batch: 1635, Training Loss: 0.141552475047843\n",
      "Epoch: 4 - Batch: 1636, Training Loss: 0.14163699267960306\n",
      "Epoch: 4 - Batch: 1637, Training Loss: 0.14172119940716038\n",
      "Epoch: 4 - Batch: 1638, Training Loss: 0.14180382874301614\n",
      "Epoch: 4 - Batch: 1639, Training Loss: 0.14189504536724407\n",
      "Epoch: 4 - Batch: 1640, Training Loss: 0.14197903317376157\n",
      "Epoch: 4 - Batch: 1641, Training Loss: 0.14206484386428672\n",
      "Epoch: 4 - Batch: 1642, Training Loss: 0.14215631610644397\n",
      "Epoch: 4 - Batch: 1643, Training Loss: 0.14224578663830337\n",
      "Epoch: 4 - Batch: 1644, Training Loss: 0.1423312405349801\n",
      "Epoch: 4 - Batch: 1645, Training Loss: 0.14241195823472136\n",
      "Epoch: 4 - Batch: 1646, Training Loss: 0.14249730921948134\n",
      "Epoch: 4 - Batch: 1647, Training Loss: 0.14258842236960112\n",
      "Epoch: 4 - Batch: 1648, Training Loss: 0.14267358212279246\n",
      "Epoch: 4 - Batch: 1649, Training Loss: 0.14275863283556295\n",
      "Epoch: 4 - Batch: 1650, Training Loss: 0.14285147561387437\n",
      "Epoch: 4 - Batch: 1651, Training Loss: 0.14293395738375325\n",
      "Epoch: 4 - Batch: 1652, Training Loss: 0.14301682772673976\n",
      "Epoch: 4 - Batch: 1653, Training Loss: 0.1431062729043846\n",
      "Epoch: 4 - Batch: 1654, Training Loss: 0.14319129168344769\n",
      "Epoch: 4 - Batch: 1655, Training Loss: 0.14327975036962512\n",
      "Epoch: 4 - Batch: 1656, Training Loss: 0.14337221407648146\n",
      "Epoch: 4 - Batch: 1657, Training Loss: 0.14345591911925606\n",
      "Epoch: 4 - Batch: 1658, Training Loss: 0.14354402911084801\n",
      "Epoch: 4 - Batch: 1659, Training Loss: 0.14363297859629984\n",
      "Epoch: 4 - Batch: 1660, Training Loss: 0.14372141630223537\n",
      "Epoch: 4 - Batch: 1661, Training Loss: 0.143805606380278\n",
      "Epoch: 4 - Batch: 1662, Training Loss: 0.14389792654298827\n",
      "Epoch: 4 - Batch: 1663, Training Loss: 0.14398523173596137\n",
      "Epoch: 4 - Batch: 1664, Training Loss: 0.1440660676441679\n",
      "Epoch: 4 - Batch: 1665, Training Loss: 0.14415531468949902\n",
      "Epoch: 4 - Batch: 1666, Training Loss: 0.14424167445568897\n",
      "Epoch: 4 - Batch: 1667, Training Loss: 0.14432456032998525\n",
      "Epoch: 4 - Batch: 1668, Training Loss: 0.14441416864198436\n",
      "Epoch: 4 - Batch: 1669, Training Loss: 0.14450088431240116\n",
      "Epoch: 4 - Batch: 1670, Training Loss: 0.1445829898773833\n",
      "Epoch: 4 - Batch: 1671, Training Loss: 0.144675454405904\n",
      "Epoch: 4 - Batch: 1672, Training Loss: 0.14476089673132247\n",
      "Epoch: 4 - Batch: 1673, Training Loss: 0.1448439167284254\n",
      "Epoch: 4 - Batch: 1674, Training Loss: 0.14493636948799415\n",
      "Epoch: 4 - Batch: 1675, Training Loss: 0.1450271313240872\n",
      "Epoch: 4 - Batch: 1676, Training Loss: 0.14512235930754772\n",
      "Epoch: 4 - Batch: 1677, Training Loss: 0.1452082611039701\n",
      "Epoch: 4 - Batch: 1678, Training Loss: 0.1452891808281194\n",
      "Epoch: 4 - Batch: 1679, Training Loss: 0.1453687595960315\n",
      "Epoch: 4 - Batch: 1680, Training Loss: 0.1454565352814312\n",
      "Epoch: 4 - Batch: 1681, Training Loss: 0.14554078804072654\n",
      "Epoch: 4 - Batch: 1682, Training Loss: 0.145638834703621\n",
      "Epoch: 4 - Batch: 1683, Training Loss: 0.14571772275452394\n",
      "Epoch: 4 - Batch: 1684, Training Loss: 0.14580181711175746\n",
      "Epoch: 4 - Batch: 1685, Training Loss: 0.1458837939843313\n",
      "Epoch: 4 - Batch: 1686, Training Loss: 0.1459640666385294\n",
      "Epoch: 4 - Batch: 1687, Training Loss: 0.14605015177730699\n",
      "Epoch: 4 - Batch: 1688, Training Loss: 0.14613556717313939\n",
      "Epoch: 4 - Batch: 1689, Training Loss: 0.14621897346369464\n",
      "Epoch: 4 - Batch: 1690, Training Loss: 0.14631519630353645\n",
      "Epoch: 4 - Batch: 1691, Training Loss: 0.14638947284043724\n",
      "Epoch: 4 - Batch: 1692, Training Loss: 0.14647910532666675\n",
      "Epoch: 4 - Batch: 1693, Training Loss: 0.14656224256230033\n",
      "Epoch: 4 - Batch: 1694, Training Loss: 0.14664843166324235\n",
      "Epoch: 4 - Batch: 1695, Training Loss: 0.14672693291410285\n",
      "Epoch: 4 - Batch: 1696, Training Loss: 0.14681458013567758\n",
      "Epoch: 4 - Batch: 1697, Training Loss: 0.1469020120503001\n",
      "Epoch: 4 - Batch: 1698, Training Loss: 0.1469865681334811\n",
      "Epoch: 4 - Batch: 1699, Training Loss: 0.14707693536063135\n",
      "Epoch: 4 - Batch: 1700, Training Loss: 0.14716417050826214\n",
      "Epoch: 4 - Batch: 1701, Training Loss: 0.14724175413301335\n",
      "Epoch: 4 - Batch: 1702, Training Loss: 0.14732757178606284\n",
      "Epoch: 4 - Batch: 1703, Training Loss: 0.14741155130180159\n",
      "Epoch: 4 - Batch: 1704, Training Loss: 0.1474976809031236\n",
      "Epoch: 4 - Batch: 1705, Training Loss: 0.14758691931171203\n",
      "Epoch: 4 - Batch: 1706, Training Loss: 0.14767554301651162\n",
      "Epoch: 4 - Batch: 1707, Training Loss: 0.14776237957661426\n",
      "Epoch: 4 - Batch: 1708, Training Loss: 0.14785153437930948\n",
      "Epoch: 4 - Batch: 1709, Training Loss: 0.1479359810761057\n",
      "Epoch: 4 - Batch: 1710, Training Loss: 0.14802361904932293\n",
      "Epoch: 4 - Batch: 1711, Training Loss: 0.14810946640584796\n",
      "Epoch: 4 - Batch: 1712, Training Loss: 0.14819884466418184\n",
      "Epoch: 4 - Batch: 1713, Training Loss: 0.14828586222520515\n",
      "Epoch: 4 - Batch: 1714, Training Loss: 0.1483809014668018\n",
      "Epoch: 4 - Batch: 1715, Training Loss: 0.148468346107955\n",
      "Epoch: 4 - Batch: 1716, Training Loss: 0.14856027920497195\n",
      "Epoch: 4 - Batch: 1717, Training Loss: 0.14864844026331284\n",
      "Epoch: 4 - Batch: 1718, Training Loss: 0.1487407197814379\n",
      "Epoch: 4 - Batch: 1719, Training Loss: 0.14883200845883457\n",
      "Epoch: 4 - Batch: 1720, Training Loss: 0.14891463954922177\n",
      "Epoch: 4 - Batch: 1721, Training Loss: 0.14899960413550462\n",
      "Epoch: 4 - Batch: 1722, Training Loss: 0.14908638805966473\n",
      "Epoch: 4 - Batch: 1723, Training Loss: 0.14917201511747208\n",
      "Epoch: 4 - Batch: 1724, Training Loss: 0.14925877280397398\n",
      "Epoch: 4 - Batch: 1725, Training Loss: 0.14934917631409259\n",
      "Epoch: 4 - Batch: 1726, Training Loss: 0.14943366967895336\n",
      "Epoch: 4 - Batch: 1727, Training Loss: 0.14953317067541094\n",
      "Epoch: 4 - Batch: 1728, Training Loss: 0.14961443879142725\n",
      "Epoch: 4 - Batch: 1729, Training Loss: 0.14970226045296361\n",
      "Epoch: 4 - Batch: 1730, Training Loss: 0.1497798157457392\n",
      "Epoch: 4 - Batch: 1731, Training Loss: 0.14986356052494365\n",
      "Epoch: 4 - Batch: 1732, Training Loss: 0.14995803063732277\n",
      "Epoch: 4 - Batch: 1733, Training Loss: 0.1500415410074231\n",
      "Epoch: 4 - Batch: 1734, Training Loss: 0.15013309023891316\n",
      "Epoch: 4 - Batch: 1735, Training Loss: 0.15022174671144034\n",
      "Epoch: 4 - Batch: 1736, Training Loss: 0.1503029674118629\n",
      "Epoch: 4 - Batch: 1737, Training Loss: 0.15040204426270615\n",
      "Epoch: 4 - Batch: 1738, Training Loss: 0.1504830876874983\n",
      "Epoch: 4 - Batch: 1739, Training Loss: 0.15057243332637482\n",
      "Epoch: 4 - Batch: 1740, Training Loss: 0.15065881016835644\n",
      "Epoch: 4 - Batch: 1741, Training Loss: 0.15074441921132714\n",
      "Epoch: 4 - Batch: 1742, Training Loss: 0.1508292083135786\n",
      "Epoch: 4 - Batch: 1743, Training Loss: 0.1509046220895564\n",
      "Epoch: 4 - Batch: 1744, Training Loss: 0.1509819285786567\n",
      "Epoch: 4 - Batch: 1745, Training Loss: 0.15108223547958222\n",
      "Epoch: 4 - Batch: 1746, Training Loss: 0.1511600376114521\n",
      "Epoch: 4 - Batch: 1747, Training Loss: 0.1512437598412211\n",
      "Epoch: 4 - Batch: 1748, Training Loss: 0.15133395475361675\n",
      "Epoch: 4 - Batch: 1749, Training Loss: 0.15142034372904803\n",
      "Epoch: 4 - Batch: 1750, Training Loss: 0.15150083185082447\n",
      "Epoch: 4 - Batch: 1751, Training Loss: 0.15158241646206794\n",
      "Epoch: 4 - Batch: 1752, Training Loss: 0.1516746421148429\n",
      "Epoch: 4 - Batch: 1753, Training Loss: 0.15176226224620543\n",
      "Epoch: 4 - Batch: 1754, Training Loss: 0.15184832940721393\n",
      "Epoch: 4 - Batch: 1755, Training Loss: 0.15193664637791182\n",
      "Epoch: 4 - Batch: 1756, Training Loss: 0.15202027737204707\n",
      "Epoch: 4 - Batch: 1757, Training Loss: 0.15210120570822735\n",
      "Epoch: 4 - Batch: 1758, Training Loss: 0.15218751012018664\n",
      "Epoch: 4 - Batch: 1759, Training Loss: 0.15227126231908206\n",
      "Epoch: 4 - Batch: 1760, Training Loss: 0.15235850420065386\n",
      "Epoch: 4 - Batch: 1761, Training Loss: 0.1524476097467329\n",
      "Epoch: 4 - Batch: 1762, Training Loss: 0.1525289571317373\n",
      "Epoch: 4 - Batch: 1763, Training Loss: 0.15261802507277153\n",
      "Epoch: 4 - Batch: 1764, Training Loss: 0.1526956773219417\n",
      "Epoch: 4 - Batch: 1765, Training Loss: 0.15277596243701963\n",
      "Epoch: 4 - Batch: 1766, Training Loss: 0.15285220306063962\n",
      "Epoch: 4 - Batch: 1767, Training Loss: 0.15293876261482783\n",
      "Epoch: 4 - Batch: 1768, Training Loss: 0.15302000016245873\n",
      "Epoch: 4 - Batch: 1769, Training Loss: 0.15311119690611588\n",
      "Epoch: 4 - Batch: 1770, Training Loss: 0.15319724573103546\n",
      "Epoch: 4 - Batch: 1771, Training Loss: 0.15327405403206004\n",
      "Epoch: 4 - Batch: 1772, Training Loss: 0.15335992749032887\n",
      "Epoch: 4 - Batch: 1773, Training Loss: 0.15344875270059058\n",
      "Epoch: 4 - Batch: 1774, Training Loss: 0.15353356032426876\n",
      "Epoch: 4 - Batch: 1775, Training Loss: 0.15361383598711756\n",
      "Epoch: 4 - Batch: 1776, Training Loss: 0.15369104738555736\n",
      "Epoch: 4 - Batch: 1777, Training Loss: 0.15379055730912025\n",
      "Epoch: 4 - Batch: 1778, Training Loss: 0.15389877177811973\n",
      "Epoch: 4 - Batch: 1779, Training Loss: 0.15399021191358764\n",
      "Epoch: 4 - Batch: 1780, Training Loss: 0.1540714417758767\n",
      "Epoch: 4 - Batch: 1781, Training Loss: 0.1541587825042889\n",
      "Epoch: 4 - Batch: 1782, Training Loss: 0.15424120735074354\n",
      "Epoch: 4 - Batch: 1783, Training Loss: 0.15432786313850883\n",
      "Epoch: 4 - Batch: 1784, Training Loss: 0.15441219220138702\n",
      "Epoch: 4 - Batch: 1785, Training Loss: 0.15449900999912378\n",
      "Epoch: 4 - Batch: 1786, Training Loss: 0.15458187787093927\n",
      "Epoch: 4 - Batch: 1787, Training Loss: 0.15466760607377608\n",
      "Epoch: 4 - Batch: 1788, Training Loss: 0.15475551249647812\n",
      "Epoch: 4 - Batch: 1789, Training Loss: 0.15483962248386832\n",
      "Epoch: 4 - Batch: 1790, Training Loss: 0.1549329201368056\n",
      "Epoch: 4 - Batch: 1791, Training Loss: 0.15502171477250395\n",
      "Epoch: 4 - Batch: 1792, Training Loss: 0.1551061699330609\n",
      "Epoch: 4 - Batch: 1793, Training Loss: 0.15518781681781385\n",
      "Epoch: 4 - Batch: 1794, Training Loss: 0.15527644776314448\n",
      "Epoch: 4 - Batch: 1795, Training Loss: 0.15536419625948517\n",
      "Epoch: 4 - Batch: 1796, Training Loss: 0.15544698746322003\n",
      "Epoch: 4 - Batch: 1797, Training Loss: 0.15553617328528938\n",
      "Epoch: 4 - Batch: 1798, Training Loss: 0.15562622137305945\n",
      "Epoch: 4 - Batch: 1799, Training Loss: 0.1557197480047026\n",
      "Epoch: 4 - Batch: 1800, Training Loss: 0.1558108829073052\n",
      "Epoch: 4 - Batch: 1801, Training Loss: 0.15589460212224554\n",
      "Epoch: 4 - Batch: 1802, Training Loss: 0.15598796542184074\n",
      "Epoch: 4 - Batch: 1803, Training Loss: 0.15607541661777505\n",
      "Epoch: 4 - Batch: 1804, Training Loss: 0.1561720702753929\n",
      "Epoch: 4 - Batch: 1805, Training Loss: 0.15625843857947866\n",
      "Epoch: 4 - Batch: 1806, Training Loss: 0.1563434359087774\n",
      "Epoch: 4 - Batch: 1807, Training Loss: 0.15642459121583707\n",
      "Epoch: 4 - Batch: 1808, Training Loss: 0.15652221547529274\n",
      "Epoch: 4 - Batch: 1809, Training Loss: 0.15660863842314748\n",
      "Epoch: 4 - Batch: 1810, Training Loss: 0.1566891838822693\n",
      "Epoch: 4 - Batch: 1811, Training Loss: 0.15677517452346745\n",
      "Epoch: 4 - Batch: 1812, Training Loss: 0.1568663414215567\n",
      "Epoch: 4 - Batch: 1813, Training Loss: 0.15695375572290785\n",
      "Epoch: 4 - Batch: 1814, Training Loss: 0.15704065309235113\n",
      "Epoch: 4 - Batch: 1815, Training Loss: 0.1571270204759257\n",
      "Epoch: 4 - Batch: 1816, Training Loss: 0.15721255055880468\n",
      "Epoch: 4 - Batch: 1817, Training Loss: 0.15729762810849235\n",
      "Epoch: 4 - Batch: 1818, Training Loss: 0.1573906552855846\n",
      "Epoch: 4 - Batch: 1819, Training Loss: 0.15747376243471112\n",
      "Epoch: 4 - Batch: 1820, Training Loss: 0.15756314295416646\n",
      "Epoch: 4 - Batch: 1821, Training Loss: 0.15765022865773037\n",
      "Epoch: 4 - Batch: 1822, Training Loss: 0.1577367466530001\n",
      "Epoch: 4 - Batch: 1823, Training Loss: 0.15782651452851137\n",
      "Epoch: 4 - Batch: 1824, Training Loss: 0.1579093649291478\n",
      "Epoch: 4 - Batch: 1825, Training Loss: 0.15799428903147159\n",
      "Epoch: 4 - Batch: 1826, Training Loss: 0.15807803799313297\n",
      "Epoch: 4 - Batch: 1827, Training Loss: 0.15816637110675547\n",
      "Epoch: 4 - Batch: 1828, Training Loss: 0.1582558470999028\n",
      "Epoch: 4 - Batch: 1829, Training Loss: 0.15833793464387036\n",
      "Epoch: 4 - Batch: 1830, Training Loss: 0.1584211639120903\n",
      "Epoch: 4 - Batch: 1831, Training Loss: 0.15850846497202986\n",
      "Epoch: 4 - Batch: 1832, Training Loss: 0.15859686020471367\n",
      "Epoch: 4 - Batch: 1833, Training Loss: 0.15869737682156698\n",
      "Epoch: 4 - Batch: 1834, Training Loss: 0.15878138772117756\n",
      "Epoch: 4 - Batch: 1835, Training Loss: 0.15886321385849766\n",
      "Epoch: 4 - Batch: 1836, Training Loss: 0.15894756676817612\n",
      "Epoch: 4 - Batch: 1837, Training Loss: 0.15903065609398173\n",
      "Epoch: 4 - Batch: 1838, Training Loss: 0.15910789200026004\n",
      "Epoch: 4 - Batch: 1839, Training Loss: 0.15919018261888332\n",
      "Epoch: 4 - Batch: 1840, Training Loss: 0.15927985429788505\n",
      "Epoch: 4 - Batch: 1841, Training Loss: 0.15936429955522416\n",
      "Epoch: 4 - Batch: 1842, Training Loss: 0.15944211312861584\n",
      "Epoch: 4 - Batch: 1843, Training Loss: 0.15952537964973876\n",
      "Epoch: 4 - Batch: 1844, Training Loss: 0.15960628977956662\n",
      "Epoch: 4 - Batch: 1845, Training Loss: 0.1596865534510581\n",
      "Epoch: 4 - Batch: 1846, Training Loss: 0.1597743036401509\n",
      "Epoch: 4 - Batch: 1847, Training Loss: 0.15985366000267207\n",
      "Epoch: 4 - Batch: 1848, Training Loss: 0.15993526391993906\n",
      "Epoch: 4 - Batch: 1849, Training Loss: 0.16003151618524966\n",
      "Epoch: 4 - Batch: 1850, Training Loss: 0.16011801622128408\n",
      "Epoch: 4 - Batch: 1851, Training Loss: 0.16021358803730107\n",
      "Epoch: 4 - Batch: 1852, Training Loss: 0.16029811630745236\n",
      "Epoch: 4 - Batch: 1853, Training Loss: 0.16038165164873572\n",
      "Epoch: 4 - Batch: 1854, Training Loss: 0.16047146913300503\n",
      "Epoch: 4 - Batch: 1855, Training Loss: 0.16056594488583195\n",
      "Epoch: 4 - Batch: 1856, Training Loss: 0.16065072657457038\n",
      "Epoch: 4 - Batch: 1857, Training Loss: 0.1607458877004992\n",
      "Epoch: 4 - Batch: 1858, Training Loss: 0.16083300846997975\n",
      "Epoch: 4 - Batch: 1859, Training Loss: 0.16090970055800963\n",
      "Epoch: 4 - Batch: 1860, Training Loss: 0.1609945552883852\n",
      "Epoch: 4 - Batch: 1861, Training Loss: 0.16107878362633302\n",
      "Epoch: 4 - Batch: 1862, Training Loss: 0.16116170380666087\n",
      "Epoch: 4 - Batch: 1863, Training Loss: 0.16124550723293132\n",
      "Epoch: 4 - Batch: 1864, Training Loss: 0.16133902648195106\n",
      "Epoch: 4 - Batch: 1865, Training Loss: 0.16143148241697458\n",
      "Epoch: 4 - Batch: 1866, Training Loss: 0.16152158524736046\n",
      "Epoch: 4 - Batch: 1867, Training Loss: 0.1616134923538363\n",
      "Epoch: 4 - Batch: 1868, Training Loss: 0.16170244041218687\n",
      "Epoch: 4 - Batch: 1869, Training Loss: 0.1617880670993186\n",
      "Epoch: 4 - Batch: 1870, Training Loss: 0.16187435412041187\n",
      "Epoch: 4 - Batch: 1871, Training Loss: 0.16196002613499785\n",
      "Epoch: 4 - Batch: 1872, Training Loss: 0.16204493383492402\n",
      "Epoch: 4 - Batch: 1873, Training Loss: 0.16213389308767928\n",
      "Epoch: 4 - Batch: 1874, Training Loss: 0.16222226826063238\n",
      "Epoch: 4 - Batch: 1875, Training Loss: 0.16231301441714538\n",
      "Epoch: 4 - Batch: 1876, Training Loss: 0.1623899740465047\n",
      "Epoch: 4 - Batch: 1877, Training Loss: 0.162459900117731\n",
      "Epoch: 4 - Batch: 1878, Training Loss: 0.16254252525259608\n",
      "Epoch: 4 - Batch: 1879, Training Loss: 0.16263457033072737\n",
      "Epoch: 4 - Batch: 1880, Training Loss: 0.16272330334777657\n",
      "Epoch: 4 - Batch: 1881, Training Loss: 0.16281259661018355\n",
      "Epoch: 4 - Batch: 1882, Training Loss: 0.1629000938255\n",
      "Epoch: 4 - Batch: 1883, Training Loss: 0.16297899244065903\n",
      "Epoch: 4 - Batch: 1884, Training Loss: 0.16306013007884596\n",
      "Epoch: 4 - Batch: 1885, Training Loss: 0.16314369871298076\n",
      "Epoch: 4 - Batch: 1886, Training Loss: 0.16322398348827266\n",
      "Epoch: 4 - Batch: 1887, Training Loss: 0.16330573065932316\n",
      "Epoch: 4 - Batch: 1888, Training Loss: 0.163380628621598\n",
      "Epoch: 4 - Batch: 1889, Training Loss: 0.1634630018205785\n",
      "Epoch: 4 - Batch: 1890, Training Loss: 0.1635473783299401\n",
      "Epoch: 4 - Batch: 1891, Training Loss: 0.1636242754016053\n",
      "Epoch: 4 - Batch: 1892, Training Loss: 0.163705668280868\n",
      "Epoch: 4 - Batch: 1893, Training Loss: 0.1637915937819983\n",
      "Epoch: 4 - Batch: 1894, Training Loss: 0.16389179553459732\n",
      "Epoch: 4 - Batch: 1895, Training Loss: 0.16398178950990017\n",
      "Epoch: 4 - Batch: 1896, Training Loss: 0.164070931048585\n",
      "Epoch: 4 - Batch: 1897, Training Loss: 0.16415436972383637\n",
      "Epoch: 4 - Batch: 1898, Training Loss: 0.16423918117782962\n",
      "Epoch: 4 - Batch: 1899, Training Loss: 0.16432958558374772\n",
      "Epoch: 4 - Batch: 1900, Training Loss: 0.16440940692180622\n",
      "Epoch: 4 - Batch: 1901, Training Loss: 0.1644929883627848\n",
      "Epoch: 4 - Batch: 1902, Training Loss: 0.1645806620135335\n",
      "Epoch: 4 - Batch: 1903, Training Loss: 0.16467610601440788\n",
      "Epoch: 4 - Batch: 1904, Training Loss: 0.16476774667304744\n",
      "Epoch: 4 - Batch: 1905, Training Loss: 0.16485399376619514\n",
      "Epoch: 4 - Batch: 1906, Training Loss: 0.164943899601006\n",
      "Epoch: 4 - Batch: 1907, Training Loss: 0.16503120763582574\n",
      "Epoch: 4 - Batch: 1908, Training Loss: 0.165115004260198\n",
      "Epoch: 4 - Batch: 1909, Training Loss: 0.16520477776997916\n",
      "Epoch: 4 - Batch: 1910, Training Loss: 0.16528942153030762\n",
      "Epoch: 4 - Batch: 1911, Training Loss: 0.1653750474699101\n",
      "Epoch: 4 - Batch: 1912, Training Loss: 0.16547133834108982\n",
      "Epoch: 4 - Batch: 1913, Training Loss: 0.16556650614535828\n",
      "Epoch: 4 - Batch: 1914, Training Loss: 0.16564972835220707\n",
      "Epoch: 4 - Batch: 1915, Training Loss: 0.16572851937875818\n",
      "Epoch: 4 - Batch: 1916, Training Loss: 0.16581020869969532\n",
      "Epoch: 4 - Batch: 1917, Training Loss: 0.16590184551566395\n",
      "Epoch: 4 - Batch: 1918, Training Loss: 0.16598885611416292\n",
      "Epoch: 4 - Batch: 1919, Training Loss: 0.16607343015657927\n",
      "Epoch: 4 - Batch: 1920, Training Loss: 0.16615507418095177\n",
      "Epoch: 4 - Batch: 1921, Training Loss: 0.16624000562265343\n",
      "Epoch: 4 - Batch: 1922, Training Loss: 0.16632540142827762\n",
      "Epoch: 4 - Batch: 1923, Training Loss: 0.16640163768410288\n",
      "Epoch: 4 - Batch: 1924, Training Loss: 0.1664899330938337\n",
      "Epoch: 4 - Batch: 1925, Training Loss: 0.16658207211353096\n",
      "Epoch: 4 - Batch: 1926, Training Loss: 0.1666716498695005\n",
      "Epoch: 4 - Batch: 1927, Training Loss: 0.16675723484326554\n",
      "Epoch: 4 - Batch: 1928, Training Loss: 0.16683994599959348\n",
      "Epoch: 4 - Batch: 1929, Training Loss: 0.16693078625829857\n",
      "Epoch: 4 - Batch: 1930, Training Loss: 0.16702272917822028\n",
      "Epoch: 4 - Batch: 1931, Training Loss: 0.16710679946007026\n",
      "Epoch: 4 - Batch: 1932, Training Loss: 0.16720043536689547\n",
      "Epoch: 4 - Batch: 1933, Training Loss: 0.16728873361847293\n",
      "Epoch: 4 - Batch: 1934, Training Loss: 0.1673750454748349\n",
      "Epoch: 4 - Batch: 1935, Training Loss: 0.1674644585272566\n",
      "Epoch: 4 - Batch: 1936, Training Loss: 0.16754673098277295\n",
      "Epoch: 4 - Batch: 1937, Training Loss: 0.16763287918583472\n",
      "Epoch: 4 - Batch: 1938, Training Loss: 0.16771577249217784\n",
      "Epoch: 4 - Batch: 1939, Training Loss: 0.1677977642906246\n",
      "Epoch: 4 - Batch: 1940, Training Loss: 0.16788235674326496\n",
      "Epoch: 4 - Batch: 1941, Training Loss: 0.1679753636320432\n",
      "Epoch: 4 - Batch: 1942, Training Loss: 0.16805479529114506\n",
      "Epoch: 4 - Batch: 1943, Training Loss: 0.16814635357329896\n",
      "Epoch: 4 - Batch: 1944, Training Loss: 0.1682376599998814\n",
      "Epoch: 4 - Batch: 1945, Training Loss: 0.1683273232707238\n",
      "Epoch: 4 - Batch: 1946, Training Loss: 0.16841493750216555\n",
      "Epoch: 4 - Batch: 1947, Training Loss: 0.16850110475177787\n",
      "Epoch: 4 - Batch: 1948, Training Loss: 0.16858355031900146\n",
      "Epoch: 4 - Batch: 1949, Training Loss: 0.16867756944884907\n",
      "Epoch: 4 - Batch: 1950, Training Loss: 0.1687690420246144\n",
      "Epoch: 4 - Batch: 1951, Training Loss: 0.16885347929389322\n",
      "Epoch: 4 - Batch: 1952, Training Loss: 0.1689408388649844\n",
      "Epoch: 4 - Batch: 1953, Training Loss: 0.16902727963616007\n",
      "Epoch: 4 - Batch: 1954, Training Loss: 0.16911224288801055\n",
      "Epoch: 4 - Batch: 1955, Training Loss: 0.1692041276859901\n",
      "Epoch: 4 - Batch: 1956, Training Loss: 0.16929233121140483\n",
      "Epoch: 4 - Batch: 1957, Training Loss: 0.16937536607260134\n",
      "Epoch: 4 - Batch: 1958, Training Loss: 0.16945788919134916\n",
      "Epoch: 4 - Batch: 1959, Training Loss: 0.16954082783750238\n",
      "Epoch: 4 - Batch: 1960, Training Loss: 0.1696271154887443\n",
      "Epoch: 4 - Batch: 1961, Training Loss: 0.16970262528824964\n",
      "Epoch: 4 - Batch: 1962, Training Loss: 0.16978493238958356\n",
      "Epoch: 4 - Batch: 1963, Training Loss: 0.16987054161789208\n",
      "Epoch: 4 - Batch: 1964, Training Loss: 0.1699573991829483\n",
      "Epoch: 4 - Batch: 1965, Training Loss: 0.17004663053634353\n",
      "Epoch: 4 - Batch: 1966, Training Loss: 0.17013783172193056\n",
      "Epoch: 4 - Batch: 1967, Training Loss: 0.17021676694748808\n",
      "Epoch: 4 - Batch: 1968, Training Loss: 0.17030722581183733\n",
      "Epoch: 4 - Batch: 1969, Training Loss: 0.17039424386709484\n",
      "Epoch: 4 - Batch: 1970, Training Loss: 0.17048095513882724\n",
      "Epoch: 4 - Batch: 1971, Training Loss: 0.17056893040835364\n",
      "Epoch: 4 - Batch: 1972, Training Loss: 0.17065277231075673\n",
      "Epoch: 4 - Batch: 1973, Training Loss: 0.1707390383145406\n",
      "Epoch: 4 - Batch: 1974, Training Loss: 0.17082730044995376\n",
      "Epoch: 4 - Batch: 1975, Training Loss: 0.17091464463800538\n",
      "Epoch: 4 - Batch: 1976, Training Loss: 0.17100252411258754\n",
      "Epoch: 4 - Batch: 1977, Training Loss: 0.17107874134039602\n",
      "Epoch: 4 - Batch: 1978, Training Loss: 0.17116898999458324\n",
      "Epoch: 4 - Batch: 1979, Training Loss: 0.1712573321650475\n",
      "Epoch: 4 - Batch: 1980, Training Loss: 0.17133365079118046\n",
      "Epoch: 4 - Batch: 1981, Training Loss: 0.17143180281245096\n",
      "Epoch: 4 - Batch: 1982, Training Loss: 0.17151270823469802\n",
      "Epoch: 4 - Batch: 1983, Training Loss: 0.171602288684244\n",
      "Epoch: 4 - Batch: 1984, Training Loss: 0.17170090632355628\n",
      "Epoch: 4 - Batch: 1985, Training Loss: 0.17178753096838892\n",
      "Epoch: 4 - Batch: 1986, Training Loss: 0.17187405127420355\n",
      "Epoch: 4 - Batch: 1987, Training Loss: 0.17196678860764797\n",
      "Epoch: 4 - Batch: 1988, Training Loss: 0.17204781879314143\n",
      "Epoch: 4 - Batch: 1989, Training Loss: 0.17212543619262244\n",
      "Epoch: 4 - Batch: 1990, Training Loss: 0.17220007293358766\n",
      "Epoch: 4 - Batch: 1991, Training Loss: 0.17228167783932305\n",
      "Epoch: 4 - Batch: 1992, Training Loss: 0.172367556060774\n",
      "Epoch: 4 - Batch: 1993, Training Loss: 0.1724539661770733\n",
      "Epoch: 4 - Batch: 1994, Training Loss: 0.172540681174096\n",
      "Epoch: 4 - Batch: 1995, Training Loss: 0.17263211807850778\n",
      "Epoch: 4 - Batch: 1996, Training Loss: 0.17272478914750156\n",
      "Epoch: 4 - Batch: 1997, Training Loss: 0.17280745942448306\n",
      "Epoch: 4 - Batch: 1998, Training Loss: 0.17289666366295434\n",
      "Epoch: 4 - Batch: 1999, Training Loss: 0.17298325632465616\n",
      "Epoch: 4 - Batch: 2000, Training Loss: 0.17306767170553777\n",
      "Epoch: 4 - Batch: 2001, Training Loss: 0.17314873468964848\n",
      "Epoch: 4 - Batch: 2002, Training Loss: 0.17324141240609225\n",
      "Epoch: 4 - Batch: 2003, Training Loss: 0.17332492495082305\n",
      "Epoch: 4 - Batch: 2004, Training Loss: 0.17340365023804738\n",
      "Epoch: 4 - Batch: 2005, Training Loss: 0.17348691751287745\n",
      "Epoch: 4 - Batch: 2006, Training Loss: 0.1735772643157993\n",
      "Epoch: 4 - Batch: 2007, Training Loss: 0.17366780034246335\n",
      "Epoch: 4 - Batch: 2008, Training Loss: 0.17375905772247915\n",
      "Epoch: 4 - Batch: 2009, Training Loss: 0.1738430168325233\n",
      "Epoch: 4 - Batch: 2010, Training Loss: 0.173933609319267\n",
      "Epoch: 4 - Batch: 2011, Training Loss: 0.1740188262738893\n",
      "Epoch: 4 - Batch: 2012, Training Loss: 0.17410564723472494\n",
      "Epoch: 4 - Batch: 2013, Training Loss: 0.1741952178181206\n",
      "Epoch: 4 - Batch: 2014, Training Loss: 0.17427495026815787\n",
      "Epoch: 4 - Batch: 2015, Training Loss: 0.17435804090368412\n",
      "Epoch: 4 - Batch: 2016, Training Loss: 0.17444273331519186\n",
      "Epoch: 4 - Batch: 2017, Training Loss: 0.17452368179141586\n",
      "Epoch: 4 - Batch: 2018, Training Loss: 0.17461584769127578\n",
      "Epoch: 4 - Batch: 2019, Training Loss: 0.1747017894650672\n",
      "Epoch: 4 - Batch: 2020, Training Loss: 0.17478497444990262\n",
      "Epoch: 4 - Batch: 2021, Training Loss: 0.17487583804263998\n",
      "Epoch: 4 - Batch: 2022, Training Loss: 0.17495080777389888\n",
      "Epoch: 4 - Batch: 2023, Training Loss: 0.17503477954869445\n",
      "Epoch: 4 - Batch: 2024, Training Loss: 0.17512229763922801\n",
      "Epoch: 4 - Batch: 2025, Training Loss: 0.1752084260667438\n",
      "Epoch: 4 - Batch: 2026, Training Loss: 0.17529441578125282\n",
      "Epoch: 4 - Batch: 2027, Training Loss: 0.17537526555890665\n",
      "Epoch: 4 - Batch: 2028, Training Loss: 0.17546438880474807\n",
      "Epoch: 4 - Batch: 2029, Training Loss: 0.17555036874577576\n",
      "Epoch: 4 - Batch: 2030, Training Loss: 0.17563318964680827\n",
      "Epoch: 4 - Batch: 2031, Training Loss: 0.17571877443523548\n",
      "Epoch: 4 - Batch: 2032, Training Loss: 0.17580797301472517\n",
      "Epoch: 4 - Batch: 2033, Training Loss: 0.17589351936408734\n",
      "Epoch: 4 - Batch: 2034, Training Loss: 0.17597761615542432\n",
      "Epoch: 4 - Batch: 2035, Training Loss: 0.17606757666489378\n",
      "Epoch: 4 - Batch: 2036, Training Loss: 0.17615366859321374\n",
      "Epoch: 4 - Batch: 2037, Training Loss: 0.1762392744916193\n",
      "Epoch: 4 - Batch: 2038, Training Loss: 0.17632382020206\n",
      "Epoch: 4 - Batch: 2039, Training Loss: 0.17639415333693104\n",
      "Epoch: 4 - Batch: 2040, Training Loss: 0.1764753450319838\n",
      "Epoch: 4 - Batch: 2041, Training Loss: 0.17655728693748785\n",
      "Epoch: 4 - Batch: 2042, Training Loss: 0.17664284916412376\n",
      "Epoch: 4 - Batch: 2043, Training Loss: 0.17672779251069176\n",
      "Epoch: 4 - Batch: 2044, Training Loss: 0.17681064938579627\n",
      "Epoch: 4 - Batch: 2045, Training Loss: 0.17690032094123945\n",
      "Epoch: 4 - Batch: 2046, Training Loss: 0.1769693206693007\n",
      "Epoch: 4 - Batch: 2047, Training Loss: 0.17706640759733186\n",
      "Epoch: 4 - Batch: 2048, Training Loss: 0.17714507746236835\n",
      "Epoch: 4 - Batch: 2049, Training Loss: 0.17722184155414353\n",
      "Epoch: 4 - Batch: 2050, Training Loss: 0.17730497040632945\n",
      "Epoch: 4 - Batch: 2051, Training Loss: 0.17739013929049768\n",
      "Epoch: 4 - Batch: 2052, Training Loss: 0.17747703412081273\n",
      "Epoch: 4 - Batch: 2053, Training Loss: 0.17756986239335035\n",
      "Epoch: 4 - Batch: 2054, Training Loss: 0.17765602626981428\n",
      "Epoch: 4 - Batch: 2055, Training Loss: 0.1777436206577528\n",
      "Epoch: 4 - Batch: 2056, Training Loss: 0.17782328102373168\n",
      "Epoch: 4 - Batch: 2057, Training Loss: 0.17790964369354753\n",
      "Epoch: 4 - Batch: 2058, Training Loss: 0.17798580384348361\n",
      "Epoch: 4 - Batch: 2059, Training Loss: 0.1780746395809338\n",
      "Epoch: 4 - Batch: 2060, Training Loss: 0.17815420011160385\n",
      "Epoch: 4 - Batch: 2061, Training Loss: 0.17824274169939075\n",
      "Epoch: 4 - Batch: 2062, Training Loss: 0.17832619537465014\n",
      "Epoch: 4 - Batch: 2063, Training Loss: 0.17841982795157244\n",
      "Epoch: 4 - Batch: 2064, Training Loss: 0.17850557913629966\n",
      "Epoch: 4 - Batch: 2065, Training Loss: 0.1785978310205549\n",
      "Epoch: 4 - Batch: 2066, Training Loss: 0.1786864214510368\n",
      "Epoch: 4 - Batch: 2067, Training Loss: 0.17877135157066198\n",
      "Epoch: 4 - Batch: 2068, Training Loss: 0.17885185050381158\n",
      "Epoch: 4 - Batch: 2069, Training Loss: 0.17893642331682033\n",
      "Epoch: 4 - Batch: 2070, Training Loss: 0.17901848016266603\n",
      "Epoch: 4 - Batch: 2071, Training Loss: 0.17910668911847902\n",
      "Epoch: 4 - Batch: 2072, Training Loss: 0.17919141652176826\n",
      "Epoch: 4 - Batch: 2073, Training Loss: 0.17928426866581784\n",
      "Epoch: 4 - Batch: 2074, Training Loss: 0.1793648445635886\n",
      "Epoch: 4 - Batch: 2075, Training Loss: 0.17944960453691175\n",
      "Epoch: 4 - Batch: 2076, Training Loss: 0.17953320700755562\n",
      "Epoch: 4 - Batch: 2077, Training Loss: 0.17961699441446\n",
      "Epoch: 4 - Batch: 2078, Training Loss: 0.17971121893568617\n",
      "Epoch: 4 - Batch: 2079, Training Loss: 0.17979828005727647\n",
      "Epoch: 4 - Batch: 2080, Training Loss: 0.17988492437263034\n",
      "Epoch: 4 - Batch: 2081, Training Loss: 0.17997494428287295\n",
      "Epoch: 4 - Batch: 2082, Training Loss: 0.18006430138180504\n",
      "Epoch: 4 - Batch: 2083, Training Loss: 0.18015289986830446\n",
      "Epoch: 4 - Batch: 2084, Training Loss: 0.1802354761419407\n",
      "Epoch: 4 - Batch: 2085, Training Loss: 0.18032634135947298\n",
      "Epoch: 4 - Batch: 2086, Training Loss: 0.1804129348984405\n",
      "Epoch: 4 - Batch: 2087, Training Loss: 0.18049404018899892\n",
      "Epoch: 4 - Batch: 2088, Training Loss: 0.1805853891753241\n",
      "Epoch: 4 - Batch: 2089, Training Loss: 0.1806708547606397\n",
      "Epoch: 4 - Batch: 2090, Training Loss: 0.18076388335247737\n",
      "Epoch: 4 - Batch: 2091, Training Loss: 0.18085113084360735\n",
      "Epoch: 4 - Batch: 2092, Training Loss: 0.18093049895654667\n",
      "Epoch: 4 - Batch: 2093, Training Loss: 0.18101803780491674\n",
      "Epoch: 4 - Batch: 2094, Training Loss: 0.18109631602023768\n",
      "Epoch: 4 - Batch: 2095, Training Loss: 0.1811782832676004\n",
      "Epoch: 4 - Batch: 2096, Training Loss: 0.18126086306265535\n",
      "Epoch: 4 - Batch: 2097, Training Loss: 0.1813488407477514\n",
      "Epoch: 4 - Batch: 2098, Training Loss: 0.181436074035826\n",
      "Epoch: 4 - Batch: 2099, Training Loss: 0.18152078158943413\n",
      "Epoch: 4 - Batch: 2100, Training Loss: 0.18160591411032093\n",
      "Epoch: 4 - Batch: 2101, Training Loss: 0.1816899189740963\n",
      "Epoch: 4 - Batch: 2102, Training Loss: 0.18176305377438887\n",
      "Epoch: 4 - Batch: 2103, Training Loss: 0.18184640455008738\n",
      "Epoch: 4 - Batch: 2104, Training Loss: 0.18193805288789086\n",
      "Epoch: 4 - Batch: 2105, Training Loss: 0.18202489089980647\n",
      "Epoch: 4 - Batch: 2106, Training Loss: 0.18210256015681114\n",
      "Epoch: 4 - Batch: 2107, Training Loss: 0.18218742551939998\n",
      "Epoch: 4 - Batch: 2108, Training Loss: 0.18226518179251386\n",
      "Epoch: 4 - Batch: 2109, Training Loss: 0.18235340275204004\n",
      "Epoch: 4 - Batch: 2110, Training Loss: 0.18244024805391013\n",
      "Epoch: 4 - Batch: 2111, Training Loss: 0.18252480532325321\n",
      "Epoch: 4 - Batch: 2112, Training Loss: 0.18261506159482507\n",
      "Epoch: 4 - Batch: 2113, Training Loss: 0.18269933086772067\n",
      "Epoch: 4 - Batch: 2114, Training Loss: 0.18278838967777802\n",
      "Epoch: 4 - Batch: 2115, Training Loss: 0.18287989479886557\n",
      "Epoch: 4 - Batch: 2116, Training Loss: 0.1829616871182104\n",
      "Epoch: 4 - Batch: 2117, Training Loss: 0.1830476882557074\n",
      "Epoch: 4 - Batch: 2118, Training Loss: 0.18313238533772838\n",
      "Epoch: 4 - Batch: 2119, Training Loss: 0.1832167760130778\n",
      "Epoch: 4 - Batch: 2120, Training Loss: 0.18330550067756898\n",
      "Epoch: 4 - Batch: 2121, Training Loss: 0.18340036859287948\n",
      "Epoch: 4 - Batch: 2122, Training Loss: 0.18348879824897543\n",
      "Epoch: 4 - Batch: 2123, Training Loss: 0.1835862641187252\n",
      "Epoch: 4 - Batch: 2124, Training Loss: 0.1836797707029935\n",
      "Epoch: 4 - Batch: 2125, Training Loss: 0.1837653987801589\n",
      "Epoch: 4 - Batch: 2126, Training Loss: 0.18385411571606278\n",
      "Epoch: 4 - Batch: 2127, Training Loss: 0.18394310624766508\n",
      "Epoch: 4 - Batch: 2128, Training Loss: 0.18403378468791445\n",
      "Epoch: 4 - Batch: 2129, Training Loss: 0.18411462339472218\n",
      "Epoch: 4 - Batch: 2130, Training Loss: 0.18421087566003277\n",
      "Epoch: 4 - Batch: 2131, Training Loss: 0.18429316047140418\n",
      "Epoch: 4 - Batch: 2132, Training Loss: 0.18437871470380185\n",
      "Epoch: 4 - Batch: 2133, Training Loss: 0.18446086155587366\n",
      "Epoch: 4 - Batch: 2134, Training Loss: 0.1845475658774376\n",
      "Epoch: 4 - Batch: 2135, Training Loss: 0.18463523876212326\n",
      "Epoch: 4 - Batch: 2136, Training Loss: 0.18471825697998304\n",
      "Epoch: 4 - Batch: 2137, Training Loss: 0.18480706785745288\n",
      "Epoch: 4 - Batch: 2138, Training Loss: 0.18488985764026444\n",
      "Epoch: 4 - Batch: 2139, Training Loss: 0.18498021831263356\n",
      "Epoch: 4 - Batch: 2140, Training Loss: 0.1850743559042415\n",
      "Epoch: 4 - Batch: 2141, Training Loss: 0.18516895992937768\n",
      "Epoch: 4 - Batch: 2142, Training Loss: 0.1852554336597967\n",
      "Epoch: 4 - Batch: 2143, Training Loss: 0.18534254411831622\n",
      "Epoch: 4 - Batch: 2144, Training Loss: 0.18542544221255317\n",
      "Epoch: 4 - Batch: 2145, Training Loss: 0.18551499928482137\n",
      "Epoch: 4 - Batch: 2146, Training Loss: 0.18559933965948486\n",
      "Epoch: 4 - Batch: 2147, Training Loss: 0.1856892866331151\n",
      "Epoch: 4 - Batch: 2148, Training Loss: 0.18577424208842105\n",
      "Epoch: 4 - Batch: 2149, Training Loss: 0.18585993468168363\n",
      "Epoch: 4 - Batch: 2150, Training Loss: 0.18594936073246485\n",
      "Epoch: 4 - Batch: 2151, Training Loss: 0.1860388562972866\n",
      "Epoch: 4 - Batch: 2152, Training Loss: 0.1861211471753828\n",
      "Epoch: 4 - Batch: 2153, Training Loss: 0.18621289544734196\n",
      "Epoch: 4 - Batch: 2154, Training Loss: 0.18629375155119357\n",
      "Epoch: 4 - Batch: 2155, Training Loss: 0.18637282392056428\n",
      "Epoch: 4 - Batch: 2156, Training Loss: 0.18646043065817994\n",
      "Epoch: 4 - Batch: 2157, Training Loss: 0.186547830558782\n",
      "Epoch: 4 - Batch: 2158, Training Loss: 0.18662869151230674\n",
      "Epoch: 4 - Batch: 2159, Training Loss: 0.1867134733493154\n",
      "Epoch: 4 - Batch: 2160, Training Loss: 0.186801965180074\n",
      "Epoch: 4 - Batch: 2161, Training Loss: 0.18688698397149298\n",
      "Epoch: 4 - Batch: 2162, Training Loss: 0.1869754479830439\n",
      "Epoch: 4 - Batch: 2163, Training Loss: 0.18706959909195725\n",
      "Epoch: 4 - Batch: 2164, Training Loss: 0.18715551987932888\n",
      "Epoch: 4 - Batch: 2165, Training Loss: 0.18724425988361412\n",
      "Epoch: 4 - Batch: 2166, Training Loss: 0.18732880409899635\n",
      "Epoch: 4 - Batch: 2167, Training Loss: 0.18741437357851917\n",
      "Epoch: 4 - Batch: 2168, Training Loss: 0.18750072488094838\n",
      "Epoch: 4 - Batch: 2169, Training Loss: 0.1875880554837374\n",
      "Epoch: 4 - Batch: 2170, Training Loss: 0.18767382303379462\n",
      "Epoch: 4 - Batch: 2171, Training Loss: 0.1877615915466205\n",
      "Epoch: 4 - Batch: 2172, Training Loss: 0.18786166968506762\n",
      "Epoch: 4 - Batch: 2173, Training Loss: 0.18794125770751516\n",
      "Epoch: 4 - Batch: 2174, Training Loss: 0.18802053940978217\n",
      "Epoch: 4 - Batch: 2175, Training Loss: 0.1881132642699909\n",
      "Epoch: 4 - Batch: 2176, Training Loss: 0.18820327124983122\n",
      "Epoch: 4 - Batch: 2177, Training Loss: 0.18828208211988556\n",
      "Epoch: 4 - Batch: 2178, Training Loss: 0.18836842129504305\n",
      "Epoch: 4 - Batch: 2179, Training Loss: 0.18845445346427003\n",
      "Epoch: 4 - Batch: 2180, Training Loss: 0.18854351902680216\n",
      "Epoch: 4 - Batch: 2181, Training Loss: 0.18862210375381938\n",
      "Epoch: 4 - Batch: 2182, Training Loss: 0.18870608661799487\n",
      "Epoch: 4 - Batch: 2183, Training Loss: 0.18880457710938073\n",
      "Epoch: 4 - Batch: 2184, Training Loss: 0.18888620226018465\n",
      "Epoch: 4 - Batch: 2185, Training Loss: 0.18896794957332747\n",
      "Epoch: 4 - Batch: 2186, Training Loss: 0.18905137816002318\n",
      "Epoch: 4 - Batch: 2187, Training Loss: 0.18912913608882162\n",
      "Epoch: 4 - Batch: 2188, Training Loss: 0.18922023044568587\n",
      "Epoch: 4 - Batch: 2189, Training Loss: 0.18930437557312782\n",
      "Epoch: 4 - Batch: 2190, Training Loss: 0.18938610135001527\n",
      "Epoch: 4 - Batch: 2191, Training Loss: 0.18946294881910036\n",
      "Epoch: 4 - Batch: 2192, Training Loss: 0.18954133174411494\n",
      "Epoch: 4 - Batch: 2193, Training Loss: 0.18962840586406476\n",
      "Epoch: 4 - Batch: 2194, Training Loss: 0.18971945803803986\n",
      "Epoch: 4 - Batch: 2195, Training Loss: 0.18979623687652805\n",
      "Epoch: 4 - Batch: 2196, Training Loss: 0.18988318808660973\n",
      "Epoch: 4 - Batch: 2197, Training Loss: 0.18997286195877575\n",
      "Epoch: 4 - Batch: 2198, Training Loss: 0.19005516284511456\n",
      "Epoch: 4 - Batch: 2199, Training Loss: 0.19014197981500902\n",
      "Epoch: 4 - Batch: 2200, Training Loss: 0.1902215388073751\n",
      "Epoch: 4 - Batch: 2201, Training Loss: 0.19031453683082736\n",
      "Epoch: 4 - Batch: 2202, Training Loss: 0.19039608099517932\n",
      "Epoch: 4 - Batch: 2203, Training Loss: 0.1904906843716331\n",
      "Epoch: 4 - Batch: 2204, Training Loss: 0.1905725069813566\n",
      "Epoch: 4 - Batch: 2205, Training Loss: 0.19066455186797215\n",
      "Epoch: 4 - Batch: 2206, Training Loss: 0.1907478691345918\n",
      "Epoch: 4 - Batch: 2207, Training Loss: 0.19083568626770728\n",
      "Epoch: 4 - Batch: 2208, Training Loss: 0.19092650379163906\n",
      "Epoch: 4 - Batch: 2209, Training Loss: 0.19102402865441878\n",
      "Epoch: 4 - Batch: 2210, Training Loss: 0.19111420870889875\n",
      "Epoch: 4 - Batch: 2211, Training Loss: 0.19120246488878975\n",
      "Epoch: 4 - Batch: 2212, Training Loss: 0.1912929269162379\n",
      "Epoch: 4 - Batch: 2213, Training Loss: 0.1913868326872933\n",
      "Epoch: 4 - Batch: 2214, Training Loss: 0.19147495452197236\n",
      "Epoch: 4 - Batch: 2215, Training Loss: 0.19156270443305842\n",
      "Epoch: 4 - Batch: 2216, Training Loss: 0.19165293871738623\n",
      "Epoch: 4 - Batch: 2217, Training Loss: 0.19174744047928804\n",
      "Epoch: 4 - Batch: 2218, Training Loss: 0.19183341696685424\n",
      "Epoch: 4 - Batch: 2219, Training Loss: 0.1919180615797367\n",
      "Epoch: 4 - Batch: 2220, Training Loss: 0.19201008392927263\n",
      "Epoch: 4 - Batch: 2221, Training Loss: 0.19210073899842217\n",
      "Epoch: 4 - Batch: 2222, Training Loss: 0.19218314713358287\n",
      "Epoch: 4 - Batch: 2223, Training Loss: 0.1922703838874748\n",
      "Epoch: 4 - Batch: 2224, Training Loss: 0.1923541115291083\n",
      "Epoch: 4 - Batch: 2225, Training Loss: 0.19244134289584744\n",
      "Epoch: 4 - Batch: 2226, Training Loss: 0.1925372862524259\n",
      "Epoch: 4 - Batch: 2227, Training Loss: 0.19262247252128215\n",
      "Epoch: 4 - Batch: 2228, Training Loss: 0.19270750609030377\n",
      "Epoch: 4 - Batch: 2229, Training Loss: 0.19279342457948634\n",
      "Epoch: 4 - Batch: 2230, Training Loss: 0.19287742063353705\n",
      "Epoch: 4 - Batch: 2231, Training Loss: 0.19296515947377701\n",
      "Epoch: 4 - Batch: 2232, Training Loss: 0.19305483671909146\n",
      "Epoch: 4 - Batch: 2233, Training Loss: 0.1931417725214219\n",
      "Epoch: 4 - Batch: 2234, Training Loss: 0.19322921947312\n",
      "Epoch: 4 - Batch: 2235, Training Loss: 0.19331472979898673\n",
      "Epoch: 4 - Batch: 2236, Training Loss: 0.19339695852095412\n",
      "Epoch: 4 - Batch: 2237, Training Loss: 0.1934794792055985\n",
      "Epoch: 4 - Batch: 2238, Training Loss: 0.1935690539220277\n",
      "Epoch: 4 - Batch: 2239, Training Loss: 0.19365597606496035\n",
      "Epoch: 4 - Batch: 2240, Training Loss: 0.19374375175036007\n",
      "Epoch: 4 - Batch: 2241, Training Loss: 0.19382565867643847\n",
      "Epoch: 4 - Batch: 2242, Training Loss: 0.1939091596251993\n",
      "Epoch: 4 - Batch: 2243, Training Loss: 0.19399813243850547\n",
      "Epoch: 4 - Batch: 2244, Training Loss: 0.19408633876869927\n",
      "Epoch: 4 - Batch: 2245, Training Loss: 0.19417809974865533\n",
      "Epoch: 4 - Batch: 2246, Training Loss: 0.1942632559433604\n",
      "Epoch: 4 - Batch: 2247, Training Loss: 0.1943510093820431\n",
      "Epoch: 4 - Batch: 2248, Training Loss: 0.19443508671290838\n",
      "Epoch: 4 - Batch: 2249, Training Loss: 0.19452592713841751\n",
      "Epoch: 4 - Batch: 2250, Training Loss: 0.19461106884588253\n",
      "Epoch: 4 - Batch: 2251, Training Loss: 0.19470023331703437\n",
      "Epoch: 4 - Batch: 2252, Training Loss: 0.19479564505059327\n",
      "Epoch: 4 - Batch: 2253, Training Loss: 0.19489006683304536\n",
      "Epoch: 4 - Batch: 2254, Training Loss: 0.19498020023181664\n",
      "Epoch: 4 - Batch: 2255, Training Loss: 0.19506308623584942\n",
      "Epoch: 4 - Batch: 2256, Training Loss: 0.1951486997639955\n",
      "Epoch: 4 - Batch: 2257, Training Loss: 0.19523609058922203\n",
      "Epoch: 4 - Batch: 2258, Training Loss: 0.1953320395924262\n",
      "Epoch: 4 - Batch: 2259, Training Loss: 0.1954258460656525\n",
      "Epoch: 4 - Batch: 2260, Training Loss: 0.19550236804444793\n",
      "Epoch: 4 - Batch: 2261, Training Loss: 0.19558599774739635\n",
      "Epoch: 4 - Batch: 2262, Training Loss: 0.19567055119260232\n",
      "Epoch: 4 - Batch: 2263, Training Loss: 0.19575972641952596\n",
      "Epoch: 4 - Batch: 2264, Training Loss: 0.19584992107244867\n",
      "Epoch: 4 - Batch: 2265, Training Loss: 0.19593282014279817\n",
      "Epoch: 4 - Batch: 2266, Training Loss: 0.1960131770577498\n",
      "Epoch: 4 - Batch: 2267, Training Loss: 0.19610025293840896\n",
      "Epoch: 4 - Batch: 2268, Training Loss: 0.1961885123987795\n",
      "Epoch: 4 - Batch: 2269, Training Loss: 0.19627046055016825\n",
      "Epoch: 4 - Batch: 2270, Training Loss: 0.1963529187512536\n",
      "Epoch: 4 - Batch: 2271, Training Loss: 0.19643445371049356\n",
      "Epoch: 4 - Batch: 2272, Training Loss: 0.19651638830133142\n",
      "Epoch: 4 - Batch: 2273, Training Loss: 0.19659749930647277\n",
      "Epoch: 4 - Batch: 2274, Training Loss: 0.19668202424672113\n",
      "Epoch: 4 - Batch: 2275, Training Loss: 0.19676982899309192\n",
      "Epoch: 4 - Batch: 2276, Training Loss: 0.1968580732001594\n",
      "Epoch: 4 - Batch: 2277, Training Loss: 0.1969428340136807\n",
      "Epoch: 4 - Batch: 2278, Training Loss: 0.1970292782692075\n",
      "Epoch: 4 - Batch: 2279, Training Loss: 0.197120018587579\n",
      "Epoch: 4 - Batch: 2280, Training Loss: 0.19720150998625194\n",
      "Epoch: 4 - Batch: 2281, Training Loss: 0.19729239543031896\n",
      "Epoch: 4 - Batch: 2282, Training Loss: 0.19737767491792368\n",
      "Epoch: 4 - Batch: 2283, Training Loss: 0.1974660578132862\n",
      "Epoch: 4 - Batch: 2284, Training Loss: 0.19755418085266108\n",
      "Epoch: 4 - Batch: 2285, Training Loss: 0.1976424250473727\n",
      "Epoch: 4 - Batch: 2286, Training Loss: 0.19773157139248515\n",
      "Epoch: 4 - Batch: 2287, Training Loss: 0.19781705187278403\n",
      "Epoch: 4 - Batch: 2288, Training Loss: 0.19790702291561993\n",
      "Epoch: 4 - Batch: 2289, Training Loss: 0.1979993047442899\n",
      "Epoch: 4 - Batch: 2290, Training Loss: 0.1980849344894957\n",
      "Epoch: 4 - Batch: 2291, Training Loss: 0.1981660169526119\n",
      "Epoch: 4 - Batch: 2292, Training Loss: 0.1982450817787331\n",
      "Epoch: 4 - Batch: 2293, Training Loss: 0.19834089387906328\n",
      "Epoch: 4 - Batch: 2294, Training Loss: 0.19842489389316556\n",
      "Epoch: 4 - Batch: 2295, Training Loss: 0.19851418467204568\n",
      "Epoch: 4 - Batch: 2296, Training Loss: 0.19859893122691025\n",
      "Epoch: 4 - Batch: 2297, Training Loss: 0.19867816881259678\n",
      "Epoch: 4 - Batch: 2298, Training Loss: 0.19876535260311604\n",
      "Epoch: 4 - Batch: 2299, Training Loss: 0.1988496089826769\n",
      "Epoch: 4 - Batch: 2300, Training Loss: 0.19893892150786188\n",
      "Epoch: 4 - Batch: 2301, Training Loss: 0.1990234626466362\n",
      "Epoch: 4 - Batch: 2302, Training Loss: 0.19910965187113677\n",
      "Epoch: 4 - Batch: 2303, Training Loss: 0.1992003539604927\n",
      "Epoch: 4 - Batch: 2304, Training Loss: 0.19928826485601428\n",
      "Epoch: 4 - Batch: 2305, Training Loss: 0.199374963741002\n",
      "Epoch: 4 - Batch: 2306, Training Loss: 0.1994702091215064\n",
      "Epoch: 4 - Batch: 2307, Training Loss: 0.19954698857778735\n",
      "Epoch: 4 - Batch: 2308, Training Loss: 0.19962909085611205\n",
      "Epoch: 4 - Batch: 2309, Training Loss: 0.19971508806444718\n",
      "Epoch: 4 - Batch: 2310, Training Loss: 0.19980325866768608\n",
      "Epoch: 4 - Batch: 2311, Training Loss: 0.1998865194865819\n",
      "Epoch: 4 - Batch: 2312, Training Loss: 0.19997144984714624\n",
      "Epoch: 4 - Batch: 2313, Training Loss: 0.20005697441313594\n",
      "Epoch: 4 - Batch: 2314, Training Loss: 0.20014283233391705\n",
      "Epoch: 4 - Batch: 2315, Training Loss: 0.2002194426756099\n",
      "Epoch: 4 - Batch: 2316, Training Loss: 0.20029861361381426\n",
      "Epoch: 4 - Batch: 2317, Training Loss: 0.2003832600368295\n",
      "Epoch: 4 - Batch: 2318, Training Loss: 0.20047057949463723\n",
      "Epoch: 4 - Batch: 2319, Training Loss: 0.2005583704951193\n",
      "Epoch: 4 - Batch: 2320, Training Loss: 0.2006357590579868\n",
      "Epoch: 4 - Batch: 2321, Training Loss: 0.20072903568125877\n",
      "Epoch: 4 - Batch: 2322, Training Loss: 0.20081741447447743\n",
      "Epoch: 4 - Batch: 2323, Training Loss: 0.20090432742465392\n",
      "Epoch: 4 - Batch: 2324, Training Loss: 0.2009871867894632\n",
      "Epoch: 4 - Batch: 2325, Training Loss: 0.2010706852608751\n",
      "Epoch: 4 - Batch: 2326, Training Loss: 0.20116198975376623\n",
      "Epoch: 4 - Batch: 2327, Training Loss: 0.2012444408378791\n",
      "Epoch: 4 - Batch: 2328, Training Loss: 0.20132829549770254\n",
      "Epoch: 4 - Batch: 2329, Training Loss: 0.2014138786119114\n",
      "Epoch: 4 - Batch: 2330, Training Loss: 0.20150466774208825\n",
      "Epoch: 4 - Batch: 2331, Training Loss: 0.2015918922883954\n",
      "Epoch: 4 - Batch: 2332, Training Loss: 0.20167979176710693\n",
      "Epoch: 4 - Batch: 2333, Training Loss: 0.20176667773827392\n",
      "Epoch: 4 - Batch: 2334, Training Loss: 0.20185515498294562\n",
      "Epoch: 4 - Batch: 2335, Training Loss: 0.2019386858390536\n",
      "Epoch: 4 - Batch: 2336, Training Loss: 0.2020219186225143\n",
      "Epoch: 4 - Batch: 2337, Training Loss: 0.20210430033467897\n",
      "Epoch: 4 - Batch: 2338, Training Loss: 0.20217973829724303\n",
      "Epoch: 4 - Batch: 2339, Training Loss: 0.20226950147752937\n",
      "Epoch: 4 - Batch: 2340, Training Loss: 0.20236204326103377\n",
      "Epoch: 4 - Batch: 2341, Training Loss: 0.20244357851063632\n",
      "Epoch: 4 - Batch: 2342, Training Loss: 0.2025254159103183\n",
      "Epoch: 4 - Batch: 2343, Training Loss: 0.20261697792394046\n",
      "Epoch: 4 - Batch: 2344, Training Loss: 0.20270654491796028\n",
      "Epoch: 4 - Batch: 2345, Training Loss: 0.20279287868743118\n",
      "Epoch: 4 - Batch: 2346, Training Loss: 0.20287952533804166\n",
      "Epoch: 4 - Batch: 2347, Training Loss: 0.20296333269347402\n",
      "Epoch: 4 - Batch: 2348, Training Loss: 0.20304776484742884\n",
      "Epoch: 4 - Batch: 2349, Training Loss: 0.20312884551894606\n",
      "Epoch: 4 - Batch: 2350, Training Loss: 0.20322106662476636\n",
      "Epoch: 4 - Batch: 2351, Training Loss: 0.20331540040264082\n",
      "Epoch: 4 - Batch: 2352, Training Loss: 0.20339851177129184\n",
      "Epoch: 4 - Batch: 2353, Training Loss: 0.20348709176725416\n",
      "Epoch: 4 - Batch: 2354, Training Loss: 0.20357457581122915\n",
      "Epoch: 4 - Batch: 2355, Training Loss: 0.20365598440096153\n",
      "Epoch: 4 - Batch: 2356, Training Loss: 0.2037440247302427\n",
      "Epoch: 4 - Batch: 2357, Training Loss: 0.20383218838331316\n",
      "Epoch: 4 - Batch: 2358, Training Loss: 0.20392379828205157\n",
      "Epoch: 4 - Batch: 2359, Training Loss: 0.20401071054030018\n",
      "Epoch: 4 - Batch: 2360, Training Loss: 0.20410177183907424\n",
      "Epoch: 4 - Batch: 2361, Training Loss: 0.20418420564970172\n",
      "Epoch: 4 - Batch: 2362, Training Loss: 0.20427099032756899\n",
      "Epoch: 4 - Batch: 2363, Training Loss: 0.20435713460146887\n",
      "Epoch: 4 - Batch: 2364, Training Loss: 0.20444490031569357\n",
      "Epoch: 4 - Batch: 2365, Training Loss: 0.20452675821373908\n",
      "Epoch: 4 - Batch: 2366, Training Loss: 0.2046126135521465\n",
      "Epoch: 4 - Batch: 2367, Training Loss: 0.20469538838437343\n",
      "Epoch: 4 - Batch: 2368, Training Loss: 0.2047864939232865\n",
      "Epoch: 4 - Batch: 2369, Training Loss: 0.2048784415755007\n",
      "Epoch: 4 - Batch: 2370, Training Loss: 0.2049622063403007\n",
      "Epoch: 4 - Batch: 2371, Training Loss: 0.20504516709107862\n",
      "Epoch: 4 - Batch: 2372, Training Loss: 0.2051264704151039\n",
      "Epoch: 4 - Batch: 2373, Training Loss: 0.20520916107306827\n",
      "Epoch: 4 - Batch: 2374, Training Loss: 0.20529738101323644\n",
      "Epoch: 4 - Batch: 2375, Training Loss: 0.2053887630576518\n",
      "Epoch: 4 - Batch: 2376, Training Loss: 0.20547764371481306\n",
      "Epoch: 4 - Batch: 2377, Training Loss: 0.20557351297950666\n",
      "Epoch: 4 - Batch: 2378, Training Loss: 0.20565452202413212\n",
      "Epoch: 4 - Batch: 2379, Training Loss: 0.20573189113270585\n",
      "Epoch: 4 - Batch: 2380, Training Loss: 0.20581893513525895\n",
      "Epoch: 4 - Batch: 2381, Training Loss: 0.20590326818290042\n",
      "Epoch: 4 - Batch: 2382, Training Loss: 0.20599336802316937\n",
      "Epoch: 4 - Batch: 2383, Training Loss: 0.20608256298239352\n",
      "Epoch: 4 - Batch: 2384, Training Loss: 0.20616250716236298\n",
      "Epoch: 4 - Batch: 2385, Training Loss: 0.20624425196108928\n",
      "Epoch: 4 - Batch: 2386, Training Loss: 0.20632725464146134\n",
      "Epoch: 4 - Batch: 2387, Training Loss: 0.2064212112568603\n",
      "Epoch: 4 - Batch: 2388, Training Loss: 0.20650666614818336\n",
      "Epoch: 4 - Batch: 2389, Training Loss: 0.20658965490968467\n",
      "Epoch: 4 - Batch: 2390, Training Loss: 0.2066778311224818\n",
      "Epoch: 4 - Batch: 2391, Training Loss: 0.20676504385659153\n",
      "Epoch: 4 - Batch: 2392, Training Loss: 0.20684373760549582\n",
      "Epoch: 4 - Batch: 2393, Training Loss: 0.20692172032436526\n",
      "Epoch: 4 - Batch: 2394, Training Loss: 0.20700654895299703\n",
      "Epoch: 4 - Batch: 2395, Training Loss: 0.20709393266125106\n",
      "Epoch: 4 - Batch: 2396, Training Loss: 0.2071764436033037\n",
      "Epoch: 4 - Batch: 2397, Training Loss: 0.20726418782451853\n",
      "Epoch: 4 - Batch: 2398, Training Loss: 0.20735200700252804\n",
      "Epoch: 4 - Batch: 2399, Training Loss: 0.20744120353094578\n",
      "Epoch: 4 - Batch: 2400, Training Loss: 0.20753045212693674\n",
      "Epoch: 4 - Batch: 2401, Training Loss: 0.20762138349771697\n",
      "Epoch: 4 - Batch: 2402, Training Loss: 0.20771719877967984\n",
      "Epoch: 4 - Batch: 2403, Training Loss: 0.20780069098667323\n",
      "Epoch: 4 - Batch: 2404, Training Loss: 0.20789453494756377\n",
      "Epoch: 4 - Batch: 2405, Training Loss: 0.20798419011296523\n",
      "Epoch: 4 - Batch: 2406, Training Loss: 0.20806571696058632\n",
      "Epoch: 4 - Batch: 2407, Training Loss: 0.2081519299083286\n",
      "Epoch: 4 - Batch: 2408, Training Loss: 0.20824111760230998\n",
      "Epoch: 4 - Batch: 2409, Training Loss: 0.2083288384400691\n",
      "Epoch: 4 - Batch: 2410, Training Loss: 0.20841125287671589\n",
      "Epoch: 4 - Batch: 2411, Training Loss: 0.20851160837939722\n",
      "Epoch: 4 - Batch: 2412, Training Loss: 0.20858566409245652\n",
      "Epoch 4 - Batch 2412, Training Loss: 0.20858566409245652, Validation Loss: 0.20841029686714285\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch: 5 - Batch: 1, Training Loss: 8.568042892326368e-05\n",
      "Epoch: 5 - Batch: 2, Training Loss: 0.00017368236856278693\n",
      "Epoch: 5 - Batch: 3, Training Loss: 0.0002684679073876212\n",
      "Epoch: 5 - Batch: 4, Training Loss: 0.00035724069175633227\n",
      "Epoch: 5 - Batch: 5, Training Loss: 0.000435272481904101\n",
      "Epoch: 5 - Batch: 6, Training Loss: 0.0005257950221523519\n",
      "Epoch: 5 - Batch: 7, Training Loss: 0.0006140884549463567\n",
      "Epoch: 5 - Batch: 8, Training Loss: 0.0007012069818392322\n",
      "Epoch: 5 - Batch: 9, Training Loss: 0.0007832084054970622\n",
      "Epoch: 5 - Batch: 10, Training Loss: 0.0008651041865941896\n",
      "Epoch: 5 - Batch: 11, Training Loss: 0.0009476245190966782\n",
      "Epoch: 5 - Batch: 12, Training Loss: 0.0010212683435498581\n",
      "Epoch: 5 - Batch: 13, Training Loss: 0.0011192425208206398\n",
      "Epoch: 5 - Batch: 14, Training Loss: 0.0012041820185397989\n",
      "Epoch: 5 - Batch: 15, Training Loss: 0.0012902283537546003\n",
      "Epoch: 5 - Batch: 16, Training Loss: 0.0013773909354486671\n",
      "Epoch: 5 - Batch: 17, Training Loss: 0.001457557075998281\n",
      "Epoch: 5 - Batch: 18, Training Loss: 0.0015510585943660136\n",
      "Epoch: 5 - Batch: 19, Training Loss: 0.001640694626725926\n",
      "Epoch: 5 - Batch: 20, Training Loss: 0.00172203120035714\n",
      "Epoch: 5 - Batch: 21, Training Loss: 0.0018103431978233615\n",
      "Epoch: 5 - Batch: 22, Training Loss: 0.0019012073465741886\n",
      "Epoch: 5 - Batch: 23, Training Loss: 0.0019979053740675373\n",
      "Epoch: 5 - Batch: 24, Training Loss: 0.0020821771737354906\n",
      "Epoch: 5 - Batch: 25, Training Loss: 0.0021706977874584262\n",
      "Epoch: 5 - Batch: 26, Training Loss: 0.002253147215886693\n",
      "Epoch: 5 - Batch: 27, Training Loss: 0.0023417406611972386\n",
      "Epoch: 5 - Batch: 28, Training Loss: 0.002426228620371415\n",
      "Epoch: 5 - Batch: 29, Training Loss: 0.002517947732512631\n",
      "Epoch: 5 - Batch: 30, Training Loss: 0.0026029154077592377\n",
      "Epoch: 5 - Batch: 31, Training Loss: 0.002691767096618317\n",
      "Epoch: 5 - Batch: 32, Training Loss: 0.0027758819151478225\n",
      "Epoch: 5 - Batch: 33, Training Loss: 0.0028613251301878523\n",
      "Epoch: 5 - Batch: 34, Training Loss: 0.002950761609310732\n",
      "Epoch: 5 - Batch: 35, Training Loss: 0.0030322437843378898\n",
      "Epoch: 5 - Batch: 36, Training Loss: 0.0031174502055937576\n",
      "Epoch: 5 - Batch: 37, Training Loss: 0.003211173202663314\n",
      "Epoch: 5 - Batch: 38, Training Loss: 0.003297658319002756\n",
      "Epoch: 5 - Batch: 39, Training Loss: 0.003383489958880157\n",
      "Epoch: 5 - Batch: 40, Training Loss: 0.003467587015868024\n",
      "Epoch: 5 - Batch: 41, Training Loss: 0.0035551358292351906\n",
      "Epoch: 5 - Batch: 42, Training Loss: 0.0036495780050260314\n",
      "Epoch: 5 - Batch: 43, Training Loss: 0.0037362913525420833\n",
      "Epoch: 5 - Batch: 44, Training Loss: 0.0038149182995160422\n",
      "Epoch: 5 - Batch: 45, Training Loss: 0.003915564331595182\n",
      "Epoch: 5 - Batch: 46, Training Loss: 0.0040073879186687976\n",
      "Epoch: 5 - Batch: 47, Training Loss: 0.004094102267009109\n",
      "Epoch: 5 - Batch: 48, Training Loss: 0.004178992588425157\n",
      "Epoch: 5 - Batch: 49, Training Loss: 0.00427325430695295\n",
      "Epoch: 5 - Batch: 50, Training Loss: 0.004362743774160223\n",
      "Epoch: 5 - Batch: 51, Training Loss: 0.004443197089492978\n",
      "Epoch: 5 - Batch: 52, Training Loss: 0.004531829721397824\n",
      "Epoch: 5 - Batch: 53, Training Loss: 0.004625294240157601\n",
      "Epoch: 5 - Batch: 54, Training Loss: 0.0047148561868106155\n",
      "Epoch: 5 - Batch: 55, Training Loss: 0.004803070165278702\n",
      "Epoch: 5 - Batch: 56, Training Loss: 0.004893943315269936\n",
      "Epoch: 5 - Batch: 57, Training Loss: 0.00498154772998484\n",
      "Epoch: 5 - Batch: 58, Training Loss: 0.005077097589647394\n",
      "Epoch: 5 - Batch: 59, Training Loss: 0.005166393150243395\n",
      "Epoch: 5 - Batch: 60, Training Loss: 0.005260806011768123\n",
      "Epoch: 5 - Batch: 61, Training Loss: 0.005353271874721173\n",
      "Epoch: 5 - Batch: 62, Training Loss: 0.005435967085818153\n",
      "Epoch: 5 - Batch: 63, Training Loss: 0.005529731555315194\n",
      "Epoch: 5 - Batch: 64, Training Loss: 0.005611865279291004\n",
      "Epoch: 5 - Batch: 65, Training Loss: 0.005703260735987035\n",
      "Epoch: 5 - Batch: 66, Training Loss: 0.005788815437907208\n",
      "Epoch: 5 - Batch: 67, Training Loss: 0.0058773281933063295\n",
      "Epoch: 5 - Batch: 68, Training Loss: 0.005967865362886964\n",
      "Epoch: 5 - Batch: 69, Training Loss: 0.0060545241407691745\n",
      "Epoch: 5 - Batch: 70, Training Loss: 0.006136560253490065\n",
      "Epoch: 5 - Batch: 71, Training Loss: 0.0062240144024737435\n",
      "Epoch: 5 - Batch: 72, Training Loss: 0.006309648799659008\n",
      "Epoch: 5 - Batch: 73, Training Loss: 0.006396171508687448\n",
      "Epoch: 5 - Batch: 74, Training Loss: 0.0064742990134663845\n",
      "Epoch: 5 - Batch: 75, Training Loss: 0.006561016759666835\n",
      "Epoch: 5 - Batch: 76, Training Loss: 0.006656339178927502\n",
      "Epoch: 5 - Batch: 77, Training Loss: 0.006745697328107274\n",
      "Epoch: 5 - Batch: 78, Training Loss: 0.006831750910031064\n",
      "Epoch: 5 - Batch: 79, Training Loss: 0.006925826425823209\n",
      "Epoch: 5 - Batch: 80, Training Loss: 0.007023697505839428\n",
      "Epoch: 5 - Batch: 81, Training Loss: 0.007106849173111702\n",
      "Epoch: 5 - Batch: 82, Training Loss: 0.007196207279045981\n",
      "Epoch: 5 - Batch: 83, Training Loss: 0.007283869920727823\n",
      "Epoch: 5 - Batch: 84, Training Loss: 0.007369954874167592\n",
      "Epoch: 5 - Batch: 85, Training Loss: 0.007468380126638792\n",
      "Epoch: 5 - Batch: 86, Training Loss: 0.0075573813336998665\n",
      "Epoch: 5 - Batch: 87, Training Loss: 0.007641176073804226\n",
      "Epoch: 5 - Batch: 88, Training Loss: 0.007732912749793399\n",
      "Epoch: 5 - Batch: 89, Training Loss: 0.007819055355652846\n",
      "Epoch: 5 - Batch: 90, Training Loss: 0.007910396841974006\n",
      "Epoch: 5 - Batch: 91, Training Loss: 0.008010744220680661\n",
      "Epoch: 5 - Batch: 92, Training Loss: 0.008096568959813016\n",
      "Epoch: 5 - Batch: 93, Training Loss: 0.00818993002917636\n",
      "Epoch: 5 - Batch: 94, Training Loss: 0.008273034114050826\n",
      "Epoch: 5 - Batch: 95, Training Loss: 0.008359905517664714\n",
      "Epoch: 5 - Batch: 96, Training Loss: 0.008444786732815589\n",
      "Epoch: 5 - Batch: 97, Training Loss: 0.008528885711138917\n",
      "Epoch: 5 - Batch: 98, Training Loss: 0.008612663863507867\n",
      "Epoch: 5 - Batch: 99, Training Loss: 0.008692892981850686\n",
      "Epoch: 5 - Batch: 100, Training Loss: 0.008782663050526213\n",
      "Epoch: 5 - Batch: 101, Training Loss: 0.008870326464448994\n",
      "Epoch: 5 - Batch: 102, Training Loss: 0.00895368687771446\n",
      "Epoch: 5 - Batch: 103, Training Loss: 0.009037854776997273\n",
      "Epoch: 5 - Batch: 104, Training Loss: 0.009122389638997232\n",
      "Epoch: 5 - Batch: 105, Training Loss: 0.00920173829764276\n",
      "Epoch: 5 - Batch: 106, Training Loss: 0.009277405702257236\n",
      "Epoch: 5 - Batch: 107, Training Loss: 0.009361522930178477\n",
      "Epoch: 5 - Batch: 108, Training Loss: 0.009459803180512701\n",
      "Epoch: 5 - Batch: 109, Training Loss: 0.009538709518081117\n",
      "Epoch: 5 - Batch: 110, Training Loss: 0.009629114257607294\n",
      "Epoch: 5 - Batch: 111, Training Loss: 0.009712590754130982\n",
      "Epoch: 5 - Batch: 112, Training Loss: 0.009802663535077378\n",
      "Epoch: 5 - Batch: 113, Training Loss: 0.009882989813686406\n",
      "Epoch: 5 - Batch: 114, Training Loss: 0.00996306851184981\n",
      "Epoch: 5 - Batch: 115, Training Loss: 0.010049612466771014\n",
      "Epoch: 5 - Batch: 116, Training Loss: 0.010139092654731143\n",
      "Epoch: 5 - Batch: 117, Training Loss: 0.010226315001696101\n",
      "Epoch: 5 - Batch: 118, Training Loss: 0.010315476414773792\n",
      "Epoch: 5 - Batch: 119, Training Loss: 0.010401874119616662\n",
      "Epoch: 5 - Batch: 120, Training Loss: 0.010489140546094521\n",
      "Epoch: 5 - Batch: 121, Training Loss: 0.010577359219787528\n",
      "Epoch: 5 - Batch: 122, Training Loss: 0.010661519186611398\n",
      "Epoch: 5 - Batch: 123, Training Loss: 0.010753729221585576\n",
      "Epoch: 5 - Batch: 124, Training Loss: 0.010832105814224452\n",
      "Epoch: 5 - Batch: 125, Training Loss: 0.010922289736087049\n",
      "Epoch: 5 - Batch: 126, Training Loss: 0.011011130341744146\n",
      "Epoch: 5 - Batch: 127, Training Loss: 0.011099161904546158\n",
      "Epoch: 5 - Batch: 128, Training Loss: 0.01118183975566679\n",
      "Epoch: 5 - Batch: 129, Training Loss: 0.01126667435217655\n",
      "Epoch: 5 - Batch: 130, Training Loss: 0.011345643210371533\n",
      "Epoch: 5 - Batch: 131, Training Loss: 0.011423750155007661\n",
      "Epoch: 5 - Batch: 132, Training Loss: 0.011513480338983077\n",
      "Epoch: 5 - Batch: 133, Training Loss: 0.01159860097971524\n",
      "Epoch: 5 - Batch: 134, Training Loss: 0.011680634411215585\n",
      "Epoch: 5 - Batch: 135, Training Loss: 0.0117684830146939\n",
      "Epoch: 5 - Batch: 136, Training Loss: 0.011856209264317555\n",
      "Epoch: 5 - Batch: 137, Training Loss: 0.011938922416116072\n",
      "Epoch: 5 - Batch: 138, Training Loss: 0.012021280120854355\n",
      "Epoch: 5 - Batch: 139, Training Loss: 0.012103307850127592\n",
      "Epoch: 5 - Batch: 140, Training Loss: 0.012189140113975674\n",
      "Epoch: 5 - Batch: 141, Training Loss: 0.012277015060136962\n",
      "Epoch: 5 - Batch: 142, Training Loss: 0.01236661354305337\n",
      "Epoch: 5 - Batch: 143, Training Loss: 0.012452268784566107\n",
      "Epoch: 5 - Batch: 144, Training Loss: 0.012530947595241652\n",
      "Epoch: 5 - Batch: 145, Training Loss: 0.012620837558956684\n",
      "Epoch: 5 - Batch: 146, Training Loss: 0.012703470267960879\n",
      "Epoch: 5 - Batch: 147, Training Loss: 0.012794691334118693\n",
      "Epoch: 5 - Batch: 148, Training Loss: 0.0128746740643559\n",
      "Epoch: 5 - Batch: 149, Training Loss: 0.012961519773969208\n",
      "Epoch: 5 - Batch: 150, Training Loss: 0.013043832861714892\n",
      "Epoch: 5 - Batch: 151, Training Loss: 0.013120607418899314\n",
      "Epoch: 5 - Batch: 152, Training Loss: 0.013210194342913319\n",
      "Epoch: 5 - Batch: 153, Training Loss: 0.013296457053861808\n",
      "Epoch: 5 - Batch: 154, Training Loss: 0.01337409583514998\n",
      "Epoch: 5 - Batch: 155, Training Loss: 0.01345577996045005\n",
      "Epoch: 5 - Batch: 156, Training Loss: 0.013534985093483285\n",
      "Epoch: 5 - Batch: 157, Training Loss: 0.0136312825626896\n",
      "Epoch: 5 - Batch: 158, Training Loss: 0.013714454697435768\n",
      "Epoch: 5 - Batch: 159, Training Loss: 0.013795603023437323\n",
      "Epoch: 5 - Batch: 160, Training Loss: 0.01387880555976485\n",
      "Epoch: 5 - Batch: 161, Training Loss: 0.013967108271805998\n",
      "Epoch: 5 - Batch: 162, Training Loss: 0.014054661860711144\n",
      "Epoch: 5 - Batch: 163, Training Loss: 0.014141154736527559\n",
      "Epoch: 5 - Batch: 164, Training Loss: 0.014226113849166614\n",
      "Epoch: 5 - Batch: 165, Training Loss: 0.014320362093634471\n",
      "Epoch: 5 - Batch: 166, Training Loss: 0.014407942748040109\n",
      "Epoch: 5 - Batch: 167, Training Loss: 0.014498320409709936\n",
      "Epoch: 5 - Batch: 168, Training Loss: 0.0145955821113108\n",
      "Epoch: 5 - Batch: 169, Training Loss: 0.014684392852866235\n",
      "Epoch: 5 - Batch: 170, Training Loss: 0.014769099473607283\n",
      "Epoch: 5 - Batch: 171, Training Loss: 0.014859459719699413\n",
      "Epoch: 5 - Batch: 172, Training Loss: 0.014945432568466288\n",
      "Epoch: 5 - Batch: 173, Training Loss: 0.015033662677503145\n",
      "Epoch: 5 - Batch: 174, Training Loss: 0.015122816720313298\n",
      "Epoch: 5 - Batch: 175, Training Loss: 0.015203911749334083\n",
      "Epoch: 5 - Batch: 176, Training Loss: 0.015287596720022152\n",
      "Epoch: 5 - Batch: 177, Training Loss: 0.01537611687657845\n",
      "Epoch: 5 - Batch: 178, Training Loss: 0.015461304838186868\n",
      "Epoch: 5 - Batch: 179, Training Loss: 0.015550204418340133\n",
      "Epoch: 5 - Batch: 180, Training Loss: 0.015637521516179562\n",
      "Epoch: 5 - Batch: 181, Training Loss: 0.01573670992186018\n",
      "Epoch: 5 - Batch: 182, Training Loss: 0.01582035440241124\n",
      "Epoch: 5 - Batch: 183, Training Loss: 0.015894056824507008\n",
      "Epoch: 5 - Batch: 184, Training Loss: 0.01596721334021483\n",
      "Epoch: 5 - Batch: 185, Training Loss: 0.01605301408427667\n",
      "Epoch: 5 - Batch: 186, Training Loss: 0.01614261885014537\n",
      "Epoch: 5 - Batch: 187, Training Loss: 0.016232499095980405\n",
      "Epoch: 5 - Batch: 188, Training Loss: 0.016320583449173132\n",
      "Epoch: 5 - Batch: 189, Training Loss: 0.01640207930830976\n",
      "Epoch: 5 - Batch: 190, Training Loss: 0.016481376702512675\n",
      "Epoch: 5 - Batch: 191, Training Loss: 0.016563873941922068\n",
      "Epoch: 5 - Batch: 192, Training Loss: 0.016647974001382713\n",
      "Epoch: 5 - Batch: 193, Training Loss: 0.016730818570055574\n",
      "Epoch: 5 - Batch: 194, Training Loss: 0.01682087813641499\n",
      "Epoch: 5 - Batch: 195, Training Loss: 0.01691271535199673\n",
      "Epoch: 5 - Batch: 196, Training Loss: 0.01700275133720678\n",
      "Epoch: 5 - Batch: 197, Training Loss: 0.01708343943231932\n",
      "Epoch: 5 - Batch: 198, Training Loss: 0.01717320657883513\n",
      "Epoch: 5 - Batch: 199, Training Loss: 0.017259573109855698\n",
      "Epoch: 5 - Batch: 200, Training Loss: 0.017347298902312718\n",
      "Epoch: 5 - Batch: 201, Training Loss: 0.017442833996728482\n",
      "Epoch: 5 - Batch: 202, Training Loss: 0.01752128823356051\n",
      "Epoch: 5 - Batch: 203, Training Loss: 0.01760560524948003\n",
      "Epoch: 5 - Batch: 204, Training Loss: 0.017691710472700014\n",
      "Epoch: 5 - Batch: 205, Training Loss: 0.017766611622785453\n",
      "Epoch: 5 - Batch: 206, Training Loss: 0.017860000159214583\n",
      "Epoch: 5 - Batch: 207, Training Loss: 0.01795913471807888\n",
      "Epoch: 5 - Batch: 208, Training Loss: 0.018046202771303862\n",
      "Epoch: 5 - Batch: 209, Training Loss: 0.01813390266588869\n",
      "Epoch: 5 - Batch: 210, Training Loss: 0.018222562974908853\n",
      "Epoch: 5 - Batch: 211, Training Loss: 0.018305215576839683\n",
      "Epoch: 5 - Batch: 212, Training Loss: 0.01839041680863643\n",
      "Epoch: 5 - Batch: 213, Training Loss: 0.018484991643065045\n",
      "Epoch: 5 - Batch: 214, Training Loss: 0.018571698034234703\n",
      "Epoch: 5 - Batch: 215, Training Loss: 0.018654419866021395\n",
      "Epoch: 5 - Batch: 216, Training Loss: 0.018739439460570936\n",
      "Epoch: 5 - Batch: 217, Training Loss: 0.01882588463043099\n",
      "Epoch: 5 - Batch: 218, Training Loss: 0.018909008293157788\n",
      "Epoch: 5 - Batch: 219, Training Loss: 0.01900087991979585\n",
      "Epoch: 5 - Batch: 220, Training Loss: 0.019086244360081987\n",
      "Epoch: 5 - Batch: 221, Training Loss: 0.0191752567306581\n",
      "Epoch: 5 - Batch: 222, Training Loss: 0.019255084141619368\n",
      "Epoch: 5 - Batch: 223, Training Loss: 0.01933465472995543\n",
      "Epoch: 5 - Batch: 224, Training Loss: 0.019420422527129774\n",
      "Epoch: 5 - Batch: 225, Training Loss: 0.019509352428551337\n",
      "Epoch: 5 - Batch: 226, Training Loss: 0.01958999739555182\n",
      "Epoch: 5 - Batch: 227, Training Loss: 0.019679373312451155\n",
      "Epoch: 5 - Batch: 228, Training Loss: 0.019772466791061618\n",
      "Epoch: 5 - Batch: 229, Training Loss: 0.019865225407466366\n",
      "Epoch: 5 - Batch: 230, Training Loss: 0.019953225660531676\n",
      "Epoch: 5 - Batch: 231, Training Loss: 0.020043422259501557\n",
      "Epoch: 5 - Batch: 232, Training Loss: 0.020128876644234555\n",
      "Epoch: 5 - Batch: 233, Training Loss: 0.020208000395428482\n",
      "Epoch: 5 - Batch: 234, Training Loss: 0.02030624162597245\n",
      "Epoch: 5 - Batch: 235, Training Loss: 0.020395195250635715\n",
      "Epoch: 5 - Batch: 236, Training Loss: 0.02048932714001654\n",
      "Epoch: 5 - Batch: 237, Training Loss: 0.02056864542179242\n",
      "Epoch: 5 - Batch: 238, Training Loss: 0.02065057064071421\n",
      "Epoch: 5 - Batch: 239, Training Loss: 0.02072832074825641\n",
      "Epoch: 5 - Batch: 240, Training Loss: 0.020815482619488814\n",
      "Epoch: 5 - Batch: 241, Training Loss: 0.020900055407785854\n",
      "Epoch: 5 - Batch: 242, Training Loss: 0.020989674617649112\n",
      "Epoch: 5 - Batch: 243, Training Loss: 0.021077175440875253\n",
      "Epoch: 5 - Batch: 244, Training Loss: 0.02117222097778004\n",
      "Epoch: 5 - Batch: 245, Training Loss: 0.021253868906602733\n",
      "Epoch: 5 - Batch: 246, Training Loss: 0.02133655716135332\n",
      "Epoch: 5 - Batch: 247, Training Loss: 0.02142123505473137\n",
      "Epoch: 5 - Batch: 248, Training Loss: 0.021495949792031625\n",
      "Epoch: 5 - Batch: 249, Training Loss: 0.021581895334358833\n",
      "Epoch: 5 - Batch: 250, Training Loss: 0.021671047900644305\n",
      "Epoch: 5 - Batch: 251, Training Loss: 0.02175461164803845\n",
      "Epoch: 5 - Batch: 252, Training Loss: 0.021842168004655126\n",
      "Epoch: 5 - Batch: 253, Training Loss: 0.021927857385395377\n",
      "Epoch: 5 - Batch: 254, Training Loss: 0.022009172150942422\n",
      "Epoch: 5 - Batch: 255, Training Loss: 0.02210441961340841\n",
      "Epoch: 5 - Batch: 256, Training Loss: 0.022190763724729987\n",
      "Epoch: 5 - Batch: 257, Training Loss: 0.022277569623778314\n",
      "Epoch: 5 - Batch: 258, Training Loss: 0.02235817164182663\n",
      "Epoch: 5 - Batch: 259, Training Loss: 0.02243935391553994\n",
      "Epoch: 5 - Batch: 260, Training Loss: 0.022526661740310157\n",
      "Epoch: 5 - Batch: 261, Training Loss: 0.02260351234778243\n",
      "Epoch: 5 - Batch: 262, Training Loss: 0.022691790817635966\n",
      "Epoch: 5 - Batch: 263, Training Loss: 0.022780308954009964\n",
      "Epoch: 5 - Batch: 264, Training Loss: 0.022870553895262737\n",
      "Epoch: 5 - Batch: 265, Training Loss: 0.022953298918989364\n",
      "Epoch: 5 - Batch: 266, Training Loss: 0.023043736067923346\n",
      "Epoch: 5 - Batch: 267, Training Loss: 0.023126829928327753\n",
      "Epoch: 5 - Batch: 268, Training Loss: 0.023211586306097102\n",
      "Epoch: 5 - Batch: 269, Training Loss: 0.0232997654101643\n",
      "Epoch: 5 - Batch: 270, Training Loss: 0.0233825078020938\n",
      "Epoch: 5 - Batch: 271, Training Loss: 0.023468637798803166\n",
      "Epoch: 5 - Batch: 272, Training Loss: 0.0235558482903252\n",
      "Epoch: 5 - Batch: 273, Training Loss: 0.023643440287405778\n",
      "Epoch: 5 - Batch: 274, Training Loss: 0.023725657512250035\n",
      "Epoch: 5 - Batch: 275, Training Loss: 0.023812893493901043\n",
      "Epoch: 5 - Batch: 276, Training Loss: 0.023907998555137547\n",
      "Epoch: 5 - Batch: 277, Training Loss: 0.023997424346445806\n",
      "Epoch: 5 - Batch: 278, Training Loss: 0.02408394948339976\n",
      "Epoch: 5 - Batch: 279, Training Loss: 0.024169014325090506\n",
      "Epoch: 5 - Batch: 280, Training Loss: 0.02425535382767815\n",
      "Epoch: 5 - Batch: 281, Training Loss: 0.0243462880959064\n",
      "Epoch: 5 - Batch: 282, Training Loss: 0.02443561840116681\n",
      "Epoch: 5 - Batch: 283, Training Loss: 0.024525837784333412\n",
      "Epoch: 5 - Batch: 284, Training Loss: 0.024618225638595583\n",
      "Epoch: 5 - Batch: 285, Training Loss: 0.024705121018746203\n",
      "Epoch: 5 - Batch: 286, Training Loss: 0.024789412748408356\n",
      "Epoch: 5 - Batch: 287, Training Loss: 0.0248770920210711\n",
      "Epoch: 5 - Batch: 288, Training Loss: 0.024962225962881227\n",
      "Epoch: 5 - Batch: 289, Training Loss: 0.02504512694221033\n",
      "Epoch: 5 - Batch: 290, Training Loss: 0.025137778248013946\n",
      "Epoch: 5 - Batch: 291, Training Loss: 0.02522512438211275\n",
      "Epoch: 5 - Batch: 292, Training Loss: 0.025304004420243685\n",
      "Epoch: 5 - Batch: 293, Training Loss: 0.02539030040550983\n",
      "Epoch: 5 - Batch: 294, Training Loss: 0.02548160690622741\n",
      "Epoch: 5 - Batch: 295, Training Loss: 0.025563078764115597\n",
      "Epoch: 5 - Batch: 296, Training Loss: 0.025649217379033862\n",
      "Epoch: 5 - Batch: 297, Training Loss: 0.025732409332571534\n",
      "Epoch: 5 - Batch: 298, Training Loss: 0.025811680149330232\n",
      "Epoch: 5 - Batch: 299, Training Loss: 0.0258934470465033\n",
      "Epoch: 5 - Batch: 300, Training Loss: 0.025986835471729735\n",
      "Epoch: 5 - Batch: 301, Training Loss: 0.026077531525250493\n",
      "Epoch: 5 - Batch: 302, Training Loss: 0.026163810669486202\n",
      "Epoch: 5 - Batch: 303, Training Loss: 0.026250885197179233\n",
      "Epoch: 5 - Batch: 304, Training Loss: 0.02633802068248317\n",
      "Epoch: 5 - Batch: 305, Training Loss: 0.026421391938011445\n",
      "Epoch: 5 - Batch: 306, Training Loss: 0.02650408481385182\n",
      "Epoch: 5 - Batch: 307, Training Loss: 0.026587307854720806\n",
      "Epoch: 5 - Batch: 308, Training Loss: 0.02666579899849188\n",
      "Epoch: 5 - Batch: 309, Training Loss: 0.02675202790999887\n",
      "Epoch: 5 - Batch: 310, Training Loss: 0.026839852017994544\n",
      "Epoch: 5 - Batch: 311, Training Loss: 0.02692941463186373\n",
      "Epoch: 5 - Batch: 312, Training Loss: 0.027017172895507827\n",
      "Epoch: 5 - Batch: 313, Training Loss: 0.027104981626641888\n",
      "Epoch: 5 - Batch: 314, Training Loss: 0.027185030188874817\n",
      "Epoch: 5 - Batch: 315, Training Loss: 0.027270870323402568\n",
      "Epoch: 5 - Batch: 316, Training Loss: 0.02736144087232563\n",
      "Epoch: 5 - Batch: 317, Training Loss: 0.02744219168575842\n",
      "Epoch: 5 - Batch: 318, Training Loss: 0.027535795294378527\n",
      "Epoch: 5 - Batch: 319, Training Loss: 0.027622207461749735\n",
      "Epoch: 5 - Batch: 320, Training Loss: 0.027704665372472497\n",
      "Epoch: 5 - Batch: 321, Training Loss: 0.027794913056725094\n",
      "Epoch: 5 - Batch: 322, Training Loss: 0.027881736420774537\n",
      "Epoch: 5 - Batch: 323, Training Loss: 0.02797322466376409\n",
      "Epoch: 5 - Batch: 324, Training Loss: 0.028061314243483505\n",
      "Epoch: 5 - Batch: 325, Training Loss: 0.028142512758968282\n",
      "Epoch: 5 - Batch: 326, Training Loss: 0.0282261497333374\n",
      "Epoch: 5 - Batch: 327, Training Loss: 0.0283070579912532\n",
      "Epoch: 5 - Batch: 328, Training Loss: 0.028387237364923577\n",
      "Epoch: 5 - Batch: 329, Training Loss: 0.028472144527370062\n",
      "Epoch: 5 - Batch: 330, Training Loss: 0.028551797937002545\n",
      "Epoch: 5 - Batch: 331, Training Loss: 0.02863172053747114\n",
      "Epoch: 5 - Batch: 332, Training Loss: 0.0287207893866607\n",
      "Epoch: 5 - Batch: 333, Training Loss: 0.02880017033733341\n",
      "Epoch: 5 - Batch: 334, Training Loss: 0.02888416545851709\n",
      "Epoch: 5 - Batch: 335, Training Loss: 0.028979755474708567\n",
      "Epoch: 5 - Batch: 336, Training Loss: 0.029066551161642692\n",
      "Epoch: 5 - Batch: 337, Training Loss: 0.02915782389998634\n",
      "Epoch: 5 - Batch: 338, Training Loss: 0.02924450295382669\n",
      "Epoch: 5 - Batch: 339, Training Loss: 0.02932837484389\n",
      "Epoch: 5 - Batch: 340, Training Loss: 0.029407382178217617\n",
      "Epoch: 5 - Batch: 341, Training Loss: 0.029481672739013906\n",
      "Epoch: 5 - Batch: 342, Training Loss: 0.029561722604789543\n",
      "Epoch: 5 - Batch: 343, Training Loss: 0.029645999507425635\n",
      "Epoch: 5 - Batch: 344, Training Loss: 0.029731695968070827\n",
      "Epoch: 5 - Batch: 345, Training Loss: 0.02981290037112054\n",
      "Epoch: 5 - Batch: 346, Training Loss: 0.02989287223438323\n",
      "Epoch: 5 - Batch: 347, Training Loss: 0.029975841399498444\n",
      "Epoch: 5 - Batch: 348, Training Loss: 0.03007065603636193\n",
      "Epoch: 5 - Batch: 349, Training Loss: 0.030163081050858173\n",
      "Epoch: 5 - Batch: 350, Training Loss: 0.030256343594633327\n",
      "Epoch: 5 - Batch: 351, Training Loss: 0.030339318912964357\n",
      "Epoch: 5 - Batch: 352, Training Loss: 0.030426636245565034\n",
      "Epoch: 5 - Batch: 353, Training Loss: 0.03050656810674699\n",
      "Epoch: 5 - Batch: 354, Training Loss: 0.03059339000662761\n",
      "Epoch: 5 - Batch: 355, Training Loss: 0.030680161284570077\n",
      "Epoch: 5 - Batch: 356, Training Loss: 0.030769155356124858\n",
      "Epoch: 5 - Batch: 357, Training Loss: 0.030850067246662047\n",
      "Epoch: 5 - Batch: 358, Training Loss: 0.030936581621666254\n",
      "Epoch: 5 - Batch: 359, Training Loss: 0.031023653338402263\n",
      "Epoch: 5 - Batch: 360, Training Loss: 0.031116755015122555\n",
      "Epoch: 5 - Batch: 361, Training Loss: 0.031207825790837432\n",
      "Epoch: 5 - Batch: 362, Training Loss: 0.03129844101853236\n",
      "Epoch: 5 - Batch: 363, Training Loss: 0.03138957619296377\n",
      "Epoch: 5 - Batch: 364, Training Loss: 0.03146966762383581\n",
      "Epoch: 5 - Batch: 365, Training Loss: 0.0315612972919423\n",
      "Epoch: 5 - Batch: 366, Training Loss: 0.03164798902906786\n",
      "Epoch: 5 - Batch: 367, Training Loss: 0.031734400844297204\n",
      "Epoch: 5 - Batch: 368, Training Loss: 0.031812984385152364\n",
      "Epoch: 5 - Batch: 369, Training Loss: 0.03189252667240242\n",
      "Epoch: 5 - Batch: 370, Training Loss: 0.03197569054725949\n",
      "Epoch: 5 - Batch: 371, Training Loss: 0.03206758048527474\n",
      "Epoch: 5 - Batch: 372, Training Loss: 0.03215869930055407\n",
      "Epoch: 5 - Batch: 373, Training Loss: 0.03224104313584505\n",
      "Epoch: 5 - Batch: 374, Training Loss: 0.032316517375199555\n",
      "Epoch: 5 - Batch: 375, Training Loss: 0.032405402295130795\n",
      "Epoch: 5 - Batch: 376, Training Loss: 0.032496038805786054\n",
      "Epoch: 5 - Batch: 377, Training Loss: 0.032584336334546014\n",
      "Epoch: 5 - Batch: 378, Training Loss: 0.03267236698919268\n",
      "Epoch: 5 - Batch: 379, Training Loss: 0.032751465120521156\n",
      "Epoch: 5 - Batch: 380, Training Loss: 0.03283781002261745\n",
      "Epoch: 5 - Batch: 381, Training Loss: 0.032916737334249824\n",
      "Epoch: 5 - Batch: 382, Training Loss: 0.03299051178480262\n",
      "Epoch: 5 - Batch: 383, Training Loss: 0.03307167305974027\n",
      "Epoch: 5 - Batch: 384, Training Loss: 0.03315505608422049\n",
      "Epoch: 5 - Batch: 385, Training Loss: 0.03323680609711763\n",
      "Epoch: 5 - Batch: 386, Training Loss: 0.03332396578754161\n",
      "Epoch: 5 - Batch: 387, Training Loss: 0.03340481109237592\n",
      "Epoch: 5 - Batch: 388, Training Loss: 0.03348338920283278\n",
      "Epoch: 5 - Batch: 389, Training Loss: 0.033574193987878004\n",
      "Epoch: 5 - Batch: 390, Training Loss: 0.03366797024486077\n",
      "Epoch: 5 - Batch: 391, Training Loss: 0.033752090079867424\n",
      "Epoch: 5 - Batch: 392, Training Loss: 0.03384071337074585\n",
      "Epoch: 5 - Batch: 393, Training Loss: 0.03393511417295605\n",
      "Epoch: 5 - Batch: 394, Training Loss: 0.034022805968277885\n",
      "Epoch: 5 - Batch: 395, Training Loss: 0.034104328726110966\n",
      "Epoch: 5 - Batch: 396, Training Loss: 0.034198327474975664\n",
      "Epoch: 5 - Batch: 397, Training Loss: 0.03428985685060669\n",
      "Epoch: 5 - Batch: 398, Training Loss: 0.03437819613597879\n",
      "Epoch: 5 - Batch: 399, Training Loss: 0.03445936108060539\n",
      "Epoch: 5 - Batch: 400, Training Loss: 0.034541178173439616\n",
      "Epoch: 5 - Batch: 401, Training Loss: 0.034626560783010615\n",
      "Epoch: 5 - Batch: 402, Training Loss: 0.03471077596815071\n",
      "Epoch: 5 - Batch: 403, Training Loss: 0.034802408527527286\n",
      "Epoch: 5 - Batch: 404, Training Loss: 0.034878971916970924\n",
      "Epoch: 5 - Batch: 405, Training Loss: 0.03495680659416303\n",
      "Epoch: 5 - Batch: 406, Training Loss: 0.03504430717027207\n",
      "Epoch: 5 - Batch: 407, Training Loss: 0.03513518332273608\n",
      "Epoch: 5 - Batch: 408, Training Loss: 0.03521193651664712\n",
      "Epoch: 5 - Batch: 409, Training Loss: 0.03529818525312354\n",
      "Epoch: 5 - Batch: 410, Training Loss: 0.03538515483018375\n",
      "Epoch: 5 - Batch: 411, Training Loss: 0.03546792626455056\n",
      "Epoch: 5 - Batch: 412, Training Loss: 0.035555972067475516\n",
      "Epoch: 5 - Batch: 413, Training Loss: 0.03564101198122869\n",
      "Epoch: 5 - Batch: 414, Training Loss: 0.035723774642938404\n",
      "Epoch: 5 - Batch: 415, Training Loss: 0.03581339537257183\n",
      "Epoch: 5 - Batch: 416, Training Loss: 0.03589596881671727\n",
      "Epoch: 5 - Batch: 417, Training Loss: 0.03598217659971212\n",
      "Epoch: 5 - Batch: 418, Training Loss: 0.03607285363757195\n",
      "Epoch: 5 - Batch: 419, Training Loss: 0.036161088676594975\n",
      "Epoch: 5 - Batch: 420, Training Loss: 0.036248508791375915\n",
      "Epoch: 5 - Batch: 421, Training Loss: 0.03632833785184383\n",
      "Epoch: 5 - Batch: 422, Training Loss: 0.03641865300524294\n",
      "Epoch: 5 - Batch: 423, Training Loss: 0.03648879213465585\n",
      "Epoch: 5 - Batch: 424, Training Loss: 0.03658408062473854\n",
      "Epoch: 5 - Batch: 425, Training Loss: 0.03666605973372214\n",
      "Epoch: 5 - Batch: 426, Training Loss: 0.03674946118078224\n",
      "Epoch: 5 - Batch: 427, Training Loss: 0.03683997808676059\n",
      "Epoch: 5 - Batch: 428, Training Loss: 0.036921126072976124\n",
      "Epoch: 5 - Batch: 429, Training Loss: 0.03701027574799152\n",
      "Epoch: 5 - Batch: 430, Training Loss: 0.03709050641081622\n",
      "Epoch: 5 - Batch: 431, Training Loss: 0.03718004971248396\n",
      "Epoch: 5 - Batch: 432, Training Loss: 0.03726446519839032\n",
      "Epoch: 5 - Batch: 433, Training Loss: 0.03735251837776075\n",
      "Epoch: 5 - Batch: 434, Training Loss: 0.037445404796952235\n",
      "Epoch: 5 - Batch: 435, Training Loss: 0.03753962537666063\n",
      "Epoch: 5 - Batch: 436, Training Loss: 0.03762073764827714\n",
      "Epoch: 5 - Batch: 437, Training Loss: 0.037713307411615334\n",
      "Epoch: 5 - Batch: 438, Training Loss: 0.03779290495424919\n",
      "Epoch: 5 - Batch: 439, Training Loss: 0.03788368443450327\n",
      "Epoch: 5 - Batch: 440, Training Loss: 0.037973670897446264\n",
      "Epoch: 5 - Batch: 441, Training Loss: 0.03806473530989579\n",
      "Epoch: 5 - Batch: 442, Training Loss: 0.03815201804610232\n",
      "Epoch: 5 - Batch: 443, Training Loss: 0.038243132339138695\n",
      "Epoch: 5 - Batch: 444, Training Loss: 0.03832552162544249\n",
      "Epoch: 5 - Batch: 445, Training Loss: 0.038417662164909924\n",
      "Epoch: 5 - Batch: 446, Training Loss: 0.038503860977554004\n",
      "Epoch: 5 - Batch: 447, Training Loss: 0.03858867797932617\n",
      "Epoch: 5 - Batch: 448, Training Loss: 0.03868062401910129\n",
      "Epoch: 5 - Batch: 449, Training Loss: 0.0387674174819815\n",
      "Epoch: 5 - Batch: 450, Training Loss: 0.038848647449295316\n",
      "Epoch: 5 - Batch: 451, Training Loss: 0.038942912948714756\n",
      "Epoch: 5 - Batch: 452, Training Loss: 0.039019300329003165\n",
      "Epoch: 5 - Batch: 453, Training Loss: 0.039103604414520376\n",
      "Epoch: 5 - Batch: 454, Training Loss: 0.039190819749537587\n",
      "Epoch: 5 - Batch: 455, Training Loss: 0.03927982901879409\n",
      "Epoch: 5 - Batch: 456, Training Loss: 0.03936403471463751\n",
      "Epoch: 5 - Batch: 457, Training Loss: 0.03946036370980799\n",
      "Epoch: 5 - Batch: 458, Training Loss: 0.039550640844241106\n",
      "Epoch: 5 - Batch: 459, Training Loss: 0.039635554672671394\n",
      "Epoch: 5 - Batch: 460, Training Loss: 0.03972417311387671\n",
      "Epoch: 5 - Batch: 461, Training Loss: 0.03980362764984419\n",
      "Epoch: 5 - Batch: 462, Training Loss: 0.03988183049444931\n",
      "Epoch: 5 - Batch: 463, Training Loss: 0.03996302175086924\n",
      "Epoch: 5 - Batch: 464, Training Loss: 0.040044987515529394\n",
      "Epoch: 5 - Batch: 465, Training Loss: 0.04012478626090694\n",
      "Epoch: 5 - Batch: 466, Training Loss: 0.04020539726166188\n",
      "Epoch: 5 - Batch: 467, Training Loss: 0.0402913491363648\n",
      "Epoch: 5 - Batch: 468, Training Loss: 0.04038008333957611\n",
      "Epoch: 5 - Batch: 469, Training Loss: 0.04046836357631691\n",
      "Epoch: 5 - Batch: 470, Training Loss: 0.04056235012995268\n",
      "Epoch: 5 - Batch: 471, Training Loss: 0.04065516489754071\n",
      "Epoch: 5 - Batch: 472, Training Loss: 0.0407443560264498\n",
      "Epoch: 5 - Batch: 473, Training Loss: 0.04083100746495411\n",
      "Epoch: 5 - Batch: 474, Training Loss: 0.04090905742438674\n",
      "Epoch: 5 - Batch: 475, Training Loss: 0.04099522966576453\n",
      "Epoch: 5 - Batch: 476, Training Loss: 0.041084355802876046\n",
      "Epoch: 5 - Batch: 477, Training Loss: 0.041164838766083\n",
      "Epoch: 5 - Batch: 478, Training Loss: 0.041246369518154295\n",
      "Epoch: 5 - Batch: 479, Training Loss: 0.04133524631411084\n",
      "Epoch: 5 - Batch: 480, Training Loss: 0.041422236519271065\n",
      "Epoch: 5 - Batch: 481, Training Loss: 0.04150872521263055\n",
      "Epoch: 5 - Batch: 482, Training Loss: 0.041589872754035306\n",
      "Epoch: 5 - Batch: 483, Training Loss: 0.041674705046178095\n",
      "Epoch: 5 - Batch: 484, Training Loss: 0.04176083460425461\n",
      "Epoch: 5 - Batch: 485, Training Loss: 0.04184288138625634\n",
      "Epoch: 5 - Batch: 486, Training Loss: 0.041939121456338005\n",
      "Epoch: 5 - Batch: 487, Training Loss: 0.04201382957089995\n",
      "Epoch: 5 - Batch: 488, Training Loss: 0.042094901723055106\n",
      "Epoch: 5 - Batch: 489, Training Loss: 0.04218736376804894\n",
      "Epoch: 5 - Batch: 490, Training Loss: 0.042274601646323705\n",
      "Epoch: 5 - Batch: 491, Training Loss: 0.04236433044026543\n",
      "Epoch: 5 - Batch: 492, Training Loss: 0.04244757166277512\n",
      "Epoch: 5 - Batch: 493, Training Loss: 0.04252944039072761\n",
      "Epoch: 5 - Batch: 494, Training Loss: 0.042616812157047726\n",
      "Epoch: 5 - Batch: 495, Training Loss: 0.042705420670323506\n",
      "Epoch: 5 - Batch: 496, Training Loss: 0.04279689559987924\n",
      "Epoch: 5 - Batch: 497, Training Loss: 0.042881371085817736\n",
      "Epoch: 5 - Batch: 498, Training Loss: 0.042960573939195716\n",
      "Epoch: 5 - Batch: 499, Training Loss: 0.04305024088590497\n",
      "Epoch: 5 - Batch: 500, Training Loss: 0.04313637841350799\n",
      "Epoch: 5 - Batch: 501, Training Loss: 0.043223459557761405\n",
      "Epoch: 5 - Batch: 502, Training Loss: 0.04331821203849604\n",
      "Epoch: 5 - Batch: 503, Training Loss: 0.043406690376660916\n",
      "Epoch: 5 - Batch: 504, Training Loss: 0.04350217009831226\n",
      "Epoch: 5 - Batch: 505, Training Loss: 0.043585770079251344\n",
      "Epoch: 5 - Batch: 506, Training Loss: 0.043670075079101825\n",
      "Epoch: 5 - Batch: 507, Training Loss: 0.04375035799483755\n",
      "Epoch: 5 - Batch: 508, Training Loss: 0.04384029211837854\n",
      "Epoch: 5 - Batch: 509, Training Loss: 0.04392960998129291\n",
      "Epoch: 5 - Batch: 510, Training Loss: 0.04401844165984473\n",
      "Epoch: 5 - Batch: 511, Training Loss: 0.04409627079543585\n",
      "Epoch: 5 - Batch: 512, Training Loss: 0.04418115422228478\n",
      "Epoch: 5 - Batch: 513, Training Loss: 0.044265334724539744\n",
      "Epoch: 5 - Batch: 514, Training Loss: 0.044351845096246914\n",
      "Epoch: 5 - Batch: 515, Training Loss: 0.04443219105773304\n",
      "Epoch: 5 - Batch: 516, Training Loss: 0.04452152856027902\n",
      "Epoch: 5 - Batch: 517, Training Loss: 0.04461535527239589\n",
      "Epoch: 5 - Batch: 518, Training Loss: 0.044705882205150614\n",
      "Epoch: 5 - Batch: 519, Training Loss: 0.04479615696725956\n",
      "Epoch: 5 - Batch: 520, Training Loss: 0.04487992102159789\n",
      "Epoch: 5 - Batch: 521, Training Loss: 0.044964616800076136\n",
      "Epoch: 5 - Batch: 522, Training Loss: 0.04504974376700609\n",
      "Epoch: 5 - Batch: 523, Training Loss: 0.04514791151728005\n",
      "Epoch: 5 - Batch: 524, Training Loss: 0.04522313836770469\n",
      "Epoch: 5 - Batch: 525, Training Loss: 0.04531939449422\n",
      "Epoch: 5 - Batch: 526, Training Loss: 0.04541229768648472\n",
      "Epoch: 5 - Batch: 527, Training Loss: 0.04549688548714565\n",
      "Epoch: 5 - Batch: 528, Training Loss: 0.045579981707518376\n",
      "Epoch: 5 - Batch: 529, Training Loss: 0.045665760204863196\n",
      "Epoch: 5 - Batch: 530, Training Loss: 0.045752225743350305\n",
      "Epoch: 5 - Batch: 531, Training Loss: 0.0458369970346367\n",
      "Epoch: 5 - Batch: 532, Training Loss: 0.0459200386668418\n",
      "Epoch: 5 - Batch: 533, Training Loss: 0.04601507501716835\n",
      "Epoch: 5 - Batch: 534, Training Loss: 0.04609004201160537\n",
      "Epoch: 5 - Batch: 535, Training Loss: 0.04618573873913901\n",
      "Epoch: 5 - Batch: 536, Training Loss: 0.04627007199065207\n",
      "Epoch: 5 - Batch: 537, Training Loss: 0.04635411634991814\n",
      "Epoch: 5 - Batch: 538, Training Loss: 0.046438044582680485\n",
      "Epoch: 5 - Batch: 539, Training Loss: 0.046517374281266434\n",
      "Epoch: 5 - Batch: 540, Training Loss: 0.04660961481049088\n",
      "Epoch: 5 - Batch: 541, Training Loss: 0.04670046733238211\n",
      "Epoch: 5 - Batch: 542, Training Loss: 0.046788983442395876\n",
      "Epoch: 5 - Batch: 543, Training Loss: 0.046878124585693355\n",
      "Epoch: 5 - Batch: 544, Training Loss: 0.04695863209380637\n",
      "Epoch: 5 - Batch: 545, Training Loss: 0.047045530260202306\n",
      "Epoch: 5 - Batch: 546, Training Loss: 0.04713775103241452\n",
      "Epoch: 5 - Batch: 547, Training Loss: 0.047229709094436606\n",
      "Epoch: 5 - Batch: 548, Training Loss: 0.047323638292193215\n",
      "Epoch: 5 - Batch: 549, Training Loss: 0.04741033357544918\n",
      "Epoch: 5 - Batch: 550, Training Loss: 0.04750365602040963\n",
      "Epoch: 5 - Batch: 551, Training Loss: 0.04758083211817156\n",
      "Epoch: 5 - Batch: 552, Training Loss: 0.0476645252066564\n",
      "Epoch: 5 - Batch: 553, Training Loss: 0.047751958288907215\n",
      "Epoch: 5 - Batch: 554, Training Loss: 0.0478398178149614\n",
      "Epoch: 5 - Batch: 555, Training Loss: 0.047922893314565194\n",
      "Epoch: 5 - Batch: 556, Training Loss: 0.04801481018834446\n",
      "Epoch: 5 - Batch: 557, Training Loss: 0.04810328202115165\n",
      "Epoch: 5 - Batch: 558, Training Loss: 0.048182232388809546\n",
      "Epoch: 5 - Batch: 559, Training Loss: 0.048265451679676524\n",
      "Epoch: 5 - Batch: 560, Training Loss: 0.04835131556215769\n",
      "Epoch: 5 - Batch: 561, Training Loss: 0.048439428790983674\n",
      "Epoch: 5 - Batch: 562, Training Loss: 0.048529517591296145\n",
      "Epoch: 5 - Batch: 563, Training Loss: 0.048616938194133354\n",
      "Epoch: 5 - Batch: 564, Training Loss: 0.04870311867686646\n",
      "Epoch: 5 - Batch: 565, Training Loss: 0.04878887024462519\n",
      "Epoch: 5 - Batch: 566, Training Loss: 0.0488695523842156\n",
      "Epoch: 5 - Batch: 567, Training Loss: 0.048953877487042254\n",
      "Epoch: 5 - Batch: 568, Training Loss: 0.04903838706278485\n",
      "Epoch: 5 - Batch: 569, Training Loss: 0.04912724044439607\n",
      "Epoch: 5 - Batch: 570, Training Loss: 0.04921630155264244\n",
      "Epoch: 5 - Batch: 571, Training Loss: 0.04930544613704555\n",
      "Epoch: 5 - Batch: 572, Training Loss: 0.04938445825308907\n",
      "Epoch: 5 - Batch: 573, Training Loss: 0.04947533404105536\n",
      "Epoch: 5 - Batch: 574, Training Loss: 0.04956896014350959\n",
      "Epoch: 5 - Batch: 575, Training Loss: 0.04964658022421114\n",
      "Epoch: 5 - Batch: 576, Training Loss: 0.049723750477653636\n",
      "Epoch: 5 - Batch: 577, Training Loss: 0.049807354060324466\n",
      "Epoch: 5 - Batch: 578, Training Loss: 0.049891190093696414\n",
      "Epoch: 5 - Batch: 579, Training Loss: 0.04997726325965046\n",
      "Epoch: 5 - Batch: 580, Training Loss: 0.050063866831572296\n",
      "Epoch: 5 - Batch: 581, Training Loss: 0.0501500397710559\n",
      "Epoch: 5 - Batch: 582, Training Loss: 0.05023570893143935\n",
      "Epoch: 5 - Batch: 583, Training Loss: 0.050335007866421345\n",
      "Epoch: 5 - Batch: 584, Training Loss: 0.050421889519217\n",
      "Epoch: 5 - Batch: 585, Training Loss: 0.050506123460545074\n",
      "Epoch: 5 - Batch: 586, Training Loss: 0.05058739944106311\n",
      "Epoch: 5 - Batch: 587, Training Loss: 0.050666701518135085\n",
      "Epoch: 5 - Batch: 588, Training Loss: 0.05074766750878362\n",
      "Epoch: 5 - Batch: 589, Training Loss: 0.050827021085059466\n",
      "Epoch: 5 - Batch: 590, Training Loss: 0.05091852045449649\n",
      "Epoch: 5 - Batch: 591, Training Loss: 0.05100160602412216\n",
      "Epoch: 5 - Batch: 592, Training Loss: 0.051088461346590695\n",
      "Epoch: 5 - Batch: 593, Training Loss: 0.051170920807973266\n",
      "Epoch: 5 - Batch: 594, Training Loss: 0.051246976606535474\n",
      "Epoch: 5 - Batch: 595, Training Loss: 0.05133942695671251\n",
      "Epoch: 5 - Batch: 596, Training Loss: 0.05142709528208767\n",
      "Epoch: 5 - Batch: 597, Training Loss: 0.05151480976275939\n",
      "Epoch: 5 - Batch: 598, Training Loss: 0.05159765635779248\n",
      "Epoch: 5 - Batch: 599, Training Loss: 0.051690886275932366\n",
      "Epoch: 5 - Batch: 600, Training Loss: 0.05177780880807446\n",
      "Epoch: 5 - Batch: 601, Training Loss: 0.0518717601413454\n",
      "Epoch: 5 - Batch: 602, Training Loss: 0.05195559303633016\n",
      "Epoch: 5 - Batch: 603, Training Loss: 0.05204809811676715\n",
      "Epoch: 5 - Batch: 604, Training Loss: 0.05213863713354811\n",
      "Epoch: 5 - Batch: 605, Training Loss: 0.05223016932631409\n",
      "Epoch: 5 - Batch: 606, Training Loss: 0.052318086165001935\n",
      "Epoch: 5 - Batch: 607, Training Loss: 0.05241300326897137\n",
      "Epoch: 5 - Batch: 608, Training Loss: 0.05249387059811731\n",
      "Epoch: 5 - Batch: 609, Training Loss: 0.052579981369116215\n",
      "Epoch: 5 - Batch: 610, Training Loss: 0.052663888269494814\n",
      "Epoch: 5 - Batch: 611, Training Loss: 0.0527547037732443\n",
      "Epoch: 5 - Batch: 612, Training Loss: 0.05283676354742762\n",
      "Epoch: 5 - Batch: 613, Training Loss: 0.05292537854134938\n",
      "Epoch: 5 - Batch: 614, Training Loss: 0.053014767716081185\n",
      "Epoch: 5 - Batch: 615, Training Loss: 0.05309958172773643\n",
      "Epoch: 5 - Batch: 616, Training Loss: 0.05319106819148285\n",
      "Epoch: 5 - Batch: 617, Training Loss: 0.0532748193215968\n",
      "Epoch: 5 - Batch: 618, Training Loss: 0.053360949769294876\n",
      "Epoch: 5 - Batch: 619, Training Loss: 0.05346021772814825\n",
      "Epoch: 5 - Batch: 620, Training Loss: 0.05355436574191992\n",
      "Epoch: 5 - Batch: 621, Training Loss: 0.05364690048258696\n",
      "Epoch: 5 - Batch: 622, Training Loss: 0.05374601181862169\n",
      "Epoch: 5 - Batch: 623, Training Loss: 0.053825312616862665\n",
      "Epoch: 5 - Batch: 624, Training Loss: 0.05391526068398608\n",
      "Epoch: 5 - Batch: 625, Training Loss: 0.05400358016910047\n",
      "Epoch: 5 - Batch: 626, Training Loss: 0.05410115604920569\n",
      "Epoch: 5 - Batch: 627, Training Loss: 0.054189180785397785\n",
      "Epoch: 5 - Batch: 628, Training Loss: 0.05427249078677464\n",
      "Epoch: 5 - Batch: 629, Training Loss: 0.05435593444143559\n",
      "Epoch: 5 - Batch: 630, Training Loss: 0.054442200074543805\n",
      "Epoch: 5 - Batch: 631, Training Loss: 0.0545193229062146\n",
      "Epoch: 5 - Batch: 632, Training Loss: 0.054598845763882596\n",
      "Epoch: 5 - Batch: 633, Training Loss: 0.05468906800742964\n",
      "Epoch: 5 - Batch: 634, Training Loss: 0.05477760840905444\n",
      "Epoch: 5 - Batch: 635, Training Loss: 0.054856963807818904\n",
      "Epoch: 5 - Batch: 636, Training Loss: 0.05494137509273455\n",
      "Epoch: 5 - Batch: 637, Training Loss: 0.05501828142511311\n",
      "Epoch: 5 - Batch: 638, Training Loss: 0.05510284954289694\n",
      "Epoch: 5 - Batch: 639, Training Loss: 0.05519229173042486\n",
      "Epoch: 5 - Batch: 640, Training Loss: 0.05528029793283437\n",
      "Epoch: 5 - Batch: 641, Training Loss: 0.055359518158544556\n",
      "Epoch: 5 - Batch: 642, Training Loss: 0.05544955035668503\n",
      "Epoch: 5 - Batch: 643, Training Loss: 0.05553551667538251\n",
      "Epoch: 5 - Batch: 644, Training Loss: 0.055621806390052216\n",
      "Epoch: 5 - Batch: 645, Training Loss: 0.05570898696187124\n",
      "Epoch: 5 - Batch: 646, Training Loss: 0.05579657203967298\n",
      "Epoch: 5 - Batch: 647, Training Loss: 0.05588357385315903\n",
      "Epoch: 5 - Batch: 648, Training Loss: 0.05597443868765981\n",
      "Epoch: 5 - Batch: 649, Training Loss: 0.05605922368776739\n",
      "Epoch: 5 - Batch: 650, Training Loss: 0.056147056951451654\n",
      "Epoch: 5 - Batch: 651, Training Loss: 0.05623187995199145\n",
      "Epoch: 5 - Batch: 652, Training Loss: 0.05632037867113923\n",
      "Epoch: 5 - Batch: 653, Training Loss: 0.056410770504974804\n",
      "Epoch: 5 - Batch: 654, Training Loss: 0.056496522326028566\n",
      "Epoch: 5 - Batch: 655, Training Loss: 0.056582973173404015\n",
      "Epoch: 5 - Batch: 656, Training Loss: 0.05667242098284598\n",
      "Epoch: 5 - Batch: 657, Training Loss: 0.056759131074594224\n",
      "Epoch: 5 - Batch: 658, Training Loss: 0.05685076057589667\n",
      "Epoch: 5 - Batch: 659, Training Loss: 0.056931392470402506\n",
      "Epoch: 5 - Batch: 660, Training Loss: 0.05701999906852075\n",
      "Epoch: 5 - Batch: 661, Training Loss: 0.057106187601093435\n",
      "Epoch: 5 - Batch: 662, Training Loss: 0.05718649449954381\n",
      "Epoch: 5 - Batch: 663, Training Loss: 0.057276082047528494\n",
      "Epoch: 5 - Batch: 664, Training Loss: 0.0573629411570666\n",
      "Epoch: 5 - Batch: 665, Training Loss: 0.057457726238724804\n",
      "Epoch: 5 - Batch: 666, Training Loss: 0.057541616767595456\n",
      "Epoch: 5 - Batch: 667, Training Loss: 0.057625289283581634\n",
      "Epoch: 5 - Batch: 668, Training Loss: 0.0577173424362642\n",
      "Epoch: 5 - Batch: 669, Training Loss: 0.05780184540162434\n",
      "Epoch: 5 - Batch: 670, Training Loss: 0.0578817318365052\n",
      "Epoch: 5 - Batch: 671, Training Loss: 0.0579774120380827\n",
      "Epoch: 5 - Batch: 672, Training Loss: 0.058059079254395136\n",
      "Epoch: 5 - Batch: 673, Training Loss: 0.058142275791955036\n",
      "Epoch: 5 - Batch: 674, Training Loss: 0.05822576258090598\n",
      "Epoch: 5 - Batch: 675, Training Loss: 0.05831255381355436\n",
      "Epoch: 5 - Batch: 676, Training Loss: 0.0583840011870782\n",
      "Epoch: 5 - Batch: 677, Training Loss: 0.058473438277815905\n",
      "Epoch: 5 - Batch: 678, Training Loss: 0.05855934128511209\n",
      "Epoch: 5 - Batch: 679, Training Loss: 0.05864487766329922\n",
      "Epoch: 5 - Batch: 680, Training Loss: 0.05872783968673021\n",
      "Epoch: 5 - Batch: 681, Training Loss: 0.05881688093293959\n",
      "Epoch: 5 - Batch: 682, Training Loss: 0.058907970940542856\n",
      "Epoch: 5 - Batch: 683, Training Loss: 0.058990889940886554\n",
      "Epoch: 5 - Batch: 684, Training Loss: 0.05907577244104635\n",
      "Epoch: 5 - Batch: 685, Training Loss: 0.059156927389786215\n",
      "Epoch: 5 - Batch: 686, Training Loss: 0.05923793874495658\n",
      "Epoch: 5 - Batch: 687, Training Loss: 0.05933152804549654\n",
      "Epoch: 5 - Batch: 688, Training Loss: 0.05941725463589428\n",
      "Epoch: 5 - Batch: 689, Training Loss: 0.05951621529845456\n",
      "Epoch: 5 - Batch: 690, Training Loss: 0.05960403487185143\n",
      "Epoch: 5 - Batch: 691, Training Loss: 0.059691526366407004\n",
      "Epoch: 5 - Batch: 692, Training Loss: 0.05978044252193982\n",
      "Epoch: 5 - Batch: 693, Training Loss: 0.059866377580324016\n",
      "Epoch: 5 - Batch: 694, Training Loss: 0.05995107692346644\n",
      "Epoch: 5 - Batch: 695, Training Loss: 0.060038805823421006\n",
      "Epoch: 5 - Batch: 696, Training Loss: 0.060130272406596644\n",
      "Epoch: 5 - Batch: 697, Training Loss: 0.06021718932918055\n",
      "Epoch: 5 - Batch: 698, Training Loss: 0.06030099740125251\n",
      "Epoch: 5 - Batch: 699, Training Loss: 0.06038681818651125\n",
      "Epoch: 5 - Batch: 700, Training Loss: 0.060476073905652634\n",
      "Epoch: 5 - Batch: 701, Training Loss: 0.06056930639381037\n",
      "Epoch: 5 - Batch: 702, Training Loss: 0.06064592372273924\n",
      "Epoch: 5 - Batch: 703, Training Loss: 0.06073241116197944\n",
      "Epoch: 5 - Batch: 704, Training Loss: 0.06081938616326001\n",
      "Epoch: 5 - Batch: 705, Training Loss: 0.06090337290099604\n",
      "Epoch: 5 - Batch: 706, Training Loss: 0.06099191869595157\n",
      "Epoch: 5 - Batch: 707, Training Loss: 0.06107917545397286\n",
      "Epoch: 5 - Batch: 708, Training Loss: 0.06116595053751868\n",
      "Epoch: 5 - Batch: 709, Training Loss: 0.06124829668953249\n",
      "Epoch: 5 - Batch: 710, Training Loss: 0.06132898959107264\n",
      "Epoch: 5 - Batch: 711, Training Loss: 0.0614067158085691\n",
      "Epoch: 5 - Batch: 712, Training Loss: 0.061486899327628845\n",
      "Epoch: 5 - Batch: 713, Training Loss: 0.061569875992748074\n",
      "Epoch: 5 - Batch: 714, Training Loss: 0.06166521170073085\n",
      "Epoch: 5 - Batch: 715, Training Loss: 0.061751666749097024\n",
      "Epoch: 5 - Batch: 716, Training Loss: 0.061844539539780384\n",
      "Epoch: 5 - Batch: 717, Training Loss: 0.06193201104256249\n",
      "Epoch: 5 - Batch: 718, Training Loss: 0.062014597089680074\n",
      "Epoch: 5 - Batch: 719, Training Loss: 0.06210096187852508\n",
      "Epoch: 5 - Batch: 720, Training Loss: 0.06218486052519251\n",
      "Epoch: 5 - Batch: 721, Training Loss: 0.06227854757907972\n",
      "Epoch: 5 - Batch: 722, Training Loss: 0.06236380347935715\n",
      "Epoch: 5 - Batch: 723, Training Loss: 0.06245430668887016\n",
      "Epoch: 5 - Batch: 724, Training Loss: 0.06253119913944558\n",
      "Epoch: 5 - Batch: 725, Training Loss: 0.06262184418799667\n",
      "Epoch: 5 - Batch: 726, Training Loss: 0.06271814134212869\n",
      "Epoch: 5 - Batch: 727, Training Loss: 0.0628075654159731\n",
      "Epoch: 5 - Batch: 728, Training Loss: 0.0628900286088239\n",
      "Epoch: 5 - Batch: 729, Training Loss: 0.06298077414754413\n",
      "Epoch: 5 - Batch: 730, Training Loss: 0.06306469123532522\n",
      "Epoch: 5 - Batch: 731, Training Loss: 0.06315112921407764\n",
      "Epoch: 5 - Batch: 732, Training Loss: 0.06322593398231574\n",
      "Epoch: 5 - Batch: 733, Training Loss: 0.06332157191451311\n",
      "Epoch: 5 - Batch: 734, Training Loss: 0.06341178209874562\n",
      "Epoch: 5 - Batch: 735, Training Loss: 0.06350381591451504\n",
      "Epoch: 5 - Batch: 736, Training Loss: 0.06358504077886072\n",
      "Epoch: 5 - Batch: 737, Training Loss: 0.06367188790321943\n",
      "Epoch: 5 - Batch: 738, Training Loss: 0.06375480477217814\n",
      "Epoch: 5 - Batch: 739, Training Loss: 0.06384835891413254\n",
      "Epoch: 5 - Batch: 740, Training Loss: 0.0639278779723751\n",
      "Epoch: 5 - Batch: 741, Training Loss: 0.06401409672119131\n",
      "Epoch: 5 - Batch: 742, Training Loss: 0.06409786778747739\n",
      "Epoch: 5 - Batch: 743, Training Loss: 0.06417569351285252\n",
      "Epoch: 5 - Batch: 744, Training Loss: 0.06426531702873126\n",
      "Epoch: 5 - Batch: 745, Training Loss: 0.06434754308801188\n",
      "Epoch: 5 - Batch: 746, Training Loss: 0.06443604852874481\n",
      "Epoch: 5 - Batch: 747, Training Loss: 0.06451914701641694\n",
      "Epoch: 5 - Batch: 748, Training Loss: 0.06460632396797042\n",
      "Epoch: 5 - Batch: 749, Training Loss: 0.06469237013020325\n",
      "Epoch: 5 - Batch: 750, Training Loss: 0.06478435029685004\n",
      "Epoch: 5 - Batch: 751, Training Loss: 0.06487890516421688\n",
      "Epoch: 5 - Batch: 752, Training Loss: 0.06496372903584445\n",
      "Epoch: 5 - Batch: 753, Training Loss: 0.06504342797062487\n",
      "Epoch: 5 - Batch: 754, Training Loss: 0.06512602892508158\n",
      "Epoch: 5 - Batch: 755, Training Loss: 0.06521245294789572\n",
      "Epoch: 5 - Batch: 756, Training Loss: 0.06529279651852389\n",
      "Epoch: 5 - Batch: 757, Training Loss: 0.06537598309726463\n",
      "Epoch: 5 - Batch: 758, Training Loss: 0.06546281265777537\n",
      "Epoch: 5 - Batch: 759, Training Loss: 0.06554014651509463\n",
      "Epoch: 5 - Batch: 760, Training Loss: 0.06562841108461122\n",
      "Epoch: 5 - Batch: 761, Training Loss: 0.06570965305563822\n",
      "Epoch: 5 - Batch: 762, Training Loss: 0.06578926760610657\n",
      "Epoch: 5 - Batch: 763, Training Loss: 0.06587700949206479\n",
      "Epoch: 5 - Batch: 764, Training Loss: 0.06595522771353152\n",
      "Epoch: 5 - Batch: 765, Training Loss: 0.06603891492680729\n",
      "Epoch: 5 - Batch: 766, Training Loss: 0.06612599101751598\n",
      "Epoch: 5 - Batch: 767, Training Loss: 0.06621932565770537\n",
      "Epoch: 5 - Batch: 768, Training Loss: 0.06630188042597589\n",
      "Epoch: 5 - Batch: 769, Training Loss: 0.06638337137366013\n",
      "Epoch: 5 - Batch: 770, Training Loss: 0.06647220498590327\n",
      "Epoch: 5 - Batch: 771, Training Loss: 0.06655473606182173\n",
      "Epoch: 5 - Batch: 772, Training Loss: 0.06663972392653549\n",
      "Epoch: 5 - Batch: 773, Training Loss: 0.06672721015749682\n",
      "Epoch: 5 - Batch: 774, Training Loss: 0.06680992648475008\n",
      "Epoch: 5 - Batch: 775, Training Loss: 0.06689657869549533\n",
      "Epoch: 5 - Batch: 776, Training Loss: 0.0669776505572879\n",
      "Epoch: 5 - Batch: 777, Training Loss: 0.06706905378581675\n",
      "Epoch: 5 - Batch: 778, Training Loss: 0.06715375273974973\n",
      "Epoch: 5 - Batch: 779, Training Loss: 0.06723921716361497\n",
      "Epoch: 5 - Batch: 780, Training Loss: 0.06732327864191821\n",
      "Epoch: 5 - Batch: 781, Training Loss: 0.06741024708223975\n",
      "Epoch: 5 - Batch: 782, Training Loss: 0.06749160188075717\n",
      "Epoch: 5 - Batch: 783, Training Loss: 0.06757491009053504\n",
      "Epoch: 5 - Batch: 784, Training Loss: 0.0676597430251823\n",
      "Epoch: 5 - Batch: 785, Training Loss: 0.0677492974718313\n",
      "Epoch: 5 - Batch: 786, Training Loss: 0.0678419733966761\n",
      "Epoch: 5 - Batch: 787, Training Loss: 0.0679309403841966\n",
      "Epoch: 5 - Batch: 788, Training Loss: 0.068022062177237\n",
      "Epoch: 5 - Batch: 789, Training Loss: 0.06811093647731081\n",
      "Epoch: 5 - Batch: 790, Training Loss: 0.06819924371777285\n",
      "Epoch: 5 - Batch: 791, Training Loss: 0.06828538866506682\n",
      "Epoch: 5 - Batch: 792, Training Loss: 0.06836583149645657\n",
      "Epoch: 5 - Batch: 793, Training Loss: 0.06844897022097067\n",
      "Epoch: 5 - Batch: 794, Training Loss: 0.06853475933199499\n",
      "Epoch: 5 - Batch: 795, Training Loss: 0.06862683770297771\n",
      "Epoch: 5 - Batch: 796, Training Loss: 0.06871738073541157\n",
      "Epoch: 5 - Batch: 797, Training Loss: 0.06879926718783813\n",
      "Epoch: 5 - Batch: 798, Training Loss: 0.06888782480116902\n",
      "Epoch: 5 - Batch: 799, Training Loss: 0.06897169086544667\n",
      "Epoch: 5 - Batch: 800, Training Loss: 0.06905776017019602\n",
      "Epoch: 5 - Batch: 801, Training Loss: 0.06914110417488598\n",
      "Epoch: 5 - Batch: 802, Training Loss: 0.06922297205028446\n",
      "Epoch: 5 - Batch: 803, Training Loss: 0.06931147813969979\n",
      "Epoch: 5 - Batch: 804, Training Loss: 0.06939670835215456\n",
      "Epoch: 5 - Batch: 805, Training Loss: 0.06947607143008294\n",
      "Epoch: 5 - Batch: 806, Training Loss: 0.0695581488978507\n",
      "Epoch: 5 - Batch: 807, Training Loss: 0.06964844704959325\n",
      "Epoch: 5 - Batch: 808, Training Loss: 0.06973276169936653\n",
      "Epoch: 5 - Batch: 809, Training Loss: 0.06981636377462305\n",
      "Epoch: 5 - Batch: 810, Training Loss: 0.06990596388851232\n",
      "Epoch: 5 - Batch: 811, Training Loss: 0.06998870667582918\n",
      "Epoch: 5 - Batch: 812, Training Loss: 0.07007005277582465\n",
      "Epoch: 5 - Batch: 813, Training Loss: 0.07014985088487564\n",
      "Epoch: 5 - Batch: 814, Training Loss: 0.07023609863288367\n",
      "Epoch: 5 - Batch: 815, Training Loss: 0.0703255952479531\n",
      "Epoch: 5 - Batch: 816, Training Loss: 0.07041331144608866\n",
      "Epoch: 5 - Batch: 817, Training Loss: 0.07049870082666823\n",
      "Epoch: 5 - Batch: 818, Training Loss: 0.0705837675835165\n",
      "Epoch: 5 - Batch: 819, Training Loss: 0.07066395370966166\n",
      "Epoch: 5 - Batch: 820, Training Loss: 0.07075397977929804\n",
      "Epoch: 5 - Batch: 821, Training Loss: 0.0708430091515307\n",
      "Epoch: 5 - Batch: 822, Training Loss: 0.07093564644579468\n",
      "Epoch: 5 - Batch: 823, Training Loss: 0.0710177572558373\n",
      "Epoch: 5 - Batch: 824, Training Loss: 0.07110491011965137\n",
      "Epoch: 5 - Batch: 825, Training Loss: 0.07118724524406454\n",
      "Epoch: 5 - Batch: 826, Training Loss: 0.07127593307575184\n",
      "Epoch: 5 - Batch: 827, Training Loss: 0.07135551771887301\n",
      "Epoch: 5 - Batch: 828, Training Loss: 0.07144636938203232\n",
      "Epoch: 5 - Batch: 829, Training Loss: 0.07152945023172134\n",
      "Epoch: 5 - Batch: 830, Training Loss: 0.0716182619432114\n",
      "Epoch: 5 - Batch: 831, Training Loss: 0.07170241910525618\n",
      "Epoch: 5 - Batch: 832, Training Loss: 0.0717877512473372\n",
      "Epoch: 5 - Batch: 833, Training Loss: 0.071868908673426\n",
      "Epoch: 5 - Batch: 834, Training Loss: 0.07194890482829973\n",
      "Epoch: 5 - Batch: 835, Training Loss: 0.07203689924322353\n",
      "Epoch: 5 - Batch: 836, Training Loss: 0.07213998025601381\n",
      "Epoch: 5 - Batch: 837, Training Loss: 0.07222391134297867\n",
      "Epoch: 5 - Batch: 838, Training Loss: 0.07229916650066724\n",
      "Epoch: 5 - Batch: 839, Training Loss: 0.07238698022356674\n",
      "Epoch: 5 - Batch: 840, Training Loss: 0.07247207830250757\n",
      "Epoch: 5 - Batch: 841, Training Loss: 0.07256487276942576\n",
      "Epoch: 5 - Batch: 842, Training Loss: 0.07264180393707298\n",
      "Epoch: 5 - Batch: 843, Training Loss: 0.07272476825251509\n",
      "Epoch: 5 - Batch: 844, Training Loss: 0.07280595825481573\n",
      "Epoch: 5 - Batch: 845, Training Loss: 0.07288607672647655\n",
      "Epoch: 5 - Batch: 846, Training Loss: 0.07297002938058642\n",
      "Epoch: 5 - Batch: 847, Training Loss: 0.07306195096812439\n",
      "Epoch: 5 - Batch: 848, Training Loss: 0.07315161383740147\n",
      "Epoch: 5 - Batch: 849, Training Loss: 0.07323277057774032\n",
      "Epoch: 5 - Batch: 850, Training Loss: 0.07331794474403656\n",
      "Epoch: 5 - Batch: 851, Training Loss: 0.07339794999281961\n",
      "Epoch: 5 - Batch: 852, Training Loss: 0.07348358552674354\n",
      "Epoch: 5 - Batch: 853, Training Loss: 0.0735712595975913\n",
      "Epoch: 5 - Batch: 854, Training Loss: 0.07365149139097674\n",
      "Epoch: 5 - Batch: 855, Training Loss: 0.07373648559424414\n",
      "Epoch: 5 - Batch: 856, Training Loss: 0.07382472242486615\n",
      "Epoch: 5 - Batch: 857, Training Loss: 0.07390357619444925\n",
      "Epoch: 5 - Batch: 858, Training Loss: 0.0739987698533444\n",
      "Epoch: 5 - Batch: 859, Training Loss: 0.07408065416820804\n",
      "Epoch: 5 - Batch: 860, Training Loss: 0.07416361787203532\n",
      "Epoch: 5 - Batch: 861, Training Loss: 0.07424633217910628\n",
      "Epoch: 5 - Batch: 862, Training Loss: 0.07433253623681084\n",
      "Epoch: 5 - Batch: 863, Training Loss: 0.07441582939715725\n",
      "Epoch: 5 - Batch: 864, Training Loss: 0.074503251062598\n",
      "Epoch: 5 - Batch: 865, Training Loss: 0.07458977554469164\n",
      "Epoch: 5 - Batch: 866, Training Loss: 0.07467341198158106\n",
      "Epoch: 5 - Batch: 867, Training Loss: 0.0747526118880876\n",
      "Epoch: 5 - Batch: 868, Training Loss: 0.07484319324220591\n",
      "Epoch: 5 - Batch: 869, Training Loss: 0.07492434786969354\n",
      "Epoch: 5 - Batch: 870, Training Loss: 0.07501077761326856\n",
      "Epoch: 5 - Batch: 871, Training Loss: 0.07509891482480921\n",
      "Epoch: 5 - Batch: 872, Training Loss: 0.07518079405523849\n",
      "Epoch: 5 - Batch: 873, Training Loss: 0.07527352551347383\n",
      "Epoch: 5 - Batch: 874, Training Loss: 0.07535007396468871\n",
      "Epoch: 5 - Batch: 875, Training Loss: 0.07543634298330121\n",
      "Epoch: 5 - Batch: 876, Training Loss: 0.07552123375570596\n",
      "Epoch: 5 - Batch: 877, Training Loss: 0.07560645664113869\n",
      "Epoch: 5 - Batch: 878, Training Loss: 0.0756946940339521\n",
      "Epoch: 5 - Batch: 879, Training Loss: 0.07577758653716464\n",
      "Epoch: 5 - Batch: 880, Training Loss: 0.0758638162270905\n",
      "Epoch: 5 - Batch: 881, Training Loss: 0.07594613392974804\n",
      "Epoch: 5 - Batch: 882, Training Loss: 0.07602969346997354\n",
      "Epoch: 5 - Batch: 883, Training Loss: 0.07611424089170016\n",
      "Epoch: 5 - Batch: 884, Training Loss: 0.07619490685129837\n",
      "Epoch: 5 - Batch: 885, Training Loss: 0.0762872545428537\n",
      "Epoch: 5 - Batch: 886, Training Loss: 0.07636962814957744\n",
      "Epoch: 5 - Batch: 887, Training Loss: 0.07645490881716632\n",
      "Epoch: 5 - Batch: 888, Training Loss: 0.07653541953162372\n",
      "Epoch: 5 - Batch: 889, Training Loss: 0.07661508884941959\n",
      "Epoch: 5 - Batch: 890, Training Loss: 0.07670815281607025\n",
      "Epoch: 5 - Batch: 891, Training Loss: 0.07679066526553721\n",
      "Epoch: 5 - Batch: 892, Training Loss: 0.07687616217294538\n",
      "Epoch: 5 - Batch: 893, Training Loss: 0.07696392114705114\n",
      "Epoch: 5 - Batch: 894, Training Loss: 0.07704967418515662\n",
      "Epoch: 5 - Batch: 895, Training Loss: 0.07713342555003182\n",
      "Epoch: 5 - Batch: 896, Training Loss: 0.07723484975386219\n",
      "Epoch: 5 - Batch: 897, Training Loss: 0.07733060708068694\n",
      "Epoch: 5 - Batch: 898, Training Loss: 0.07741370522857305\n",
      "Epoch: 5 - Batch: 899, Training Loss: 0.07750985470552547\n",
      "Epoch: 5 - Batch: 900, Training Loss: 0.07759844521014252\n",
      "Epoch: 5 - Batch: 901, Training Loss: 0.07769109502088767\n",
      "Epoch: 5 - Batch: 902, Training Loss: 0.07777100543848317\n",
      "Epoch: 5 - Batch: 903, Training Loss: 0.07785681244696353\n",
      "Epoch: 5 - Batch: 904, Training Loss: 0.0779472713854479\n",
      "Epoch: 5 - Batch: 905, Training Loss: 0.0780259926075959\n",
      "Epoch: 5 - Batch: 906, Training Loss: 0.07811168964278836\n",
      "Epoch: 5 - Batch: 907, Training Loss: 0.0782028382912797\n",
      "Epoch: 5 - Batch: 908, Training Loss: 0.07829336624339248\n",
      "Epoch: 5 - Batch: 909, Training Loss: 0.07837891471178378\n",
      "Epoch: 5 - Batch: 910, Training Loss: 0.0784697547789731\n",
      "Epoch: 5 - Batch: 911, Training Loss: 0.07855934786238085\n",
      "Epoch: 5 - Batch: 912, Training Loss: 0.07864640408487462\n",
      "Epoch: 5 - Batch: 913, Training Loss: 0.07872848698304068\n",
      "Epoch: 5 - Batch: 914, Training Loss: 0.07881093435421907\n",
      "Epoch: 5 - Batch: 915, Training Loss: 0.07889323700126724\n",
      "Epoch: 5 - Batch: 916, Training Loss: 0.07898483166523636\n",
      "Epoch: 5 - Batch: 917, Training Loss: 0.07906439770043984\n",
      "Epoch: 5 - Batch: 918, Training Loss: 0.07915607400572122\n",
      "Epoch: 5 - Batch: 919, Training Loss: 0.07923946669248008\n",
      "Epoch: 5 - Batch: 920, Training Loss: 0.0793258348730073\n",
      "Epoch: 5 - Batch: 921, Training Loss: 0.07941188327782783\n",
      "Epoch: 5 - Batch: 922, Training Loss: 0.07949395149106014\n",
      "Epoch: 5 - Batch: 923, Training Loss: 0.07957893068814159\n",
      "Epoch: 5 - Batch: 924, Training Loss: 0.07966878505463822\n",
      "Epoch: 5 - Batch: 925, Training Loss: 0.07975888524087112\n",
      "Epoch: 5 - Batch: 926, Training Loss: 0.0798420697005827\n",
      "Epoch: 5 - Batch: 927, Training Loss: 0.0799250148706274\n",
      "Epoch: 5 - Batch: 928, Training Loss: 0.08000981025583115\n",
      "Epoch: 5 - Batch: 929, Training Loss: 0.08008567262966043\n",
      "Epoch: 5 - Batch: 930, Training Loss: 0.08017321823050529\n",
      "Epoch: 5 - Batch: 931, Training Loss: 0.08026925284742914\n",
      "Epoch: 5 - Batch: 932, Training Loss: 0.08034808469154744\n",
      "Epoch: 5 - Batch: 933, Training Loss: 0.0804387057388501\n",
      "Epoch: 5 - Batch: 934, Training Loss: 0.08052341720308633\n",
      "Epoch: 5 - Batch: 935, Training Loss: 0.08061391875691477\n",
      "Epoch: 5 - Batch: 936, Training Loss: 0.0806874627398812\n",
      "Epoch: 5 - Batch: 937, Training Loss: 0.08076665622134312\n",
      "Epoch: 5 - Batch: 938, Training Loss: 0.08086126966164084\n",
      "Epoch: 5 - Batch: 939, Training Loss: 0.08095917425073597\n",
      "Epoch: 5 - Batch: 940, Training Loss: 0.08104640450544816\n",
      "Epoch: 5 - Batch: 941, Training Loss: 0.08113376728906165\n",
      "Epoch: 5 - Batch: 942, Training Loss: 0.08121992841634781\n",
      "Epoch: 5 - Batch: 943, Training Loss: 0.08132096279295127\n",
      "Epoch: 5 - Batch: 944, Training Loss: 0.08141248639222005\n",
      "Epoch: 5 - Batch: 945, Training Loss: 0.08150289426395549\n",
      "Epoch: 5 - Batch: 946, Training Loss: 0.08158708381687428\n",
      "Epoch: 5 - Batch: 947, Training Loss: 0.08167476757471241\n",
      "Epoch: 5 - Batch: 948, Training Loss: 0.08175728699288162\n",
      "Epoch: 5 - Batch: 949, Training Loss: 0.08183562345988121\n",
      "Epoch: 5 - Batch: 950, Training Loss: 0.0819219723961642\n",
      "Epoch: 5 - Batch: 951, Training Loss: 0.08200460249808297\n",
      "Epoch: 5 - Batch: 952, Training Loss: 0.08208095130956984\n",
      "Epoch: 5 - Batch: 953, Training Loss: 0.08216192911241581\n",
      "Epoch: 5 - Batch: 954, Training Loss: 0.08225249943275555\n",
      "Epoch: 5 - Batch: 955, Training Loss: 0.08234106835787174\n",
      "Epoch: 5 - Batch: 956, Training Loss: 0.08242593405406866\n",
      "Epoch: 5 - Batch: 957, Training Loss: 0.08251791480761855\n",
      "Epoch: 5 - Batch: 958, Training Loss: 0.08260073809950902\n",
      "Epoch: 5 - Batch: 959, Training Loss: 0.08268354027524319\n",
      "Epoch: 5 - Batch: 960, Training Loss: 0.08276068903567581\n",
      "Epoch: 5 - Batch: 961, Training Loss: 0.08284755190139387\n",
      "Epoch: 5 - Batch: 962, Training Loss: 0.08293162926314877\n",
      "Epoch: 5 - Batch: 963, Training Loss: 0.08302891740627945\n",
      "Epoch: 5 - Batch: 964, Training Loss: 0.08311844006749132\n",
      "Epoch: 5 - Batch: 965, Training Loss: 0.08320329054127483\n",
      "Epoch: 5 - Batch: 966, Training Loss: 0.08328222098511645\n",
      "Epoch: 5 - Batch: 967, Training Loss: 0.08336536336078573\n",
      "Epoch: 5 - Batch: 968, Training Loss: 0.08346119474860567\n",
      "Epoch: 5 - Batch: 969, Training Loss: 0.08355687524672369\n",
      "Epoch: 5 - Batch: 970, Training Loss: 0.08365003939464713\n",
      "Epoch: 5 - Batch: 971, Training Loss: 0.0837318800871645\n",
      "Epoch: 5 - Batch: 972, Training Loss: 0.08381465771181469\n",
      "Epoch: 5 - Batch: 973, Training Loss: 0.08390800750028236\n",
      "Epoch: 5 - Batch: 974, Training Loss: 0.08399429942047221\n",
      "Epoch: 5 - Batch: 975, Training Loss: 0.08407806049087155\n",
      "Epoch: 5 - Batch: 976, Training Loss: 0.08416427720097167\n",
      "Epoch: 5 - Batch: 977, Training Loss: 0.08425458558954015\n",
      "Epoch: 5 - Batch: 978, Training Loss: 0.08434446296263888\n",
      "Epoch: 5 - Batch: 979, Training Loss: 0.08443255674334901\n",
      "Epoch: 5 - Batch: 980, Training Loss: 0.08451000289067898\n",
      "Epoch: 5 - Batch: 981, Training Loss: 0.08459015888089366\n",
      "Epoch: 5 - Batch: 982, Training Loss: 0.08466816483494852\n",
      "Epoch: 5 - Batch: 983, Training Loss: 0.08475949788222067\n",
      "Epoch: 5 - Batch: 984, Training Loss: 0.08485825677984588\n",
      "Epoch: 5 - Batch: 985, Training Loss: 0.08494968861835711\n",
      "Epoch: 5 - Batch: 986, Training Loss: 0.08503344202194839\n",
      "Epoch: 5 - Batch: 987, Training Loss: 0.08512375579766966\n",
      "Epoch: 5 - Batch: 988, Training Loss: 0.08521010976896357\n",
      "Epoch: 5 - Batch: 989, Training Loss: 0.08530552387484666\n",
      "Epoch: 5 - Batch: 990, Training Loss: 0.08538827043069931\n",
      "Epoch: 5 - Batch: 991, Training Loss: 0.08547754342332606\n",
      "Epoch: 5 - Batch: 992, Training Loss: 0.08556535396459289\n",
      "Epoch: 5 - Batch: 993, Training Loss: 0.08565122507524925\n",
      "Epoch: 5 - Batch: 994, Training Loss: 0.0857325272934255\n",
      "Epoch: 5 - Batch: 995, Training Loss: 0.08581540096631493\n",
      "Epoch: 5 - Batch: 996, Training Loss: 0.08590701471390218\n",
      "Epoch: 5 - Batch: 997, Training Loss: 0.08598846551123543\n",
      "Epoch: 5 - Batch: 998, Training Loss: 0.08607345933764926\n",
      "Epoch: 5 - Batch: 999, Training Loss: 0.08616710500559997\n",
      "Epoch: 5 - Batch: 1000, Training Loss: 0.0862467938209351\n",
      "Epoch: 5 - Batch: 1001, Training Loss: 0.08634032365892262\n",
      "Epoch: 5 - Batch: 1002, Training Loss: 0.08642096365886937\n",
      "Epoch: 5 - Batch: 1003, Training Loss: 0.08649197123808845\n",
      "Epoch: 5 - Batch: 1004, Training Loss: 0.08658138865212699\n",
      "Epoch: 5 - Batch: 1005, Training Loss: 0.08667283168504289\n",
      "Epoch: 5 - Batch: 1006, Training Loss: 0.08676543000894003\n",
      "Epoch: 5 - Batch: 1007, Training Loss: 0.08685101548311722\n",
      "Epoch: 5 - Batch: 1008, Training Loss: 0.08694181535053214\n",
      "Epoch: 5 - Batch: 1009, Training Loss: 0.08703149492492525\n",
      "Epoch: 5 - Batch: 1010, Training Loss: 0.08711066989113837\n",
      "Epoch: 5 - Batch: 1011, Training Loss: 0.08720551206920277\n",
      "Epoch: 5 - Batch: 1012, Training Loss: 0.08730057843190125\n",
      "Epoch: 5 - Batch: 1013, Training Loss: 0.08738521955425467\n",
      "Epoch: 5 - Batch: 1014, Training Loss: 0.08747652895859818\n",
      "Epoch: 5 - Batch: 1015, Training Loss: 0.08756922367669852\n",
      "Epoch: 5 - Batch: 1016, Training Loss: 0.08765773078547189\n",
      "Epoch: 5 - Batch: 1017, Training Loss: 0.08774504798833609\n",
      "Epoch: 5 - Batch: 1018, Training Loss: 0.08782969862469789\n",
      "Epoch: 5 - Batch: 1019, Training Loss: 0.08791546571141057\n",
      "Epoch: 5 - Batch: 1020, Training Loss: 0.08800151962694244\n",
      "Epoch: 5 - Batch: 1021, Training Loss: 0.08808379584481665\n",
      "Epoch: 5 - Batch: 1022, Training Loss: 0.08816801482320424\n",
      "Epoch: 5 - Batch: 1023, Training Loss: 0.08825293067181683\n",
      "Epoch: 5 - Batch: 1024, Training Loss: 0.08833205936535278\n",
      "Epoch: 5 - Batch: 1025, Training Loss: 0.08841752530898818\n",
      "Epoch: 5 - Batch: 1026, Training Loss: 0.08850897894940567\n",
      "Epoch: 5 - Batch: 1027, Training Loss: 0.08860238813246858\n",
      "Epoch: 5 - Batch: 1028, Training Loss: 0.08869712211031028\n",
      "Epoch: 5 - Batch: 1029, Training Loss: 0.0887767035285533\n",
      "Epoch: 5 - Batch: 1030, Training Loss: 0.08886056882676793\n",
      "Epoch: 5 - Batch: 1031, Training Loss: 0.08894992984868401\n",
      "Epoch: 5 - Batch: 1032, Training Loss: 0.08903366823373347\n",
      "Epoch: 5 - Batch: 1033, Training Loss: 0.08912061723211709\n",
      "Epoch: 5 - Batch: 1034, Training Loss: 0.08920688418112387\n",
      "Epoch: 5 - Batch: 1035, Training Loss: 0.08929439993022291\n",
      "Epoch: 5 - Batch: 1036, Training Loss: 0.08937537076436662\n",
      "Epoch: 5 - Batch: 1037, Training Loss: 0.08945712296425011\n",
      "Epoch: 5 - Batch: 1038, Training Loss: 0.0895427670854933\n",
      "Epoch: 5 - Batch: 1039, Training Loss: 0.08962951077281144\n",
      "Epoch: 5 - Batch: 1040, Training Loss: 0.08971757684551661\n",
      "Epoch: 5 - Batch: 1041, Training Loss: 0.08980110715178906\n",
      "Epoch: 5 - Batch: 1042, Training Loss: 0.08988570491126915\n",
      "Epoch: 5 - Batch: 1043, Training Loss: 0.08997040768316136\n",
      "Epoch: 5 - Batch: 1044, Training Loss: 0.09006121639119057\n",
      "Epoch: 5 - Batch: 1045, Training Loss: 0.09015609545766022\n",
      "Epoch: 5 - Batch: 1046, Training Loss: 0.09024824013016117\n",
      "Epoch: 5 - Batch: 1047, Training Loss: 0.09033519547945429\n",
      "Epoch: 5 - Batch: 1048, Training Loss: 0.09043472666126579\n",
      "Epoch: 5 - Batch: 1049, Training Loss: 0.09051941786189972\n",
      "Epoch: 5 - Batch: 1050, Training Loss: 0.09061045536329695\n",
      "Epoch: 5 - Batch: 1051, Training Loss: 0.09070662660508805\n",
      "Epoch: 5 - Batch: 1052, Training Loss: 0.09079561924954156\n",
      "Epoch: 5 - Batch: 1053, Training Loss: 0.0908851188300162\n",
      "Epoch: 5 - Batch: 1054, Training Loss: 0.09096803206635352\n",
      "Epoch: 5 - Batch: 1055, Training Loss: 0.09105242613338514\n",
      "Epoch: 5 - Batch: 1056, Training Loss: 0.09114011267746859\n",
      "Epoch: 5 - Batch: 1057, Training Loss: 0.09122844928779808\n",
      "Epoch: 5 - Batch: 1058, Training Loss: 0.09131492810882937\n",
      "Epoch: 5 - Batch: 1059, Training Loss: 0.09140058286846375\n",
      "Epoch: 5 - Batch: 1060, Training Loss: 0.09150302192662683\n",
      "Epoch: 5 - Batch: 1061, Training Loss: 0.0915999226889899\n",
      "Epoch: 5 - Batch: 1062, Training Loss: 0.09168380812724827\n",
      "Epoch: 5 - Batch: 1063, Training Loss: 0.09176832529566378\n",
      "Epoch: 5 - Batch: 1064, Training Loss: 0.09185452117997023\n",
      "Epoch: 5 - Batch: 1065, Training Loss: 0.09194761628955356\n",
      "Epoch: 5 - Batch: 1066, Training Loss: 0.0920364760282522\n",
      "Epoch: 5 - Batch: 1067, Training Loss: 0.09212020138405251\n",
      "Epoch: 5 - Batch: 1068, Training Loss: 0.09220291030397067\n",
      "Epoch: 5 - Batch: 1069, Training Loss: 0.09228946650354423\n",
      "Epoch: 5 - Batch: 1070, Training Loss: 0.0923849251769965\n",
      "Epoch: 5 - Batch: 1071, Training Loss: 0.09247099369097111\n",
      "Epoch: 5 - Batch: 1072, Training Loss: 0.09255544346437525\n",
      "Epoch: 5 - Batch: 1073, Training Loss: 0.09263589726158636\n",
      "Epoch: 5 - Batch: 1074, Training Loss: 0.09271970989296884\n",
      "Epoch: 5 - Batch: 1075, Training Loss: 0.0928105962946146\n",
      "Epoch: 5 - Batch: 1076, Training Loss: 0.09289504171257984\n",
      "Epoch: 5 - Batch: 1077, Training Loss: 0.09298307423296062\n",
      "Epoch: 5 - Batch: 1078, Training Loss: 0.09307842940759303\n",
      "Epoch: 5 - Batch: 1079, Training Loss: 0.09317018839825643\n",
      "Epoch: 5 - Batch: 1080, Training Loss: 0.09325197591117365\n",
      "Epoch: 5 - Batch: 1081, Training Loss: 0.09333111346385768\n",
      "Epoch: 5 - Batch: 1082, Training Loss: 0.09341687255633213\n",
      "Epoch: 5 - Batch: 1083, Training Loss: 0.09349567527770007\n",
      "Epoch: 5 - Batch: 1084, Training Loss: 0.09358457097811486\n",
      "Epoch: 5 - Batch: 1085, Training Loss: 0.09367519026470816\n",
      "Epoch: 5 - Batch: 1086, Training Loss: 0.09376079866517441\n",
      "Epoch: 5 - Batch: 1087, Training Loss: 0.09384321805034111\n",
      "Epoch: 5 - Batch: 1088, Training Loss: 0.09393339702368375\n",
      "Epoch: 5 - Batch: 1089, Training Loss: 0.09401757865649946\n",
      "Epoch: 5 - Batch: 1090, Training Loss: 0.09410614284314524\n",
      "Epoch: 5 - Batch: 1091, Training Loss: 0.09419827511036771\n",
      "Epoch: 5 - Batch: 1092, Training Loss: 0.09428535216483311\n",
      "Epoch: 5 - Batch: 1093, Training Loss: 0.09437459206848002\n",
      "Epoch: 5 - Batch: 1094, Training Loss: 0.0944630936171186\n",
      "Epoch: 5 - Batch: 1095, Training Loss: 0.09454950556208443\n",
      "Epoch: 5 - Batch: 1096, Training Loss: 0.09462406586973031\n",
      "Epoch: 5 - Batch: 1097, Training Loss: 0.09471157964807048\n",
      "Epoch: 5 - Batch: 1098, Training Loss: 0.09480262822031382\n",
      "Epoch: 5 - Batch: 1099, Training Loss: 0.09489720327714782\n",
      "Epoch: 5 - Batch: 1100, Training Loss: 0.09497135849190787\n",
      "Epoch: 5 - Batch: 1101, Training Loss: 0.09505292191285993\n",
      "Epoch: 5 - Batch: 1102, Training Loss: 0.09514867490895747\n",
      "Epoch: 5 - Batch: 1103, Training Loss: 0.09524294260154119\n",
      "Epoch: 5 - Batch: 1104, Training Loss: 0.09533391271410495\n",
      "Epoch: 5 - Batch: 1105, Training Loss: 0.09542128635851503\n",
      "Epoch: 5 - Batch: 1106, Training Loss: 0.09550829227414495\n",
      "Epoch: 5 - Batch: 1107, Training Loss: 0.09559338563932708\n",
      "Epoch: 5 - Batch: 1108, Training Loss: 0.0956805851773836\n",
      "Epoch: 5 - Batch: 1109, Training Loss: 0.09577084580439438\n",
      "Epoch: 5 - Batch: 1110, Training Loss: 0.09585947238193025\n",
      "Epoch: 5 - Batch: 1111, Training Loss: 0.09595327656932336\n",
      "Epoch: 5 - Batch: 1112, Training Loss: 0.09604359126555584\n",
      "Epoch: 5 - Batch: 1113, Training Loss: 0.09612805937504887\n",
      "Epoch: 5 - Batch: 1114, Training Loss: 0.09621788680578149\n",
      "Epoch: 5 - Batch: 1115, Training Loss: 0.09630431773551859\n",
      "Epoch: 5 - Batch: 1116, Training Loss: 0.09639231417062469\n",
      "Epoch: 5 - Batch: 1117, Training Loss: 0.09647300389053216\n",
      "Epoch: 5 - Batch: 1118, Training Loss: 0.09655625435522144\n",
      "Epoch: 5 - Batch: 1119, Training Loss: 0.09664752844653121\n",
      "Epoch: 5 - Batch: 1120, Training Loss: 0.09673193696991325\n",
      "Epoch: 5 - Batch: 1121, Training Loss: 0.09680834169782217\n",
      "Epoch: 5 - Batch: 1122, Training Loss: 0.09689862385491035\n",
      "Epoch: 5 - Batch: 1123, Training Loss: 0.09698828632880009\n",
      "Epoch: 5 - Batch: 1124, Training Loss: 0.09707792859468888\n",
      "Epoch: 5 - Batch: 1125, Training Loss: 0.09715611497511713\n",
      "Epoch: 5 - Batch: 1126, Training Loss: 0.09724890557465268\n",
      "Epoch: 5 - Batch: 1127, Training Loss: 0.09733133460974219\n",
      "Epoch: 5 - Batch: 1128, Training Loss: 0.09742451621648882\n",
      "Epoch: 5 - Batch: 1129, Training Loss: 0.09751340796302997\n",
      "Epoch: 5 - Batch: 1130, Training Loss: 0.09759285019874375\n",
      "Epoch: 5 - Batch: 1131, Training Loss: 0.09767877711461946\n",
      "Epoch: 5 - Batch: 1132, Training Loss: 0.09775693711529719\n",
      "Epoch: 5 - Batch: 1133, Training Loss: 0.09783361690307335\n",
      "Epoch: 5 - Batch: 1134, Training Loss: 0.09791967109661197\n",
      "Epoch: 5 - Batch: 1135, Training Loss: 0.09801104848904792\n",
      "Epoch: 5 - Batch: 1136, Training Loss: 0.09810370955597703\n",
      "Epoch: 5 - Batch: 1137, Training Loss: 0.09820581805177193\n",
      "Epoch: 5 - Batch: 1138, Training Loss: 0.09828944979343644\n",
      "Epoch: 5 - Batch: 1139, Training Loss: 0.09838325192975761\n",
      "Epoch: 5 - Batch: 1140, Training Loss: 0.09846688116318353\n",
      "Epoch: 5 - Batch: 1141, Training Loss: 0.0985489993929171\n",
      "Epoch: 5 - Batch: 1142, Training Loss: 0.09864103673010521\n",
      "Epoch: 5 - Batch: 1143, Training Loss: 0.09872556503114613\n",
      "Epoch: 5 - Batch: 1144, Training Loss: 0.098813055222159\n",
      "Epoch: 5 - Batch: 1145, Training Loss: 0.09890149504094575\n",
      "Epoch: 5 - Batch: 1146, Training Loss: 0.09898698242198967\n",
      "Epoch: 5 - Batch: 1147, Training Loss: 0.09907328024827822\n",
      "Epoch: 5 - Batch: 1148, Training Loss: 0.09915586215000644\n",
      "Epoch: 5 - Batch: 1149, Training Loss: 0.09925092567703617\n",
      "Epoch: 5 - Batch: 1150, Training Loss: 0.0993391738688669\n",
      "Epoch: 5 - Batch: 1151, Training Loss: 0.09942244073595376\n",
      "Epoch: 5 - Batch: 1152, Training Loss: 0.09950866346953323\n",
      "Epoch: 5 - Batch: 1153, Training Loss: 0.09959344816692235\n",
      "Epoch: 5 - Batch: 1154, Training Loss: 0.09968296713373356\n",
      "Epoch: 5 - Batch: 1155, Training Loss: 0.09976790930649534\n",
      "Epoch: 5 - Batch: 1156, Training Loss: 0.0998573996695021\n",
      "Epoch: 5 - Batch: 1157, Training Loss: 0.09995327996179634\n",
      "Epoch: 5 - Batch: 1158, Training Loss: 0.10005228468533574\n",
      "Epoch: 5 - Batch: 1159, Training Loss: 0.1001407623253948\n",
      "Epoch: 5 - Batch: 1160, Training Loss: 0.10022346994177025\n",
      "Epoch: 5 - Batch: 1161, Training Loss: 0.10030872289517626\n",
      "Epoch: 5 - Batch: 1162, Training Loss: 0.10039787596187386\n",
      "Epoch: 5 - Batch: 1163, Training Loss: 0.10048402976831591\n",
      "Epoch: 5 - Batch: 1164, Training Loss: 0.10057365161615422\n",
      "Epoch: 5 - Batch: 1165, Training Loss: 0.10066616236175076\n",
      "Epoch: 5 - Batch: 1166, Training Loss: 0.10075092177288252\n",
      "Epoch: 5 - Batch: 1167, Training Loss: 0.10083633877497605\n",
      "Epoch: 5 - Batch: 1168, Training Loss: 0.10092143825630644\n",
      "Epoch: 5 - Batch: 1169, Training Loss: 0.10100257147727519\n",
      "Epoch: 5 - Batch: 1170, Training Loss: 0.10107770754305483\n",
      "Epoch: 5 - Batch: 1171, Training Loss: 0.1011720289588963\n",
      "Epoch: 5 - Batch: 1172, Training Loss: 0.10127320535987566\n",
      "Epoch: 5 - Batch: 1173, Training Loss: 0.10135529296562248\n",
      "Epoch: 5 - Batch: 1174, Training Loss: 0.10144415224097657\n",
      "Epoch: 5 - Batch: 1175, Training Loss: 0.10153578981683029\n",
      "Epoch: 5 - Batch: 1176, Training Loss: 0.10161721162732759\n",
      "Epoch: 5 - Batch: 1177, Training Loss: 0.10171300264239114\n",
      "Epoch: 5 - Batch: 1178, Training Loss: 0.10179725446881939\n",
      "Epoch: 5 - Batch: 1179, Training Loss: 0.10188581515258027\n",
      "Epoch: 5 - Batch: 1180, Training Loss: 0.10197154282411533\n",
      "Epoch: 5 - Batch: 1181, Training Loss: 0.10206450100611296\n",
      "Epoch: 5 - Batch: 1182, Training Loss: 0.1021527588725782\n",
      "Epoch: 5 - Batch: 1183, Training Loss: 0.10223374516389659\n",
      "Epoch: 5 - Batch: 1184, Training Loss: 0.10231604377058015\n",
      "Epoch: 5 - Batch: 1185, Training Loss: 0.1023987252172546\n",
      "Epoch: 5 - Batch: 1186, Training Loss: 0.10248129143940275\n",
      "Epoch: 5 - Batch: 1187, Training Loss: 0.10256383066400762\n",
      "Epoch: 5 - Batch: 1188, Training Loss: 0.10264430410702825\n",
      "Epoch: 5 - Batch: 1189, Training Loss: 0.10273420139776533\n",
      "Epoch: 5 - Batch: 1190, Training Loss: 0.10282725418854512\n",
      "Epoch: 5 - Batch: 1191, Training Loss: 0.10291510625166284\n",
      "Epoch: 5 - Batch: 1192, Training Loss: 0.10300093946691176\n",
      "Epoch: 5 - Batch: 1193, Training Loss: 0.10308799963710122\n",
      "Epoch: 5 - Batch: 1194, Training Loss: 0.10316399885202522\n",
      "Epoch: 5 - Batch: 1195, Training Loss: 0.10324184659553405\n",
      "Epoch: 5 - Batch: 1196, Training Loss: 0.10332749710475429\n",
      "Epoch: 5 - Batch: 1197, Training Loss: 0.10341124222374476\n",
      "Epoch: 5 - Batch: 1198, Training Loss: 0.10349428702522669\n",
      "Epoch: 5 - Batch: 1199, Training Loss: 0.10357887806312165\n",
      "Epoch: 5 - Batch: 1200, Training Loss: 0.10366037957506195\n",
      "Epoch: 5 - Batch: 1201, Training Loss: 0.10375314844064847\n",
      "Epoch: 5 - Batch: 1202, Training Loss: 0.10384666914148118\n",
      "Epoch: 5 - Batch: 1203, Training Loss: 0.10394237415979356\n",
      "Epoch: 5 - Batch: 1204, Training Loss: 0.10403156902017087\n",
      "Epoch: 5 - Batch: 1205, Training Loss: 0.10412063940148646\n",
      "Epoch: 5 - Batch: 1206, Training Loss: 0.10420722296927896\n",
      "Epoch: 5 - Batch: 1207, Training Loss: 0.10429761364166416\n",
      "Epoch: 5 - Batch: 1208, Training Loss: 0.10438536368866465\n",
      "Epoch: 5 - Batch: 1209, Training Loss: 0.10447305439667125\n",
      "Epoch: 5 - Batch: 1210, Training Loss: 0.1045537106692791\n",
      "Epoch: 5 - Batch: 1211, Training Loss: 0.10464368915651766\n",
      "Epoch: 5 - Batch: 1212, Training Loss: 0.10472916032667974\n",
      "Epoch: 5 - Batch: 1213, Training Loss: 0.1048047027147528\n",
      "Epoch: 5 - Batch: 1214, Training Loss: 0.10488678973359651\n",
      "Epoch: 5 - Batch: 1215, Training Loss: 0.10497107354315557\n",
      "Epoch: 5 - Batch: 1216, Training Loss: 0.10506539347011652\n",
      "Epoch: 5 - Batch: 1217, Training Loss: 0.10515800012185997\n",
      "Epoch: 5 - Batch: 1218, Training Loss: 0.10523496717091026\n",
      "Epoch: 5 - Batch: 1219, Training Loss: 0.10531658343167645\n",
      "Epoch: 5 - Batch: 1220, Training Loss: 0.1054068790442908\n",
      "Epoch: 5 - Batch: 1221, Training Loss: 0.10549267092920457\n",
      "Epoch: 5 - Batch: 1222, Training Loss: 0.10558108250868459\n",
      "Epoch: 5 - Batch: 1223, Training Loss: 0.10567719325620935\n",
      "Epoch: 5 - Batch: 1224, Training Loss: 0.10576811609527167\n",
      "Epoch: 5 - Batch: 1225, Training Loss: 0.10585190963685809\n",
      "Epoch: 5 - Batch: 1226, Training Loss: 0.10593705046705741\n",
      "Epoch: 5 - Batch: 1227, Training Loss: 0.10603051570863471\n",
      "Epoch: 5 - Batch: 1228, Training Loss: 0.10612473287812711\n",
      "Epoch: 5 - Batch: 1229, Training Loss: 0.10621359895537939\n",
      "Epoch: 5 - Batch: 1230, Training Loss: 0.1062990228211504\n",
      "Epoch: 5 - Batch: 1231, Training Loss: 0.10638887009538624\n",
      "Epoch: 5 - Batch: 1232, Training Loss: 0.10648405721185615\n",
      "Epoch: 5 - Batch: 1233, Training Loss: 0.10657915451361567\n",
      "Epoch: 5 - Batch: 1234, Training Loss: 0.10666440146825404\n",
      "Epoch: 5 - Batch: 1235, Training Loss: 0.10674517436159982\n",
      "Epoch: 5 - Batch: 1236, Training Loss: 0.10684157300672523\n",
      "Epoch: 5 - Batch: 1237, Training Loss: 0.10692799248258471\n",
      "Epoch: 5 - Batch: 1238, Training Loss: 0.10701393109945516\n",
      "Epoch: 5 - Batch: 1239, Training Loss: 0.10709600540412402\n",
      "Epoch: 5 - Batch: 1240, Training Loss: 0.10717931709825301\n",
      "Epoch: 5 - Batch: 1241, Training Loss: 0.1072615212011792\n",
      "Epoch: 5 - Batch: 1242, Training Loss: 0.1073445347547037\n",
      "Epoch: 5 - Batch: 1243, Training Loss: 0.10743321848424711\n",
      "Epoch: 5 - Batch: 1244, Training Loss: 0.1075107800414412\n",
      "Epoch: 5 - Batch: 1245, Training Loss: 0.10759993989150322\n",
      "Epoch: 5 - Batch: 1246, Training Loss: 0.10768916317034717\n",
      "Epoch: 5 - Batch: 1247, Training Loss: 0.10776643061494551\n",
      "Epoch: 5 - Batch: 1248, Training Loss: 0.10785166465771534\n",
      "Epoch: 5 - Batch: 1249, Training Loss: 0.10793510367275273\n",
      "Epoch: 5 - Batch: 1250, Training Loss: 0.10801363912462002\n",
      "Epoch: 5 - Batch: 1251, Training Loss: 0.10810403254000504\n",
      "Epoch: 5 - Batch: 1252, Training Loss: 0.1081868811490425\n",
      "Epoch: 5 - Batch: 1253, Training Loss: 0.10826709185098336\n",
      "Epoch: 5 - Batch: 1254, Training Loss: 0.10835194322674428\n",
      "Epoch: 5 - Batch: 1255, Training Loss: 0.1084352226288461\n",
      "Epoch: 5 - Batch: 1256, Training Loss: 0.10852786410556702\n",
      "Epoch: 5 - Batch: 1257, Training Loss: 0.10860820281416622\n",
      "Epoch: 5 - Batch: 1258, Training Loss: 0.10870031069093082\n",
      "Epoch: 5 - Batch: 1259, Training Loss: 0.10878394607757257\n",
      "Epoch: 5 - Batch: 1260, Training Loss: 0.10886960129437359\n",
      "Epoch: 5 - Batch: 1261, Training Loss: 0.10896202203992192\n",
      "Epoch: 5 - Batch: 1262, Training Loss: 0.1090460772414508\n",
      "Epoch: 5 - Batch: 1263, Training Loss: 0.10913857813325292\n",
      "Epoch: 5 - Batch: 1264, Training Loss: 0.10922486740928977\n",
      "Epoch: 5 - Batch: 1265, Training Loss: 0.10931088704967973\n",
      "Epoch: 5 - Batch: 1266, Training Loss: 0.10940333743692433\n",
      "Epoch: 5 - Batch: 1267, Training Loss: 0.1094990486702318\n",
      "Epoch: 5 - Batch: 1268, Training Loss: 0.10958395263251183\n",
      "Epoch: 5 - Batch: 1269, Training Loss: 0.10966527384781521\n",
      "Epoch: 5 - Batch: 1270, Training Loss: 0.10975299030542374\n",
      "Epoch: 5 - Batch: 1271, Training Loss: 0.10983454112958156\n",
      "Epoch: 5 - Batch: 1272, Training Loss: 0.10991665563402485\n",
      "Epoch: 5 - Batch: 1273, Training Loss: 0.11000037203183023\n",
      "Epoch: 5 - Batch: 1274, Training Loss: 0.11008395829159229\n",
      "Epoch: 5 - Batch: 1275, Training Loss: 0.11017450563004164\n",
      "Epoch: 5 - Batch: 1276, Training Loss: 0.1102734838626278\n",
      "Epoch: 5 - Batch: 1277, Training Loss: 0.11035757881912031\n",
      "Epoch: 5 - Batch: 1278, Training Loss: 0.11044556730323368\n",
      "Epoch: 5 - Batch: 1279, Training Loss: 0.11053427268641307\n",
      "Epoch: 5 - Batch: 1280, Training Loss: 0.11062581006246024\n",
      "Epoch: 5 - Batch: 1281, Training Loss: 0.11071295579694594\n",
      "Epoch: 5 - Batch: 1282, Training Loss: 0.11080591623388712\n",
      "Epoch: 5 - Batch: 1283, Training Loss: 0.11089592486941202\n",
      "Epoch: 5 - Batch: 1284, Training Loss: 0.1109771578824441\n",
      "Epoch: 5 - Batch: 1285, Training Loss: 0.11106600817509156\n",
      "Epoch: 5 - Batch: 1286, Training Loss: 0.11114989251367884\n",
      "Epoch: 5 - Batch: 1287, Training Loss: 0.11123103283308632\n",
      "Epoch: 5 - Batch: 1288, Training Loss: 0.11131318856901791\n",
      "Epoch: 5 - Batch: 1289, Training Loss: 0.11140385393059471\n",
      "Epoch: 5 - Batch: 1290, Training Loss: 0.11149112405765116\n",
      "Epoch: 5 - Batch: 1291, Training Loss: 0.11159424801321567\n",
      "Epoch: 5 - Batch: 1292, Training Loss: 0.11167788992374889\n",
      "Epoch: 5 - Batch: 1293, Training Loss: 0.11176211378269924\n",
      "Epoch: 5 - Batch: 1294, Training Loss: 0.1118410168768557\n",
      "Epoch: 5 - Batch: 1295, Training Loss: 0.11191621844102297\n",
      "Epoch: 5 - Batch: 1296, Training Loss: 0.11200093830724063\n",
      "Epoch: 5 - Batch: 1297, Training Loss: 0.11208184674431633\n",
      "Epoch: 5 - Batch: 1298, Training Loss: 0.11216808361917191\n",
      "Epoch: 5 - Batch: 1299, Training Loss: 0.11224895160317816\n",
      "Epoch: 5 - Batch: 1300, Training Loss: 0.11233125318144487\n",
      "Epoch: 5 - Batch: 1301, Training Loss: 0.11240960683543884\n",
      "Epoch: 5 - Batch: 1302, Training Loss: 0.1124951056518266\n",
      "Epoch: 5 - Batch: 1303, Training Loss: 0.11257670327996337\n",
      "Epoch: 5 - Batch: 1304, Training Loss: 0.11266620011743819\n",
      "Epoch: 5 - Batch: 1305, Training Loss: 0.11275449843697287\n",
      "Epoch: 5 - Batch: 1306, Training Loss: 0.11284272048999223\n",
      "Epoch: 5 - Batch: 1307, Training Loss: 0.112928463359229\n",
      "Epoch: 5 - Batch: 1308, Training Loss: 0.11302085202897762\n",
      "Epoch: 5 - Batch: 1309, Training Loss: 0.11310927288152685\n",
      "Epoch: 5 - Batch: 1310, Training Loss: 0.11319440765117927\n",
      "Epoch: 5 - Batch: 1311, Training Loss: 0.11327149178431205\n",
      "Epoch: 5 - Batch: 1312, Training Loss: 0.1133563086069243\n",
      "Epoch: 5 - Batch: 1313, Training Loss: 0.11343741609064699\n",
      "Epoch: 5 - Batch: 1314, Training Loss: 0.1135233924052312\n",
      "Epoch: 5 - Batch: 1315, Training Loss: 0.11361497547940828\n",
      "Epoch: 5 - Batch: 1316, Training Loss: 0.11370084528034402\n",
      "Epoch: 5 - Batch: 1317, Training Loss: 0.11378955622983611\n",
      "Epoch: 5 - Batch: 1318, Training Loss: 0.11388046194368334\n",
      "Epoch: 5 - Batch: 1319, Training Loss: 0.11396449316249756\n",
      "Epoch: 5 - Batch: 1320, Training Loss: 0.11405086133066893\n",
      "Epoch: 5 - Batch: 1321, Training Loss: 0.11413741062331952\n",
      "Epoch: 5 - Batch: 1322, Training Loss: 0.1142244521052782\n",
      "Epoch: 5 - Batch: 1323, Training Loss: 0.11431060151303586\n",
      "Epoch: 5 - Batch: 1324, Training Loss: 0.11440760493402062\n",
      "Epoch: 5 - Batch: 1325, Training Loss: 0.1144858439936369\n",
      "Epoch: 5 - Batch: 1326, Training Loss: 0.11457165018663082\n",
      "Epoch: 5 - Batch: 1327, Training Loss: 0.11465957705209505\n",
      "Epoch: 5 - Batch: 1328, Training Loss: 0.11474306869689703\n",
      "Epoch: 5 - Batch: 1329, Training Loss: 0.11483135405513975\n",
      "Epoch: 5 - Batch: 1330, Training Loss: 0.11491592800488717\n",
      "Epoch: 5 - Batch: 1331, Training Loss: 0.11499754469810829\n",
      "Epoch: 5 - Batch: 1332, Training Loss: 0.11508624954998592\n",
      "Epoch: 5 - Batch: 1333, Training Loss: 0.11516886286029768\n",
      "Epoch: 5 - Batch: 1334, Training Loss: 0.1152629334769933\n",
      "Epoch: 5 - Batch: 1335, Training Loss: 0.11534610040992449\n",
      "Epoch: 5 - Batch: 1336, Training Loss: 0.11542905576119375\n",
      "Epoch: 5 - Batch: 1337, Training Loss: 0.11551996705370954\n",
      "Epoch: 5 - Batch: 1338, Training Loss: 0.11561243249038558\n",
      "Epoch: 5 - Batch: 1339, Training Loss: 0.11569467748501408\n",
      "Epoch: 5 - Batch: 1340, Training Loss: 0.1157739652540059\n",
      "Epoch: 5 - Batch: 1341, Training Loss: 0.11585679325965506\n",
      "Epoch: 5 - Batch: 1342, Training Loss: 0.1159413299564995\n",
      "Epoch: 5 - Batch: 1343, Training Loss: 0.11603762206326472\n",
      "Epoch: 5 - Batch: 1344, Training Loss: 0.11611952107541794\n",
      "Epoch: 5 - Batch: 1345, Training Loss: 0.1162064251046671\n",
      "Epoch: 5 - Batch: 1346, Training Loss: 0.11628398715609539\n",
      "Epoch: 5 - Batch: 1347, Training Loss: 0.11637556481840797\n",
      "Epoch: 5 - Batch: 1348, Training Loss: 0.11646753602088189\n",
      "Epoch: 5 - Batch: 1349, Training Loss: 0.11655972841169505\n",
      "Epoch: 5 - Batch: 1350, Training Loss: 0.11664620173437085\n",
      "Epoch: 5 - Batch: 1351, Training Loss: 0.1167274608615026\n",
      "Epoch: 5 - Batch: 1352, Training Loss: 0.11680928138926452\n",
      "Epoch: 5 - Batch: 1353, Training Loss: 0.11689204238293381\n",
      "Epoch: 5 - Batch: 1354, Training Loss: 0.11697088434031946\n",
      "Epoch: 5 - Batch: 1355, Training Loss: 0.117064087267094\n",
      "Epoch: 5 - Batch: 1356, Training Loss: 0.11715805460119721\n",
      "Epoch: 5 - Batch: 1357, Training Loss: 0.11725149108452187\n",
      "Epoch: 5 - Batch: 1358, Training Loss: 0.11734035774249936\n",
      "Epoch: 5 - Batch: 1359, Training Loss: 0.1174205661277174\n",
      "Epoch: 5 - Batch: 1360, Training Loss: 0.1175065182248257\n",
      "Epoch: 5 - Batch: 1361, Training Loss: 0.1175989283258049\n",
      "Epoch: 5 - Batch: 1362, Training Loss: 0.11768342570014063\n",
      "Epoch: 5 - Batch: 1363, Training Loss: 0.1177696052994301\n",
      "Epoch: 5 - Batch: 1364, Training Loss: 0.11785688715837092\n",
      "Epoch: 5 - Batch: 1365, Training Loss: 0.11793022921577614\n",
      "Epoch: 5 - Batch: 1366, Training Loss: 0.11802186514830115\n",
      "Epoch: 5 - Batch: 1367, Training Loss: 0.11810478046660004\n",
      "Epoch: 5 - Batch: 1368, Training Loss: 0.11818848933968971\n",
      "Epoch: 5 - Batch: 1369, Training Loss: 0.11827907348005333\n",
      "Epoch: 5 - Batch: 1370, Training Loss: 0.11836161662764218\n",
      "Epoch: 5 - Batch: 1371, Training Loss: 0.11845134039275089\n",
      "Epoch: 5 - Batch: 1372, Training Loss: 0.11853496456027623\n",
      "Epoch: 5 - Batch: 1373, Training Loss: 0.11862200095780058\n",
      "Epoch: 5 - Batch: 1374, Training Loss: 0.11871436319837522\n",
      "Epoch: 5 - Batch: 1375, Training Loss: 0.11880171596139028\n",
      "Epoch: 5 - Batch: 1376, Training Loss: 0.11888454885378011\n",
      "Epoch: 5 - Batch: 1377, Training Loss: 0.11897393197414294\n",
      "Epoch: 5 - Batch: 1378, Training Loss: 0.1190663183209908\n",
      "Epoch: 5 - Batch: 1379, Training Loss: 0.11914646385815211\n",
      "Epoch: 5 - Batch: 1380, Training Loss: 0.11923045573592383\n",
      "Epoch: 5 - Batch: 1381, Training Loss: 0.11931784349072039\n",
      "Epoch: 5 - Batch: 1382, Training Loss: 0.11940683870519177\n",
      "Epoch: 5 - Batch: 1383, Training Loss: 0.11949450299020233\n",
      "Epoch: 5 - Batch: 1384, Training Loss: 0.11956802769803487\n",
      "Epoch: 5 - Batch: 1385, Training Loss: 0.11965357932334714\n",
      "Epoch: 5 - Batch: 1386, Training Loss: 0.11973780927373402\n",
      "Epoch: 5 - Batch: 1387, Training Loss: 0.11982808749548238\n",
      "Epoch: 5 - Batch: 1388, Training Loss: 0.11990945571863632\n",
      "Epoch: 5 - Batch: 1389, Training Loss: 0.11999090532980748\n",
      "Epoch: 5 - Batch: 1390, Training Loss: 0.12008383553197134\n",
      "Epoch: 5 - Batch: 1391, Training Loss: 0.1201773621018352\n",
      "Epoch: 5 - Batch: 1392, Training Loss: 0.12026397846824494\n",
      "Epoch: 5 - Batch: 1393, Training Loss: 0.12035031771135962\n",
      "Epoch: 5 - Batch: 1394, Training Loss: 0.12044403765035507\n",
      "Epoch: 5 - Batch: 1395, Training Loss: 0.12052824563820959\n",
      "Epoch: 5 - Batch: 1396, Training Loss: 0.12061740744667464\n",
      "Epoch: 5 - Batch: 1397, Training Loss: 0.12069964015340529\n",
      "Epoch: 5 - Batch: 1398, Training Loss: 0.12078709530939115\n",
      "Epoch: 5 - Batch: 1399, Training Loss: 0.12087786952605097\n",
      "Epoch: 5 - Batch: 1400, Training Loss: 0.12096324054582992\n",
      "Epoch: 5 - Batch: 1401, Training Loss: 0.12104647838283535\n",
      "Epoch: 5 - Batch: 1402, Training Loss: 0.12113156055979073\n",
      "Epoch: 5 - Batch: 1403, Training Loss: 0.12121412940755807\n",
      "Epoch: 5 - Batch: 1404, Training Loss: 0.12130167250634229\n",
      "Epoch: 5 - Batch: 1405, Training Loss: 0.1213885594906894\n",
      "Epoch: 5 - Batch: 1406, Training Loss: 0.12147308548736335\n",
      "Epoch: 5 - Batch: 1407, Training Loss: 0.1215509348742009\n",
      "Epoch: 5 - Batch: 1408, Training Loss: 0.12162813997115464\n",
      "Epoch: 5 - Batch: 1409, Training Loss: 0.12171509465940951\n",
      "Epoch: 5 - Batch: 1410, Training Loss: 0.12180530687618019\n",
      "Epoch: 5 - Batch: 1411, Training Loss: 0.12189100094596743\n",
      "Epoch: 5 - Batch: 1412, Training Loss: 0.12198509856020633\n",
      "Epoch: 5 - Batch: 1413, Training Loss: 0.12205954537932355\n",
      "Epoch: 5 - Batch: 1414, Training Loss: 0.12213826776909986\n",
      "Epoch: 5 - Batch: 1415, Training Loss: 0.12222266981183592\n",
      "Epoch: 5 - Batch: 1416, Training Loss: 0.12231763902662406\n",
      "Epoch: 5 - Batch: 1417, Training Loss: 0.12240320405186113\n",
      "Epoch: 5 - Batch: 1418, Training Loss: 0.12248843878655885\n",
      "Epoch: 5 - Batch: 1419, Training Loss: 0.12257775747113758\n",
      "Epoch: 5 - Batch: 1420, Training Loss: 0.12266208943146378\n",
      "Epoch: 5 - Batch: 1421, Training Loss: 0.12274745388410578\n",
      "Epoch: 5 - Batch: 1422, Training Loss: 0.1228334278881451\n",
      "Epoch: 5 - Batch: 1423, Training Loss: 0.12291810902493511\n",
      "Epoch: 5 - Batch: 1424, Training Loss: 0.12300164608368233\n",
      "Epoch: 5 - Batch: 1425, Training Loss: 0.12308814784953645\n",
      "Epoch: 5 - Batch: 1426, Training Loss: 0.12316888311312567\n",
      "Epoch: 5 - Batch: 1427, Training Loss: 0.12325765339543372\n",
      "Epoch: 5 - Batch: 1428, Training Loss: 0.12333880602127284\n",
      "Epoch: 5 - Batch: 1429, Training Loss: 0.123424245560446\n",
      "Epoch: 5 - Batch: 1430, Training Loss: 0.12351252912708974\n",
      "Epoch: 5 - Batch: 1431, Training Loss: 0.12360148359195114\n",
      "Epoch: 5 - Batch: 1432, Training Loss: 0.12369361327062199\n",
      "Epoch: 5 - Batch: 1433, Training Loss: 0.1237846925842327\n",
      "Epoch: 5 - Batch: 1434, Training Loss: 0.1238694459657072\n",
      "Epoch: 5 - Batch: 1435, Training Loss: 0.1239630967489819\n",
      "Epoch: 5 - Batch: 1436, Training Loss: 0.12404099756088818\n",
      "Epoch: 5 - Batch: 1437, Training Loss: 0.12413931512590466\n",
      "Epoch: 5 - Batch: 1438, Training Loss: 0.12422845500890493\n",
      "Epoch: 5 - Batch: 1439, Training Loss: 0.12431921194590147\n",
      "Epoch: 5 - Batch: 1440, Training Loss: 0.12440600491454748\n",
      "Epoch: 5 - Batch: 1441, Training Loss: 0.12449070415266512\n",
      "Epoch: 5 - Batch: 1442, Training Loss: 0.12457179174346117\n",
      "Epoch: 5 - Batch: 1443, Training Loss: 0.12465573849765024\n",
      "Epoch: 5 - Batch: 1444, Training Loss: 0.12474021816604568\n",
      "Epoch: 5 - Batch: 1445, Training Loss: 0.1248320681699374\n",
      "Epoch: 5 - Batch: 1446, Training Loss: 0.12492346749401606\n",
      "Epoch: 5 - Batch: 1447, Training Loss: 0.125010488069868\n",
      "Epoch: 5 - Batch: 1448, Training Loss: 0.12509901410163338\n",
      "Epoch: 5 - Batch: 1449, Training Loss: 0.12519903104770835\n",
      "Epoch: 5 - Batch: 1450, Training Loss: 0.12527701984565848\n",
      "Epoch: 5 - Batch: 1451, Training Loss: 0.12535908666267917\n",
      "Epoch: 5 - Batch: 1452, Training Loss: 0.12545004352976633\n",
      "Epoch: 5 - Batch: 1453, Training Loss: 0.12553554901831582\n",
      "Epoch: 5 - Batch: 1454, Training Loss: 0.1256358556721242\n",
      "Epoch: 5 - Batch: 1455, Training Loss: 0.12572366747986619\n",
      "Epoch: 5 - Batch: 1456, Training Loss: 0.12581678432339855\n",
      "Epoch: 5 - Batch: 1457, Training Loss: 0.12588981600790278\n",
      "Epoch: 5 - Batch: 1458, Training Loss: 0.12597463785317012\n",
      "Epoch: 5 - Batch: 1459, Training Loss: 0.12605595905611763\n",
      "Epoch: 5 - Batch: 1460, Training Loss: 0.12614791602464656\n",
      "Epoch: 5 - Batch: 1461, Training Loss: 0.12623469265885218\n",
      "Epoch: 5 - Batch: 1462, Training Loss: 0.1263232987998038\n",
      "Epoch: 5 - Batch: 1463, Training Loss: 0.12640691121073308\n",
      "Epoch: 5 - Batch: 1464, Training Loss: 0.1264959512090011\n",
      "Epoch: 5 - Batch: 1465, Training Loss: 0.12659187814464815\n",
      "Epoch: 5 - Batch: 1466, Training Loss: 0.12667312812844714\n",
      "Epoch: 5 - Batch: 1467, Training Loss: 0.12676356215752774\n",
      "Epoch: 5 - Batch: 1468, Training Loss: 0.12684458448469738\n",
      "Epoch: 5 - Batch: 1469, Training Loss: 0.12693407715824903\n",
      "Epoch: 5 - Batch: 1470, Training Loss: 0.12701734522385383\n",
      "Epoch: 5 - Batch: 1471, Training Loss: 0.12710614349217358\n",
      "Epoch: 5 - Batch: 1472, Training Loss: 0.12718961162996134\n",
      "Epoch: 5 - Batch: 1473, Training Loss: 0.12727467553878502\n",
      "Epoch: 5 - Batch: 1474, Training Loss: 0.12737049254438967\n",
      "Epoch: 5 - Batch: 1475, Training Loss: 0.1274525634511984\n",
      "Epoch: 5 - Batch: 1476, Training Loss: 0.1275398892043143\n",
      "Epoch: 5 - Batch: 1477, Training Loss: 0.12762425347520145\n",
      "Epoch: 5 - Batch: 1478, Training Loss: 0.12771110174247677\n",
      "Epoch: 5 - Batch: 1479, Training Loss: 0.12779463136917718\n",
      "Epoch: 5 - Batch: 1480, Training Loss: 0.12788255914897467\n",
      "Epoch: 5 - Batch: 1481, Training Loss: 0.12796779363037739\n",
      "Epoch: 5 - Batch: 1482, Training Loss: 0.12805411130016914\n",
      "Epoch: 5 - Batch: 1483, Training Loss: 0.12813351101320775\n",
      "Epoch: 5 - Batch: 1484, Training Loss: 0.12822369350179116\n",
      "Epoch: 5 - Batch: 1485, Training Loss: 0.1283123279747184\n",
      "Epoch: 5 - Batch: 1486, Training Loss: 0.12840995385896903\n",
      "Epoch: 5 - Batch: 1487, Training Loss: 0.12849707014945214\n",
      "Epoch: 5 - Batch: 1488, Training Loss: 0.12858765563042604\n",
      "Epoch: 5 - Batch: 1489, Training Loss: 0.1286838009867008\n",
      "Epoch: 5 - Batch: 1490, Training Loss: 0.128762854982915\n",
      "Epoch: 5 - Batch: 1491, Training Loss: 0.1288504995850485\n",
      "Epoch: 5 - Batch: 1492, Training Loss: 0.12893212708346483\n",
      "Epoch: 5 - Batch: 1493, Training Loss: 0.12901757547560813\n",
      "Epoch: 5 - Batch: 1494, Training Loss: 0.12910277376053345\n",
      "Epoch: 5 - Batch: 1495, Training Loss: 0.12918991062351523\n",
      "Epoch: 5 - Batch: 1496, Training Loss: 0.12926735724654562\n",
      "Epoch: 5 - Batch: 1497, Training Loss: 0.1293496219055174\n",
      "Epoch: 5 - Batch: 1498, Training Loss: 0.1294352169431264\n",
      "Epoch: 5 - Batch: 1499, Training Loss: 0.12951786039554658\n",
      "Epoch: 5 - Batch: 1500, Training Loss: 0.12960322567603086\n",
      "Epoch: 5 - Batch: 1501, Training Loss: 0.12968603462895154\n",
      "Epoch: 5 - Batch: 1502, Training Loss: 0.12977346345461027\n",
      "Epoch: 5 - Batch: 1503, Training Loss: 0.12986371382626136\n",
      "Epoch: 5 - Batch: 1504, Training Loss: 0.1299463269079898\n",
      "Epoch: 5 - Batch: 1505, Training Loss: 0.13003523178880488\n",
      "Epoch: 5 - Batch: 1506, Training Loss: 0.13012175304602033\n",
      "Epoch: 5 - Batch: 1507, Training Loss: 0.1302100852761992\n",
      "Epoch: 5 - Batch: 1508, Training Loss: 0.13030087818108982\n",
      "Epoch: 5 - Batch: 1509, Training Loss: 0.13038659872947442\n",
      "Epoch: 5 - Batch: 1510, Training Loss: 0.13047526805826878\n",
      "Epoch: 5 - Batch: 1511, Training Loss: 0.13055658641936965\n",
      "Epoch: 5 - Batch: 1512, Training Loss: 0.1306440508175351\n",
      "Epoch: 5 - Batch: 1513, Training Loss: 0.13072823737415903\n",
      "Epoch: 5 - Batch: 1514, Training Loss: 0.13082208949609184\n",
      "Epoch: 5 - Batch: 1515, Training Loss: 0.13090730586998894\n",
      "Epoch: 5 - Batch: 1516, Training Loss: 0.13099391410624606\n",
      "Epoch: 5 - Batch: 1517, Training Loss: 0.13108967467648275\n",
      "Epoch: 5 - Batch: 1518, Training Loss: 0.13118165012319288\n",
      "Epoch: 5 - Batch: 1519, Training Loss: 0.13126343117399794\n",
      "Epoch: 5 - Batch: 1520, Training Loss: 0.1313588972733191\n",
      "Epoch: 5 - Batch: 1521, Training Loss: 0.13144172705821136\n",
      "Epoch: 5 - Batch: 1522, Training Loss: 0.13153032863985248\n",
      "Epoch: 5 - Batch: 1523, Training Loss: 0.13161571926219545\n",
      "Epoch: 5 - Batch: 1524, Training Loss: 0.1317092484329668\n",
      "Epoch: 5 - Batch: 1525, Training Loss: 0.1317949592758273\n",
      "Epoch: 5 - Batch: 1526, Training Loss: 0.1318812504052424\n",
      "Epoch: 5 - Batch: 1527, Training Loss: 0.13197209892383657\n",
      "Epoch: 5 - Batch: 1528, Training Loss: 0.13205645505221528\n",
      "Epoch: 5 - Batch: 1529, Training Loss: 0.13215095127869403\n",
      "Epoch: 5 - Batch: 1530, Training Loss: 0.13223144571431242\n",
      "Epoch: 5 - Batch: 1531, Training Loss: 0.1323109520027788\n",
      "Epoch: 5 - Batch: 1532, Training Loss: 0.1323979925951159\n",
      "Epoch: 5 - Batch: 1533, Training Loss: 0.1324823896399086\n",
      "Epoch: 5 - Batch: 1534, Training Loss: 0.13257072454513008\n",
      "Epoch: 5 - Batch: 1535, Training Loss: 0.1326566266133813\n",
      "Epoch: 5 - Batch: 1536, Training Loss: 0.1327414127440496\n",
      "Epoch: 5 - Batch: 1537, Training Loss: 0.13282678391209882\n",
      "Epoch: 5 - Batch: 1538, Training Loss: 0.13291033425956816\n",
      "Epoch: 5 - Batch: 1539, Training Loss: 0.132998101326759\n",
      "Epoch: 5 - Batch: 1540, Training Loss: 0.13309145209751713\n",
      "Epoch: 5 - Batch: 1541, Training Loss: 0.13317423084037222\n",
      "Epoch: 5 - Batch: 1542, Training Loss: 0.13326295274950775\n",
      "Epoch: 5 - Batch: 1543, Training Loss: 0.13335585533633557\n",
      "Epoch: 5 - Batch: 1544, Training Loss: 0.13343713790870226\n",
      "Epoch: 5 - Batch: 1545, Training Loss: 0.13352386800457985\n",
      "Epoch: 5 - Batch: 1546, Training Loss: 0.13360793605002005\n",
      "Epoch: 5 - Batch: 1547, Training Loss: 0.13370042069387278\n",
      "Epoch: 5 - Batch: 1548, Training Loss: 0.13379385553386872\n",
      "Epoch: 5 - Batch: 1549, Training Loss: 0.13388138901361976\n",
      "Epoch: 5 - Batch: 1550, Training Loss: 0.13398151442556833\n",
      "Epoch: 5 - Batch: 1551, Training Loss: 0.13406429699873845\n",
      "Epoch: 5 - Batch: 1552, Training Loss: 0.1341461346826051\n",
      "Epoch: 5 - Batch: 1553, Training Loss: 0.13421869091998484\n",
      "Epoch: 5 - Batch: 1554, Training Loss: 0.13430628596278368\n",
      "Epoch: 5 - Batch: 1555, Training Loss: 0.13439516773197188\n",
      "Epoch: 5 - Batch: 1556, Training Loss: 0.13448308127782435\n",
      "Epoch: 5 - Batch: 1557, Training Loss: 0.13456962509683115\n",
      "Epoch: 5 - Batch: 1558, Training Loss: 0.13465781495667017\n",
      "Epoch: 5 - Batch: 1559, Training Loss: 0.1347453512844458\n",
      "Epoch: 5 - Batch: 1560, Training Loss: 0.13483474949130766\n",
      "Epoch: 5 - Batch: 1561, Training Loss: 0.13492430458663907\n",
      "Epoch: 5 - Batch: 1562, Training Loss: 0.13501302193646408\n",
      "Epoch: 5 - Batch: 1563, Training Loss: 0.13510169246377637\n",
      "Epoch: 5 - Batch: 1564, Training Loss: 0.13517794459069743\n",
      "Epoch: 5 - Batch: 1565, Training Loss: 0.13524948853758437\n",
      "Epoch: 5 - Batch: 1566, Training Loss: 0.1353419868964362\n",
      "Epoch: 5 - Batch: 1567, Training Loss: 0.13543484950589502\n",
      "Epoch: 5 - Batch: 1568, Training Loss: 0.13551862431600517\n",
      "Epoch: 5 - Batch: 1569, Training Loss: 0.13560412318181636\n",
      "Epoch: 5 - Batch: 1570, Training Loss: 0.13569542420271222\n",
      "Epoch: 5 - Batch: 1571, Training Loss: 0.13578246535106284\n",
      "Epoch: 5 - Batch: 1572, Training Loss: 0.13587253513571437\n",
      "Epoch: 5 - Batch: 1573, Training Loss: 0.13595888010576787\n",
      "Epoch: 5 - Batch: 1574, Training Loss: 0.1360434864896348\n",
      "Epoch: 5 - Batch: 1575, Training Loss: 0.1361302799030916\n",
      "Epoch: 5 - Batch: 1576, Training Loss: 0.13621603443007763\n",
      "Epoch: 5 - Batch: 1577, Training Loss: 0.13630753181022198\n",
      "Epoch: 5 - Batch: 1578, Training Loss: 0.13639580288509626\n",
      "Epoch: 5 - Batch: 1579, Training Loss: 0.13648670949474298\n",
      "Epoch: 5 - Batch: 1580, Training Loss: 0.13656633655551456\n",
      "Epoch: 5 - Batch: 1581, Training Loss: 0.13666290482181814\n",
      "Epoch: 5 - Batch: 1582, Training Loss: 0.13674417024425803\n",
      "Epoch: 5 - Batch: 1583, Training Loss: 0.13682762500683268\n",
      "Epoch: 5 - Batch: 1584, Training Loss: 0.1369135736690133\n",
      "Epoch: 5 - Batch: 1585, Training Loss: 0.1370067807226434\n",
      "Epoch: 5 - Batch: 1586, Training Loss: 0.1370970679270984\n",
      "Epoch: 5 - Batch: 1587, Training Loss: 0.13719405869568757\n",
      "Epoch: 5 - Batch: 1588, Training Loss: 0.13727995921327898\n",
      "Epoch: 5 - Batch: 1589, Training Loss: 0.1373710416220314\n",
      "Epoch: 5 - Batch: 1590, Training Loss: 0.13744860537238976\n",
      "Epoch: 5 - Batch: 1591, Training Loss: 0.1375345435876949\n",
      "Epoch: 5 - Batch: 1592, Training Loss: 0.1376125787262794\n",
      "Epoch: 5 - Batch: 1593, Training Loss: 0.1377128029912463\n",
      "Epoch: 5 - Batch: 1594, Training Loss: 0.13779747311279153\n",
      "Epoch: 5 - Batch: 1595, Training Loss: 0.13788866747176864\n",
      "Epoch: 5 - Batch: 1596, Training Loss: 0.13797437724584766\n",
      "Epoch: 5 - Batch: 1597, Training Loss: 0.13806965998427984\n",
      "Epoch: 5 - Batch: 1598, Training Loss: 0.13815594918618154\n",
      "Epoch: 5 - Batch: 1599, Training Loss: 0.13823737593284294\n",
      "Epoch: 5 - Batch: 1600, Training Loss: 0.1383232051139052\n",
      "Epoch: 5 - Batch: 1601, Training Loss: 0.13841825540163624\n",
      "Epoch: 5 - Batch: 1602, Training Loss: 0.13851010075354853\n",
      "Epoch: 5 - Batch: 1603, Training Loss: 0.13860575282084409\n",
      "Epoch: 5 - Batch: 1604, Training Loss: 0.13869420282703332\n",
      "Epoch: 5 - Batch: 1605, Training Loss: 0.13878137747421984\n",
      "Epoch: 5 - Batch: 1606, Training Loss: 0.13887000076509826\n",
      "Epoch: 5 - Batch: 1607, Training Loss: 0.13895922109089284\n",
      "Epoch: 5 - Batch: 1608, Training Loss: 0.1390424590699906\n",
      "Epoch: 5 - Batch: 1609, Training Loss: 0.13911860001274998\n",
      "Epoch: 5 - Batch: 1610, Training Loss: 0.13920925695381156\n",
      "Epoch: 5 - Batch: 1611, Training Loss: 0.13929902638616057\n",
      "Epoch: 5 - Batch: 1612, Training Loss: 0.13938024240989194\n",
      "Epoch: 5 - Batch: 1613, Training Loss: 0.1394654136849181\n",
      "Epoch: 5 - Batch: 1614, Training Loss: 0.1395511701332396\n",
      "Epoch: 5 - Batch: 1615, Training Loss: 0.13964201815142166\n",
      "Epoch: 5 - Batch: 1616, Training Loss: 0.13972508133841588\n",
      "Epoch: 5 - Batch: 1617, Training Loss: 0.13980937798570836\n",
      "Epoch: 5 - Batch: 1618, Training Loss: 0.13989241183372478\n",
      "Epoch: 5 - Batch: 1619, Training Loss: 0.13997920580937298\n",
      "Epoch: 5 - Batch: 1620, Training Loss: 0.14006094972465563\n",
      "Epoch: 5 - Batch: 1621, Training Loss: 0.14014936277448242\n",
      "Epoch: 5 - Batch: 1622, Training Loss: 0.14023336001469525\n",
      "Epoch: 5 - Batch: 1623, Training Loss: 0.1403187580134837\n",
      "Epoch: 5 - Batch: 1624, Training Loss: 0.14040732975573483\n",
      "Epoch: 5 - Batch: 1625, Training Loss: 0.14049768250800088\n",
      "Epoch: 5 - Batch: 1626, Training Loss: 0.14059113934759673\n",
      "Epoch: 5 - Batch: 1627, Training Loss: 0.14067287963025804\n",
      "Epoch: 5 - Batch: 1628, Training Loss: 0.14075245241175838\n",
      "Epoch: 5 - Batch: 1629, Training Loss: 0.14083306871935306\n",
      "Epoch: 5 - Batch: 1630, Training Loss: 0.1409256250951223\n",
      "Epoch: 5 - Batch: 1631, Training Loss: 0.14101505435842582\n",
      "Epoch: 5 - Batch: 1632, Training Loss: 0.14110384772764906\n",
      "Epoch: 5 - Batch: 1633, Training Loss: 0.14118377160077072\n",
      "Epoch: 5 - Batch: 1634, Training Loss: 0.14127764959586397\n",
      "Epoch: 5 - Batch: 1635, Training Loss: 0.14136921560660523\n",
      "Epoch: 5 - Batch: 1636, Training Loss: 0.141452676497684\n",
      "Epoch: 5 - Batch: 1637, Training Loss: 0.14153671383882438\n",
      "Epoch: 5 - Batch: 1638, Training Loss: 0.14162302821578077\n",
      "Epoch: 5 - Batch: 1639, Training Loss: 0.14171646918428082\n",
      "Epoch: 5 - Batch: 1640, Training Loss: 0.14180500387750058\n",
      "Epoch: 5 - Batch: 1641, Training Loss: 0.14189527660089346\n",
      "Epoch: 5 - Batch: 1642, Training Loss: 0.14198362380636864\n",
      "Epoch: 5 - Batch: 1643, Training Loss: 0.14206607482252429\n",
      "Epoch: 5 - Batch: 1644, Training Loss: 0.14214863649153986\n",
      "Epoch: 5 - Batch: 1645, Training Loss: 0.14224894979279828\n",
      "Epoch: 5 - Batch: 1646, Training Loss: 0.1423295334315122\n",
      "Epoch: 5 - Batch: 1647, Training Loss: 0.14240808067086522\n",
      "Epoch: 5 - Batch: 1648, Training Loss: 0.14249211048728988\n",
      "Epoch: 5 - Batch: 1649, Training Loss: 0.1425854778334276\n",
      "Epoch: 5 - Batch: 1650, Training Loss: 0.14266654625411454\n",
      "Epoch: 5 - Batch: 1651, Training Loss: 0.14274434296794197\n",
      "Epoch: 5 - Batch: 1652, Training Loss: 0.14282493469356305\n",
      "Epoch: 5 - Batch: 1653, Training Loss: 0.14291271330112248\n",
      "Epoch: 5 - Batch: 1654, Training Loss: 0.1430099603610351\n",
      "Epoch: 5 - Batch: 1655, Training Loss: 0.1430964678908956\n",
      "Epoch: 5 - Batch: 1656, Training Loss: 0.1431802261379821\n",
      "Epoch: 5 - Batch: 1657, Training Loss: 0.1432635566253666\n",
      "Epoch: 5 - Batch: 1658, Training Loss: 0.14335662786343797\n",
      "Epoch: 5 - Batch: 1659, Training Loss: 0.1434439435650658\n",
      "Epoch: 5 - Batch: 1660, Training Loss: 0.14354004308729623\n",
      "Epoch: 5 - Batch: 1661, Training Loss: 0.14362134721445208\n",
      "Epoch: 5 - Batch: 1662, Training Loss: 0.14369855050127306\n",
      "Epoch: 5 - Batch: 1663, Training Loss: 0.14378088930773103\n",
      "Epoch: 5 - Batch: 1664, Training Loss: 0.14386883814190554\n",
      "Epoch: 5 - Batch: 1665, Training Loss: 0.14395303400248832\n",
      "Epoch: 5 - Batch: 1666, Training Loss: 0.14404364242828505\n",
      "Epoch: 5 - Batch: 1667, Training Loss: 0.14413412419421162\n",
      "Epoch: 5 - Batch: 1668, Training Loss: 0.1442250753467158\n",
      "Epoch: 5 - Batch: 1669, Training Loss: 0.14431243965009946\n",
      "Epoch: 5 - Batch: 1670, Training Loss: 0.14440245507516672\n",
      "Epoch: 5 - Batch: 1671, Training Loss: 0.14449677606473119\n",
      "Epoch: 5 - Batch: 1672, Training Loss: 0.14457910629004783\n",
      "Epoch: 5 - Batch: 1673, Training Loss: 0.14466951781293844\n",
      "Epoch: 5 - Batch: 1674, Training Loss: 0.1447441039999227\n",
      "Epoch: 5 - Batch: 1675, Training Loss: 0.14482643303289935\n",
      "Epoch: 5 - Batch: 1676, Training Loss: 0.14491163745868463\n",
      "Epoch: 5 - Batch: 1677, Training Loss: 0.14499852472721642\n",
      "Epoch: 5 - Batch: 1678, Training Loss: 0.14508515321471996\n",
      "Epoch: 5 - Batch: 1679, Training Loss: 0.14518617406166212\n",
      "Epoch: 5 - Batch: 1680, Training Loss: 0.14526871520142454\n",
      "Epoch: 5 - Batch: 1681, Training Loss: 0.14535668421888825\n",
      "Epoch: 5 - Batch: 1682, Training Loss: 0.14544415308987324\n",
      "Epoch: 5 - Batch: 1683, Training Loss: 0.14552988580877507\n",
      "Epoch: 5 - Batch: 1684, Training Loss: 0.14561438454843278\n",
      "Epoch: 5 - Batch: 1685, Training Loss: 0.14569591275519794\n",
      "Epoch: 5 - Batch: 1686, Training Loss: 0.14577805323164855\n",
      "Epoch: 5 - Batch: 1687, Training Loss: 0.14586597316547809\n",
      "Epoch: 5 - Batch: 1688, Training Loss: 0.1459459822692879\n",
      "Epoch: 5 - Batch: 1689, Training Loss: 0.14603233681512906\n",
      "Epoch: 5 - Batch: 1690, Training Loss: 0.146120150161175\n",
      "Epoch: 5 - Batch: 1691, Training Loss: 0.14620378525127622\n",
      "Epoch: 5 - Batch: 1692, Training Loss: 0.14628979168325712\n",
      "Epoch: 5 - Batch: 1693, Training Loss: 0.146377007642245\n",
      "Epoch: 5 - Batch: 1694, Training Loss: 0.14647530482010065\n",
      "Epoch: 5 - Batch: 1695, Training Loss: 0.146558539679345\n",
      "Epoch: 5 - Batch: 1696, Training Loss: 0.14664217650544387\n",
      "Epoch: 5 - Batch: 1697, Training Loss: 0.1467355509685541\n",
      "Epoch: 5 - Batch: 1698, Training Loss: 0.14682883577140213\n",
      "Epoch: 5 - Batch: 1699, Training Loss: 0.1469144400079452\n",
      "Epoch: 5 - Batch: 1700, Training Loss: 0.14700318515844407\n",
      "Epoch: 5 - Batch: 1701, Training Loss: 0.1470832153351647\n",
      "Epoch: 5 - Batch: 1702, Training Loss: 0.14716728034106455\n",
      "Epoch: 5 - Batch: 1703, Training Loss: 0.14725439851581557\n",
      "Epoch: 5 - Batch: 1704, Training Loss: 0.14734155440063618\n",
      "Epoch: 5 - Batch: 1705, Training Loss: 0.14741952898935298\n",
      "Epoch: 5 - Batch: 1706, Training Loss: 0.1475017477092557\n",
      "Epoch: 5 - Batch: 1707, Training Loss: 0.14759408714092193\n",
      "Epoch: 5 - Batch: 1708, Training Loss: 0.14767786740614208\n",
      "Epoch: 5 - Batch: 1709, Training Loss: 0.14776079804570125\n",
      "Epoch: 5 - Batch: 1710, Training Loss: 0.147854545661812\n",
      "Epoch: 5 - Batch: 1711, Training Loss: 0.1479292528682186\n",
      "Epoch: 5 - Batch: 1712, Training Loss: 0.1480169039633539\n",
      "Epoch: 5 - Batch: 1713, Training Loss: 0.14809666478861228\n",
      "Epoch: 5 - Batch: 1714, Training Loss: 0.14818007895602517\n",
      "Epoch: 5 - Batch: 1715, Training Loss: 0.14826299110504723\n",
      "Epoch: 5 - Batch: 1716, Training Loss: 0.14835390861873604\n",
      "Epoch: 5 - Batch: 1717, Training Loss: 0.14843333259867397\n",
      "Epoch: 5 - Batch: 1718, Training Loss: 0.14852548403600555\n",
      "Epoch: 5 - Batch: 1719, Training Loss: 0.148614244995821\n",
      "Epoch: 5 - Batch: 1720, Training Loss: 0.14869868755958368\n",
      "Epoch: 5 - Batch: 1721, Training Loss: 0.1487860676290384\n",
      "Epoch: 5 - Batch: 1722, Training Loss: 0.1488827818550577\n",
      "Epoch: 5 - Batch: 1723, Training Loss: 0.14897485409753636\n",
      "Epoch: 5 - Batch: 1724, Training Loss: 0.14907095840766063\n",
      "Epoch: 5 - Batch: 1725, Training Loss: 0.14916171943444517\n",
      "Epoch: 5 - Batch: 1726, Training Loss: 0.14924525869871252\n",
      "Epoch: 5 - Batch: 1727, Training Loss: 0.1493396352772689\n",
      "Epoch: 5 - Batch: 1728, Training Loss: 0.14943337808077412\n",
      "Epoch: 5 - Batch: 1729, Training Loss: 0.14951539968772115\n",
      "Epoch: 5 - Batch: 1730, Training Loss: 0.14960478537810185\n",
      "Epoch: 5 - Batch: 1731, Training Loss: 0.14968928386446453\n",
      "Epoch: 5 - Batch: 1732, Training Loss: 0.14977923363669596\n",
      "Epoch: 5 - Batch: 1733, Training Loss: 0.14986684972769387\n",
      "Epoch: 5 - Batch: 1734, Training Loss: 0.1499460315822962\n",
      "Epoch: 5 - Batch: 1735, Training Loss: 0.15003044656779044\n",
      "Epoch: 5 - Batch: 1736, Training Loss: 0.1501160790807078\n",
      "Epoch: 5 - Batch: 1737, Training Loss: 0.15020741631043688\n",
      "Epoch: 5 - Batch: 1738, Training Loss: 0.1502947224733446\n",
      "Epoch: 5 - Batch: 1739, Training Loss: 0.15037829325122026\n",
      "Epoch: 5 - Batch: 1740, Training Loss: 0.15046596662396222\n",
      "Epoch: 5 - Batch: 1741, Training Loss: 0.15055727178406952\n",
      "Epoch: 5 - Batch: 1742, Training Loss: 0.15064085664761995\n",
      "Epoch: 5 - Batch: 1743, Training Loss: 0.15072632324611568\n",
      "Epoch: 5 - Batch: 1744, Training Loss: 0.1508026838574441\n",
      "Epoch: 5 - Batch: 1745, Training Loss: 0.1508814865479224\n",
      "Epoch: 5 - Batch: 1746, Training Loss: 0.15095669665616346\n",
      "Epoch: 5 - Batch: 1747, Training Loss: 0.15104400390638642\n",
      "Epoch: 5 - Batch: 1748, Training Loss: 0.15111815159643072\n",
      "Epoch: 5 - Batch: 1749, Training Loss: 0.1512008473017619\n",
      "Epoch: 5 - Batch: 1750, Training Loss: 0.15128130690622488\n",
      "Epoch: 5 - Batch: 1751, Training Loss: 0.15137690414441363\n",
      "Epoch: 5 - Batch: 1752, Training Loss: 0.15146202135639603\n",
      "Epoch: 5 - Batch: 1753, Training Loss: 0.15154951793538596\n",
      "Epoch: 5 - Batch: 1754, Training Loss: 0.15163529092819733\n",
      "Epoch: 5 - Batch: 1755, Training Loss: 0.15172146470170117\n",
      "Epoch: 5 - Batch: 1756, Training Loss: 0.15179990010203215\n",
      "Epoch: 5 - Batch: 1757, Training Loss: 0.15188300309959138\n",
      "Epoch: 5 - Batch: 1758, Training Loss: 0.15196131008552083\n",
      "Epoch: 5 - Batch: 1759, Training Loss: 0.1520521481448837\n",
      "Epoch: 5 - Batch: 1760, Training Loss: 0.15214428386556766\n",
      "Epoch: 5 - Batch: 1761, Training Loss: 0.15223265939066263\n",
      "Epoch: 5 - Batch: 1762, Training Loss: 0.15231389166852136\n",
      "Epoch: 5 - Batch: 1763, Training Loss: 0.1524042476827331\n",
      "Epoch: 5 - Batch: 1764, Training Loss: 0.15249394234980318\n",
      "Epoch: 5 - Batch: 1765, Training Loss: 0.15258433223141366\n",
      "Epoch: 5 - Batch: 1766, Training Loss: 0.15267329811308514\n",
      "Epoch: 5 - Batch: 1767, Training Loss: 0.1527668907126384\n",
      "Epoch: 5 - Batch: 1768, Training Loss: 0.152849501137858\n",
      "Epoch: 5 - Batch: 1769, Training Loss: 0.15293679123197623\n",
      "Epoch: 5 - Batch: 1770, Training Loss: 0.15302415515850631\n",
      "Epoch: 5 - Batch: 1771, Training Loss: 0.15311355201240204\n",
      "Epoch: 5 - Batch: 1772, Training Loss: 0.15318926413903386\n",
      "Epoch: 5 - Batch: 1773, Training Loss: 0.15327030726110757\n",
      "Epoch: 5 - Batch: 1774, Training Loss: 0.15335620124245164\n",
      "Epoch: 5 - Batch: 1775, Training Loss: 0.15344256260254688\n",
      "Epoch: 5 - Batch: 1776, Training Loss: 0.15352977185230549\n",
      "Epoch: 5 - Batch: 1777, Training Loss: 0.1536243897777786\n",
      "Epoch: 5 - Batch: 1778, Training Loss: 0.1537025890762533\n",
      "Epoch: 5 - Batch: 1779, Training Loss: 0.1537873448794754\n",
      "Epoch: 5 - Batch: 1780, Training Loss: 0.15387281585194382\n",
      "Epoch: 5 - Batch: 1781, Training Loss: 0.1539623717565837\n",
      "Epoch: 5 - Batch: 1782, Training Loss: 0.15405095272865857\n",
      "Epoch: 5 - Batch: 1783, Training Loss: 0.1541314089352614\n",
      "Epoch: 5 - Batch: 1784, Training Loss: 0.1542127760958118\n",
      "Epoch: 5 - Batch: 1785, Training Loss: 0.15430626064959055\n",
      "Epoch: 5 - Batch: 1786, Training Loss: 0.15439070727842957\n",
      "Epoch: 5 - Batch: 1787, Training Loss: 0.15448640283215698\n",
      "Epoch: 5 - Batch: 1788, Training Loss: 0.1545690468591244\n",
      "Epoch: 5 - Batch: 1789, Training Loss: 0.15464401210520792\n",
      "Epoch: 5 - Batch: 1790, Training Loss: 0.1547358684661871\n",
      "Epoch: 5 - Batch: 1791, Training Loss: 0.1548154858399683\n",
      "Epoch: 5 - Batch: 1792, Training Loss: 0.15491419292331532\n",
      "Epoch: 5 - Batch: 1793, Training Loss: 0.154998050548544\n",
      "Epoch: 5 - Batch: 1794, Training Loss: 0.15508732875534156\n",
      "Epoch: 5 - Batch: 1795, Training Loss: 0.15517050796381474\n",
      "Epoch: 5 - Batch: 1796, Training Loss: 0.15525860234996178\n",
      "Epoch: 5 - Batch: 1797, Training Loss: 0.15534378900802748\n",
      "Epoch: 5 - Batch: 1798, Training Loss: 0.15542776327870575\n",
      "Epoch: 5 - Batch: 1799, Training Loss: 0.15552061128354389\n",
      "Epoch: 5 - Batch: 1800, Training Loss: 0.15560982844623958\n",
      "Epoch: 5 - Batch: 1801, Training Loss: 0.15570862003745725\n",
      "Epoch: 5 - Batch: 1802, Training Loss: 0.15579208855209856\n",
      "Epoch: 5 - Batch: 1803, Training Loss: 0.15587532701925258\n",
      "Epoch: 5 - Batch: 1804, Training Loss: 0.15595642706475052\n",
      "Epoch: 5 - Batch: 1805, Training Loss: 0.1560427212831294\n",
      "Epoch: 5 - Batch: 1806, Training Loss: 0.1561311893721125\n",
      "Epoch: 5 - Batch: 1807, Training Loss: 0.15621494764391067\n",
      "Epoch: 5 - Batch: 1808, Training Loss: 0.15629959160811074\n",
      "Epoch: 5 - Batch: 1809, Training Loss: 0.1563803453066357\n",
      "Epoch: 5 - Batch: 1810, Training Loss: 0.1564805784801741\n",
      "Epoch: 5 - Batch: 1811, Training Loss: 0.1565628948236876\n",
      "Epoch: 5 - Batch: 1812, Training Loss: 0.1566459617977514\n",
      "Epoch: 5 - Batch: 1813, Training Loss: 0.15672689039340462\n",
      "Epoch: 5 - Batch: 1814, Training Loss: 0.15680897167913158\n",
      "Epoch: 5 - Batch: 1815, Training Loss: 0.15689136719278632\n",
      "Epoch: 5 - Batch: 1816, Training Loss: 0.1569770803709034\n",
      "Epoch: 5 - Batch: 1817, Training Loss: 0.15707268139616173\n",
      "Epoch: 5 - Batch: 1818, Training Loss: 0.15716085177081734\n",
      "Epoch: 5 - Batch: 1819, Training Loss: 0.15724952663503475\n",
      "Epoch: 5 - Batch: 1820, Training Loss: 0.15733576672859056\n",
      "Epoch: 5 - Batch: 1821, Training Loss: 0.15742647007824373\n",
      "Epoch: 5 - Batch: 1822, Training Loss: 0.15750946902714758\n",
      "Epoch: 5 - Batch: 1823, Training Loss: 0.15758729988638046\n",
      "Epoch: 5 - Batch: 1824, Training Loss: 0.15767821660929454\n",
      "Epoch: 5 - Batch: 1825, Training Loss: 0.15776645321751115\n",
      "Epoch: 5 - Batch: 1826, Training Loss: 0.1578539380213712\n",
      "Epoch: 5 - Batch: 1827, Training Loss: 0.15794640659643444\n",
      "Epoch: 5 - Batch: 1828, Training Loss: 0.15802662167234802\n",
      "Epoch: 5 - Batch: 1829, Training Loss: 0.15811199685605012\n",
      "Epoch: 5 - Batch: 1830, Training Loss: 0.15819792716978598\n",
      "Epoch: 5 - Batch: 1831, Training Loss: 0.158291244789785\n",
      "Epoch: 5 - Batch: 1832, Training Loss: 0.1583788811814528\n",
      "Epoch: 5 - Batch: 1833, Training Loss: 0.1584707018463667\n",
      "Epoch: 5 - Batch: 1834, Training Loss: 0.15855909051191353\n",
      "Epoch: 5 - Batch: 1835, Training Loss: 0.15864319937492088\n",
      "Epoch: 5 - Batch: 1836, Training Loss: 0.15872663739531195\n",
      "Epoch: 5 - Batch: 1837, Training Loss: 0.158803957630301\n",
      "Epoch: 5 - Batch: 1838, Training Loss: 0.158884980865626\n",
      "Epoch: 5 - Batch: 1839, Training Loss: 0.1589690366602359\n",
      "Epoch: 5 - Batch: 1840, Training Loss: 0.15905355820262412\n",
      "Epoch: 5 - Batch: 1841, Training Loss: 0.15913809497360368\n",
      "Epoch: 5 - Batch: 1842, Training Loss: 0.1592215198350091\n",
      "Epoch: 5 - Batch: 1843, Training Loss: 0.15930877857614512\n",
      "Epoch: 5 - Batch: 1844, Training Loss: 0.15938338296948182\n",
      "Epoch: 5 - Batch: 1845, Training Loss: 0.15947427394279398\n",
      "Epoch: 5 - Batch: 1846, Training Loss: 0.15955754669126784\n",
      "Epoch: 5 - Batch: 1847, Training Loss: 0.15964427759645392\n",
      "Epoch: 5 - Batch: 1848, Training Loss: 0.15973451231941457\n",
      "Epoch: 5 - Batch: 1849, Training Loss: 0.1598193988846507\n",
      "Epoch: 5 - Batch: 1850, Training Loss: 0.159914551954562\n",
      "Epoch: 5 - Batch: 1851, Training Loss: 0.16000428407840667\n",
      "Epoch: 5 - Batch: 1852, Training Loss: 0.1600947866454152\n",
      "Epoch: 5 - Batch: 1853, Training Loss: 0.16018178315206152\n",
      "Epoch: 5 - Batch: 1854, Training Loss: 0.16026806046145275\n",
      "Epoch: 5 - Batch: 1855, Training Loss: 0.16035231337519626\n",
      "Epoch: 5 - Batch: 1856, Training Loss: 0.1604349295520664\n",
      "Epoch: 5 - Batch: 1857, Training Loss: 0.16052141571247558\n",
      "Epoch: 5 - Batch: 1858, Training Loss: 0.16060891624533913\n",
      "Epoch: 5 - Batch: 1859, Training Loss: 0.16069920707005964\n",
      "Epoch: 5 - Batch: 1860, Training Loss: 0.16078510920626804\n",
      "Epoch: 5 - Batch: 1861, Training Loss: 0.1608662522563195\n",
      "Epoch: 5 - Batch: 1862, Training Loss: 0.16095558609535446\n",
      "Epoch: 5 - Batch: 1863, Training Loss: 0.16104199294970797\n",
      "Epoch: 5 - Batch: 1864, Training Loss: 0.16112941971193895\n",
      "Epoch: 5 - Batch: 1865, Training Loss: 0.16121655032903598\n",
      "Epoch: 5 - Batch: 1866, Training Loss: 0.16130023702954377\n",
      "Epoch: 5 - Batch: 1867, Training Loss: 0.16138595151738147\n",
      "Epoch: 5 - Batch: 1868, Training Loss: 0.16147599809773724\n",
      "Epoch: 5 - Batch: 1869, Training Loss: 0.16155760650635753\n",
      "Epoch: 5 - Batch: 1870, Training Loss: 0.16164435651369555\n",
      "Epoch: 5 - Batch: 1871, Training Loss: 0.1617238263276778\n",
      "Epoch: 5 - Batch: 1872, Training Loss: 0.16180347603055376\n",
      "Epoch: 5 - Batch: 1873, Training Loss: 0.1618762349657356\n",
      "Epoch: 5 - Batch: 1874, Training Loss: 0.16197169462765626\n",
      "Epoch: 5 - Batch: 1875, Training Loss: 0.16205584507429383\n",
      "Epoch: 5 - Batch: 1876, Training Loss: 0.1621560599055654\n",
      "Epoch: 5 - Batch: 1877, Training Loss: 0.16224955843381622\n",
      "Epoch: 5 - Batch: 1878, Training Loss: 0.16233135712260433\n",
      "Epoch: 5 - Batch: 1879, Training Loss: 0.16242088181017644\n",
      "Epoch: 5 - Batch: 1880, Training Loss: 0.16250377152714365\n",
      "Epoch: 5 - Batch: 1881, Training Loss: 0.16258348601176767\n",
      "Epoch: 5 - Batch: 1882, Training Loss: 0.16266708711708955\n",
      "Epoch: 5 - Batch: 1883, Training Loss: 0.1627481844442994\n",
      "Epoch: 5 - Batch: 1884, Training Loss: 0.16284070926939276\n",
      "Epoch: 5 - Batch: 1885, Training Loss: 0.16292342649862343\n",
      "Epoch: 5 - Batch: 1886, Training Loss: 0.16301529306553886\n",
      "Epoch: 5 - Batch: 1887, Training Loss: 0.16310185215020456\n",
      "Epoch: 5 - Batch: 1888, Training Loss: 0.16319192199663538\n",
      "Epoch: 5 - Batch: 1889, Training Loss: 0.16327589002760687\n",
      "Epoch: 5 - Batch: 1890, Training Loss: 0.16336470262254055\n",
      "Epoch: 5 - Batch: 1891, Training Loss: 0.1634471298042518\n",
      "Epoch: 5 - Batch: 1892, Training Loss: 0.16352383044253338\n",
      "Epoch: 5 - Batch: 1893, Training Loss: 0.16360673139715076\n",
      "Epoch: 5 - Batch: 1894, Training Loss: 0.16369660784356035\n",
      "Epoch: 5 - Batch: 1895, Training Loss: 0.16378258811819613\n",
      "Epoch: 5 - Batch: 1896, Training Loss: 0.16386999051468093\n",
      "Epoch: 5 - Batch: 1897, Training Loss: 0.16395262305688107\n",
      "Epoch: 5 - Batch: 1898, Training Loss: 0.16404102178627183\n",
      "Epoch: 5 - Batch: 1899, Training Loss: 0.16412291112379054\n",
      "Epoch: 5 - Batch: 1900, Training Loss: 0.16421287433919227\n",
      "Epoch: 5 - Batch: 1901, Training Loss: 0.16429546754652785\n",
      "Epoch: 5 - Batch: 1902, Training Loss: 0.1643814007577117\n",
      "Epoch: 5 - Batch: 1903, Training Loss: 0.16446035189143263\n",
      "Epoch: 5 - Batch: 1904, Training Loss: 0.16454112355537082\n",
      "Epoch: 5 - Batch: 1905, Training Loss: 0.16462960549526745\n",
      "Epoch: 5 - Batch: 1906, Training Loss: 0.16471592097189494\n",
      "Epoch: 5 - Batch: 1907, Training Loss: 0.16480631295400078\n",
      "Epoch: 5 - Batch: 1908, Training Loss: 0.16489716265875703\n",
      "Epoch: 5 - Batch: 1909, Training Loss: 0.16499657604602438\n",
      "Epoch: 5 - Batch: 1910, Training Loss: 0.16508910095760873\n",
      "Epoch: 5 - Batch: 1911, Training Loss: 0.1651767582121378\n",
      "Epoch: 5 - Batch: 1912, Training Loss: 0.16527141775830864\n",
      "Epoch: 5 - Batch: 1913, Training Loss: 0.16535248832891433\n",
      "Epoch: 5 - Batch: 1914, Training Loss: 0.16543800418402624\n",
      "Epoch: 5 - Batch: 1915, Training Loss: 0.16552291712901288\n",
      "Epoch: 5 - Batch: 1916, Training Loss: 0.1656104465189759\n",
      "Epoch: 5 - Batch: 1917, Training Loss: 0.16570095509710794\n",
      "Epoch: 5 - Batch: 1918, Training Loss: 0.16578437371880655\n",
      "Epoch: 5 - Batch: 1919, Training Loss: 0.16587825589017885\n",
      "Epoch: 5 - Batch: 1920, Training Loss: 0.1659625758097243\n",
      "Epoch: 5 - Batch: 1921, Training Loss: 0.1660458537538352\n",
      "Epoch: 5 - Batch: 1922, Training Loss: 0.16612313876845944\n",
      "Epoch: 5 - Batch: 1923, Training Loss: 0.16620598915674004\n",
      "Epoch: 5 - Batch: 1924, Training Loss: 0.16629462647151394\n",
      "Epoch: 5 - Batch: 1925, Training Loss: 0.1663775397634526\n",
      "Epoch: 5 - Batch: 1926, Training Loss: 0.16647366476963407\n",
      "Epoch: 5 - Batch: 1927, Training Loss: 0.16656048667569262\n",
      "Epoch: 5 - Batch: 1928, Training Loss: 0.16664999998970015\n",
      "Epoch: 5 - Batch: 1929, Training Loss: 0.16673510575398284\n",
      "Epoch: 5 - Batch: 1930, Training Loss: 0.16682775742428418\n",
      "Epoch: 5 - Batch: 1931, Training Loss: 0.1669105039245355\n",
      "Epoch: 5 - Batch: 1932, Training Loss: 0.16699853249104263\n",
      "Epoch: 5 - Batch: 1933, Training Loss: 0.16708484345160513\n",
      "Epoch: 5 - Batch: 1934, Training Loss: 0.16716749179694387\n",
      "Epoch: 5 - Batch: 1935, Training Loss: 0.16725436211324252\n",
      "Epoch: 5 - Batch: 1936, Training Loss: 0.1673473153220085\n",
      "Epoch: 5 - Batch: 1937, Training Loss: 0.16743995589057406\n",
      "Epoch: 5 - Batch: 1938, Training Loss: 0.16752546930540457\n",
      "Epoch: 5 - Batch: 1939, Training Loss: 0.16761374185680356\n",
      "Epoch: 5 - Batch: 1940, Training Loss: 0.16770108386404675\n",
      "Epoch: 5 - Batch: 1941, Training Loss: 0.1677885570348693\n",
      "Epoch: 5 - Batch: 1942, Training Loss: 0.1678708427235064\n",
      "Epoch: 5 - Batch: 1943, Training Loss: 0.16795902769660476\n",
      "Epoch: 5 - Batch: 1944, Training Loss: 0.16805065432517088\n",
      "Epoch: 5 - Batch: 1945, Training Loss: 0.16813574875913448\n",
      "Epoch: 5 - Batch: 1946, Training Loss: 0.16822926619184353\n",
      "Epoch: 5 - Batch: 1947, Training Loss: 0.1683190240158667\n",
      "Epoch: 5 - Batch: 1948, Training Loss: 0.16840946154783218\n",
      "Epoch: 5 - Batch: 1949, Training Loss: 0.168492536874454\n",
      "Epoch: 5 - Batch: 1950, Training Loss: 0.16859169284601513\n",
      "Epoch: 5 - Batch: 1951, Training Loss: 0.16867576360974343\n",
      "Epoch: 5 - Batch: 1952, Training Loss: 0.16876445472808818\n",
      "Epoch: 5 - Batch: 1953, Training Loss: 0.16885413110849276\n",
      "Epoch: 5 - Batch: 1954, Training Loss: 0.16893925081635786\n",
      "Epoch: 5 - Batch: 1955, Training Loss: 0.16901619068252705\n",
      "Epoch: 5 - Batch: 1956, Training Loss: 0.16910426127129724\n",
      "Epoch: 5 - Batch: 1957, Training Loss: 0.1691853722517269\n",
      "Epoch: 5 - Batch: 1958, Training Loss: 0.16927274770009182\n",
      "Epoch: 5 - Batch: 1959, Training Loss: 0.16936542562658513\n",
      "Epoch: 5 - Batch: 1960, Training Loss: 0.16946166540630422\n",
      "Epoch: 5 - Batch: 1961, Training Loss: 0.1695481750675497\n",
      "Epoch: 5 - Batch: 1962, Training Loss: 0.1696331317584412\n",
      "Epoch: 5 - Batch: 1963, Training Loss: 0.16971816868825537\n",
      "Epoch: 5 - Batch: 1964, Training Loss: 0.1698012973180359\n",
      "Epoch: 5 - Batch: 1965, Training Loss: 0.16988849847470944\n",
      "Epoch: 5 - Batch: 1966, Training Loss: 0.16997256764453245\n",
      "Epoch: 5 - Batch: 1967, Training Loss: 0.1700463525234269\n",
      "Epoch: 5 - Batch: 1968, Training Loss: 0.17013570660135244\n",
      "Epoch: 5 - Batch: 1969, Training Loss: 0.1702228766336271\n",
      "Epoch: 5 - Batch: 1970, Training Loss: 0.17030618008640078\n",
      "Epoch: 5 - Batch: 1971, Training Loss: 0.17038939720263727\n",
      "Epoch: 5 - Batch: 1972, Training Loss: 0.17047530362632737\n",
      "Epoch: 5 - Batch: 1973, Training Loss: 0.17056568636625363\n",
      "Epoch: 5 - Batch: 1974, Training Loss: 0.1706530669917218\n",
      "Epoch: 5 - Batch: 1975, Training Loss: 0.1707386322455422\n",
      "Epoch: 5 - Batch: 1976, Training Loss: 0.17082247226985534\n",
      "Epoch: 5 - Batch: 1977, Training Loss: 0.1709120165229239\n",
      "Epoch: 5 - Batch: 1978, Training Loss: 0.17100192696646868\n",
      "Epoch: 5 - Batch: 1979, Training Loss: 0.17109036134250127\n",
      "Epoch: 5 - Batch: 1980, Training Loss: 0.17118455734641397\n",
      "Epoch: 5 - Batch: 1981, Training Loss: 0.17126587024622691\n",
      "Epoch: 5 - Batch: 1982, Training Loss: 0.17135896530638683\n",
      "Epoch: 5 - Batch: 1983, Training Loss: 0.17144342068317123\n",
      "Epoch: 5 - Batch: 1984, Training Loss: 0.17152685955611627\n",
      "Epoch: 5 - Batch: 1985, Training Loss: 0.17161614539883227\n",
      "Epoch: 5 - Batch: 1986, Training Loss: 0.17169870841463605\n",
      "Epoch: 5 - Batch: 1987, Training Loss: 0.17178328887591907\n",
      "Epoch: 5 - Batch: 1988, Training Loss: 0.17187035385253616\n",
      "Epoch: 5 - Batch: 1989, Training Loss: 0.17195002368927792\n",
      "Epoch: 5 - Batch: 1990, Training Loss: 0.17203580218662273\n",
      "Epoch: 5 - Batch: 1991, Training Loss: 0.1721170881136041\n",
      "Epoch: 5 - Batch: 1992, Training Loss: 0.17220466661191303\n",
      "Epoch: 5 - Batch: 1993, Training Loss: 0.17228329438055134\n",
      "Epoch: 5 - Batch: 1994, Training Loss: 0.17236632658523904\n",
      "Epoch: 5 - Batch: 1995, Training Loss: 0.1724545666839847\n",
      "Epoch: 5 - Batch: 1996, Training Loss: 0.1725417478735965\n",
      "Epoch: 5 - Batch: 1997, Training Loss: 0.1726200101436865\n",
      "Epoch: 5 - Batch: 1998, Training Loss: 0.17270065454849556\n",
      "Epoch: 5 - Batch: 1999, Training Loss: 0.17278946799598324\n",
      "Epoch: 5 - Batch: 2000, Training Loss: 0.172872433621146\n",
      "Epoch: 5 - Batch: 2001, Training Loss: 0.17296653873538892\n",
      "Epoch: 5 - Batch: 2002, Training Loss: 0.1730477454428056\n",
      "Epoch: 5 - Batch: 2003, Training Loss: 0.17313059722111987\n",
      "Epoch: 5 - Batch: 2004, Training Loss: 0.17321479626333536\n",
      "Epoch: 5 - Batch: 2005, Training Loss: 0.17330592260333041\n",
      "Epoch: 5 - Batch: 2006, Training Loss: 0.17339278448058004\n",
      "Epoch: 5 - Batch: 2007, Training Loss: 0.17348046661362324\n",
      "Epoch: 5 - Batch: 2008, Training Loss: 0.17356564842201583\n",
      "Epoch: 5 - Batch: 2009, Training Loss: 0.17365532327647232\n",
      "Epoch: 5 - Batch: 2010, Training Loss: 0.17374277161820414\n",
      "Epoch: 5 - Batch: 2011, Training Loss: 0.1738254384787917\n",
      "Epoch: 5 - Batch: 2012, Training Loss: 0.17391446427400434\n",
      "Epoch: 5 - Batch: 2013, Training Loss: 0.17400558840847924\n",
      "Epoch: 5 - Batch: 2014, Training Loss: 0.17408504519321236\n",
      "Epoch: 5 - Batch: 2015, Training Loss: 0.17416659468293783\n",
      "Epoch: 5 - Batch: 2016, Training Loss: 0.17425460369630438\n",
      "Epoch: 5 - Batch: 2017, Training Loss: 0.17433958377065153\n",
      "Epoch: 5 - Batch: 2018, Training Loss: 0.1744322892877692\n",
      "Epoch: 5 - Batch: 2019, Training Loss: 0.17453068482440898\n",
      "Epoch: 5 - Batch: 2020, Training Loss: 0.17461521504060742\n",
      "Epoch: 5 - Batch: 2021, Training Loss: 0.17470952190125166\n",
      "Epoch: 5 - Batch: 2022, Training Loss: 0.1747977616107879\n",
      "Epoch: 5 - Batch: 2023, Training Loss: 0.17488382550367274\n",
      "Epoch: 5 - Batch: 2024, Training Loss: 0.17496898523215235\n",
      "Epoch: 5 - Batch: 2025, Training Loss: 0.17505727387705253\n",
      "Epoch: 5 - Batch: 2026, Training Loss: 0.17514623068334847\n",
      "Epoch: 5 - Batch: 2027, Training Loss: 0.17523262554634467\n",
      "Epoch: 5 - Batch: 2028, Training Loss: 0.17532239958124968\n",
      "Epoch: 5 - Batch: 2029, Training Loss: 0.1754094592324932\n",
      "Epoch: 5 - Batch: 2030, Training Loss: 0.17549334265674724\n",
      "Epoch: 5 - Batch: 2031, Training Loss: 0.17557856483171827\n",
      "Epoch: 5 - Batch: 2032, Training Loss: 0.17566714312257262\n",
      "Epoch: 5 - Batch: 2033, Training Loss: 0.17576472636676743\n",
      "Epoch: 5 - Batch: 2034, Training Loss: 0.17585400727949727\n",
      "Epoch: 5 - Batch: 2035, Training Loss: 0.1759439716192818\n",
      "Epoch: 5 - Batch: 2036, Training Loss: 0.17603691481980518\n",
      "Epoch: 5 - Batch: 2037, Training Loss: 0.17612464461555924\n",
      "Epoch: 5 - Batch: 2038, Training Loss: 0.17620593660926542\n",
      "Epoch: 5 - Batch: 2039, Training Loss: 0.1762929366126187\n",
      "Epoch: 5 - Batch: 2040, Training Loss: 0.17638051283127237\n",
      "Epoch: 5 - Batch: 2041, Training Loss: 0.17646615010119393\n",
      "Epoch: 5 - Batch: 2042, Training Loss: 0.17655051996310553\n",
      "Epoch: 5 - Batch: 2043, Training Loss: 0.17663645229702368\n",
      "Epoch: 5 - Batch: 2044, Training Loss: 0.17671923711895943\n",
      "Epoch: 5 - Batch: 2045, Training Loss: 0.17680570012449626\n",
      "Epoch: 5 - Batch: 2046, Training Loss: 0.17689580516658018\n",
      "Epoch: 5 - Batch: 2047, Training Loss: 0.1769820023914969\n",
      "Epoch: 5 - Batch: 2048, Training Loss: 0.17706731624691247\n",
      "Epoch: 5 - Batch: 2049, Training Loss: 0.17715852184971767\n",
      "Epoch: 5 - Batch: 2050, Training Loss: 0.17724821476843425\n",
      "Epoch: 5 - Batch: 2051, Training Loss: 0.17733377107043766\n",
      "Epoch: 5 - Batch: 2052, Training Loss: 0.17741829905022632\n",
      "Epoch: 5 - Batch: 2053, Training Loss: 0.17750924532216777\n",
      "Epoch: 5 - Batch: 2054, Training Loss: 0.17759427965725239\n",
      "Epoch: 5 - Batch: 2055, Training Loss: 0.17768454917430088\n",
      "Epoch: 5 - Batch: 2056, Training Loss: 0.177772266774826\n",
      "Epoch: 5 - Batch: 2057, Training Loss: 0.17784933200967845\n",
      "Epoch: 5 - Batch: 2058, Training Loss: 0.17793342826971367\n",
      "Epoch: 5 - Batch: 2059, Training Loss: 0.17802854241250363\n",
      "Epoch: 5 - Batch: 2060, Training Loss: 0.1781135864778241\n",
      "Epoch: 5 - Batch: 2061, Training Loss: 0.17820345588139633\n",
      "Epoch: 5 - Batch: 2062, Training Loss: 0.17829261966679819\n",
      "Epoch: 5 - Batch: 2063, Training Loss: 0.17837781493071694\n",
      "Epoch: 5 - Batch: 2064, Training Loss: 0.1784659607604665\n",
      "Epoch: 5 - Batch: 2065, Training Loss: 0.17854892805366968\n",
      "Epoch: 5 - Batch: 2066, Training Loss: 0.1786358922250433\n",
      "Epoch: 5 - Batch: 2067, Training Loss: 0.17871824538900485\n",
      "Epoch: 5 - Batch: 2068, Training Loss: 0.17881274496392033\n",
      "Epoch: 5 - Batch: 2069, Training Loss: 0.17890049393596144\n",
      "Epoch: 5 - Batch: 2070, Training Loss: 0.17899230813876313\n",
      "Epoch: 5 - Batch: 2071, Training Loss: 0.1790841259000511\n",
      "Epoch: 5 - Batch: 2072, Training Loss: 0.17916943329335444\n",
      "Epoch: 5 - Batch: 2073, Training Loss: 0.1792535230294982\n",
      "Epoch: 5 - Batch: 2074, Training Loss: 0.17934956662912868\n",
      "Epoch: 5 - Batch: 2075, Training Loss: 0.17943993910133937\n",
      "Epoch: 5 - Batch: 2076, Training Loss: 0.17952852216773169\n",
      "Epoch: 5 - Batch: 2077, Training Loss: 0.17960814817825557\n",
      "Epoch: 5 - Batch: 2078, Training Loss: 0.17970503382322997\n",
      "Epoch: 5 - Batch: 2079, Training Loss: 0.17978416838579708\n",
      "Epoch: 5 - Batch: 2080, Training Loss: 0.17987442950704205\n",
      "Epoch: 5 - Batch: 2081, Training Loss: 0.17995961089533558\n",
      "Epoch: 5 - Batch: 2082, Training Loss: 0.1800463679331551\n",
      "Epoch: 5 - Batch: 2083, Training Loss: 0.18012554688413146\n",
      "Epoch: 5 - Batch: 2084, Training Loss: 0.18021379962291686\n",
      "Epoch: 5 - Batch: 2085, Training Loss: 0.1803031369833705\n",
      "Epoch: 5 - Batch: 2086, Training Loss: 0.18039188450619356\n",
      "Epoch: 5 - Batch: 2087, Training Loss: 0.18047849627622523\n",
      "Epoch: 5 - Batch: 2088, Training Loss: 0.1805761748520197\n",
      "Epoch: 5 - Batch: 2089, Training Loss: 0.18065890059443454\n",
      "Epoch: 5 - Batch: 2090, Training Loss: 0.18074820991945306\n",
      "Epoch: 5 - Batch: 2091, Training Loss: 0.1808252763404676\n",
      "Epoch: 5 - Batch: 2092, Training Loss: 0.18091449456576683\n",
      "Epoch: 5 - Batch: 2093, Training Loss: 0.1809960816234696\n",
      "Epoch: 5 - Batch: 2094, Training Loss: 0.18107753297681634\n",
      "Epoch: 5 - Batch: 2095, Training Loss: 0.18116525477215425\n",
      "Epoch: 5 - Batch: 2096, Training Loss: 0.18124838537269367\n",
      "Epoch: 5 - Batch: 2097, Training Loss: 0.1813318265561836\n",
      "Epoch: 5 - Batch: 2098, Training Loss: 0.1814123123983048\n",
      "Epoch: 5 - Batch: 2099, Training Loss: 0.18149802668844883\n",
      "Epoch: 5 - Batch: 2100, Training Loss: 0.18158142507125688\n",
      "Epoch: 5 - Batch: 2101, Training Loss: 0.1816697590003658\n",
      "Epoch: 5 - Batch: 2102, Training Loss: 0.18175961228572513\n",
      "Epoch: 5 - Batch: 2103, Training Loss: 0.18185596669688944\n",
      "Epoch: 5 - Batch: 2104, Training Loss: 0.18194335430959366\n",
      "Epoch: 5 - Batch: 2105, Training Loss: 0.1820297428711038\n",
      "Epoch: 5 - Batch: 2106, Training Loss: 0.18212708360075358\n",
      "Epoch: 5 - Batch: 2107, Training Loss: 0.18222217588603595\n",
      "Epoch: 5 - Batch: 2108, Training Loss: 0.18230703203115692\n",
      "Epoch: 5 - Batch: 2109, Training Loss: 0.18239749353965914\n",
      "Epoch: 5 - Batch: 2110, Training Loss: 0.1824861195550036\n",
      "Epoch: 5 - Batch: 2111, Training Loss: 0.18257303733014152\n",
      "Epoch: 5 - Batch: 2112, Training Loss: 0.18265858940644247\n",
      "Epoch: 5 - Batch: 2113, Training Loss: 0.18275326463837133\n",
      "Epoch: 5 - Batch: 2114, Training Loss: 0.18283509286383096\n",
      "Epoch: 5 - Batch: 2115, Training Loss: 0.1829259038206258\n",
      "Epoch: 5 - Batch: 2116, Training Loss: 0.18301265998735752\n",
      "Epoch: 5 - Batch: 2117, Training Loss: 0.18309291603528643\n",
      "Epoch: 5 - Batch: 2118, Training Loss: 0.18317707112772547\n",
      "Epoch: 5 - Batch: 2119, Training Loss: 0.1832569411725646\n",
      "Epoch: 5 - Batch: 2120, Training Loss: 0.18333894836022882\n",
      "Epoch: 5 - Batch: 2121, Training Loss: 0.18342811301671846\n",
      "Epoch: 5 - Batch: 2122, Training Loss: 0.1835134549261029\n",
      "Epoch: 5 - Batch: 2123, Training Loss: 0.18359910250698552\n",
      "Epoch: 5 - Batch: 2124, Training Loss: 0.1836847803164675\n",
      "Epoch: 5 - Batch: 2125, Training Loss: 0.18377364193943405\n",
      "Epoch: 5 - Batch: 2126, Training Loss: 0.18385053115895336\n",
      "Epoch: 5 - Batch: 2127, Training Loss: 0.18392882249619238\n",
      "Epoch: 5 - Batch: 2128, Training Loss: 0.18401249971975933\n",
      "Epoch: 5 - Batch: 2129, Training Loss: 0.18409253717407856\n",
      "Epoch: 5 - Batch: 2130, Training Loss: 0.1841854888754302\n",
      "Epoch: 5 - Batch: 2131, Training Loss: 0.18427603060432138\n",
      "Epoch: 5 - Batch: 2132, Training Loss: 0.1843610858393348\n",
      "Epoch: 5 - Batch: 2133, Training Loss: 0.1844430154816527\n",
      "Epoch: 5 - Batch: 2134, Training Loss: 0.18452969160097749\n",
      "Epoch: 5 - Batch: 2135, Training Loss: 0.18461461249695685\n",
      "Epoch: 5 - Batch: 2136, Training Loss: 0.1846993000402577\n",
      "Epoch: 5 - Batch: 2137, Training Loss: 0.1847895123496973\n",
      "Epoch: 5 - Batch: 2138, Training Loss: 0.18489064092733967\n",
      "Epoch: 5 - Batch: 2139, Training Loss: 0.18497746726915018\n",
      "Epoch: 5 - Batch: 2140, Training Loss: 0.18506390023760336\n",
      "Epoch: 5 - Batch: 2141, Training Loss: 0.18515216863743503\n",
      "Epoch: 5 - Batch: 2142, Training Loss: 0.18524353747344136\n",
      "Epoch: 5 - Batch: 2143, Training Loss: 0.18532811709452623\n",
      "Epoch: 5 - Batch: 2144, Training Loss: 0.1854061919510068\n",
      "Epoch: 5 - Batch: 2145, Training Loss: 0.18548990133686447\n",
      "Epoch: 5 - Batch: 2146, Training Loss: 0.18557433000646817\n",
      "Epoch: 5 - Batch: 2147, Training Loss: 0.18565743407280885\n",
      "Epoch: 5 - Batch: 2148, Training Loss: 0.18574286402383847\n",
      "Epoch: 5 - Batch: 2149, Training Loss: 0.18582555534901904\n",
      "Epoch: 5 - Batch: 2150, Training Loss: 0.18591062917341641\n",
      "Epoch: 5 - Batch: 2151, Training Loss: 0.1859953454564361\n",
      "Epoch: 5 - Batch: 2152, Training Loss: 0.18607359260913744\n",
      "Epoch: 5 - Batch: 2153, Training Loss: 0.18615075003102446\n",
      "Epoch: 5 - Batch: 2154, Training Loss: 0.18623630170576014\n",
      "Epoch: 5 - Batch: 2155, Training Loss: 0.1863295580098285\n",
      "Epoch: 5 - Batch: 2156, Training Loss: 0.1864109143281161\n",
      "Epoch: 5 - Batch: 2157, Training Loss: 0.18648813087894747\n",
      "Epoch: 5 - Batch: 2158, Training Loss: 0.18657154237749565\n",
      "Epoch: 5 - Batch: 2159, Training Loss: 0.18666991669913233\n",
      "Epoch: 5 - Batch: 2160, Training Loss: 0.18675202791074022\n",
      "Epoch: 5 - Batch: 2161, Training Loss: 0.18684347058533635\n",
      "Epoch: 5 - Batch: 2162, Training Loss: 0.1869216617227688\n",
      "Epoch: 5 - Batch: 2163, Training Loss: 0.18701192874393455\n",
      "Epoch: 5 - Batch: 2164, Training Loss: 0.18709815435642824\n",
      "Epoch: 5 - Batch: 2165, Training Loss: 0.1871910475898738\n",
      "Epoch: 5 - Batch: 2166, Training Loss: 0.18728238544357356\n",
      "Epoch: 5 - Batch: 2167, Training Loss: 0.18737188969456142\n",
      "Epoch: 5 - Batch: 2168, Training Loss: 0.1874623735300937\n",
      "Epoch: 5 - Batch: 2169, Training Loss: 0.18755452019188731\n",
      "Epoch: 5 - Batch: 2170, Training Loss: 0.18765029012786216\n",
      "Epoch: 5 - Batch: 2171, Training Loss: 0.18773153843126486\n",
      "Epoch: 5 - Batch: 2172, Training Loss: 0.18782042028076612\n",
      "Epoch: 5 - Batch: 2173, Training Loss: 0.18791078213929735\n",
      "Epoch: 5 - Batch: 2174, Training Loss: 0.1879899120931305\n",
      "Epoch: 5 - Batch: 2175, Training Loss: 0.18807304420108423\n",
      "Epoch: 5 - Batch: 2176, Training Loss: 0.18815840006640697\n",
      "Epoch: 5 - Batch: 2177, Training Loss: 0.18823649439571508\n",
      "Epoch: 5 - Batch: 2178, Training Loss: 0.18832880987555628\n",
      "Epoch: 5 - Batch: 2179, Training Loss: 0.1884169015248814\n",
      "Epoch: 5 - Batch: 2180, Training Loss: 0.18850284928508462\n",
      "Epoch: 5 - Batch: 2181, Training Loss: 0.18859024608436706\n",
      "Epoch: 5 - Batch: 2182, Training Loss: 0.1886707399516161\n",
      "Epoch: 5 - Batch: 2183, Training Loss: 0.18875803995513007\n",
      "Epoch: 5 - Batch: 2184, Training Loss: 0.18884859221738765\n",
      "Epoch: 5 - Batch: 2185, Training Loss: 0.18892811842349236\n",
      "Epoch: 5 - Batch: 2186, Training Loss: 0.1890198694137396\n",
      "Epoch: 5 - Batch: 2187, Training Loss: 0.18910088336981745\n",
      "Epoch: 5 - Batch: 2188, Training Loss: 0.1891874369623056\n",
      "Epoch: 5 - Batch: 2189, Training Loss: 0.18926603578735346\n",
      "Epoch: 5 - Batch: 2190, Training Loss: 0.18935185969864354\n",
      "Epoch: 5 - Batch: 2191, Training Loss: 0.18943430630993685\n",
      "Epoch: 5 - Batch: 2192, Training Loss: 0.18951369273425334\n",
      "Epoch: 5 - Batch: 2193, Training Loss: 0.18959359538619397\n",
      "Epoch: 5 - Batch: 2194, Training Loss: 0.18968339145776644\n",
      "Epoch: 5 - Batch: 2195, Training Loss: 0.18976982855925315\n",
      "Epoch: 5 - Batch: 2196, Training Loss: 0.18985221793204793\n",
      "Epoch: 5 - Batch: 2197, Training Loss: 0.18993463492635668\n",
      "Epoch: 5 - Batch: 2198, Training Loss: 0.19002939425223503\n",
      "Epoch: 5 - Batch: 2199, Training Loss: 0.19011795972388973\n",
      "Epoch: 5 - Batch: 2200, Training Loss: 0.19022044675366004\n",
      "Epoch: 5 - Batch: 2201, Training Loss: 0.19030483857192607\n",
      "Epoch: 5 - Batch: 2202, Training Loss: 0.1903921827414439\n",
      "Epoch: 5 - Batch: 2203, Training Loss: 0.19047674006638834\n",
      "Epoch: 5 - Batch: 2204, Training Loss: 0.19056746319776546\n",
      "Epoch: 5 - Batch: 2205, Training Loss: 0.19065438089876824\n",
      "Epoch: 5 - Batch: 2206, Training Loss: 0.19074161489112657\n",
      "Epoch: 5 - Batch: 2207, Training Loss: 0.19082241368852246\n",
      "Epoch: 5 - Batch: 2208, Training Loss: 0.1909115297741459\n",
      "Epoch: 5 - Batch: 2209, Training Loss: 0.19100553088297892\n",
      "Epoch: 5 - Batch: 2210, Training Loss: 0.1910878529904988\n",
      "Epoch: 5 - Batch: 2211, Training Loss: 0.19116841127499815\n",
      "Epoch: 5 - Batch: 2212, Training Loss: 0.19125215563410353\n",
      "Epoch: 5 - Batch: 2213, Training Loss: 0.19134164619480398\n",
      "Epoch: 5 - Batch: 2214, Training Loss: 0.19142922151148023\n",
      "Epoch: 5 - Batch: 2215, Training Loss: 0.19151778391270496\n",
      "Epoch: 5 - Batch: 2216, Training Loss: 0.1915977191593912\n",
      "Epoch: 5 - Batch: 2217, Training Loss: 0.19168068216511266\n",
      "Epoch: 5 - Batch: 2218, Training Loss: 0.1917724345886391\n",
      "Epoch: 5 - Batch: 2219, Training Loss: 0.19185041854309404\n",
      "Epoch: 5 - Batch: 2220, Training Loss: 0.19193566182804345\n",
      "Epoch: 5 - Batch: 2221, Training Loss: 0.1920221043475726\n",
      "Epoch: 5 - Batch: 2222, Training Loss: 0.1921054389123893\n",
      "Epoch: 5 - Batch: 2223, Training Loss: 0.19218751072735335\n",
      "Epoch: 5 - Batch: 2224, Training Loss: 0.19227275591510445\n",
      "Epoch: 5 - Batch: 2225, Training Loss: 0.19236606833328854\n",
      "Epoch: 5 - Batch: 2226, Training Loss: 0.19244771507594913\n",
      "Epoch: 5 - Batch: 2227, Training Loss: 0.1925331639129032\n",
      "Epoch: 5 - Batch: 2228, Training Loss: 0.1926198862863714\n",
      "Epoch: 5 - Batch: 2229, Training Loss: 0.19270432747245625\n",
      "Epoch: 5 - Batch: 2230, Training Loss: 0.1927964647129104\n",
      "Epoch: 5 - Batch: 2231, Training Loss: 0.19288750740994465\n",
      "Epoch: 5 - Batch: 2232, Training Loss: 0.19297547839198934\n",
      "Epoch: 5 - Batch: 2233, Training Loss: 0.19306670545073687\n",
      "Epoch: 5 - Batch: 2234, Training Loss: 0.1931425303121902\n",
      "Epoch: 5 - Batch: 2235, Training Loss: 0.1932295497760152\n",
      "Epoch: 5 - Batch: 2236, Training Loss: 0.1933149346528856\n",
      "Epoch: 5 - Batch: 2237, Training Loss: 0.19341308058889747\n",
      "Epoch: 5 - Batch: 2238, Training Loss: 0.1934988942386499\n",
      "Epoch: 5 - Batch: 2239, Training Loss: 0.19358629753711212\n",
      "Epoch: 5 - Batch: 2240, Training Loss: 0.19367318694938473\n",
      "Epoch: 5 - Batch: 2241, Training Loss: 0.19376535555517693\n",
      "Epoch: 5 - Batch: 2242, Training Loss: 0.19385500083589435\n",
      "Epoch: 5 - Batch: 2243, Training Loss: 0.1939335884603596\n",
      "Epoch: 5 - Batch: 2244, Training Loss: 0.19402196747598363\n",
      "Epoch: 5 - Batch: 2245, Training Loss: 0.19410118082566047\n",
      "Epoch: 5 - Batch: 2246, Training Loss: 0.194190640861221\n",
      "Epoch: 5 - Batch: 2247, Training Loss: 0.1942772464842148\n",
      "Epoch: 5 - Batch: 2248, Training Loss: 0.1943639679927731\n",
      "Epoch: 5 - Batch: 2249, Training Loss: 0.19444708661431104\n",
      "Epoch: 5 - Batch: 2250, Training Loss: 0.19452821287275546\n",
      "Epoch: 5 - Batch: 2251, Training Loss: 0.19461471617691356\n",
      "Epoch: 5 - Batch: 2252, Training Loss: 0.19470124473643935\n",
      "Epoch: 5 - Batch: 2253, Training Loss: 0.19479189490649237\n",
      "Epoch: 5 - Batch: 2254, Training Loss: 0.19488500929738753\n",
      "Epoch: 5 - Batch: 2255, Training Loss: 0.19497604469828939\n",
      "Epoch: 5 - Batch: 2256, Training Loss: 0.19505835485151948\n",
      "Epoch: 5 - Batch: 2257, Training Loss: 0.19513611318806115\n",
      "Epoch: 5 - Batch: 2258, Training Loss: 0.19523102613428536\n",
      "Epoch: 5 - Batch: 2259, Training Loss: 0.1953124946375589\n",
      "Epoch: 5 - Batch: 2260, Training Loss: 0.19540098651156299\n",
      "Epoch: 5 - Batch: 2261, Training Loss: 0.1954858294853425\n",
      "Epoch: 5 - Batch: 2262, Training Loss: 0.19556603590597957\n",
      "Epoch: 5 - Batch: 2263, Training Loss: 0.1956452979130136\n",
      "Epoch: 5 - Batch: 2264, Training Loss: 0.1957319465158492\n",
      "Epoch: 5 - Batch: 2265, Training Loss: 0.19582002039127683\n",
      "Epoch: 5 - Batch: 2266, Training Loss: 0.1959025666401853\n",
      "Epoch: 5 - Batch: 2267, Training Loss: 0.19598690288181525\n",
      "Epoch: 5 - Batch: 2268, Training Loss: 0.19607151586988672\n",
      "Epoch: 5 - Batch: 2269, Training Loss: 0.19615765925416503\n",
      "Epoch: 5 - Batch: 2270, Training Loss: 0.19624135372032772\n",
      "Epoch: 5 - Batch: 2271, Training Loss: 0.19633245177777053\n",
      "Epoch: 5 - Batch: 2272, Training Loss: 0.19641127324914853\n",
      "Epoch: 5 - Batch: 2273, Training Loss: 0.196499381152601\n",
      "Epoch: 5 - Batch: 2274, Training Loss: 0.19658002521762405\n",
      "Epoch: 5 - Batch: 2275, Training Loss: 0.19666959365110095\n",
      "Epoch: 5 - Batch: 2276, Training Loss: 0.1967558697063729\n",
      "Epoch: 5 - Batch: 2277, Training Loss: 0.1968362925707009\n",
      "Epoch: 5 - Batch: 2278, Training Loss: 0.19692686058049574\n",
      "Epoch: 5 - Batch: 2279, Training Loss: 0.197011628560413\n",
      "Epoch: 5 - Batch: 2280, Training Loss: 0.1970989036607011\n",
      "Epoch: 5 - Batch: 2281, Training Loss: 0.197178761022255\n",
      "Epoch: 5 - Batch: 2282, Training Loss: 0.1972706021793545\n",
      "Epoch: 5 - Batch: 2283, Training Loss: 0.19735545127546017\n",
      "Epoch: 5 - Batch: 2284, Training Loss: 0.19743954198153854\n",
      "Epoch: 5 - Batch: 2285, Training Loss: 0.19752090559595853\n",
      "Epoch: 5 - Batch: 2286, Training Loss: 0.1976100526886002\n",
      "Epoch: 5 - Batch: 2287, Training Loss: 0.19769085812109027\n",
      "Epoch: 5 - Batch: 2288, Training Loss: 0.19777836754399153\n",
      "Epoch: 5 - Batch: 2289, Training Loss: 0.1978674196324637\n",
      "Epoch: 5 - Batch: 2290, Training Loss: 0.19795520334916922\n",
      "Epoch: 5 - Batch: 2291, Training Loss: 0.1980369513295282\n",
      "Epoch: 5 - Batch: 2292, Training Loss: 0.19811928133243945\n",
      "Epoch: 5 - Batch: 2293, Training Loss: 0.19820532443659816\n",
      "Epoch: 5 - Batch: 2294, Training Loss: 0.1982885500969084\n",
      "Epoch: 5 - Batch: 2295, Training Loss: 0.19838114435572926\n",
      "Epoch: 5 - Batch: 2296, Training Loss: 0.19846686725172633\n",
      "Epoch: 5 - Batch: 2297, Training Loss: 0.1985504283549675\n",
      "Epoch: 5 - Batch: 2298, Training Loss: 0.19863173694876493\n",
      "Epoch: 5 - Batch: 2299, Training Loss: 0.19872710276796648\n",
      "Epoch: 5 - Batch: 2300, Training Loss: 0.19880999162002386\n",
      "Epoch: 5 - Batch: 2301, Training Loss: 0.1988979862511751\n",
      "Epoch: 5 - Batch: 2302, Training Loss: 0.19899184516882817\n",
      "Epoch: 5 - Batch: 2303, Training Loss: 0.19907526021350674\n",
      "Epoch: 5 - Batch: 2304, Training Loss: 0.19916074988038385\n",
      "Epoch: 5 - Batch: 2305, Training Loss: 0.19924505187158364\n",
      "Epoch: 5 - Batch: 2306, Training Loss: 0.19933846338372524\n",
      "Epoch: 5 - Batch: 2307, Training Loss: 0.1994259105948963\n",
      "Epoch: 5 - Batch: 2308, Training Loss: 0.19951172268163309\n",
      "Epoch: 5 - Batch: 2309, Training Loss: 0.19959017621418138\n",
      "Epoch: 5 - Batch: 2310, Training Loss: 0.19967186059277647\n",
      "Epoch: 5 - Batch: 2311, Training Loss: 0.19976171758489228\n",
      "Epoch: 5 - Batch: 2312, Training Loss: 0.19984496914307473\n",
      "Epoch: 5 - Batch: 2313, Training Loss: 0.19992487291322023\n",
      "Epoch: 5 - Batch: 2314, Training Loss: 0.2000067511551811\n",
      "Epoch: 5 - Batch: 2315, Training Loss: 0.20009049481000277\n",
      "Epoch: 5 - Batch: 2316, Training Loss: 0.20017322053388378\n",
      "Epoch: 5 - Batch: 2317, Training Loss: 0.20025502780381324\n",
      "Epoch: 5 - Batch: 2318, Training Loss: 0.20033828639865514\n",
      "Epoch: 5 - Batch: 2319, Training Loss: 0.20042779889099832\n",
      "Epoch: 5 - Batch: 2320, Training Loss: 0.20050560808423937\n",
      "Epoch: 5 - Batch: 2321, Training Loss: 0.2005901031047847\n",
      "Epoch: 5 - Batch: 2322, Training Loss: 0.20067492077995097\n",
      "Epoch: 5 - Batch: 2323, Training Loss: 0.20076894842916065\n",
      "Epoch: 5 - Batch: 2324, Training Loss: 0.20085323462957766\n",
      "Epoch: 5 - Batch: 2325, Training Loss: 0.20093688641861698\n",
      "Epoch: 5 - Batch: 2326, Training Loss: 0.20102550240100714\n",
      "Epoch: 5 - Batch: 2327, Training Loss: 0.20111586404331092\n",
      "Epoch: 5 - Batch: 2328, Training Loss: 0.2012083336377322\n",
      "Epoch: 5 - Batch: 2329, Training Loss: 0.20129959002927958\n",
      "Epoch: 5 - Batch: 2330, Training Loss: 0.2013843277249961\n",
      "Epoch: 5 - Batch: 2331, Training Loss: 0.20146895599108233\n",
      "Epoch: 5 - Batch: 2332, Training Loss: 0.20154779300612596\n",
      "Epoch: 5 - Batch: 2333, Training Loss: 0.2016366902448447\n",
      "Epoch: 5 - Batch: 2334, Training Loss: 0.20172222609173007\n",
      "Epoch: 5 - Batch: 2335, Training Loss: 0.20181708940448453\n",
      "Epoch: 5 - Batch: 2336, Training Loss: 0.20191219516382683\n",
      "Epoch: 5 - Batch: 2337, Training Loss: 0.2019915039254164\n",
      "Epoch: 5 - Batch: 2338, Training Loss: 0.20208242955690198\n",
      "Epoch: 5 - Batch: 2339, Training Loss: 0.20216798368427488\n",
      "Epoch: 5 - Batch: 2340, Training Loss: 0.2022530133550242\n",
      "Epoch: 5 - Batch: 2341, Training Loss: 0.20233748476970848\n",
      "Epoch: 5 - Batch: 2342, Training Loss: 0.20243043003983757\n",
      "Epoch: 5 - Batch: 2343, Training Loss: 0.20251415695865355\n",
      "Epoch: 5 - Batch: 2344, Training Loss: 0.20262572813043941\n",
      "Epoch: 5 - Batch: 2345, Training Loss: 0.20270395510602945\n",
      "Epoch: 5 - Batch: 2346, Training Loss: 0.20279448750007212\n",
      "Epoch: 5 - Batch: 2347, Training Loss: 0.20288465223946975\n",
      "Epoch: 5 - Batch: 2348, Training Loss: 0.2029681670823896\n",
      "Epoch: 5 - Batch: 2349, Training Loss: 0.20305147036835922\n",
      "Epoch: 5 - Batch: 2350, Training Loss: 0.20314024868197306\n",
      "Epoch: 5 - Batch: 2351, Training Loss: 0.20322771142445392\n",
      "Epoch: 5 - Batch: 2352, Training Loss: 0.20331024288958183\n",
      "Epoch: 5 - Batch: 2353, Training Loss: 0.20339602594623715\n",
      "Epoch: 5 - Batch: 2354, Training Loss: 0.2034864909390905\n",
      "Epoch: 5 - Batch: 2355, Training Loss: 0.20357327402305248\n",
      "Epoch: 5 - Batch: 2356, Training Loss: 0.20365272921388028\n",
      "Epoch: 5 - Batch: 2357, Training Loss: 0.20374374488865954\n",
      "Epoch: 5 - Batch: 2358, Training Loss: 0.2038341490412826\n",
      "Epoch: 5 - Batch: 2359, Training Loss: 0.2039159474829536\n",
      "Epoch: 5 - Batch: 2360, Training Loss: 0.2040068343355881\n",
      "Epoch: 5 - Batch: 2361, Training Loss: 0.20409557000914616\n",
      "Epoch: 5 - Batch: 2362, Training Loss: 0.20419307555972444\n",
      "Epoch: 5 - Batch: 2363, Training Loss: 0.20428212024703943\n",
      "Epoch: 5 - Batch: 2364, Training Loss: 0.20437596508519568\n",
      "Epoch: 5 - Batch: 2365, Training Loss: 0.20446361303576585\n",
      "Epoch: 5 - Batch: 2366, Training Loss: 0.20455386636046627\n",
      "Epoch: 5 - Batch: 2367, Training Loss: 0.20464063836122626\n",
      "Epoch: 5 - Batch: 2368, Training Loss: 0.20472136847242392\n",
      "Epoch: 5 - Batch: 2369, Training Loss: 0.20481095961596243\n",
      "Epoch: 5 - Batch: 2370, Training Loss: 0.20489395500020205\n",
      "Epoch: 5 - Batch: 2371, Training Loss: 0.20497762013974277\n",
      "Epoch: 5 - Batch: 2372, Training Loss: 0.20506040827304767\n",
      "Epoch: 5 - Batch: 2373, Training Loss: 0.2051462930183901\n",
      "Epoch: 5 - Batch: 2374, Training Loss: 0.20523084847760042\n",
      "Epoch: 5 - Batch: 2375, Training Loss: 0.20531466888699365\n",
      "Epoch: 5 - Batch: 2376, Training Loss: 0.2053938447366504\n",
      "Epoch: 5 - Batch: 2377, Training Loss: 0.20547890391935955\n",
      "Epoch: 5 - Batch: 2378, Training Loss: 0.20557504150380146\n",
      "Epoch: 5 - Batch: 2379, Training Loss: 0.20566989131160637\n",
      "Epoch: 5 - Batch: 2380, Training Loss: 0.20575413849223312\n",
      "Epoch: 5 - Batch: 2381, Training Loss: 0.20584614448879487\n",
      "Epoch: 5 - Batch: 2382, Training Loss: 0.20592989283884144\n",
      "Epoch: 5 - Batch: 2383, Training Loss: 0.2060136344055236\n",
      "Epoch: 5 - Batch: 2384, Training Loss: 0.20610446401965954\n",
      "Epoch: 5 - Batch: 2385, Training Loss: 0.20619338467890153\n",
      "Epoch: 5 - Batch: 2386, Training Loss: 0.20627129957648258\n",
      "Epoch: 5 - Batch: 2387, Training Loss: 0.20635654063120015\n",
      "Epoch: 5 - Batch: 2388, Training Loss: 0.206441107383662\n",
      "Epoch: 5 - Batch: 2389, Training Loss: 0.20652646026504573\n",
      "Epoch: 5 - Batch: 2390, Training Loss: 0.20661537066892802\n",
      "Epoch: 5 - Batch: 2391, Training Loss: 0.20670465013602282\n",
      "Epoch: 5 - Batch: 2392, Training Loss: 0.2067914878266862\n",
      "Epoch: 5 - Batch: 2393, Training Loss: 0.2068680150505421\n",
      "Epoch: 5 - Batch: 2394, Training Loss: 0.20695638591747972\n",
      "Epoch: 5 - Batch: 2395, Training Loss: 0.20704005945282394\n",
      "Epoch: 5 - Batch: 2396, Training Loss: 0.20712936623253633\n",
      "Epoch: 5 - Batch: 2397, Training Loss: 0.20721462482639016\n",
      "Epoch: 5 - Batch: 2398, Training Loss: 0.20730613252985142\n",
      "Epoch: 5 - Batch: 2399, Training Loss: 0.2073988776413066\n",
      "Epoch: 5 - Batch: 2400, Training Loss: 0.2074775649960559\n",
      "Epoch: 5 - Batch: 2401, Training Loss: 0.20756608045738728\n",
      "Epoch: 5 - Batch: 2402, Training Loss: 0.20765112881018946\n",
      "Epoch: 5 - Batch: 2403, Training Loss: 0.2077345965217002\n",
      "Epoch: 5 - Batch: 2404, Training Loss: 0.20781737355944727\n",
      "Epoch: 5 - Batch: 2405, Training Loss: 0.20790195112328227\n",
      "Epoch: 5 - Batch: 2406, Training Loss: 0.20799200963939402\n",
      "Epoch: 5 - Batch: 2407, Training Loss: 0.2080916446659893\n",
      "Epoch: 5 - Batch: 2408, Training Loss: 0.20818146301516846\n",
      "Epoch: 5 - Batch: 2409, Training Loss: 0.20827158882744473\n",
      "Epoch: 5 - Batch: 2410, Training Loss: 0.20836043659331985\n",
      "Epoch: 5 - Batch: 2411, Training Loss: 0.20845374107286704\n",
      "Epoch: 5 - Batch: 2412, Training Loss: 0.20854321985843763\n",
      "Epoch 5 - Batch 2412, Training Loss: 0.20854321985843763, Validation Loss: 0.2087984591673065\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch: 6 - Batch: 1, Training Loss: 7.969316459611478e-05\n",
      "Epoch: 6 - Batch: 2, Training Loss: 0.00016863824518561165\n",
      "Epoch: 6 - Batch: 3, Training Loss: 0.0002578751957831691\n",
      "Epoch: 6 - Batch: 4, Training Loss: 0.0003486927197149539\n",
      "Epoch: 6 - Batch: 5, Training Loss: 0.00044380739380668843\n",
      "Epoch: 6 - Batch: 6, Training Loss: 0.0005314405111728814\n",
      "Epoch: 6 - Batch: 7, Training Loss: 0.0006152691866331433\n",
      "Epoch: 6 - Batch: 8, Training Loss: 0.000706988712695503\n",
      "Epoch: 6 - Batch: 9, Training Loss: 0.0007979622045856211\n",
      "Epoch: 6 - Batch: 10, Training Loss: 0.0008938035944702218\n",
      "Epoch: 6 - Batch: 11, Training Loss: 0.0009759437682023689\n",
      "Epoch: 6 - Batch: 12, Training Loss: 0.0010673379213556918\n",
      "Epoch: 6 - Batch: 13, Training Loss: 0.001147606695815303\n",
      "Epoch: 6 - Batch: 14, Training Loss: 0.0012291325426121453\n",
      "Epoch: 6 - Batch: 15, Training Loss: 0.0013105399770720878\n",
      "Epoch: 6 - Batch: 16, Training Loss: 0.001396711575945416\n",
      "Epoch: 6 - Batch: 17, Training Loss: 0.0014819766442672926\n",
      "Epoch: 6 - Batch: 18, Training Loss: 0.001563307497137617\n",
      "Epoch: 6 - Batch: 19, Training Loss: 0.0016569881321581244\n",
      "Epoch: 6 - Batch: 20, Training Loss: 0.0017444704092458312\n",
      "Epoch: 6 - Batch: 21, Training Loss: 0.0018253125262695364\n",
      "Epoch: 6 - Batch: 22, Training Loss: 0.001912534823811074\n",
      "Epoch: 6 - Batch: 23, Training Loss: 0.001994841579181045\n",
      "Epoch: 6 - Batch: 24, Training Loss: 0.0020889594014208905\n",
      "Epoch: 6 - Batch: 25, Training Loss: 0.0021779908741489176\n",
      "Epoch: 6 - Batch: 26, Training Loss: 0.002265177716564381\n",
      "Epoch: 6 - Batch: 27, Training Loss: 0.0023577282974376014\n",
      "Epoch: 6 - Batch: 28, Training Loss: 0.002447216702041341\n",
      "Epoch: 6 - Batch: 29, Training Loss: 0.00252365848515955\n",
      "Epoch: 6 - Batch: 30, Training Loss: 0.0026069022097298953\n",
      "Epoch: 6 - Batch: 31, Training Loss: 0.0026882847162522684\n",
      "Epoch: 6 - Batch: 32, Training Loss: 0.0027692000417171626\n",
      "Epoch: 6 - Batch: 33, Training Loss: 0.002860254754820471\n",
      "Epoch: 6 - Batch: 34, Training Loss: 0.0029462228712948597\n",
      "Epoch: 6 - Batch: 35, Training Loss: 0.0030383797699143835\n",
      "Epoch: 6 - Batch: 36, Training Loss: 0.0031237929108032143\n",
      "Epoch: 6 - Batch: 37, Training Loss: 0.0032077953404751583\n",
      "Epoch: 6 - Batch: 38, Training Loss: 0.0032872401214950712\n",
      "Epoch: 6 - Batch: 39, Training Loss: 0.003378379323935232\n",
      "Epoch: 6 - Batch: 40, Training Loss: 0.003467314970829396\n",
      "Epoch: 6 - Batch: 41, Training Loss: 0.003553234016025442\n",
      "Epoch: 6 - Batch: 42, Training Loss: 0.0036473780388559273\n",
      "Epoch: 6 - Batch: 43, Training Loss: 0.003736989983476414\n",
      "Epoch: 6 - Batch: 44, Training Loss: 0.0038220903914959275\n",
      "Epoch: 6 - Batch: 45, Training Loss: 0.0038988359309547575\n",
      "Epoch: 6 - Batch: 46, Training Loss: 0.00398494682551221\n",
      "Epoch: 6 - Batch: 47, Training Loss: 0.004070807866146711\n",
      "Epoch: 6 - Batch: 48, Training Loss: 0.004146471860545191\n",
      "Epoch: 6 - Batch: 49, Training Loss: 0.004237077598943441\n",
      "Epoch: 6 - Batch: 50, Training Loss: 0.0043189185818234095\n",
      "Epoch: 6 - Batch: 51, Training Loss: 0.004396976232775803\n",
      "Epoch: 6 - Batch: 52, Training Loss: 0.004487881260873073\n",
      "Epoch: 6 - Batch: 53, Training Loss: 0.004571243416314101\n",
      "Epoch: 6 - Batch: 54, Training Loss: 0.004660205362645746\n",
      "Epoch: 6 - Batch: 55, Training Loss: 0.004745337352230775\n",
      "Epoch: 6 - Batch: 56, Training Loss: 0.004825767179083073\n",
      "Epoch: 6 - Batch: 57, Training Loss: 0.004903565591840602\n",
      "Epoch: 6 - Batch: 58, Training Loss: 0.004994803950017562\n",
      "Epoch: 6 - Batch: 59, Training Loss: 0.0050890522253750564\n",
      "Epoch: 6 - Batch: 60, Training Loss: 0.005173224399783718\n",
      "Epoch: 6 - Batch: 61, Training Loss: 0.0052579926144622055\n",
      "Epoch: 6 - Batch: 62, Training Loss: 0.005341004160160251\n",
      "Epoch: 6 - Batch: 63, Training Loss: 0.005428399519985588\n",
      "Epoch: 6 - Batch: 64, Training Loss: 0.005511405907114149\n",
      "Epoch: 6 - Batch: 65, Training Loss: 0.00558858280800666\n",
      "Epoch: 6 - Batch: 66, Training Loss: 0.005672805542574198\n",
      "Epoch: 6 - Batch: 67, Training Loss: 0.005760570416600747\n",
      "Epoch: 6 - Batch: 68, Training Loss: 0.005847377860130955\n",
      "Epoch: 6 - Batch: 69, Training Loss: 0.005929806203292575\n",
      "Epoch: 6 - Batch: 70, Training Loss: 0.006021212959418052\n",
      "Epoch: 6 - Batch: 71, Training Loss: 0.006106579660825666\n",
      "Epoch: 6 - Batch: 72, Training Loss: 0.00619832407775803\n",
      "Epoch: 6 - Batch: 73, Training Loss: 0.006282538453589624\n",
      "Epoch: 6 - Batch: 74, Training Loss: 0.00636965418188133\n",
      "Epoch: 6 - Batch: 75, Training Loss: 0.006449701854492697\n",
      "Epoch: 6 - Batch: 76, Training Loss: 0.00653965720539267\n",
      "Epoch: 6 - Batch: 77, Training Loss: 0.00663396226208206\n",
      "Epoch: 6 - Batch: 78, Training Loss: 0.006719686029166922\n",
      "Epoch: 6 - Batch: 79, Training Loss: 0.0068061265408696225\n",
      "Epoch: 6 - Batch: 80, Training Loss: 0.006898330021062696\n",
      "Epoch: 6 - Batch: 81, Training Loss: 0.0069877262324539585\n",
      "Epoch: 6 - Batch: 82, Training Loss: 0.007075267063938761\n",
      "Epoch: 6 - Batch: 83, Training Loss: 0.0071616770257898425\n",
      "Epoch: 6 - Batch: 84, Training Loss: 0.007241549541799979\n",
      "Epoch: 6 - Batch: 85, Training Loss: 0.007330773092472731\n",
      "Epoch: 6 - Batch: 86, Training Loss: 0.007420553163212923\n",
      "Epoch: 6 - Batch: 87, Training Loss: 0.007504070891224923\n",
      "Epoch: 6 - Batch: 88, Training Loss: 0.0075931306773948035\n",
      "Epoch: 6 - Batch: 89, Training Loss: 0.007678327479617513\n",
      "Epoch: 6 - Batch: 90, Training Loss: 0.007761045437843645\n",
      "Epoch: 6 - Batch: 91, Training Loss: 0.007845235454107003\n",
      "Epoch: 6 - Batch: 92, Training Loss: 0.0079311966821922\n",
      "Epoch: 6 - Batch: 93, Training Loss: 0.008023209035841387\n",
      "Epoch: 6 - Batch: 94, Training Loss: 0.00811227166385793\n",
      "Epoch: 6 - Batch: 95, Training Loss: 0.008193272930472645\n",
      "Epoch: 6 - Batch: 96, Training Loss: 0.008272108697574925\n",
      "Epoch: 6 - Batch: 97, Training Loss: 0.008362564534739674\n",
      "Epoch: 6 - Batch: 98, Training Loss: 0.008447779469179672\n",
      "Epoch: 6 - Batch: 99, Training Loss: 0.008533421502283358\n",
      "Epoch: 6 - Batch: 100, Training Loss: 0.00862997596091892\n",
      "Epoch: 6 - Batch: 101, Training Loss: 0.00871971769761881\n",
      "Epoch: 6 - Batch: 102, Training Loss: 0.008802261648338233\n",
      "Epoch: 6 - Batch: 103, Training Loss: 0.008889989621603667\n",
      "Epoch: 6 - Batch: 104, Training Loss: 0.00898161313031641\n",
      "Epoch: 6 - Batch: 105, Training Loss: 0.009069413743653701\n",
      "Epoch: 6 - Batch: 106, Training Loss: 0.009156945901328256\n",
      "Epoch: 6 - Batch: 107, Training Loss: 0.00925043849876864\n",
      "Epoch: 6 - Batch: 108, Training Loss: 0.009335842446901312\n",
      "Epoch: 6 - Batch: 109, Training Loss: 0.009414398458911412\n",
      "Epoch: 6 - Batch: 110, Training Loss: 0.009506270301776936\n",
      "Epoch: 6 - Batch: 111, Training Loss: 0.009587039525433758\n",
      "Epoch: 6 - Batch: 112, Training Loss: 0.009674174022269288\n",
      "Epoch: 6 - Batch: 113, Training Loss: 0.009762499042806736\n",
      "Epoch: 6 - Batch: 114, Training Loss: 0.009847318713443591\n",
      "Epoch: 6 - Batch: 115, Training Loss: 0.00993659147748702\n",
      "Epoch: 6 - Batch: 116, Training Loss: 0.010015413881732061\n",
      "Epoch: 6 - Batch: 117, Training Loss: 0.010099772543061037\n",
      "Epoch: 6 - Batch: 118, Training Loss: 0.010186498407058257\n",
      "Epoch: 6 - Batch: 119, Training Loss: 0.010266846364014976\n",
      "Epoch: 6 - Batch: 120, Training Loss: 0.010348716389332244\n",
      "Epoch: 6 - Batch: 121, Training Loss: 0.010434716149151424\n",
      "Epoch: 6 - Batch: 122, Training Loss: 0.010528662268251526\n",
      "Epoch: 6 - Batch: 123, Training Loss: 0.010619706200871302\n",
      "Epoch: 6 - Batch: 124, Training Loss: 0.01070669851888274\n",
      "Epoch: 6 - Batch: 125, Training Loss: 0.010792510611797446\n",
      "Epoch: 6 - Batch: 126, Training Loss: 0.01087213504694983\n",
      "Epoch: 6 - Batch: 127, Training Loss: 0.010955273499635125\n",
      "Epoch: 6 - Batch: 128, Training Loss: 0.011041536377387655\n",
      "Epoch: 6 - Batch: 129, Training Loss: 0.011128739690157905\n",
      "Epoch: 6 - Batch: 130, Training Loss: 0.01122275883236137\n",
      "Epoch: 6 - Batch: 131, Training Loss: 0.01130972147160897\n",
      "Epoch: 6 - Batch: 132, Training Loss: 0.011391632172401075\n",
      "Epoch: 6 - Batch: 133, Training Loss: 0.01147591515411785\n",
      "Epoch: 6 - Batch: 134, Training Loss: 0.011571611171189825\n",
      "Epoch: 6 - Batch: 135, Training Loss: 0.011655749947722279\n",
      "Epoch: 6 - Batch: 136, Training Loss: 0.011740940547305751\n",
      "Epoch: 6 - Batch: 137, Training Loss: 0.011827817535766123\n",
      "Epoch: 6 - Batch: 138, Training Loss: 0.011913793467318834\n",
      "Epoch: 6 - Batch: 139, Training Loss: 0.011995673049889986\n",
      "Epoch: 6 - Batch: 140, Training Loss: 0.01208852935786271\n",
      "Epoch: 6 - Batch: 141, Training Loss: 0.012175078421929978\n",
      "Epoch: 6 - Batch: 142, Training Loss: 0.012274743040798118\n",
      "Epoch: 6 - Batch: 143, Training Loss: 0.012358954760120876\n",
      "Epoch: 6 - Batch: 144, Training Loss: 0.012443347838684102\n",
      "Epoch: 6 - Batch: 145, Training Loss: 0.012531059335416822\n",
      "Epoch: 6 - Batch: 146, Training Loss: 0.012622769915840123\n",
      "Epoch: 6 - Batch: 147, Training Loss: 0.012710522977669243\n",
      "Epoch: 6 - Batch: 148, Training Loss: 0.012796624141990842\n",
      "Epoch: 6 - Batch: 149, Training Loss: 0.012890361008507696\n",
      "Epoch: 6 - Batch: 150, Training Loss: 0.01297218026979448\n",
      "Epoch: 6 - Batch: 151, Training Loss: 0.013058954191889929\n",
      "Epoch: 6 - Batch: 152, Training Loss: 0.013148712627527924\n",
      "Epoch: 6 - Batch: 153, Training Loss: 0.013236402501514302\n",
      "Epoch: 6 - Batch: 154, Training Loss: 0.013318982636355246\n",
      "Epoch: 6 - Batch: 155, Training Loss: 0.013404744119687657\n",
      "Epoch: 6 - Batch: 156, Training Loss: 0.013489053567645957\n",
      "Epoch: 6 - Batch: 157, Training Loss: 0.013579260860608388\n",
      "Epoch: 6 - Batch: 158, Training Loss: 0.01366351235342856\n",
      "Epoch: 6 - Batch: 159, Training Loss: 0.013748061313459134\n",
      "Epoch: 6 - Batch: 160, Training Loss: 0.013835105433392881\n",
      "Epoch: 6 - Batch: 161, Training Loss: 0.013913389332407149\n",
      "Epoch: 6 - Batch: 162, Training Loss: 0.014002146573110203\n",
      "Epoch: 6 - Batch: 163, Training Loss: 0.014095625782981638\n",
      "Epoch: 6 - Batch: 164, Training Loss: 0.014184669871037675\n",
      "Epoch: 6 - Batch: 165, Training Loss: 0.014270116774300437\n",
      "Epoch: 6 - Batch: 166, Training Loss: 0.014353019223976294\n",
      "Epoch: 6 - Batch: 167, Training Loss: 0.014444404604472529\n",
      "Epoch: 6 - Batch: 168, Training Loss: 0.014540428959858754\n",
      "Epoch: 6 - Batch: 169, Training Loss: 0.014630702702609659\n",
      "Epoch: 6 - Batch: 170, Training Loss: 0.014710811382255348\n",
      "Epoch: 6 - Batch: 171, Training Loss: 0.01479653304884485\n",
      "Epoch: 6 - Batch: 172, Training Loss: 0.014878833916649889\n",
      "Epoch: 6 - Batch: 173, Training Loss: 0.014964177827682858\n",
      "Epoch: 6 - Batch: 174, Training Loss: 0.015050602573314512\n",
      "Epoch: 6 - Batch: 175, Training Loss: 0.015136681663901058\n",
      "Epoch: 6 - Batch: 176, Training Loss: 0.015228327876497461\n",
      "Epoch: 6 - Batch: 177, Training Loss: 0.01531514935627901\n",
      "Epoch: 6 - Batch: 178, Training Loss: 0.015403178089590215\n",
      "Epoch: 6 - Batch: 179, Training Loss: 0.015489702201008204\n",
      "Epoch: 6 - Batch: 180, Training Loss: 0.01558481057361386\n",
      "Epoch: 6 - Batch: 181, Training Loss: 0.015673385645767944\n",
      "Epoch: 6 - Batch: 182, Training Loss: 0.015764661411296076\n",
      "Epoch: 6 - Batch: 183, Training Loss: 0.015848237996423622\n",
      "Epoch: 6 - Batch: 184, Training Loss: 0.015928291494820643\n",
      "Epoch: 6 - Batch: 185, Training Loss: 0.016005613762347853\n",
      "Epoch: 6 - Batch: 186, Training Loss: 0.016084712048124516\n",
      "Epoch: 6 - Batch: 187, Training Loss: 0.0161691992906591\n",
      "Epoch: 6 - Batch: 188, Training Loss: 0.016257783123114412\n",
      "Epoch: 6 - Batch: 189, Training Loss: 0.016344044481096772\n",
      "Epoch: 6 - Batch: 190, Training Loss: 0.01642898383054567\n",
      "Epoch: 6 - Batch: 191, Training Loss: 0.016513352215932572\n",
      "Epoch: 6 - Batch: 192, Training Loss: 0.016592950981796084\n",
      "Epoch: 6 - Batch: 193, Training Loss: 0.01669087819446181\n",
      "Epoch: 6 - Batch: 194, Training Loss: 0.01677410894538435\n",
      "Epoch: 6 - Batch: 195, Training Loss: 0.01685292671000582\n",
      "Epoch: 6 - Batch: 196, Training Loss: 0.016947512337273825\n",
      "Epoch: 6 - Batch: 197, Training Loss: 0.017032382024411933\n",
      "Epoch: 6 - Batch: 198, Training Loss: 0.01712312564179672\n",
      "Epoch: 6 - Batch: 199, Training Loss: 0.017213927430547095\n",
      "Epoch: 6 - Batch: 200, Training Loss: 0.017300401037407553\n",
      "Epoch: 6 - Batch: 201, Training Loss: 0.017381443801161463\n",
      "Epoch: 6 - Batch: 202, Training Loss: 0.017462740570405626\n",
      "Epoch: 6 - Batch: 203, Training Loss: 0.017542665506130824\n",
      "Epoch: 6 - Batch: 204, Training Loss: 0.017625116837360767\n",
      "Epoch: 6 - Batch: 205, Training Loss: 0.017710426331159487\n",
      "Epoch: 6 - Batch: 206, Training Loss: 0.017797424487312436\n",
      "Epoch: 6 - Batch: 207, Training Loss: 0.017877626941720053\n",
      "Epoch: 6 - Batch: 208, Training Loss: 0.01796723878131577\n",
      "Epoch: 6 - Batch: 209, Training Loss: 0.01805222036307724\n",
      "Epoch: 6 - Batch: 210, Training Loss: 0.01813247802344523\n",
      "Epoch: 6 - Batch: 211, Training Loss: 0.01821653496097174\n",
      "Epoch: 6 - Batch: 212, Training Loss: 0.018298454397353368\n",
      "Epoch: 6 - Batch: 213, Training Loss: 0.01838512403603217\n",
      "Epoch: 6 - Batch: 214, Training Loss: 0.018470944308522922\n",
      "Epoch: 6 - Batch: 215, Training Loss: 0.018554436002748326\n",
      "Epoch: 6 - Batch: 216, Training Loss: 0.01864079849958222\n",
      "Epoch: 6 - Batch: 217, Training Loss: 0.018727929629447252\n",
      "Epoch: 6 - Batch: 218, Training Loss: 0.018813399075967557\n",
      "Epoch: 6 - Batch: 219, Training Loss: 0.018892574925624316\n",
      "Epoch: 6 - Batch: 220, Training Loss: 0.0189785437587383\n",
      "Epoch: 6 - Batch: 221, Training Loss: 0.01906833010007493\n",
      "Epoch: 6 - Batch: 222, Training Loss: 0.01916186564028955\n",
      "Epoch: 6 - Batch: 223, Training Loss: 0.01924325317564493\n",
      "Epoch: 6 - Batch: 224, Training Loss: 0.0193272856176888\n",
      "Epoch: 6 - Batch: 225, Training Loss: 0.019416001293295453\n",
      "Epoch: 6 - Batch: 226, Training Loss: 0.01950224290662144\n",
      "Epoch: 6 - Batch: 227, Training Loss: 0.019585820091007955\n",
      "Epoch: 6 - Batch: 228, Training Loss: 0.019676606410227802\n",
      "Epoch: 6 - Batch: 229, Training Loss: 0.019768944278878358\n",
      "Epoch: 6 - Batch: 230, Training Loss: 0.019856418061315717\n",
      "Epoch: 6 - Batch: 231, Training Loss: 0.01994106753004922\n",
      "Epoch: 6 - Batch: 232, Training Loss: 0.02002662830487215\n",
      "Epoch: 6 - Batch: 233, Training Loss: 0.02011513314634611\n",
      "Epoch: 6 - Batch: 234, Training Loss: 0.02020160305880591\n",
      "Epoch: 6 - Batch: 235, Training Loss: 0.02029309646654287\n",
      "Epoch: 6 - Batch: 236, Training Loss: 0.02038037165949989\n",
      "Epoch: 6 - Batch: 237, Training Loss: 0.020464512610662833\n",
      "Epoch: 6 - Batch: 238, Training Loss: 0.020542057987864733\n",
      "Epoch: 6 - Batch: 239, Training Loss: 0.020625986819886054\n",
      "Epoch: 6 - Batch: 240, Training Loss: 0.020709304364512415\n",
      "Epoch: 6 - Batch: 241, Training Loss: 0.02079574975677786\n",
      "Epoch: 6 - Batch: 242, Training Loss: 0.020890827715446304\n",
      "Epoch: 6 - Batch: 243, Training Loss: 0.020977135105166664\n",
      "Epoch: 6 - Batch: 244, Training Loss: 0.02105808736228231\n",
      "Epoch: 6 - Batch: 245, Training Loss: 0.021151029240729202\n",
      "Epoch: 6 - Batch: 246, Training Loss: 0.02123343285982486\n",
      "Epoch: 6 - Batch: 247, Training Loss: 0.021316232768259634\n",
      "Epoch: 6 - Batch: 248, Training Loss: 0.02139764856145552\n",
      "Epoch: 6 - Batch: 249, Training Loss: 0.02148348352505793\n",
      "Epoch: 6 - Batch: 250, Training Loss: 0.021564193693905526\n",
      "Epoch: 6 - Batch: 251, Training Loss: 0.021656459206668893\n",
      "Epoch: 6 - Batch: 252, Training Loss: 0.02174118176028503\n",
      "Epoch: 6 - Batch: 253, Training Loss: 0.021826116618380617\n",
      "Epoch: 6 - Batch: 254, Training Loss: 0.021916326932349607\n",
      "Epoch: 6 - Batch: 255, Training Loss: 0.0220032757886409\n",
      "Epoch: 6 - Batch: 256, Training Loss: 0.02208800415568684\n",
      "Epoch: 6 - Batch: 257, Training Loss: 0.022183591119982117\n",
      "Epoch: 6 - Batch: 258, Training Loss: 0.022273186946389686\n",
      "Epoch: 6 - Batch: 259, Training Loss: 0.022358000093135075\n",
      "Epoch: 6 - Batch: 260, Training Loss: 0.022448405518411207\n",
      "Epoch: 6 - Batch: 261, Training Loss: 0.02253975716742315\n",
      "Epoch: 6 - Batch: 262, Training Loss: 0.02262354104055299\n",
      "Epoch: 6 - Batch: 263, Training Loss: 0.022711425766373552\n",
      "Epoch: 6 - Batch: 264, Training Loss: 0.022796300309362697\n",
      "Epoch: 6 - Batch: 265, Training Loss: 0.022884177356019345\n",
      "Epoch: 6 - Batch: 266, Training Loss: 0.022985325931613124\n",
      "Epoch: 6 - Batch: 267, Training Loss: 0.023070169239000696\n",
      "Epoch: 6 - Batch: 268, Training Loss: 0.02314815340968309\n",
      "Epoch: 6 - Batch: 269, Training Loss: 0.023237022921863085\n",
      "Epoch: 6 - Batch: 270, Training Loss: 0.0233298987088413\n",
      "Epoch: 6 - Batch: 271, Training Loss: 0.023418550671481374\n",
      "Epoch: 6 - Batch: 272, Training Loss: 0.023505972750843263\n",
      "Epoch: 6 - Batch: 273, Training Loss: 0.023595010883377163\n",
      "Epoch: 6 - Batch: 274, Training Loss: 0.02367993871716915\n",
      "Epoch: 6 - Batch: 275, Training Loss: 0.02376313468018179\n",
      "Epoch: 6 - Batch: 276, Training Loss: 0.023847273580272794\n",
      "Epoch: 6 - Batch: 277, Training Loss: 0.02393092019220885\n",
      "Epoch: 6 - Batch: 278, Training Loss: 0.024014748336367346\n",
      "Epoch: 6 - Batch: 279, Training Loss: 0.02410785278079917\n",
      "Epoch: 6 - Batch: 280, Training Loss: 0.0241936680429907\n",
      "Epoch: 6 - Batch: 281, Training Loss: 0.024280329174663298\n",
      "Epoch: 6 - Batch: 282, Training Loss: 0.024364684018033062\n",
      "Epoch: 6 - Batch: 283, Training Loss: 0.024451359420718244\n",
      "Epoch: 6 - Batch: 284, Training Loss: 0.02454677265551355\n",
      "Epoch: 6 - Batch: 285, Training Loss: 0.024641931365872696\n",
      "Epoch: 6 - Batch: 286, Training Loss: 0.02472529867988321\n",
      "Epoch: 6 - Batch: 287, Training Loss: 0.02480683139653546\n",
      "Epoch: 6 - Batch: 288, Training Loss: 0.0248981746064984\n",
      "Epoch: 6 - Batch: 289, Training Loss: 0.024980392430601624\n",
      "Epoch: 6 - Batch: 290, Training Loss: 0.02506452696289787\n",
      "Epoch: 6 - Batch: 291, Training Loss: 0.025154954678136516\n",
      "Epoch: 6 - Batch: 292, Training Loss: 0.02523995437975942\n",
      "Epoch: 6 - Batch: 293, Training Loss: 0.025319984068423756\n",
      "Epoch: 6 - Batch: 294, Training Loss: 0.025402323572987544\n",
      "Epoch: 6 - Batch: 295, Training Loss: 0.02548981137314246\n",
      "Epoch: 6 - Batch: 296, Training Loss: 0.025576552521332382\n",
      "Epoch: 6 - Batch: 297, Training Loss: 0.025660291073185886\n",
      "Epoch: 6 - Batch: 298, Training Loss: 0.02574881838996019\n",
      "Epoch: 6 - Batch: 299, Training Loss: 0.02583623502039\n",
      "Epoch: 6 - Batch: 300, Training Loss: 0.02592017623915601\n",
      "Epoch: 6 - Batch: 301, Training Loss: 0.02601224530617989\n",
      "Epoch: 6 - Batch: 302, Training Loss: 0.02609244040267582\n",
      "Epoch: 6 - Batch: 303, Training Loss: 0.026186537745085916\n",
      "Epoch: 6 - Batch: 304, Training Loss: 0.026271705356601064\n",
      "Epoch: 6 - Batch: 305, Training Loss: 0.02635949258854733\n",
      "Epoch: 6 - Batch: 306, Training Loss: 0.0264439285666393\n",
      "Epoch: 6 - Batch: 307, Training Loss: 0.026532342302216027\n",
      "Epoch: 6 - Batch: 308, Training Loss: 0.02662066763782778\n",
      "Epoch: 6 - Batch: 309, Training Loss: 0.026714306312364527\n",
      "Epoch: 6 - Batch: 310, Training Loss: 0.026794074310196772\n",
      "Epoch: 6 - Batch: 311, Training Loss: 0.026880971704351764\n",
      "Epoch: 6 - Batch: 312, Training Loss: 0.026966875873098328\n",
      "Epoch: 6 - Batch: 313, Training Loss: 0.027052314442336856\n",
      "Epoch: 6 - Batch: 314, Training Loss: 0.027146292766973154\n",
      "Epoch: 6 - Batch: 315, Training Loss: 0.027222695721816858\n",
      "Epoch: 6 - Batch: 316, Training Loss: 0.027304998764252388\n",
      "Epoch: 6 - Batch: 317, Training Loss: 0.027393806867832767\n",
      "Epoch: 6 - Batch: 318, Training Loss: 0.027477072307818366\n",
      "Epoch: 6 - Batch: 319, Training Loss: 0.02756029970412626\n",
      "Epoch: 6 - Batch: 320, Training Loss: 0.02764186911766802\n",
      "Epoch: 6 - Batch: 321, Training Loss: 0.02772764778181688\n",
      "Epoch: 6 - Batch: 322, Training Loss: 0.02780356523804799\n",
      "Epoch: 6 - Batch: 323, Training Loss: 0.027892095266932475\n",
      "Epoch: 6 - Batch: 324, Training Loss: 0.027975159448573046\n",
      "Epoch: 6 - Batch: 325, Training Loss: 0.028061653837981707\n",
      "Epoch: 6 - Batch: 326, Training Loss: 0.028141854037445773\n",
      "Epoch: 6 - Batch: 327, Training Loss: 0.02823210461914638\n",
      "Epoch: 6 - Batch: 328, Training Loss: 0.028327503101050755\n",
      "Epoch: 6 - Batch: 329, Training Loss: 0.02841399980100431\n",
      "Epoch: 6 - Batch: 330, Training Loss: 0.028507506206112716\n",
      "Epoch: 6 - Batch: 331, Training Loss: 0.02859481220839431\n",
      "Epoch: 6 - Batch: 332, Training Loss: 0.028679335808032384\n",
      "Epoch: 6 - Batch: 333, Training Loss: 0.028760535954490032\n",
      "Epoch: 6 - Batch: 334, Training Loss: 0.028844152381072196\n",
      "Epoch: 6 - Batch: 335, Training Loss: 0.02893403192262349\n",
      "Epoch: 6 - Batch: 336, Training Loss: 0.029023618926950552\n",
      "Epoch: 6 - Batch: 337, Training Loss: 0.029105323853332605\n",
      "Epoch: 6 - Batch: 338, Training Loss: 0.029192211696411643\n",
      "Epoch: 6 - Batch: 339, Training Loss: 0.02928862247498672\n",
      "Epoch: 6 - Batch: 340, Training Loss: 0.02936767678627525\n",
      "Epoch: 6 - Batch: 341, Training Loss: 0.02945584617246245\n",
      "Epoch: 6 - Batch: 342, Training Loss: 0.029547292881245243\n",
      "Epoch: 6 - Batch: 343, Training Loss: 0.029636148511622083\n",
      "Epoch: 6 - Batch: 344, Training Loss: 0.029723977975880923\n",
      "Epoch: 6 - Batch: 345, Training Loss: 0.02981007016985175\n",
      "Epoch: 6 - Batch: 346, Training Loss: 0.029894819047617083\n",
      "Epoch: 6 - Batch: 347, Training Loss: 0.02998580682905357\n",
      "Epoch: 6 - Batch: 348, Training Loss: 0.030070997861091967\n",
      "Epoch: 6 - Batch: 349, Training Loss: 0.030157057670366704\n",
      "Epoch: 6 - Batch: 350, Training Loss: 0.030249116519098455\n",
      "Epoch: 6 - Batch: 351, Training Loss: 0.03033372109901055\n",
      "Epoch: 6 - Batch: 352, Training Loss: 0.030419042713903076\n",
      "Epoch: 6 - Batch: 353, Training Loss: 0.030502609519371345\n",
      "Epoch: 6 - Batch: 354, Training Loss: 0.03059341080770959\n",
      "Epoch: 6 - Batch: 355, Training Loss: 0.030674620554666614\n",
      "Epoch: 6 - Batch: 356, Training Loss: 0.030762957560383462\n",
      "Epoch: 6 - Batch: 357, Training Loss: 0.03084436518018123\n",
      "Epoch: 6 - Batch: 358, Training Loss: 0.030933808065814958\n",
      "Epoch: 6 - Batch: 359, Training Loss: 0.031020471563633797\n",
      "Epoch: 6 - Batch: 360, Training Loss: 0.03111066366507244\n",
      "Epoch: 6 - Batch: 361, Training Loss: 0.031196984473251387\n",
      "Epoch: 6 - Batch: 362, Training Loss: 0.031282370981094654\n",
      "Epoch: 6 - Batch: 363, Training Loss: 0.03137787889880723\n",
      "Epoch: 6 - Batch: 364, Training Loss: 0.03145794550676646\n",
      "Epoch: 6 - Batch: 365, Training Loss: 0.03153353620623277\n",
      "Epoch: 6 - Batch: 366, Training Loss: 0.03161826006956955\n",
      "Epoch: 6 - Batch: 367, Training Loss: 0.031699496949984265\n",
      "Epoch: 6 - Batch: 368, Training Loss: 0.03178504703699257\n",
      "Epoch: 6 - Batch: 369, Training Loss: 0.031873567483911465\n",
      "Epoch: 6 - Batch: 370, Training Loss: 0.03195958698838702\n",
      "Epoch: 6 - Batch: 371, Training Loss: 0.032044231582735706\n",
      "Epoch: 6 - Batch: 372, Training Loss: 0.03213091164975617\n",
      "Epoch: 6 - Batch: 373, Training Loss: 0.03222161849611632\n",
      "Epoch: 6 - Batch: 374, Training Loss: 0.03230278508407164\n",
      "Epoch: 6 - Batch: 375, Training Loss: 0.03239236485404557\n",
      "Epoch: 6 - Batch: 376, Training Loss: 0.03247772856533626\n",
      "Epoch: 6 - Batch: 377, Training Loss: 0.03256342280192755\n",
      "Epoch: 6 - Batch: 378, Training Loss: 0.03265352883247989\n",
      "Epoch: 6 - Batch: 379, Training Loss: 0.03274310766958676\n",
      "Epoch: 6 - Batch: 380, Training Loss: 0.03283963069492707\n",
      "Epoch: 6 - Batch: 381, Training Loss: 0.03291834387341344\n",
      "Epoch: 6 - Batch: 382, Training Loss: 0.03300005618859682\n",
      "Epoch: 6 - Batch: 383, Training Loss: 0.03308186469411178\n",
      "Epoch: 6 - Batch: 384, Training Loss: 0.03316398590160642\n",
      "Epoch: 6 - Batch: 385, Training Loss: 0.033253261186244276\n",
      "Epoch: 6 - Batch: 386, Training Loss: 0.03334225430639822\n",
      "Epoch: 6 - Batch: 387, Training Loss: 0.033423003383833375\n",
      "Epoch: 6 - Batch: 388, Training Loss: 0.033513457318196445\n",
      "Epoch: 6 - Batch: 389, Training Loss: 0.03359953928151929\n",
      "Epoch: 6 - Batch: 390, Training Loss: 0.03368325367767617\n",
      "Epoch: 6 - Batch: 391, Training Loss: 0.033761926817014246\n",
      "Epoch: 6 - Batch: 392, Training Loss: 0.033838843219414674\n",
      "Epoch: 6 - Batch: 393, Training Loss: 0.033925937677101906\n",
      "Epoch: 6 - Batch: 394, Training Loss: 0.034009880557730424\n",
      "Epoch: 6 - Batch: 395, Training Loss: 0.034103911851917336\n",
      "Epoch: 6 - Batch: 396, Training Loss: 0.03419519512980534\n",
      "Epoch: 6 - Batch: 397, Training Loss: 0.03428050218332269\n",
      "Epoch: 6 - Batch: 398, Training Loss: 0.03436297569331245\n",
      "Epoch: 6 - Batch: 399, Training Loss: 0.03444374968632932\n",
      "Epoch: 6 - Batch: 400, Training Loss: 0.03452722337189598\n",
      "Epoch: 6 - Batch: 401, Training Loss: 0.034609922184724716\n",
      "Epoch: 6 - Batch: 402, Training Loss: 0.034694512332998105\n",
      "Epoch: 6 - Batch: 403, Training Loss: 0.034787550685961253\n",
      "Epoch: 6 - Batch: 404, Training Loss: 0.034869730120422826\n",
      "Epoch: 6 - Batch: 405, Training Loss: 0.0349542485800252\n",
      "Epoch: 6 - Batch: 406, Training Loss: 0.0350356569473522\n",
      "Epoch: 6 - Batch: 407, Training Loss: 0.03512022512073738\n",
      "Epoch: 6 - Batch: 408, Training Loss: 0.035206822255258736\n",
      "Epoch: 6 - Batch: 409, Training Loss: 0.03529499734985097\n",
      "Epoch: 6 - Batch: 410, Training Loss: 0.03538092822577823\n",
      "Epoch: 6 - Batch: 411, Training Loss: 0.03546811499405856\n",
      "Epoch: 6 - Batch: 412, Training Loss: 0.03554765247488101\n",
      "Epoch: 6 - Batch: 413, Training Loss: 0.03563508209131448\n",
      "Epoch: 6 - Batch: 414, Training Loss: 0.035721077378314134\n",
      "Epoch: 6 - Batch: 415, Training Loss: 0.03580916532088275\n",
      "Epoch: 6 - Batch: 416, Training Loss: 0.03588671893943997\n",
      "Epoch: 6 - Batch: 417, Training Loss: 0.03597709185028353\n",
      "Epoch: 6 - Batch: 418, Training Loss: 0.03606195731789714\n",
      "Epoch: 6 - Batch: 419, Training Loss: 0.036151070098329345\n",
      "Epoch: 6 - Batch: 420, Training Loss: 0.03623585307207669\n",
      "Epoch: 6 - Batch: 421, Training Loss: 0.03632216286501086\n",
      "Epoch: 6 - Batch: 422, Training Loss: 0.036400699960206874\n",
      "Epoch: 6 - Batch: 423, Training Loss: 0.03648866220048411\n",
      "Epoch: 6 - Batch: 424, Training Loss: 0.03657386110938011\n",
      "Epoch: 6 - Batch: 425, Training Loss: 0.036664316415243085\n",
      "Epoch: 6 - Batch: 426, Training Loss: 0.03675729739379329\n",
      "Epoch: 6 - Batch: 427, Training Loss: 0.03684819673819724\n",
      "Epoch: 6 - Batch: 428, Training Loss: 0.03694202878804349\n",
      "Epoch: 6 - Batch: 429, Training Loss: 0.03702236741015172\n",
      "Epoch: 6 - Batch: 430, Training Loss: 0.03711172283486545\n",
      "Epoch: 6 - Batch: 431, Training Loss: 0.03720069776720075\n",
      "Epoch: 6 - Batch: 432, Training Loss: 0.03728418495153906\n",
      "Epoch: 6 - Batch: 433, Training Loss: 0.03736689515646615\n",
      "Epoch: 6 - Batch: 434, Training Loss: 0.03744867426740194\n",
      "Epoch: 6 - Batch: 435, Training Loss: 0.03753719963812907\n",
      "Epoch: 6 - Batch: 436, Training Loss: 0.03763421121397817\n",
      "Epoch: 6 - Batch: 437, Training Loss: 0.03772821908764183\n",
      "Epoch: 6 - Batch: 438, Training Loss: 0.037804886562808435\n",
      "Epoch: 6 - Batch: 439, Training Loss: 0.03789696028181174\n",
      "Epoch: 6 - Batch: 440, Training Loss: 0.03797761443539045\n",
      "Epoch: 6 - Batch: 441, Training Loss: 0.03806345298836876\n",
      "Epoch: 6 - Batch: 442, Training Loss: 0.03815107425029202\n",
      "Epoch: 6 - Batch: 443, Training Loss: 0.038231410302382404\n",
      "Epoch: 6 - Batch: 444, Training Loss: 0.038316254653839726\n",
      "Epoch: 6 - Batch: 445, Training Loss: 0.038399567651511424\n",
      "Epoch: 6 - Batch: 446, Training Loss: 0.038489122141405915\n",
      "Epoch: 6 - Batch: 447, Training Loss: 0.038564556675220206\n",
      "Epoch: 6 - Batch: 448, Training Loss: 0.0386541452116733\n",
      "Epoch: 6 - Batch: 449, Training Loss: 0.03874801634308908\n",
      "Epoch: 6 - Batch: 450, Training Loss: 0.038828056064707725\n",
      "Epoch: 6 - Batch: 451, Training Loss: 0.038916596268638845\n",
      "Epoch: 6 - Batch: 452, Training Loss: 0.039004247518222325\n",
      "Epoch: 6 - Batch: 453, Training Loss: 0.03909354900345083\n",
      "Epoch: 6 - Batch: 454, Training Loss: 0.0391793938332035\n",
      "Epoch: 6 - Batch: 455, Training Loss: 0.03926127761676537\n",
      "Epoch: 6 - Batch: 456, Training Loss: 0.039342433906115505\n",
      "Epoch: 6 - Batch: 457, Training Loss: 0.03943293074000732\n",
      "Epoch: 6 - Batch: 458, Training Loss: 0.039527761871937295\n",
      "Epoch: 6 - Batch: 459, Training Loss: 0.03960999419563643\n",
      "Epoch: 6 - Batch: 460, Training Loss: 0.03968778632544166\n",
      "Epoch: 6 - Batch: 461, Training Loss: 0.0397771585170507\n",
      "Epoch: 6 - Batch: 462, Training Loss: 0.03987185855335857\n",
      "Epoch: 6 - Batch: 463, Training Loss: 0.03996863285972309\n",
      "Epoch: 6 - Batch: 464, Training Loss: 0.040065006323615905\n",
      "Epoch: 6 - Batch: 465, Training Loss: 0.040157175028254936\n",
      "Epoch: 6 - Batch: 466, Training Loss: 0.040247985787356076\n",
      "Epoch: 6 - Batch: 467, Training Loss: 0.04033200587354489\n",
      "Epoch: 6 - Batch: 468, Training Loss: 0.04041076936507304\n",
      "Epoch: 6 - Batch: 469, Training Loss: 0.04049859875519675\n",
      "Epoch: 6 - Batch: 470, Training Loss: 0.04058628537959325\n",
      "Epoch: 6 - Batch: 471, Training Loss: 0.04067554488580421\n",
      "Epoch: 6 - Batch: 472, Training Loss: 0.040765939709756706\n",
      "Epoch: 6 - Batch: 473, Training Loss: 0.040857300576236516\n",
      "Epoch: 6 - Batch: 474, Training Loss: 0.04094105465322189\n",
      "Epoch: 6 - Batch: 475, Training Loss: 0.0410242434498386\n",
      "Epoch: 6 - Batch: 476, Training Loss: 0.04110612104311709\n",
      "Epoch: 6 - Batch: 477, Training Loss: 0.04119340655939101\n",
      "Epoch: 6 - Batch: 478, Training Loss: 0.041285686225786336\n",
      "Epoch: 6 - Batch: 479, Training Loss: 0.04136665848703131\n",
      "Epoch: 6 - Batch: 480, Training Loss: 0.04145744489892007\n",
      "Epoch: 6 - Batch: 481, Training Loss: 0.041542991835185346\n",
      "Epoch: 6 - Batch: 482, Training Loss: 0.04162903338868424\n",
      "Epoch: 6 - Batch: 483, Training Loss: 0.041714726358800386\n",
      "Epoch: 6 - Batch: 484, Training Loss: 0.041813286203600676\n",
      "Epoch: 6 - Batch: 485, Training Loss: 0.04190063306669493\n",
      "Epoch: 6 - Batch: 486, Training Loss: 0.04198359248304051\n",
      "Epoch: 6 - Batch: 487, Training Loss: 0.04206010829626427\n",
      "Epoch: 6 - Batch: 488, Training Loss: 0.04214491578402804\n",
      "Epoch: 6 - Batch: 489, Training Loss: 0.042235113006968604\n",
      "Epoch: 6 - Batch: 490, Training Loss: 0.04232825969606885\n",
      "Epoch: 6 - Batch: 491, Training Loss: 0.04241549350308937\n",
      "Epoch: 6 - Batch: 492, Training Loss: 0.042490527163542326\n",
      "Epoch: 6 - Batch: 493, Training Loss: 0.042579811134346286\n",
      "Epoch: 6 - Batch: 494, Training Loss: 0.042662412008489935\n",
      "Epoch: 6 - Batch: 495, Training Loss: 0.04275402509503894\n",
      "Epoch: 6 - Batch: 496, Training Loss: 0.04283980374683195\n",
      "Epoch: 6 - Batch: 497, Training Loss: 0.04292812208902974\n",
      "Epoch: 6 - Batch: 498, Training Loss: 0.04301823556398476\n",
      "Epoch: 6 - Batch: 499, Training Loss: 0.04309726574428837\n",
      "Epoch: 6 - Batch: 500, Training Loss: 0.043183956567080656\n",
      "Epoch: 6 - Batch: 501, Training Loss: 0.04326754631737176\n",
      "Epoch: 6 - Batch: 502, Training Loss: 0.04334944781305185\n",
      "Epoch: 6 - Batch: 503, Training Loss: 0.043434882848515835\n",
      "Epoch: 6 - Batch: 504, Training Loss: 0.04351942970187312\n",
      "Epoch: 6 - Batch: 505, Training Loss: 0.043607727755756914\n",
      "Epoch: 6 - Batch: 506, Training Loss: 0.04369578826214939\n",
      "Epoch: 6 - Batch: 507, Training Loss: 0.043786768203795845\n",
      "Epoch: 6 - Batch: 508, Training Loss: 0.0438738986664447\n",
      "Epoch: 6 - Batch: 509, Training Loss: 0.04395140480762888\n",
      "Epoch: 6 - Batch: 510, Training Loss: 0.044035284809311036\n",
      "Epoch: 6 - Batch: 511, Training Loss: 0.04412174425018368\n",
      "Epoch: 6 - Batch: 512, Training Loss: 0.04420750502923235\n",
      "Epoch: 6 - Batch: 513, Training Loss: 0.04428808101349407\n",
      "Epoch: 6 - Batch: 514, Training Loss: 0.0443789718261801\n",
      "Epoch: 6 - Batch: 515, Training Loss: 0.04446788407108479\n",
      "Epoch: 6 - Batch: 516, Training Loss: 0.044555753691872556\n",
      "Epoch: 6 - Batch: 517, Training Loss: 0.04463355465611415\n",
      "Epoch: 6 - Batch: 518, Training Loss: 0.04471800499788762\n",
      "Epoch: 6 - Batch: 519, Training Loss: 0.04480187399668085\n",
      "Epoch: 6 - Batch: 520, Training Loss: 0.044882931599816676\n",
      "Epoch: 6 - Batch: 521, Training Loss: 0.04496435699968986\n",
      "Epoch: 6 - Batch: 522, Training Loss: 0.04505562761282644\n",
      "Epoch: 6 - Batch: 523, Training Loss: 0.04514932122767268\n",
      "Epoch: 6 - Batch: 524, Training Loss: 0.045248472454585444\n",
      "Epoch: 6 - Batch: 525, Training Loss: 0.045332313170826456\n",
      "Epoch: 6 - Batch: 526, Training Loss: 0.04542406980152154\n",
      "Epoch: 6 - Batch: 527, Training Loss: 0.04550853442666345\n",
      "Epoch: 6 - Batch: 528, Training Loss: 0.04558694727138105\n",
      "Epoch: 6 - Batch: 529, Training Loss: 0.04567592062216691\n",
      "Epoch: 6 - Batch: 530, Training Loss: 0.04576763053537403\n",
      "Epoch: 6 - Batch: 531, Training Loss: 0.045846437414487205\n",
      "Epoch: 6 - Batch: 532, Training Loss: 0.045936535636139156\n",
      "Epoch: 6 - Batch: 533, Training Loss: 0.04601876350555254\n",
      "Epoch: 6 - Batch: 534, Training Loss: 0.04610342523747218\n",
      "Epoch: 6 - Batch: 535, Training Loss: 0.04619521941143284\n",
      "Epoch: 6 - Batch: 536, Training Loss: 0.04627890892289764\n",
      "Epoch: 6 - Batch: 537, Training Loss: 0.04636445698972365\n",
      "Epoch: 6 - Batch: 538, Training Loss: 0.046447547192794966\n",
      "Epoch: 6 - Batch: 539, Training Loss: 0.04653286329079821\n",
      "Epoch: 6 - Batch: 540, Training Loss: 0.04662099840184349\n",
      "Epoch: 6 - Batch: 541, Training Loss: 0.04670028715930372\n",
      "Epoch: 6 - Batch: 542, Training Loss: 0.04678851935030216\n",
      "Epoch: 6 - Batch: 543, Training Loss: 0.04687391348788592\n",
      "Epoch: 6 - Batch: 544, Training Loss: 0.04695964272861457\n",
      "Epoch: 6 - Batch: 545, Training Loss: 0.0470429342765219\n",
      "Epoch: 6 - Batch: 546, Training Loss: 0.047122213482906176\n",
      "Epoch: 6 - Batch: 547, Training Loss: 0.04720408390978873\n",
      "Epoch: 6 - Batch: 548, Training Loss: 0.04729264946175649\n",
      "Epoch: 6 - Batch: 549, Training Loss: 0.047386602957302064\n",
      "Epoch: 6 - Batch: 550, Training Loss: 0.047468603880547765\n",
      "Epoch: 6 - Batch: 551, Training Loss: 0.04755489553508672\n",
      "Epoch: 6 - Batch: 552, Training Loss: 0.0476438742544916\n",
      "Epoch: 6 - Batch: 553, Training Loss: 0.04773287707399175\n",
      "Epoch: 6 - Batch: 554, Training Loss: 0.047819344651194945\n",
      "Epoch: 6 - Batch: 555, Training Loss: 0.04789909897109565\n",
      "Epoch: 6 - Batch: 556, Training Loss: 0.047987205966392756\n",
      "Epoch: 6 - Batch: 557, Training Loss: 0.048075761090018855\n",
      "Epoch: 6 - Batch: 558, Training Loss: 0.04815914080312991\n",
      "Epoch: 6 - Batch: 559, Training Loss: 0.0482373233285314\n",
      "Epoch: 6 - Batch: 560, Training Loss: 0.04832309628427523\n",
      "Epoch: 6 - Batch: 561, Training Loss: 0.04840267399576173\n",
      "Epoch: 6 - Batch: 562, Training Loss: 0.04848997606888142\n",
      "Epoch: 6 - Batch: 563, Training Loss: 0.048576671024707224\n",
      "Epoch: 6 - Batch: 564, Training Loss: 0.04865351450902906\n",
      "Epoch: 6 - Batch: 565, Training Loss: 0.04875649279524043\n",
      "Epoch: 6 - Batch: 566, Training Loss: 0.04884416379565821\n",
      "Epoch: 6 - Batch: 567, Training Loss: 0.04893422076728807\n",
      "Epoch: 6 - Batch: 568, Training Loss: 0.0490218771445514\n",
      "Epoch: 6 - Batch: 569, Training Loss: 0.049106667019043794\n",
      "Epoch: 6 - Batch: 570, Training Loss: 0.049189521175860174\n",
      "Epoch: 6 - Batch: 571, Training Loss: 0.049277298337784575\n",
      "Epoch: 6 - Batch: 572, Training Loss: 0.049354183448982084\n",
      "Epoch: 6 - Batch: 573, Training Loss: 0.04944305449946603\n",
      "Epoch: 6 - Batch: 574, Training Loss: 0.04953432658294935\n",
      "Epoch: 6 - Batch: 575, Training Loss: 0.04962443399117954\n",
      "Epoch: 6 - Batch: 576, Training Loss: 0.04970213300726109\n",
      "Epoch: 6 - Batch: 577, Training Loss: 0.049788892089974626\n",
      "Epoch: 6 - Batch: 578, Training Loss: 0.049868219706599\n",
      "Epoch: 6 - Batch: 579, Training Loss: 0.049961588331567705\n",
      "Epoch: 6 - Batch: 580, Training Loss: 0.050044666512392054\n",
      "Epoch: 6 - Batch: 581, Training Loss: 0.05013226945058228\n",
      "Epoch: 6 - Batch: 582, Training Loss: 0.05022157553218886\n",
      "Epoch: 6 - Batch: 583, Training Loss: 0.05031190360981236\n",
      "Epoch: 6 - Batch: 584, Training Loss: 0.05040360357655617\n",
      "Epoch: 6 - Batch: 585, Training Loss: 0.050483055987316575\n",
      "Epoch: 6 - Batch: 586, Training Loss: 0.05056833027928425\n",
      "Epoch: 6 - Batch: 587, Training Loss: 0.050648171497913536\n",
      "Epoch: 6 - Batch: 588, Training Loss: 0.05074229759857627\n",
      "Epoch: 6 - Batch: 589, Training Loss: 0.050824056272927805\n",
      "Epoch: 6 - Batch: 590, Training Loss: 0.050922602847539765\n",
      "Epoch: 6 - Batch: 591, Training Loss: 0.05100827156311244\n",
      "Epoch: 6 - Batch: 592, Training Loss: 0.051095421418275805\n",
      "Epoch: 6 - Batch: 593, Training Loss: 0.05118213581603953\n",
      "Epoch: 6 - Batch: 594, Training Loss: 0.0512681128596192\n",
      "Epoch: 6 - Batch: 595, Training Loss: 0.051350457683378585\n",
      "Epoch: 6 - Batch: 596, Training Loss: 0.05144899549768932\n",
      "Epoch: 6 - Batch: 597, Training Loss: 0.0515392660032062\n",
      "Epoch: 6 - Batch: 598, Training Loss: 0.05161736938935607\n",
      "Epoch: 6 - Batch: 599, Training Loss: 0.05170970977860106\n",
      "Epoch: 6 - Batch: 600, Training Loss: 0.05179149718649352\n",
      "Epoch: 6 - Batch: 601, Training Loss: 0.051877063774746254\n",
      "Epoch: 6 - Batch: 602, Training Loss: 0.05195718435308036\n",
      "Epoch: 6 - Batch: 603, Training Loss: 0.05204642284198187\n",
      "Epoch: 6 - Batch: 604, Training Loss: 0.05213204492037964\n",
      "Epoch: 6 - Batch: 605, Training Loss: 0.05222124052210827\n",
      "Epoch: 6 - Batch: 606, Training Loss: 0.05230159234026969\n",
      "Epoch: 6 - Batch: 607, Training Loss: 0.052389196131013914\n",
      "Epoch: 6 - Batch: 608, Training Loss: 0.052476914367865564\n",
      "Epoch: 6 - Batch: 609, Training Loss: 0.05256333171851797\n",
      "Epoch: 6 - Batch: 610, Training Loss: 0.052649699787842494\n",
      "Epoch: 6 - Batch: 611, Training Loss: 0.052729071119482045\n",
      "Epoch: 6 - Batch: 612, Training Loss: 0.05281294319488318\n",
      "Epoch: 6 - Batch: 613, Training Loss: 0.052904717803347365\n",
      "Epoch: 6 - Batch: 614, Training Loss: 0.05299807786570852\n",
      "Epoch: 6 - Batch: 615, Training Loss: 0.05308942683967785\n",
      "Epoch: 6 - Batch: 616, Training Loss: 0.05317962143699921\n",
      "Epoch: 6 - Batch: 617, Training Loss: 0.053267067011019485\n",
      "Epoch: 6 - Batch: 618, Training Loss: 0.05334708727834434\n",
      "Epoch: 6 - Batch: 619, Training Loss: 0.053428090830792245\n",
      "Epoch: 6 - Batch: 620, Training Loss: 0.0535078702330787\n",
      "Epoch: 6 - Batch: 621, Training Loss: 0.05358950446543607\n",
      "Epoch: 6 - Batch: 622, Training Loss: 0.05367083317456554\n",
      "Epoch: 6 - Batch: 623, Training Loss: 0.05375610815228317\n",
      "Epoch: 6 - Batch: 624, Training Loss: 0.05383917833269137\n",
      "Epoch: 6 - Batch: 625, Training Loss: 0.05392588516249388\n",
      "Epoch: 6 - Batch: 626, Training Loss: 0.05401835978574816\n",
      "Epoch: 6 - Batch: 627, Training Loss: 0.05410582082312103\n",
      "Epoch: 6 - Batch: 628, Training Loss: 0.0541876037890836\n",
      "Epoch: 6 - Batch: 629, Training Loss: 0.054273314879061174\n",
      "Epoch: 6 - Batch: 630, Training Loss: 0.05437314074183776\n",
      "Epoch: 6 - Batch: 631, Training Loss: 0.05445951962253545\n",
      "Epoch: 6 - Batch: 632, Training Loss: 0.05455656507813911\n",
      "Epoch: 6 - Batch: 633, Training Loss: 0.0546392271199432\n",
      "Epoch: 6 - Batch: 634, Training Loss: 0.054728661683908546\n",
      "Epoch: 6 - Batch: 635, Training Loss: 0.05481736701766452\n",
      "Epoch: 6 - Batch: 636, Training Loss: 0.05490652469309606\n",
      "Epoch: 6 - Batch: 637, Training Loss: 0.055004827628780166\n",
      "Epoch: 6 - Batch: 638, Training Loss: 0.05509426952594548\n",
      "Epoch: 6 - Batch: 639, Training Loss: 0.055181722365208526\n",
      "Epoch: 6 - Batch: 640, Training Loss: 0.0552619813229413\n",
      "Epoch: 6 - Batch: 641, Training Loss: 0.055345126231560855\n",
      "Epoch: 6 - Batch: 642, Training Loss: 0.05542761240012412\n",
      "Epoch: 6 - Batch: 643, Training Loss: 0.05552201140455741\n",
      "Epoch: 6 - Batch: 644, Training Loss: 0.05560676804179971\n",
      "Epoch: 6 - Batch: 645, Training Loss: 0.05569411868578561\n",
      "Epoch: 6 - Batch: 646, Training Loss: 0.05578315842458067\n",
      "Epoch: 6 - Batch: 647, Training Loss: 0.055873634031113506\n",
      "Epoch: 6 - Batch: 648, Training Loss: 0.05596439193186673\n",
      "Epoch: 6 - Batch: 649, Training Loss: 0.056040952881029\n",
      "Epoch: 6 - Batch: 650, Training Loss: 0.05613451054440209\n",
      "Epoch: 6 - Batch: 651, Training Loss: 0.05621725968880638\n",
      "Epoch: 6 - Batch: 652, Training Loss: 0.0563054409983937\n",
      "Epoch: 6 - Batch: 653, Training Loss: 0.05638656726301606\n",
      "Epoch: 6 - Batch: 654, Training Loss: 0.056475246062573314\n",
      "Epoch: 6 - Batch: 655, Training Loss: 0.05655678609389175\n",
      "Epoch: 6 - Batch: 656, Training Loss: 0.05663908197610928\n",
      "Epoch: 6 - Batch: 657, Training Loss: 0.056719895248389364\n",
      "Epoch: 6 - Batch: 658, Training Loss: 0.056797643928830306\n",
      "Epoch: 6 - Batch: 659, Training Loss: 0.05687757378218581\n",
      "Epoch: 6 - Batch: 660, Training Loss: 0.056975257510371864\n",
      "Epoch: 6 - Batch: 661, Training Loss: 0.05706588526690382\n",
      "Epoch: 6 - Batch: 662, Training Loss: 0.05714791923588386\n",
      "Epoch: 6 - Batch: 663, Training Loss: 0.057227848292286713\n",
      "Epoch: 6 - Batch: 664, Training Loss: 0.05732241057934453\n",
      "Epoch: 6 - Batch: 665, Training Loss: 0.05740269553379633\n",
      "Epoch: 6 - Batch: 666, Training Loss: 0.0574876441686703\n",
      "Epoch: 6 - Batch: 667, Training Loss: 0.05757357668792628\n",
      "Epoch: 6 - Batch: 668, Training Loss: 0.05765913259454232\n",
      "Epoch: 6 - Batch: 669, Training Loss: 0.057754528827681076\n",
      "Epoch: 6 - Batch: 670, Training Loss: 0.057854179929243785\n",
      "Epoch: 6 - Batch: 671, Training Loss: 0.05794705164496776\n",
      "Epoch: 6 - Batch: 672, Training Loss: 0.058028974973443725\n",
      "Epoch: 6 - Batch: 673, Training Loss: 0.05812223912348004\n",
      "Epoch: 6 - Batch: 674, Training Loss: 0.058208335481374024\n",
      "Epoch: 6 - Batch: 675, Training Loss: 0.05829377376024996\n",
      "Epoch: 6 - Batch: 676, Training Loss: 0.058380049685785425\n",
      "Epoch: 6 - Batch: 677, Training Loss: 0.05846939192680182\n",
      "Epoch: 6 - Batch: 678, Training Loss: 0.05856007218953982\n",
      "Epoch: 6 - Batch: 679, Training Loss: 0.058651272127185496\n",
      "Epoch: 6 - Batch: 680, Training Loss: 0.05874275916547918\n",
      "Epoch: 6 - Batch: 681, Training Loss: 0.05882578340064036\n",
      "Epoch: 6 - Batch: 682, Training Loss: 0.05891267309091973\n",
      "Epoch: 6 - Batch: 683, Training Loss: 0.05900062253670906\n",
      "Epoch: 6 - Batch: 684, Training Loss: 0.05909001714183917\n",
      "Epoch: 6 - Batch: 685, Training Loss: 0.05918354343369628\n",
      "Epoch: 6 - Batch: 686, Training Loss: 0.05926812331425412\n",
      "Epoch: 6 - Batch: 687, Training Loss: 0.05935157860195261\n",
      "Epoch: 6 - Batch: 688, Training Loss: 0.05944369932262854\n",
      "Epoch: 6 - Batch: 689, Training Loss: 0.05952390876427219\n",
      "Epoch: 6 - Batch: 690, Training Loss: 0.05961592385474327\n",
      "Epoch: 6 - Batch: 691, Training Loss: 0.059705356077274084\n",
      "Epoch: 6 - Batch: 692, Training Loss: 0.059784937310179274\n",
      "Epoch: 6 - Batch: 693, Training Loss: 0.05987044714181182\n",
      "Epoch: 6 - Batch: 694, Training Loss: 0.05996683168890662\n",
      "Epoch: 6 - Batch: 695, Training Loss: 0.06005788883611338\n",
      "Epoch: 6 - Batch: 696, Training Loss: 0.06014070426968002\n",
      "Epoch: 6 - Batch: 697, Training Loss: 0.06022832052748199\n",
      "Epoch: 6 - Batch: 698, Training Loss: 0.06031357814522327\n",
      "Epoch: 6 - Batch: 699, Training Loss: 0.06039447313295075\n",
      "Epoch: 6 - Batch: 700, Training Loss: 0.06047138594597529\n",
      "Epoch: 6 - Batch: 701, Training Loss: 0.060572396234839315\n",
      "Epoch: 6 - Batch: 702, Training Loss: 0.0606592912257805\n",
      "Epoch: 6 - Batch: 703, Training Loss: 0.06074989297941549\n",
      "Epoch: 6 - Batch: 704, Training Loss: 0.06083139358937839\n",
      "Epoch: 6 - Batch: 705, Training Loss: 0.060917257774578\n",
      "Epoch: 6 - Batch: 706, Training Loss: 0.061001368256202386\n",
      "Epoch: 6 - Batch: 707, Training Loss: 0.06108620330987878\n",
      "Epoch: 6 - Batch: 708, Training Loss: 0.061166170193731884\n",
      "Epoch: 6 - Batch: 709, Training Loss: 0.06124314272878182\n",
      "Epoch: 6 - Batch: 710, Training Loss: 0.06133386939393347\n",
      "Epoch: 6 - Batch: 711, Training Loss: 0.061414348891323085\n",
      "Epoch: 6 - Batch: 712, Training Loss: 0.06150254638708053\n",
      "Epoch: 6 - Batch: 713, Training Loss: 0.06158374103518268\n",
      "Epoch: 6 - Batch: 714, Training Loss: 0.06167772744054818\n",
      "Epoch: 6 - Batch: 715, Training Loss: 0.061770558690846855\n",
      "Epoch: 6 - Batch: 716, Training Loss: 0.061854251241852\n",
      "Epoch: 6 - Batch: 717, Training Loss: 0.061945943758015216\n",
      "Epoch: 6 - Batch: 718, Training Loss: 0.062031338365121466\n",
      "Epoch: 6 - Batch: 719, Training Loss: 0.06211192087945258\n",
      "Epoch: 6 - Batch: 720, Training Loss: 0.062197961808980795\n",
      "Epoch: 6 - Batch: 721, Training Loss: 0.062281196501421096\n",
      "Epoch: 6 - Batch: 722, Training Loss: 0.06236875925837069\n",
      "Epoch: 6 - Batch: 723, Training Loss: 0.062448556749628945\n",
      "Epoch: 6 - Batch: 724, Training Loss: 0.06252545503216794\n",
      "Epoch: 6 - Batch: 725, Training Loss: 0.06260834456379734\n",
      "Epoch: 6 - Batch: 726, Training Loss: 0.0626902627522376\n",
      "Epoch: 6 - Batch: 727, Training Loss: 0.06277381059146837\n",
      "Epoch: 6 - Batch: 728, Training Loss: 0.06286239232342833\n",
      "Epoch: 6 - Batch: 729, Training Loss: 0.06295202939368005\n",
      "Epoch: 6 - Batch: 730, Training Loss: 0.06302923082712278\n",
      "Epoch: 6 - Batch: 731, Training Loss: 0.06311601205770649\n",
      "Epoch: 6 - Batch: 732, Training Loss: 0.0631992502962772\n",
      "Epoch: 6 - Batch: 733, Training Loss: 0.06327060116439515\n",
      "Epoch: 6 - Batch: 734, Training Loss: 0.06335739339168985\n",
      "Epoch: 6 - Batch: 735, Training Loss: 0.0634400123300936\n",
      "Epoch: 6 - Batch: 736, Training Loss: 0.0635181739432104\n",
      "Epoch: 6 - Batch: 737, Training Loss: 0.06359995637787119\n",
      "Epoch: 6 - Batch: 738, Training Loss: 0.06368071618636646\n",
      "Epoch: 6 - Batch: 739, Training Loss: 0.06375884936248287\n",
      "Epoch: 6 - Batch: 740, Training Loss: 0.06384225596193453\n",
      "Epoch: 6 - Batch: 741, Training Loss: 0.06393097439289686\n",
      "Epoch: 6 - Batch: 742, Training Loss: 0.06402134941659164\n",
      "Epoch: 6 - Batch: 743, Training Loss: 0.06411788138139307\n",
      "Epoch: 6 - Batch: 744, Training Loss: 0.06420301689857472\n",
      "Epoch: 6 - Batch: 745, Training Loss: 0.06428703512520735\n",
      "Epoch: 6 - Batch: 746, Training Loss: 0.06436802158332978\n",
      "Epoch: 6 - Batch: 747, Training Loss: 0.06446380512927895\n",
      "Epoch: 6 - Batch: 748, Training Loss: 0.0645584214484909\n",
      "Epoch: 6 - Batch: 749, Training Loss: 0.06464922221788325\n",
      "Epoch: 6 - Batch: 750, Training Loss: 0.06473977183598784\n",
      "Epoch: 6 - Batch: 751, Training Loss: 0.06482836846910899\n",
      "Epoch: 6 - Batch: 752, Training Loss: 0.06491889907773059\n",
      "Epoch: 6 - Batch: 753, Training Loss: 0.06501116649329564\n",
      "Epoch: 6 - Batch: 754, Training Loss: 0.06509135911862056\n",
      "Epoch: 6 - Batch: 755, Training Loss: 0.06517217132211918\n",
      "Epoch: 6 - Batch: 756, Training Loss: 0.06525729204934233\n",
      "Epoch: 6 - Batch: 757, Training Loss: 0.06534417042165847\n",
      "Epoch: 6 - Batch: 758, Training Loss: 0.06543075461959957\n",
      "Epoch: 6 - Batch: 759, Training Loss: 0.06551023211892366\n",
      "Epoch: 6 - Batch: 760, Training Loss: 0.06560403189527653\n",
      "Epoch: 6 - Batch: 761, Training Loss: 0.06569679232799196\n",
      "Epoch: 6 - Batch: 762, Training Loss: 0.06578527609655513\n",
      "Epoch: 6 - Batch: 763, Training Loss: 0.06587272144199209\n",
      "Epoch: 6 - Batch: 764, Training Loss: 0.06596406518325679\n",
      "Epoch: 6 - Batch: 765, Training Loss: 0.06604466751637941\n",
      "Epoch: 6 - Batch: 766, Training Loss: 0.0661348149205124\n",
      "Epoch: 6 - Batch: 767, Training Loss: 0.06621934716924902\n",
      "Epoch: 6 - Batch: 768, Training Loss: 0.06630512748908245\n",
      "Epoch: 6 - Batch: 769, Training Loss: 0.06639758930549297\n",
      "Epoch: 6 - Batch: 770, Training Loss: 0.06649014159765212\n",
      "Epoch: 6 - Batch: 771, Training Loss: 0.06657448696408105\n",
      "Epoch: 6 - Batch: 772, Training Loss: 0.0666604957374965\n",
      "Epoch: 6 - Batch: 773, Training Loss: 0.0667479455372192\n",
      "Epoch: 6 - Batch: 774, Training Loss: 0.06683895934626435\n",
      "Epoch: 6 - Batch: 775, Training Loss: 0.0669233016175516\n",
      "Epoch: 6 - Batch: 776, Training Loss: 0.06701160218585189\n",
      "Epoch: 6 - Batch: 777, Training Loss: 0.06710839068291595\n",
      "Epoch: 6 - Batch: 778, Training Loss: 0.06720105478938539\n",
      "Epoch: 6 - Batch: 779, Training Loss: 0.06728336353404803\n",
      "Epoch: 6 - Batch: 780, Training Loss: 0.06737025761065593\n",
      "Epoch: 6 - Batch: 781, Training Loss: 0.06744922244084217\n",
      "Epoch: 6 - Batch: 782, Training Loss: 0.06752825485755555\n",
      "Epoch: 6 - Batch: 783, Training Loss: 0.06761721701393673\n",
      "Epoch: 6 - Batch: 784, Training Loss: 0.06769689074895077\n",
      "Epoch: 6 - Batch: 785, Training Loss: 0.06779078384289892\n",
      "Epoch: 6 - Batch: 786, Training Loss: 0.06788684516700345\n",
      "Epoch: 6 - Batch: 787, Training Loss: 0.06796643591391704\n",
      "Epoch: 6 - Batch: 788, Training Loss: 0.06804924047433124\n",
      "Epoch: 6 - Batch: 789, Training Loss: 0.06813478802221133\n",
      "Epoch: 6 - Batch: 790, Training Loss: 0.06821481297858319\n",
      "Epoch: 6 - Batch: 791, Training Loss: 0.0683008325386601\n",
      "Epoch: 6 - Batch: 792, Training Loss: 0.0683899006156087\n",
      "Epoch: 6 - Batch: 793, Training Loss: 0.06847680310037599\n",
      "Epoch: 6 - Batch: 794, Training Loss: 0.06857198739353301\n",
      "Epoch: 6 - Batch: 795, Training Loss: 0.06865136236397189\n",
      "Epoch: 6 - Batch: 796, Training Loss: 0.06873480176204078\n",
      "Epoch: 6 - Batch: 797, Training Loss: 0.06882766746664126\n",
      "Epoch: 6 - Batch: 798, Training Loss: 0.06891657712917225\n",
      "Epoch: 6 - Batch: 799, Training Loss: 0.06900950069006402\n",
      "Epoch: 6 - Batch: 800, Training Loss: 0.06909382783778469\n",
      "Epoch: 6 - Batch: 801, Training Loss: 0.06917621571552102\n",
      "Epoch: 6 - Batch: 802, Training Loss: 0.0692662895034696\n",
      "Epoch: 6 - Batch: 803, Training Loss: 0.06935413074903622\n",
      "Epoch: 6 - Batch: 804, Training Loss: 0.06944031616793343\n",
      "Epoch: 6 - Batch: 805, Training Loss: 0.06951934827575043\n",
      "Epoch: 6 - Batch: 806, Training Loss: 0.06960055372904782\n",
      "Epoch: 6 - Batch: 807, Training Loss: 0.06969307049194576\n",
      "Epoch: 6 - Batch: 808, Training Loss: 0.06977997384780082\n",
      "Epoch: 6 - Batch: 809, Training Loss: 0.0698640809624547\n",
      "Epoch: 6 - Batch: 810, Training Loss: 0.06994942950075539\n",
      "Epoch: 6 - Batch: 811, Training Loss: 0.0700389251273564\n",
      "Epoch: 6 - Batch: 812, Training Loss: 0.07012180494728373\n",
      "Epoch: 6 - Batch: 813, Training Loss: 0.0702078803620034\n",
      "Epoch: 6 - Batch: 814, Training Loss: 0.07029035851779467\n",
      "Epoch: 6 - Batch: 815, Training Loss: 0.07037112478840213\n",
      "Epoch: 6 - Batch: 816, Training Loss: 0.07045701820755479\n",
      "Epoch: 6 - Batch: 817, Training Loss: 0.07054190114016952\n",
      "Epoch: 6 - Batch: 818, Training Loss: 0.07063382579195954\n",
      "Epoch: 6 - Batch: 819, Training Loss: 0.07072311607660543\n",
      "Epoch: 6 - Batch: 820, Training Loss: 0.07081495292768945\n",
      "Epoch: 6 - Batch: 821, Training Loss: 0.07089517702337719\n",
      "Epoch: 6 - Batch: 822, Training Loss: 0.0709883520938765\n",
      "Epoch: 6 - Batch: 823, Training Loss: 0.07107647205664348\n",
      "Epoch: 6 - Batch: 824, Training Loss: 0.07115703481396236\n",
      "Epoch: 6 - Batch: 825, Training Loss: 0.07123774931971509\n",
      "Epoch: 6 - Batch: 826, Training Loss: 0.07132021557681793\n",
      "Epoch: 6 - Batch: 827, Training Loss: 0.07140261855958706\n",
      "Epoch: 6 - Batch: 828, Training Loss: 0.07149283760296765\n",
      "Epoch: 6 - Batch: 829, Training Loss: 0.07158138320640743\n",
      "Epoch: 6 - Batch: 830, Training Loss: 0.07166768915667067\n",
      "Epoch: 6 - Batch: 831, Training Loss: 0.07174961612394595\n",
      "Epoch: 6 - Batch: 832, Training Loss: 0.07183776955872428\n",
      "Epoch: 6 - Batch: 833, Training Loss: 0.0719209849760307\n",
      "Epoch: 6 - Batch: 834, Training Loss: 0.0720130266686577\n",
      "Epoch: 6 - Batch: 835, Training Loss: 0.07209832394869371\n",
      "Epoch: 6 - Batch: 836, Training Loss: 0.07218459559910333\n",
      "Epoch: 6 - Batch: 837, Training Loss: 0.07227627216385767\n",
      "Epoch: 6 - Batch: 838, Training Loss: 0.07235981887870563\n",
      "Epoch: 6 - Batch: 839, Training Loss: 0.07244091460583221\n",
      "Epoch: 6 - Batch: 840, Training Loss: 0.07253169778048696\n",
      "Epoch: 6 - Batch: 841, Training Loss: 0.07261786572820511\n",
      "Epoch: 6 - Batch: 842, Training Loss: 0.07269742766744264\n",
      "Epoch: 6 - Batch: 843, Training Loss: 0.07278303643240659\n",
      "Epoch: 6 - Batch: 844, Training Loss: 0.07288711965948985\n",
      "Epoch: 6 - Batch: 845, Training Loss: 0.0729774786761744\n",
      "Epoch: 6 - Batch: 846, Training Loss: 0.07306532026152705\n",
      "Epoch: 6 - Batch: 847, Training Loss: 0.07314238709028482\n",
      "Epoch: 6 - Batch: 848, Training Loss: 0.0732336104360979\n",
      "Epoch: 6 - Batch: 849, Training Loss: 0.07332689360179513\n",
      "Epoch: 6 - Batch: 850, Training Loss: 0.0734114293843063\n",
      "Epoch: 6 - Batch: 851, Training Loss: 0.07350392390048721\n",
      "Epoch: 6 - Batch: 852, Training Loss: 0.07358434111201151\n",
      "Epoch: 6 - Batch: 853, Training Loss: 0.07367202123722824\n",
      "Epoch: 6 - Batch: 854, Training Loss: 0.0737571790876179\n",
      "Epoch: 6 - Batch: 855, Training Loss: 0.07384728441388651\n",
      "Epoch: 6 - Batch: 856, Training Loss: 0.07393535065339572\n",
      "Epoch: 6 - Batch: 857, Training Loss: 0.07402483483847497\n",
      "Epoch: 6 - Batch: 858, Training Loss: 0.07410742050873897\n",
      "Epoch: 6 - Batch: 859, Training Loss: 0.07419626403037786\n",
      "Epoch: 6 - Batch: 860, Training Loss: 0.07428322799170195\n",
      "Epoch: 6 - Batch: 861, Training Loss: 0.07436635665237212\n",
      "Epoch: 6 - Batch: 862, Training Loss: 0.07445385161274504\n",
      "Epoch: 6 - Batch: 863, Training Loss: 0.07453814242189599\n",
      "Epoch: 6 - Batch: 864, Training Loss: 0.07462627674216654\n",
      "Epoch: 6 - Batch: 865, Training Loss: 0.07471117945032729\n",
      "Epoch: 6 - Batch: 866, Training Loss: 0.07479556525746982\n",
      "Epoch: 6 - Batch: 867, Training Loss: 0.07487877773408273\n",
      "Epoch: 6 - Batch: 868, Training Loss: 0.07496501766701243\n",
      "Epoch: 6 - Batch: 869, Training Loss: 0.07504930488678749\n",
      "Epoch: 6 - Batch: 870, Training Loss: 0.07513529638671756\n",
      "Epoch: 6 - Batch: 871, Training Loss: 0.07522032689766504\n",
      "Epoch: 6 - Batch: 872, Training Loss: 0.07530293611818878\n",
      "Epoch: 6 - Batch: 873, Training Loss: 0.0753937018031307\n",
      "Epoch: 6 - Batch: 874, Training Loss: 0.07547735690353917\n",
      "Epoch: 6 - Batch: 875, Training Loss: 0.0755674538401822\n",
      "Epoch: 6 - Batch: 876, Training Loss: 0.07565074066197497\n",
      "Epoch: 6 - Batch: 877, Training Loss: 0.07573774557678063\n",
      "Epoch: 6 - Batch: 878, Training Loss: 0.07582775157433047\n",
      "Epoch: 6 - Batch: 879, Training Loss: 0.07591937331615593\n",
      "Epoch: 6 - Batch: 880, Training Loss: 0.07600793889901333\n",
      "Epoch: 6 - Batch: 881, Training Loss: 0.07608919306526928\n",
      "Epoch: 6 - Batch: 882, Training Loss: 0.07617326815972479\n",
      "Epoch: 6 - Batch: 883, Training Loss: 0.07625451714269953\n",
      "Epoch: 6 - Batch: 884, Training Loss: 0.07634279648364084\n",
      "Epoch: 6 - Batch: 885, Training Loss: 0.07643273409153296\n",
      "Epoch: 6 - Batch: 886, Training Loss: 0.07652098681796249\n",
      "Epoch: 6 - Batch: 887, Training Loss: 0.07661292782272668\n",
      "Epoch: 6 - Batch: 888, Training Loss: 0.07669553352800966\n",
      "Epoch: 6 - Batch: 889, Training Loss: 0.07678180252191044\n",
      "Epoch: 6 - Batch: 890, Training Loss: 0.07687076524048303\n",
      "Epoch: 6 - Batch: 891, Training Loss: 0.07696159140115749\n",
      "Epoch: 6 - Batch: 892, Training Loss: 0.07705290118235458\n",
      "Epoch: 6 - Batch: 893, Training Loss: 0.07713502907436681\n",
      "Epoch: 6 - Batch: 894, Training Loss: 0.07722004164461276\n",
      "Epoch: 6 - Batch: 895, Training Loss: 0.0773070124634364\n",
      "Epoch: 6 - Batch: 896, Training Loss: 0.07739247182757898\n",
      "Epoch: 6 - Batch: 897, Training Loss: 0.07747007727277022\n",
      "Epoch: 6 - Batch: 898, Training Loss: 0.07756070600541472\n",
      "Epoch: 6 - Batch: 899, Training Loss: 0.07763856043344114\n",
      "Epoch: 6 - Batch: 900, Training Loss: 0.07772075796305243\n",
      "Epoch: 6 - Batch: 901, Training Loss: 0.07780549977326867\n",
      "Epoch: 6 - Batch: 902, Training Loss: 0.0778911169896375\n",
      "Epoch: 6 - Batch: 903, Training Loss: 0.07797720467372123\n",
      "Epoch: 6 - Batch: 904, Training Loss: 0.07806312748127514\n",
      "Epoch: 6 - Batch: 905, Training Loss: 0.07814833014017314\n",
      "Epoch: 6 - Batch: 906, Training Loss: 0.07823893155402212\n",
      "Epoch: 6 - Batch: 907, Training Loss: 0.07832264776031177\n",
      "Epoch: 6 - Batch: 908, Training Loss: 0.07840839290920379\n",
      "Epoch: 6 - Batch: 909, Training Loss: 0.07848866203580528\n",
      "Epoch: 6 - Batch: 910, Training Loss: 0.07857509043877002\n",
      "Epoch: 6 - Batch: 911, Training Loss: 0.0786579124580074\n",
      "Epoch: 6 - Batch: 912, Training Loss: 0.07874563193044457\n",
      "Epoch: 6 - Batch: 913, Training Loss: 0.07883181936952408\n",
      "Epoch: 6 - Batch: 914, Training Loss: 0.07891981227703355\n",
      "Epoch: 6 - Batch: 915, Training Loss: 0.07900921534592437\n",
      "Epoch: 6 - Batch: 916, Training Loss: 0.07909347911428653\n",
      "Epoch: 6 - Batch: 917, Training Loss: 0.07918616475083341\n",
      "Epoch: 6 - Batch: 918, Training Loss: 0.07926581446606523\n",
      "Epoch: 6 - Batch: 919, Training Loss: 0.07934517229893315\n",
      "Epoch: 6 - Batch: 920, Training Loss: 0.07943054629029524\n",
      "Epoch: 6 - Batch: 921, Training Loss: 0.07951626095729285\n",
      "Epoch: 6 - Batch: 922, Training Loss: 0.07961362590441855\n",
      "Epoch: 6 - Batch: 923, Training Loss: 0.07970427890396237\n",
      "Epoch: 6 - Batch: 924, Training Loss: 0.0797898968863942\n",
      "Epoch: 6 - Batch: 925, Training Loss: 0.07987699859079042\n",
      "Epoch: 6 - Batch: 926, Training Loss: 0.07995662339661845\n",
      "Epoch: 6 - Batch: 927, Training Loss: 0.08003844548739604\n",
      "Epoch: 6 - Batch: 928, Training Loss: 0.08012748201984671\n",
      "Epoch: 6 - Batch: 929, Training Loss: 0.08020867560534532\n",
      "Epoch: 6 - Batch: 930, Training Loss: 0.08029047117428005\n",
      "Epoch: 6 - Batch: 931, Training Loss: 0.08037563572772106\n",
      "Epoch: 6 - Batch: 932, Training Loss: 0.08046374004179763\n",
      "Epoch: 6 - Batch: 933, Training Loss: 0.08055165680635035\n",
      "Epoch: 6 - Batch: 934, Training Loss: 0.08063427342803127\n",
      "Epoch: 6 - Batch: 935, Training Loss: 0.08072350107466997\n",
      "Epoch: 6 - Batch: 936, Training Loss: 0.08080899604838682\n",
      "Epoch: 6 - Batch: 937, Training Loss: 0.08090269673696007\n",
      "Epoch: 6 - Batch: 938, Training Loss: 0.08098651513234893\n",
      "Epoch: 6 - Batch: 939, Training Loss: 0.08106628867178216\n",
      "Epoch: 6 - Batch: 940, Training Loss: 0.08116149456915175\n",
      "Epoch: 6 - Batch: 941, Training Loss: 0.0812414960061535\n",
      "Epoch: 6 - Batch: 942, Training Loss: 0.0813257804705729\n",
      "Epoch: 6 - Batch: 943, Training Loss: 0.08140856014629502\n",
      "Epoch: 6 - Batch: 944, Training Loss: 0.0814953597067897\n",
      "Epoch: 6 - Batch: 945, Training Loss: 0.08158368624709732\n",
      "Epoch: 6 - Batch: 946, Training Loss: 0.08166989896007834\n",
      "Epoch: 6 - Batch: 947, Training Loss: 0.0817655436447505\n",
      "Epoch: 6 - Batch: 948, Training Loss: 0.08185100834662246\n",
      "Epoch: 6 - Batch: 949, Training Loss: 0.08192717291995463\n",
      "Epoch: 6 - Batch: 950, Training Loss: 0.08200820103584237\n",
      "Epoch: 6 - Batch: 951, Training Loss: 0.08209139730775139\n",
      "Epoch: 6 - Batch: 952, Training Loss: 0.08218228110231175\n",
      "Epoch: 6 - Batch: 953, Training Loss: 0.08226950487637796\n",
      "Epoch: 6 - Batch: 954, Training Loss: 0.08235035953459455\n",
      "Epoch: 6 - Batch: 955, Training Loss: 0.08243342885627082\n",
      "Epoch: 6 - Batch: 956, Training Loss: 0.08252606443307095\n",
      "Epoch: 6 - Batch: 957, Training Loss: 0.08262619891421712\n",
      "Epoch: 6 - Batch: 958, Training Loss: 0.08271123654831504\n",
      "Epoch: 6 - Batch: 959, Training Loss: 0.08280235870585315\n",
      "Epoch: 6 - Batch: 960, Training Loss: 0.08289618724663657\n",
      "Epoch: 6 - Batch: 961, Training Loss: 0.08298433202613843\n",
      "Epoch: 6 - Batch: 962, Training Loss: 0.0830675226328879\n",
      "Epoch: 6 - Batch: 963, Training Loss: 0.08314801455408977\n",
      "Epoch: 6 - Batch: 964, Training Loss: 0.08323371939818262\n",
      "Epoch: 6 - Batch: 965, Training Loss: 0.0833165108490346\n",
      "Epoch: 6 - Batch: 966, Training Loss: 0.08341462518494719\n",
      "Epoch: 6 - Batch: 967, Training Loss: 0.08349705919944628\n",
      "Epoch: 6 - Batch: 968, Training Loss: 0.08358212753166606\n",
      "Epoch: 6 - Batch: 969, Training Loss: 0.08366749958933685\n",
      "Epoch: 6 - Batch: 970, Training Loss: 0.08374633145198893\n",
      "Epoch: 6 - Batch: 971, Training Loss: 0.08383457848854721\n",
      "Epoch: 6 - Batch: 972, Training Loss: 0.08392200185909596\n",
      "Epoch: 6 - Batch: 973, Training Loss: 0.08400809459672441\n",
      "Epoch: 6 - Batch: 974, Training Loss: 0.08408288914049245\n",
      "Epoch: 6 - Batch: 975, Training Loss: 0.08416351317667448\n",
      "Epoch: 6 - Batch: 976, Training Loss: 0.0842532319191281\n",
      "Epoch: 6 - Batch: 977, Training Loss: 0.0843460152348871\n",
      "Epoch: 6 - Batch: 978, Training Loss: 0.08444008976838878\n",
      "Epoch: 6 - Batch: 979, Training Loss: 0.08453750261298658\n",
      "Epoch: 6 - Batch: 980, Training Loss: 0.08462510219038423\n",
      "Epoch: 6 - Batch: 981, Training Loss: 0.08471153312012134\n",
      "Epoch: 6 - Batch: 982, Training Loss: 0.08479248346207945\n",
      "Epoch: 6 - Batch: 983, Training Loss: 0.08488085455760039\n",
      "Epoch: 6 - Batch: 984, Training Loss: 0.08496831226507032\n",
      "Epoch: 6 - Batch: 985, Training Loss: 0.08505215772595967\n",
      "Epoch: 6 - Batch: 986, Training Loss: 0.08513692411197162\n",
      "Epoch: 6 - Batch: 987, Training Loss: 0.08522383832862326\n",
      "Epoch: 6 - Batch: 988, Training Loss: 0.08530971803277682\n",
      "Epoch: 6 - Batch: 989, Training Loss: 0.08539872547336677\n",
      "Epoch: 6 - Batch: 990, Training Loss: 0.08548952690997527\n",
      "Epoch: 6 - Batch: 991, Training Loss: 0.0855786241652756\n",
      "Epoch: 6 - Batch: 992, Training Loss: 0.08566485343510238\n",
      "Epoch: 6 - Batch: 993, Training Loss: 0.08576291573433141\n",
      "Epoch: 6 - Batch: 994, Training Loss: 0.08584634122588544\n",
      "Epoch: 6 - Batch: 995, Training Loss: 0.08594230140496052\n",
      "Epoch: 6 - Batch: 996, Training Loss: 0.08602733637637167\n",
      "Epoch: 6 - Batch: 997, Training Loss: 0.08611272832079116\n",
      "Epoch: 6 - Batch: 998, Training Loss: 0.08619726929276143\n",
      "Epoch: 6 - Batch: 999, Training Loss: 0.08628482274575218\n",
      "Epoch: 6 - Batch: 1000, Training Loss: 0.0863778217267832\n",
      "Epoch: 6 - Batch: 1001, Training Loss: 0.08646269835791184\n",
      "Epoch: 6 - Batch: 1002, Training Loss: 0.0865483948309129\n",
      "Epoch: 6 - Batch: 1003, Training Loss: 0.08662878267874766\n",
      "Epoch: 6 - Batch: 1004, Training Loss: 0.08671373563817089\n",
      "Epoch: 6 - Batch: 1005, Training Loss: 0.08679922610817857\n",
      "Epoch: 6 - Batch: 1006, Training Loss: 0.08689092191099923\n",
      "Epoch: 6 - Batch: 1007, Training Loss: 0.08697854253659597\n",
      "Epoch: 6 - Batch: 1008, Training Loss: 0.08706532156783747\n",
      "Epoch: 6 - Batch: 1009, Training Loss: 0.08715216765430436\n",
      "Epoch: 6 - Batch: 1010, Training Loss: 0.08723264389592617\n",
      "Epoch: 6 - Batch: 1011, Training Loss: 0.08732053243352801\n",
      "Epoch: 6 - Batch: 1012, Training Loss: 0.08740406126327578\n",
      "Epoch: 6 - Batch: 1013, Training Loss: 0.08749646624275306\n",
      "Epoch: 6 - Batch: 1014, Training Loss: 0.08758045480915563\n",
      "Epoch: 6 - Batch: 1015, Training Loss: 0.08766286587883189\n",
      "Epoch: 6 - Batch: 1016, Training Loss: 0.08774565973907561\n",
      "Epoch: 6 - Batch: 1017, Training Loss: 0.08782681019028424\n",
      "Epoch: 6 - Batch: 1018, Training Loss: 0.08791124076886754\n",
      "Epoch: 6 - Batch: 1019, Training Loss: 0.08800406939972495\n",
      "Epoch: 6 - Batch: 1020, Training Loss: 0.08808350543897742\n",
      "Epoch: 6 - Batch: 1021, Training Loss: 0.08817030936108892\n",
      "Epoch: 6 - Batch: 1022, Training Loss: 0.08826191774005716\n",
      "Epoch: 6 - Batch: 1023, Training Loss: 0.08833451098322276\n",
      "Epoch: 6 - Batch: 1024, Training Loss: 0.08841626806366898\n",
      "Epoch: 6 - Batch: 1025, Training Loss: 0.0885020982455555\n",
      "Epoch: 6 - Batch: 1026, Training Loss: 0.08858946477299307\n",
      "Epoch: 6 - Batch: 1027, Training Loss: 0.08867654487935465\n",
      "Epoch: 6 - Batch: 1028, Training Loss: 0.08875272560796729\n",
      "Epoch: 6 - Batch: 1029, Training Loss: 0.08883342347038325\n",
      "Epoch: 6 - Batch: 1030, Training Loss: 0.08892114409809286\n",
      "Epoch: 6 - Batch: 1031, Training Loss: 0.08900772420006803\n",
      "Epoch: 6 - Batch: 1032, Training Loss: 0.08909858841059813\n",
      "Epoch: 6 - Batch: 1033, Training Loss: 0.08919051162293104\n",
      "Epoch: 6 - Batch: 1034, Training Loss: 0.0892820190360297\n",
      "Epoch: 6 - Batch: 1035, Training Loss: 0.08936969468842691\n",
      "Epoch: 6 - Batch: 1036, Training Loss: 0.08945335631272686\n",
      "Epoch: 6 - Batch: 1037, Training Loss: 0.08954947150835943\n",
      "Epoch: 6 - Batch: 1038, Training Loss: 0.0896355485849416\n",
      "Epoch: 6 - Batch: 1039, Training Loss: 0.08971608217626464\n",
      "Epoch: 6 - Batch: 1040, Training Loss: 0.08979605421868724\n",
      "Epoch: 6 - Batch: 1041, Training Loss: 0.08988140232453298\n",
      "Epoch: 6 - Batch: 1042, Training Loss: 0.08997017955700952\n",
      "Epoch: 6 - Batch: 1043, Training Loss: 0.09005890643937671\n",
      "Epoch: 6 - Batch: 1044, Training Loss: 0.0901466716161217\n",
      "Epoch: 6 - Batch: 1045, Training Loss: 0.09023555326175137\n",
      "Epoch: 6 - Batch: 1046, Training Loss: 0.09031792561435581\n",
      "Epoch: 6 - Batch: 1047, Training Loss: 0.09040608379378248\n",
      "Epoch: 6 - Batch: 1048, Training Loss: 0.09048543421931528\n",
      "Epoch: 6 - Batch: 1049, Training Loss: 0.09056780958057042\n",
      "Epoch: 6 - Batch: 1050, Training Loss: 0.09066718862473866\n",
      "Epoch: 6 - Batch: 1051, Training Loss: 0.09073978361007981\n",
      "Epoch: 6 - Batch: 1052, Training Loss: 0.09082196982790582\n",
      "Epoch: 6 - Batch: 1053, Training Loss: 0.09090833325760676\n",
      "Epoch: 6 - Batch: 1054, Training Loss: 0.09100452386720066\n",
      "Epoch: 6 - Batch: 1055, Training Loss: 0.09109568750952211\n",
      "Epoch: 6 - Batch: 1056, Training Loss: 0.09117814280698153\n",
      "Epoch: 6 - Batch: 1057, Training Loss: 0.09126197140830666\n",
      "Epoch: 6 - Batch: 1058, Training Loss: 0.09134562670640882\n",
      "Epoch: 6 - Batch: 1059, Training Loss: 0.09143024644695506\n",
      "Epoch: 6 - Batch: 1060, Training Loss: 0.0915122587128757\n",
      "Epoch: 6 - Batch: 1061, Training Loss: 0.09159707536868393\n",
      "Epoch: 6 - Batch: 1062, Training Loss: 0.09168658563697318\n",
      "Epoch: 6 - Batch: 1063, Training Loss: 0.09178211713583513\n",
      "Epoch: 6 - Batch: 1064, Training Loss: 0.09187602527921472\n",
      "Epoch: 6 - Batch: 1065, Training Loss: 0.09196506469057962\n",
      "Epoch: 6 - Batch: 1066, Training Loss: 0.09204654564237713\n",
      "Epoch: 6 - Batch: 1067, Training Loss: 0.09213480678932189\n",
      "Epoch: 6 - Batch: 1068, Training Loss: 0.09221496827789206\n",
      "Epoch: 6 - Batch: 1069, Training Loss: 0.09229649068358328\n",
      "Epoch: 6 - Batch: 1070, Training Loss: 0.09237719949328682\n",
      "Epoch: 6 - Batch: 1071, Training Loss: 0.09246098333552702\n",
      "Epoch: 6 - Batch: 1072, Training Loss: 0.0925500180763787\n",
      "Epoch: 6 - Batch: 1073, Training Loss: 0.09263931988903737\n",
      "Epoch: 6 - Batch: 1074, Training Loss: 0.09272479188704175\n",
      "Epoch: 6 - Batch: 1075, Training Loss: 0.09280882952472266\n",
      "Epoch: 6 - Batch: 1076, Training Loss: 0.09289384995329242\n",
      "Epoch: 6 - Batch: 1077, Training Loss: 0.09297828082841625\n",
      "Epoch: 6 - Batch: 1078, Training Loss: 0.0930694804015642\n",
      "Epoch: 6 - Batch: 1079, Training Loss: 0.09314916150066785\n",
      "Epoch: 6 - Batch: 1080, Training Loss: 0.09323148252087249\n",
      "Epoch: 6 - Batch: 1081, Training Loss: 0.09331118017682191\n",
      "Epoch: 6 - Batch: 1082, Training Loss: 0.09339428103064028\n",
      "Epoch: 6 - Batch: 1083, Training Loss: 0.0934782085838306\n",
      "Epoch: 6 - Batch: 1084, Training Loss: 0.09356927824134059\n",
      "Epoch: 6 - Batch: 1085, Training Loss: 0.09365544769029514\n",
      "Epoch: 6 - Batch: 1086, Training Loss: 0.09374524384218069\n",
      "Epoch: 6 - Batch: 1087, Training Loss: 0.09383420552286145\n",
      "Epoch: 6 - Batch: 1088, Training Loss: 0.09392657392282984\n",
      "Epoch: 6 - Batch: 1089, Training Loss: 0.09401299138468494\n",
      "Epoch: 6 - Batch: 1090, Training Loss: 0.09410141077924328\n",
      "Epoch: 6 - Batch: 1091, Training Loss: 0.09418753265815588\n",
      "Epoch: 6 - Batch: 1092, Training Loss: 0.0942856165492416\n",
      "Epoch: 6 - Batch: 1093, Training Loss: 0.09436910078053055\n",
      "Epoch: 6 - Batch: 1094, Training Loss: 0.09446087432021327\n",
      "Epoch: 6 - Batch: 1095, Training Loss: 0.09455207094648979\n",
      "Epoch: 6 - Batch: 1096, Training Loss: 0.09464216803140309\n",
      "Epoch: 6 - Batch: 1097, Training Loss: 0.09472640458599449\n",
      "Epoch: 6 - Batch: 1098, Training Loss: 0.09480694520779906\n",
      "Epoch: 6 - Batch: 1099, Training Loss: 0.09489258843942068\n",
      "Epoch: 6 - Batch: 1100, Training Loss: 0.0949790090705526\n",
      "Epoch: 6 - Batch: 1101, Training Loss: 0.09506943611857507\n",
      "Epoch: 6 - Batch: 1102, Training Loss: 0.09515205944330736\n",
      "Epoch: 6 - Batch: 1103, Training Loss: 0.09523873899138191\n",
      "Epoch: 6 - Batch: 1104, Training Loss: 0.09533036279045724\n",
      "Epoch: 6 - Batch: 1105, Training Loss: 0.095418628626449\n",
      "Epoch: 6 - Batch: 1106, Training Loss: 0.09550388317993821\n",
      "Epoch: 6 - Batch: 1107, Training Loss: 0.09558482370516949\n",
      "Epoch: 6 - Batch: 1108, Training Loss: 0.0956759547712791\n",
      "Epoch: 6 - Batch: 1109, Training Loss: 0.09576599396283354\n",
      "Epoch: 6 - Batch: 1110, Training Loss: 0.09585632244202233\n",
      "Epoch: 6 - Batch: 1111, Training Loss: 0.09593781937611834\n",
      "Epoch: 6 - Batch: 1112, Training Loss: 0.09602895348794623\n",
      "Epoch: 6 - Batch: 1113, Training Loss: 0.09611185210730701\n",
      "Epoch: 6 - Batch: 1114, Training Loss: 0.09620499455834898\n",
      "Epoch: 6 - Batch: 1115, Training Loss: 0.09629909907951086\n",
      "Epoch: 6 - Batch: 1116, Training Loss: 0.09638502637841809\n",
      "Epoch: 6 - Batch: 1117, Training Loss: 0.09646572666875956\n",
      "Epoch: 6 - Batch: 1118, Training Loss: 0.09655076938111391\n",
      "Epoch: 6 - Batch: 1119, Training Loss: 0.09663960547070598\n",
      "Epoch: 6 - Batch: 1120, Training Loss: 0.09672726304648725\n",
      "Epoch: 6 - Batch: 1121, Training Loss: 0.0968150779679047\n",
      "Epoch: 6 - Batch: 1122, Training Loss: 0.09690763821105657\n",
      "Epoch: 6 - Batch: 1123, Training Loss: 0.0969854081435682\n",
      "Epoch: 6 - Batch: 1124, Training Loss: 0.09706860550897038\n",
      "Epoch: 6 - Batch: 1125, Training Loss: 0.0971575235487711\n",
      "Epoch: 6 - Batch: 1126, Training Loss: 0.09724000650481206\n",
      "Epoch: 6 - Batch: 1127, Training Loss: 0.09732174709636972\n",
      "Epoch: 6 - Batch: 1128, Training Loss: 0.0974112288473455\n",
      "Epoch: 6 - Batch: 1129, Training Loss: 0.0974993127866171\n",
      "Epoch: 6 - Batch: 1130, Training Loss: 0.09758230846739725\n",
      "Epoch: 6 - Batch: 1131, Training Loss: 0.09766530976391352\n",
      "Epoch: 6 - Batch: 1132, Training Loss: 0.09776256425587297\n",
      "Epoch: 6 - Batch: 1133, Training Loss: 0.09784520751677737\n",
      "Epoch: 6 - Batch: 1134, Training Loss: 0.09794517237056745\n",
      "Epoch: 6 - Batch: 1135, Training Loss: 0.09803219041346911\n",
      "Epoch: 6 - Batch: 1136, Training Loss: 0.0981189825913404\n",
      "Epoch: 6 - Batch: 1137, Training Loss: 0.09820204792825342\n",
      "Epoch: 6 - Batch: 1138, Training Loss: 0.09828273175441804\n",
      "Epoch: 6 - Batch: 1139, Training Loss: 0.09837352792743229\n",
      "Epoch: 6 - Batch: 1140, Training Loss: 0.09845123616098171\n",
      "Epoch: 6 - Batch: 1141, Training Loss: 0.0985298850643101\n",
      "Epoch: 6 - Batch: 1142, Training Loss: 0.09861421974910235\n",
      "Epoch: 6 - Batch: 1143, Training Loss: 0.09869492808310547\n",
      "Epoch: 6 - Batch: 1144, Training Loss: 0.09878360430028901\n",
      "Epoch: 6 - Batch: 1145, Training Loss: 0.09886721378905856\n",
      "Epoch: 6 - Batch: 1146, Training Loss: 0.09895337433561954\n",
      "Epoch: 6 - Batch: 1147, Training Loss: 0.0990396999255144\n",
      "Epoch: 6 - Batch: 1148, Training Loss: 0.09912665835141543\n",
      "Epoch: 6 - Batch: 1149, Training Loss: 0.09921208639346545\n",
      "Epoch: 6 - Batch: 1150, Training Loss: 0.09930541327417787\n",
      "Epoch: 6 - Batch: 1151, Training Loss: 0.09939042362654782\n",
      "Epoch: 6 - Batch: 1152, Training Loss: 0.09948128283295662\n",
      "Epoch: 6 - Batch: 1153, Training Loss: 0.09956738205740899\n",
      "Epoch: 6 - Batch: 1154, Training Loss: 0.0996501222994197\n",
      "Epoch: 6 - Batch: 1155, Training Loss: 0.09973555728546027\n",
      "Epoch: 6 - Batch: 1156, Training Loss: 0.09982042946230317\n",
      "Epoch: 6 - Batch: 1157, Training Loss: 0.09991342292644491\n",
      "Epoch: 6 - Batch: 1158, Training Loss: 0.0999932770260531\n",
      "Epoch: 6 - Batch: 1159, Training Loss: 0.10009468596422455\n",
      "Epoch: 6 - Batch: 1160, Training Loss: 0.10018087565206968\n",
      "Epoch: 6 - Batch: 1161, Training Loss: 0.10026340166826549\n",
      "Epoch: 6 - Batch: 1162, Training Loss: 0.10034329174194566\n",
      "Epoch: 6 - Batch: 1163, Training Loss: 0.10042814477339115\n",
      "Epoch: 6 - Batch: 1164, Training Loss: 0.10051624346555367\n",
      "Epoch: 6 - Batch: 1165, Training Loss: 0.10059655947644715\n",
      "Epoch: 6 - Batch: 1166, Training Loss: 0.10068254526797218\n",
      "Epoch: 6 - Batch: 1167, Training Loss: 0.10076768726579981\n",
      "Epoch: 6 - Batch: 1168, Training Loss: 0.10085603264878638\n",
      "Epoch: 6 - Batch: 1169, Training Loss: 0.10094832342927333\n",
      "Epoch: 6 - Batch: 1170, Training Loss: 0.1010357799320474\n",
      "Epoch: 6 - Batch: 1171, Training Loss: 0.10112146274565069\n",
      "Epoch: 6 - Batch: 1172, Training Loss: 0.10120158331162894\n",
      "Epoch: 6 - Batch: 1173, Training Loss: 0.10130209100755491\n",
      "Epoch: 6 - Batch: 1174, Training Loss: 0.10138626780923128\n",
      "Epoch: 6 - Batch: 1175, Training Loss: 0.10147180237110773\n",
      "Epoch: 6 - Batch: 1176, Training Loss: 0.10156380605712459\n",
      "Epoch: 6 - Batch: 1177, Training Loss: 0.10164622303907749\n",
      "Epoch: 6 - Batch: 1178, Training Loss: 0.10173208624198662\n",
      "Epoch: 6 - Batch: 1179, Training Loss: 0.10181963134241935\n",
      "Epoch: 6 - Batch: 1180, Training Loss: 0.1019029243544955\n",
      "Epoch: 6 - Batch: 1181, Training Loss: 0.10198923260912571\n",
      "Epoch: 6 - Batch: 1182, Training Loss: 0.10207841317377873\n",
      "Epoch: 6 - Batch: 1183, Training Loss: 0.10215681242087785\n",
      "Epoch: 6 - Batch: 1184, Training Loss: 0.10223761806959536\n",
      "Epoch: 6 - Batch: 1185, Training Loss: 0.10232762419688168\n",
      "Epoch: 6 - Batch: 1186, Training Loss: 0.10241617394571083\n",
      "Epoch: 6 - Batch: 1187, Training Loss: 0.10249382662733593\n",
      "Epoch: 6 - Batch: 1188, Training Loss: 0.1025792897600143\n",
      "Epoch: 6 - Batch: 1189, Training Loss: 0.10266601435135846\n",
      "Epoch: 6 - Batch: 1190, Training Loss: 0.10274992637323899\n",
      "Epoch: 6 - Batch: 1191, Training Loss: 0.10283624560604641\n",
      "Epoch: 6 - Batch: 1192, Training Loss: 0.10291798331251192\n",
      "Epoch: 6 - Batch: 1193, Training Loss: 0.10299918477486812\n",
      "Epoch: 6 - Batch: 1194, Training Loss: 0.10308454326581006\n",
      "Epoch: 6 - Batch: 1195, Training Loss: 0.10317400550689072\n",
      "Epoch: 6 - Batch: 1196, Training Loss: 0.10326482117744425\n",
      "Epoch: 6 - Batch: 1197, Training Loss: 0.10335094896863349\n",
      "Epoch: 6 - Batch: 1198, Training Loss: 0.10344498748481767\n",
      "Epoch: 6 - Batch: 1199, Training Loss: 0.10352376223252978\n",
      "Epoch: 6 - Batch: 1200, Training Loss: 0.10360982242483602\n",
      "Epoch: 6 - Batch: 1201, Training Loss: 0.10369737513029753\n",
      "Epoch: 6 - Batch: 1202, Training Loss: 0.10378044631770791\n",
      "Epoch: 6 - Batch: 1203, Training Loss: 0.10386604133678314\n",
      "Epoch: 6 - Batch: 1204, Training Loss: 0.10395561888270315\n",
      "Epoch: 6 - Batch: 1205, Training Loss: 0.10403917016303954\n",
      "Epoch: 6 - Batch: 1206, Training Loss: 0.10412404759729878\n",
      "Epoch: 6 - Batch: 1207, Training Loss: 0.10421000027513228\n",
      "Epoch: 6 - Batch: 1208, Training Loss: 0.1043022835309035\n",
      "Epoch: 6 - Batch: 1209, Training Loss: 0.1043832875775856\n",
      "Epoch: 6 - Batch: 1210, Training Loss: 0.1044698790469474\n",
      "Epoch: 6 - Batch: 1211, Training Loss: 0.1045576459288004\n",
      "Epoch: 6 - Batch: 1212, Training Loss: 0.10463536554827026\n",
      "Epoch: 6 - Batch: 1213, Training Loss: 0.10471639407190123\n",
      "Epoch: 6 - Batch: 1214, Training Loss: 0.10480131938509878\n",
      "Epoch: 6 - Batch: 1215, Training Loss: 0.10488350378043616\n",
      "Epoch: 6 - Batch: 1216, Training Loss: 0.1049695522346801\n",
      "Epoch: 6 - Batch: 1217, Training Loss: 0.10505402312424053\n",
      "Epoch: 6 - Batch: 1218, Training Loss: 0.1051356646959758\n",
      "Epoch: 6 - Batch: 1219, Training Loss: 0.10522204745641198\n",
      "Epoch: 6 - Batch: 1220, Training Loss: 0.10529669684359486\n",
      "Epoch: 6 - Batch: 1221, Training Loss: 0.10538942061031042\n",
      "Epoch: 6 - Batch: 1222, Training Loss: 0.10547883247039211\n",
      "Epoch: 6 - Batch: 1223, Training Loss: 0.10556808172124338\n",
      "Epoch: 6 - Batch: 1224, Training Loss: 0.10565682822470839\n",
      "Epoch: 6 - Batch: 1225, Training Loss: 0.1057344953812177\n",
      "Epoch: 6 - Batch: 1226, Training Loss: 0.10581936011983585\n",
      "Epoch: 6 - Batch: 1227, Training Loss: 0.10590173443702125\n",
      "Epoch: 6 - Batch: 1228, Training Loss: 0.10598846798018238\n",
      "Epoch: 6 - Batch: 1229, Training Loss: 0.1060797254034437\n",
      "Epoch: 6 - Batch: 1230, Training Loss: 0.10616835618197028\n",
      "Epoch: 6 - Batch: 1231, Training Loss: 0.10625231298764744\n",
      "Epoch: 6 - Batch: 1232, Training Loss: 0.10633295676848584\n",
      "Epoch: 6 - Batch: 1233, Training Loss: 0.10641188233794265\n",
      "Epoch: 6 - Batch: 1234, Training Loss: 0.10649665130015036\n",
      "Epoch: 6 - Batch: 1235, Training Loss: 0.10659225746544439\n",
      "Epoch: 6 - Batch: 1236, Training Loss: 0.10668060481301192\n",
      "Epoch: 6 - Batch: 1237, Training Loss: 0.10677015751130743\n",
      "Epoch: 6 - Batch: 1238, Training Loss: 0.1068595796699943\n",
      "Epoch: 6 - Batch: 1239, Training Loss: 0.10694807932818708\n",
      "Epoch: 6 - Batch: 1240, Training Loss: 0.10705035122398714\n",
      "Epoch: 6 - Batch: 1241, Training Loss: 0.10714494582160591\n",
      "Epoch: 6 - Batch: 1242, Training Loss: 0.10722810940610038\n",
      "Epoch: 6 - Batch: 1243, Training Loss: 0.1073140029920571\n",
      "Epoch: 6 - Batch: 1244, Training Loss: 0.1073988466886541\n",
      "Epoch: 6 - Batch: 1245, Training Loss: 0.10748435003964067\n",
      "Epoch: 6 - Batch: 1246, Training Loss: 0.10756836994049165\n",
      "Epoch: 6 - Batch: 1247, Training Loss: 0.10764844368395718\n",
      "Epoch: 6 - Batch: 1248, Training Loss: 0.10772899512949077\n",
      "Epoch: 6 - Batch: 1249, Training Loss: 0.10781484925084645\n",
      "Epoch: 6 - Batch: 1250, Training Loss: 0.10789533687838868\n",
      "Epoch: 6 - Batch: 1251, Training Loss: 0.10798577932180654\n",
      "Epoch: 6 - Batch: 1252, Training Loss: 0.10808678310531289\n",
      "Epoch: 6 - Batch: 1253, Training Loss: 0.10817074283314977\n",
      "Epoch: 6 - Batch: 1254, Training Loss: 0.10825300316139438\n",
      "Epoch: 6 - Batch: 1255, Training Loss: 0.10834685835993507\n",
      "Epoch: 6 - Batch: 1256, Training Loss: 0.10842740306534973\n",
      "Epoch: 6 - Batch: 1257, Training Loss: 0.10851804824363731\n",
      "Epoch: 6 - Batch: 1258, Training Loss: 0.10860198883843264\n",
      "Epoch: 6 - Batch: 1259, Training Loss: 0.10869330037416115\n",
      "Epoch: 6 - Batch: 1260, Training Loss: 0.10878074515122876\n",
      "Epoch: 6 - Batch: 1261, Training Loss: 0.10886721649696776\n",
      "Epoch: 6 - Batch: 1262, Training Loss: 0.10895691294945888\n",
      "Epoch: 6 - Batch: 1263, Training Loss: 0.10903635667405319\n",
      "Epoch: 6 - Batch: 1264, Training Loss: 0.10912047290115016\n",
      "Epoch: 6 - Batch: 1265, Training Loss: 0.10920178410203303\n",
      "Epoch: 6 - Batch: 1266, Training Loss: 0.10929577084100661\n",
      "Epoch: 6 - Batch: 1267, Training Loss: 0.10938204942922884\n",
      "Epoch: 6 - Batch: 1268, Training Loss: 0.10946312901960874\n",
      "Epoch: 6 - Batch: 1269, Training Loss: 0.10954248506087766\n",
      "Epoch: 6 - Batch: 1270, Training Loss: 0.1096353511361538\n",
      "Epoch: 6 - Batch: 1271, Training Loss: 0.10972468568565043\n",
      "Epoch: 6 - Batch: 1272, Training Loss: 0.10981532746607786\n",
      "Epoch: 6 - Batch: 1273, Training Loss: 0.10989785363054394\n",
      "Epoch: 6 - Batch: 1274, Training Loss: 0.10998429683582304\n",
      "Epoch: 6 - Batch: 1275, Training Loss: 0.11007475432867236\n",
      "Epoch: 6 - Batch: 1276, Training Loss: 0.11016938836000255\n",
      "Epoch: 6 - Batch: 1277, Training Loss: 0.11026367186808073\n",
      "Epoch: 6 - Batch: 1278, Training Loss: 0.11034847162725715\n",
      "Epoch: 6 - Batch: 1279, Training Loss: 0.11043874411588878\n",
      "Epoch: 6 - Batch: 1280, Training Loss: 0.11053396720025274\n",
      "Epoch: 6 - Batch: 1281, Training Loss: 0.11062042879722209\n",
      "Epoch: 6 - Batch: 1282, Training Loss: 0.11071266266046274\n",
      "Epoch: 6 - Batch: 1283, Training Loss: 0.11080610187443136\n",
      "Epoch: 6 - Batch: 1284, Training Loss: 0.11089301266851116\n",
      "Epoch: 6 - Batch: 1285, Training Loss: 0.11097954474327774\n",
      "Epoch: 6 - Batch: 1286, Training Loss: 0.11106578149457476\n",
      "Epoch: 6 - Batch: 1287, Training Loss: 0.11115208749426142\n",
      "Epoch: 6 - Batch: 1288, Training Loss: 0.11124061422413262\n",
      "Epoch: 6 - Batch: 1289, Training Loss: 0.11132132068004577\n",
      "Epoch: 6 - Batch: 1290, Training Loss: 0.11140829052275686\n",
      "Epoch: 6 - Batch: 1291, Training Loss: 0.11150531855042696\n",
      "Epoch: 6 - Batch: 1292, Training Loss: 0.11160003912834386\n",
      "Epoch: 6 - Batch: 1293, Training Loss: 0.11168269126693012\n",
      "Epoch: 6 - Batch: 1294, Training Loss: 0.11176797831631814\n",
      "Epoch: 6 - Batch: 1295, Training Loss: 0.11185757303069875\n",
      "Epoch: 6 - Batch: 1296, Training Loss: 0.11193641364129621\n",
      "Epoch: 6 - Batch: 1297, Training Loss: 0.11202332573274079\n",
      "Epoch: 6 - Batch: 1298, Training Loss: 0.1121155960148642\n",
      "Epoch: 6 - Batch: 1299, Training Loss: 0.11220288916087862\n",
      "Epoch: 6 - Batch: 1300, Training Loss: 0.11229153619353253\n",
      "Epoch: 6 - Batch: 1301, Training Loss: 0.11238490176042712\n",
      "Epoch: 6 - Batch: 1302, Training Loss: 0.11246040921232\n",
      "Epoch: 6 - Batch: 1303, Training Loss: 0.11255142087144639\n",
      "Epoch: 6 - Batch: 1304, Training Loss: 0.1126301621842147\n",
      "Epoch: 6 - Batch: 1305, Training Loss: 0.11271220514825722\n",
      "Epoch: 6 - Batch: 1306, Training Loss: 0.11279347412918338\n",
      "Epoch: 6 - Batch: 1307, Training Loss: 0.11286902789699893\n",
      "Epoch: 6 - Batch: 1308, Training Loss: 0.11294740494062651\n",
      "Epoch: 6 - Batch: 1309, Training Loss: 0.11303093794665328\n",
      "Epoch: 6 - Batch: 1310, Training Loss: 0.11311841945150006\n",
      "Epoch: 6 - Batch: 1311, Training Loss: 0.113201535941653\n",
      "Epoch: 6 - Batch: 1312, Training Loss: 0.11328585945675226\n",
      "Epoch: 6 - Batch: 1313, Training Loss: 0.11338374435061443\n",
      "Epoch: 6 - Batch: 1314, Training Loss: 0.11347023768977542\n",
      "Epoch: 6 - Batch: 1315, Training Loss: 0.11355909938687708\n",
      "Epoch: 6 - Batch: 1316, Training Loss: 0.11364869913626864\n",
      "Epoch: 6 - Batch: 1317, Training Loss: 0.11374635713338654\n",
      "Epoch: 6 - Batch: 1318, Training Loss: 0.11383086447271937\n",
      "Epoch: 6 - Batch: 1319, Training Loss: 0.11391396855759384\n",
      "Epoch: 6 - Batch: 1320, Training Loss: 0.11399302373379222\n",
      "Epoch: 6 - Batch: 1321, Training Loss: 0.11407880971260727\n",
      "Epoch: 6 - Batch: 1322, Training Loss: 0.11416848525281371\n",
      "Epoch: 6 - Batch: 1323, Training Loss: 0.11424715506224885\n",
      "Epoch: 6 - Batch: 1324, Training Loss: 0.11433472592463936\n",
      "Epoch: 6 - Batch: 1325, Training Loss: 0.11441598039361375\n",
      "Epoch: 6 - Batch: 1326, Training Loss: 0.11450858710713648\n",
      "Epoch: 6 - Batch: 1327, Training Loss: 0.1145954905000591\n",
      "Epoch: 6 - Batch: 1328, Training Loss: 0.11467523238996961\n",
      "Epoch: 6 - Batch: 1329, Training Loss: 0.11475442000857831\n",
      "Epoch: 6 - Batch: 1330, Training Loss: 0.11482902794804542\n",
      "Epoch: 6 - Batch: 1331, Training Loss: 0.11490730804145632\n",
      "Epoch: 6 - Batch: 1332, Training Loss: 0.11499268293479584\n",
      "Epoch: 6 - Batch: 1333, Training Loss: 0.11507649386107031\n",
      "Epoch: 6 - Batch: 1334, Training Loss: 0.11517288320223688\n",
      "Epoch: 6 - Batch: 1335, Training Loss: 0.11525910536744702\n",
      "Epoch: 6 - Batch: 1336, Training Loss: 0.11535126965486786\n",
      "Epoch: 6 - Batch: 1337, Training Loss: 0.11544044608030944\n",
      "Epoch: 6 - Batch: 1338, Training Loss: 0.11552235536017821\n",
      "Epoch: 6 - Batch: 1339, Training Loss: 0.11559945173796335\n",
      "Epoch: 6 - Batch: 1340, Training Loss: 0.11568148659656495\n",
      "Epoch: 6 - Batch: 1341, Training Loss: 0.11576681465503588\n",
      "Epoch: 6 - Batch: 1342, Training Loss: 0.11585008151594481\n",
      "Epoch: 6 - Batch: 1343, Training Loss: 0.11593342033117565\n",
      "Epoch: 6 - Batch: 1344, Training Loss: 0.11602601625185899\n",
      "Epoch: 6 - Batch: 1345, Training Loss: 0.11610400582822797\n",
      "Epoch: 6 - Batch: 1346, Training Loss: 0.11618776794269706\n",
      "Epoch: 6 - Batch: 1347, Training Loss: 0.11627205576790901\n",
      "Epoch: 6 - Batch: 1348, Training Loss: 0.11635599982234376\n",
      "Epoch: 6 - Batch: 1349, Training Loss: 0.11644637755197079\n",
      "Epoch: 6 - Batch: 1350, Training Loss: 0.11653003533977774\n",
      "Epoch: 6 - Batch: 1351, Training Loss: 0.11661360934253158\n",
      "Epoch: 6 - Batch: 1352, Training Loss: 0.11670331412286901\n",
      "Epoch: 6 - Batch: 1353, Training Loss: 0.11678528645466611\n",
      "Epoch: 6 - Batch: 1354, Training Loss: 0.1168607992937118\n",
      "Epoch: 6 - Batch: 1355, Training Loss: 0.1169584195375146\n",
      "Epoch: 6 - Batch: 1356, Training Loss: 0.11705317790581417\n",
      "Epoch: 6 - Batch: 1357, Training Loss: 0.11713030529679548\n",
      "Epoch: 6 - Batch: 1358, Training Loss: 0.1172208099642994\n",
      "Epoch: 6 - Batch: 1359, Training Loss: 0.11730719419508233\n",
      "Epoch: 6 - Batch: 1360, Training Loss: 0.11739242792277788\n",
      "Epoch: 6 - Batch: 1361, Training Loss: 0.11748388379960512\n",
      "Epoch: 6 - Batch: 1362, Training Loss: 0.11756688265584002\n",
      "Epoch: 6 - Batch: 1363, Training Loss: 0.11765733011573504\n",
      "Epoch: 6 - Batch: 1364, Training Loss: 0.11774612202696737\n",
      "Epoch: 6 - Batch: 1365, Training Loss: 0.1178422413432776\n",
      "Epoch: 6 - Batch: 1366, Training Loss: 0.11792833189391971\n",
      "Epoch: 6 - Batch: 1367, Training Loss: 0.1180138524195448\n",
      "Epoch: 6 - Batch: 1368, Training Loss: 0.1181052673490684\n",
      "Epoch: 6 - Batch: 1369, Training Loss: 0.11819016881546571\n",
      "Epoch: 6 - Batch: 1370, Training Loss: 0.11827963890251433\n",
      "Epoch: 6 - Batch: 1371, Training Loss: 0.11836684646569871\n",
      "Epoch: 6 - Batch: 1372, Training Loss: 0.1184542919285163\n",
      "Epoch: 6 - Batch: 1373, Training Loss: 0.11853940358036391\n",
      "Epoch: 6 - Batch: 1374, Training Loss: 0.11862962822712476\n",
      "Epoch: 6 - Batch: 1375, Training Loss: 0.11871599328162065\n",
      "Epoch: 6 - Batch: 1376, Training Loss: 0.11879453218694944\n",
      "Epoch: 6 - Batch: 1377, Training Loss: 0.11888182757555153\n",
      "Epoch: 6 - Batch: 1378, Training Loss: 0.11897195534623084\n",
      "Epoch: 6 - Batch: 1379, Training Loss: 0.11905815472106633\n",
      "Epoch: 6 - Batch: 1380, Training Loss: 0.11914675938549327\n",
      "Epoch: 6 - Batch: 1381, Training Loss: 0.11923763847247285\n",
      "Epoch: 6 - Batch: 1382, Training Loss: 0.11931899998639749\n",
      "Epoch: 6 - Batch: 1383, Training Loss: 0.1194161878370527\n",
      "Epoch: 6 - Batch: 1384, Training Loss: 0.11950326854267326\n",
      "Epoch: 6 - Batch: 1385, Training Loss: 0.11958619659985871\n",
      "Epoch: 6 - Batch: 1386, Training Loss: 0.11967531146843043\n",
      "Epoch: 6 - Batch: 1387, Training Loss: 0.11975828395727064\n",
      "Epoch: 6 - Batch: 1388, Training Loss: 0.11984103644393372\n",
      "Epoch: 6 - Batch: 1389, Training Loss: 0.11992857915968642\n",
      "Epoch: 6 - Batch: 1390, Training Loss: 0.12001289308664218\n",
      "Epoch: 6 - Batch: 1391, Training Loss: 0.12009764360773623\n",
      "Epoch: 6 - Batch: 1392, Training Loss: 0.120184813460851\n",
      "Epoch: 6 - Batch: 1393, Training Loss: 0.12027196129583205\n",
      "Epoch: 6 - Batch: 1394, Training Loss: 0.1203578997088309\n",
      "Epoch: 6 - Batch: 1395, Training Loss: 0.12043949170269776\n",
      "Epoch: 6 - Batch: 1396, Training Loss: 0.12052456768936977\n",
      "Epoch: 6 - Batch: 1397, Training Loss: 0.12061510143020063\n",
      "Epoch: 6 - Batch: 1398, Training Loss: 0.12071626184888147\n",
      "Epoch: 6 - Batch: 1399, Training Loss: 0.12080093804950738\n",
      "Epoch: 6 - Batch: 1400, Training Loss: 0.12089203425974988\n",
      "Epoch: 6 - Batch: 1401, Training Loss: 0.1209764625957455\n",
      "Epoch: 6 - Batch: 1402, Training Loss: 0.1210584355329795\n",
      "Epoch: 6 - Batch: 1403, Training Loss: 0.1211495935039635\n",
      "Epoch: 6 - Batch: 1404, Training Loss: 0.1212324348971816\n",
      "Epoch: 6 - Batch: 1405, Training Loss: 0.12131609252436244\n",
      "Epoch: 6 - Batch: 1406, Training Loss: 0.12140435882987667\n",
      "Epoch: 6 - Batch: 1407, Training Loss: 0.12149318658544452\n",
      "Epoch: 6 - Batch: 1408, Training Loss: 0.12158284859598968\n",
      "Epoch: 6 - Batch: 1409, Training Loss: 0.12166380859404852\n",
      "Epoch: 6 - Batch: 1410, Training Loss: 0.12174987073857986\n",
      "Epoch: 6 - Batch: 1411, Training Loss: 0.12184264257574952\n",
      "Epoch: 6 - Batch: 1412, Training Loss: 0.12193443227071271\n",
      "Epoch: 6 - Batch: 1413, Training Loss: 0.12202059715417883\n",
      "Epoch: 6 - Batch: 1414, Training Loss: 0.12211497018660479\n",
      "Epoch: 6 - Batch: 1415, Training Loss: 0.12219597415915176\n",
      "Epoch: 6 - Batch: 1416, Training Loss: 0.12228560125205054\n",
      "Epoch: 6 - Batch: 1417, Training Loss: 0.12238362551995771\n",
      "Epoch: 6 - Batch: 1418, Training Loss: 0.12246741336549495\n",
      "Epoch: 6 - Batch: 1419, Training Loss: 0.12256029146919599\n",
      "Epoch: 6 - Batch: 1420, Training Loss: 0.1226464126994262\n",
      "Epoch: 6 - Batch: 1421, Training Loss: 0.12273616795343151\n",
      "Epoch: 6 - Batch: 1422, Training Loss: 0.12282401719941428\n",
      "Epoch: 6 - Batch: 1423, Training Loss: 0.12291136923343388\n",
      "Epoch: 6 - Batch: 1424, Training Loss: 0.12300609527263871\n",
      "Epoch: 6 - Batch: 1425, Training Loss: 0.12309292318364282\n",
      "Epoch: 6 - Batch: 1426, Training Loss: 0.12318365954814067\n",
      "Epoch: 6 - Batch: 1427, Training Loss: 0.12327494883097424\n",
      "Epoch: 6 - Batch: 1428, Training Loss: 0.12336645250642675\n",
      "Epoch: 6 - Batch: 1429, Training Loss: 0.12344787865382917\n",
      "Epoch: 6 - Batch: 1430, Training Loss: 0.12353192098673502\n",
      "Epoch: 6 - Batch: 1431, Training Loss: 0.1236202761123133\n",
      "Epoch: 6 - Batch: 1432, Training Loss: 0.12370505337765561\n",
      "Epoch: 6 - Batch: 1433, Training Loss: 0.12378688307346199\n",
      "Epoch: 6 - Batch: 1434, Training Loss: 0.1238710474945716\n",
      "Epoch: 6 - Batch: 1435, Training Loss: 0.12395856654886188\n",
      "Epoch: 6 - Batch: 1436, Training Loss: 0.12404659108736028\n",
      "Epoch: 6 - Batch: 1437, Training Loss: 0.12413012419841182\n",
      "Epoch: 6 - Batch: 1438, Training Loss: 0.12420933688705042\n",
      "Epoch: 6 - Batch: 1439, Training Loss: 0.1242950383271052\n",
      "Epoch: 6 - Batch: 1440, Training Loss: 0.12437939842171337\n",
      "Epoch: 6 - Batch: 1441, Training Loss: 0.12446768333514531\n",
      "Epoch: 6 - Batch: 1442, Training Loss: 0.12455508856112091\n",
      "Epoch: 6 - Batch: 1443, Training Loss: 0.12464006093777037\n",
      "Epoch: 6 - Batch: 1444, Training Loss: 0.12472803437863021\n",
      "Epoch: 6 - Batch: 1445, Training Loss: 0.1248195401360443\n",
      "Epoch: 6 - Batch: 1446, Training Loss: 0.12490258214510298\n",
      "Epoch: 6 - Batch: 1447, Training Loss: 0.12499455053661988\n",
      "Epoch: 6 - Batch: 1448, Training Loss: 0.12508318513310568\n",
      "Epoch: 6 - Batch: 1449, Training Loss: 0.12517719605248762\n",
      "Epoch: 6 - Batch: 1450, Training Loss: 0.12526618317387392\n",
      "Epoch: 6 - Batch: 1451, Training Loss: 0.1253530905021364\n",
      "Epoch: 6 - Batch: 1452, Training Loss: 0.12544257743021545\n",
      "Epoch: 6 - Batch: 1453, Training Loss: 0.12552469719825296\n",
      "Epoch: 6 - Batch: 1454, Training Loss: 0.12561767126988022\n",
      "Epoch: 6 - Batch: 1455, Training Loss: 0.12570764884278549\n",
      "Epoch: 6 - Batch: 1456, Training Loss: 0.12579136514174405\n",
      "Epoch: 6 - Batch: 1457, Training Loss: 0.12588061093913383\n",
      "Epoch: 6 - Batch: 1458, Training Loss: 0.12597572228332263\n",
      "Epoch: 6 - Batch: 1459, Training Loss: 0.1260612389848106\n",
      "Epoch: 6 - Batch: 1460, Training Loss: 0.12614339097691807\n",
      "Epoch: 6 - Batch: 1461, Training Loss: 0.12623143581608634\n",
      "Epoch: 6 - Batch: 1462, Training Loss: 0.12631846617159756\n",
      "Epoch: 6 - Batch: 1463, Training Loss: 0.12640319366755573\n",
      "Epoch: 6 - Batch: 1464, Training Loss: 0.12649140882600796\n",
      "Epoch: 6 - Batch: 1465, Training Loss: 0.1265839877573906\n",
      "Epoch: 6 - Batch: 1466, Training Loss: 0.12667108039258923\n",
      "Epoch: 6 - Batch: 1467, Training Loss: 0.12675820014271175\n",
      "Epoch: 6 - Batch: 1468, Training Loss: 0.12684065015804313\n",
      "Epoch: 6 - Batch: 1469, Training Loss: 0.1269252775345078\n",
      "Epoch: 6 - Batch: 1470, Training Loss: 0.12702253993421447\n",
      "Epoch: 6 - Batch: 1471, Training Loss: 0.12711213551350495\n",
      "Epoch: 6 - Batch: 1472, Training Loss: 0.12719760035746924\n",
      "Epoch: 6 - Batch: 1473, Training Loss: 0.12728673747275798\n",
      "Epoch: 6 - Batch: 1474, Training Loss: 0.12738182383958577\n",
      "Epoch: 6 - Batch: 1475, Training Loss: 0.1274708031520717\n",
      "Epoch: 6 - Batch: 1476, Training Loss: 0.12755415853650415\n",
      "Epoch: 6 - Batch: 1477, Training Loss: 0.12764587219353538\n",
      "Epoch: 6 - Batch: 1478, Training Loss: 0.1277346746999134\n",
      "Epoch: 6 - Batch: 1479, Training Loss: 0.12782888881750962\n",
      "Epoch: 6 - Batch: 1480, Training Loss: 0.12791458802733255\n",
      "Epoch: 6 - Batch: 1481, Training Loss: 0.12799765054711063\n",
      "Epoch: 6 - Batch: 1482, Training Loss: 0.1280813574741531\n",
      "Epoch: 6 - Batch: 1483, Training Loss: 0.1281692101550636\n",
      "Epoch: 6 - Batch: 1484, Training Loss: 0.12824837951171852\n",
      "Epoch: 6 - Batch: 1485, Training Loss: 0.12833566732000357\n",
      "Epoch: 6 - Batch: 1486, Training Loss: 0.1284299340550085\n",
      "Epoch: 6 - Batch: 1487, Training Loss: 0.1285266645722227\n",
      "Epoch: 6 - Batch: 1488, Training Loss: 0.12860805007503995\n",
      "Epoch: 6 - Batch: 1489, Training Loss: 0.12869436690463357\n",
      "Epoch: 6 - Batch: 1490, Training Loss: 0.12878137692240735\n",
      "Epoch: 6 - Batch: 1491, Training Loss: 0.12886957669782007\n",
      "Epoch: 6 - Batch: 1492, Training Loss: 0.12895920237597344\n",
      "Epoch: 6 - Batch: 1493, Training Loss: 0.12904144775363344\n",
      "Epoch: 6 - Batch: 1494, Training Loss: 0.12913128001550536\n",
      "Epoch: 6 - Batch: 1495, Training Loss: 0.12921495201254563\n",
      "Epoch: 6 - Batch: 1496, Training Loss: 0.1293007316898746\n",
      "Epoch: 6 - Batch: 1497, Training Loss: 0.1293897132387703\n",
      "Epoch: 6 - Batch: 1498, Training Loss: 0.12946901486485357\n",
      "Epoch: 6 - Batch: 1499, Training Loss: 0.12955929542185854\n",
      "Epoch: 6 - Batch: 1500, Training Loss: 0.1296539609606191\n",
      "Epoch: 6 - Batch: 1501, Training Loss: 0.12974084387988988\n",
      "Epoch: 6 - Batch: 1502, Training Loss: 0.12981824816558293\n",
      "Epoch: 6 - Batch: 1503, Training Loss: 0.12990785430295154\n",
      "Epoch: 6 - Batch: 1504, Training Loss: 0.1299934461651058\n",
      "Epoch: 6 - Batch: 1505, Training Loss: 0.13007230550717952\n",
      "Epoch: 6 - Batch: 1506, Training Loss: 0.13016112167295532\n",
      "Epoch: 6 - Batch: 1507, Training Loss: 0.13024399330095074\n",
      "Epoch: 6 - Batch: 1508, Training Loss: 0.13032396717203987\n",
      "Epoch: 6 - Batch: 1509, Training Loss: 0.13041311140430112\n",
      "Epoch: 6 - Batch: 1510, Training Loss: 0.13050758735482174\n",
      "Epoch: 6 - Batch: 1511, Training Loss: 0.13058583072663144\n",
      "Epoch: 6 - Batch: 1512, Training Loss: 0.13066692995046503\n",
      "Epoch: 6 - Batch: 1513, Training Loss: 0.13075735429255522\n",
      "Epoch: 6 - Batch: 1514, Training Loss: 0.13084218684417098\n",
      "Epoch: 6 - Batch: 1515, Training Loss: 0.13093432258956667\n",
      "Epoch: 6 - Batch: 1516, Training Loss: 0.13101819216290714\n",
      "Epoch: 6 - Batch: 1517, Training Loss: 0.1311061708550054\n",
      "Epoch: 6 - Batch: 1518, Training Loss: 0.13118927395141144\n",
      "Epoch: 6 - Batch: 1519, Training Loss: 0.1312868143578568\n",
      "Epoch: 6 - Batch: 1520, Training Loss: 0.13136582255709428\n",
      "Epoch: 6 - Batch: 1521, Training Loss: 0.13144968839278862\n",
      "Epoch: 6 - Batch: 1522, Training Loss: 0.1315434069046828\n",
      "Epoch: 6 - Batch: 1523, Training Loss: 0.13164231867512463\n",
      "Epoch: 6 - Batch: 1524, Training Loss: 0.13172557846230654\n",
      "Epoch: 6 - Batch: 1525, Training Loss: 0.1318187466753063\n",
      "Epoch: 6 - Batch: 1526, Training Loss: 0.13190012518470956\n",
      "Epoch: 6 - Batch: 1527, Training Loss: 0.13198273747566328\n",
      "Epoch: 6 - Batch: 1528, Training Loss: 0.1320713105832364\n",
      "Epoch: 6 - Batch: 1529, Training Loss: 0.13215992550920097\n",
      "Epoch: 6 - Batch: 1530, Training Loss: 0.13226123559435407\n",
      "Epoch: 6 - Batch: 1531, Training Loss: 0.1323466218241906\n",
      "Epoch: 6 - Batch: 1532, Training Loss: 0.13242630853903037\n",
      "Epoch: 6 - Batch: 1533, Training Loss: 0.13250961091684466\n",
      "Epoch: 6 - Batch: 1534, Training Loss: 0.132597762133747\n",
      "Epoch: 6 - Batch: 1535, Training Loss: 0.13268194951821322\n",
      "Epoch: 6 - Batch: 1536, Training Loss: 0.13277205328146616\n",
      "Epoch: 6 - Batch: 1537, Training Loss: 0.1328638319673625\n",
      "Epoch: 6 - Batch: 1538, Training Loss: 0.1329501330555968\n",
      "Epoch: 6 - Batch: 1539, Training Loss: 0.13304358825186394\n",
      "Epoch: 6 - Batch: 1540, Training Loss: 0.13313262315951968\n",
      "Epoch: 6 - Batch: 1541, Training Loss: 0.13321569945607611\n",
      "Epoch: 6 - Batch: 1542, Training Loss: 0.13329585563162863\n",
      "Epoch: 6 - Batch: 1543, Training Loss: 0.13338937932257827\n",
      "Epoch: 6 - Batch: 1544, Training Loss: 0.1334805854196177\n",
      "Epoch: 6 - Batch: 1545, Training Loss: 0.13356974743195435\n",
      "Epoch: 6 - Batch: 1546, Training Loss: 0.13365503257236275\n",
      "Epoch: 6 - Batch: 1547, Training Loss: 0.13373648359827933\n",
      "Epoch: 6 - Batch: 1548, Training Loss: 0.13381224404866027\n",
      "Epoch: 6 - Batch: 1549, Training Loss: 0.13390342695993768\n",
      "Epoch: 6 - Batch: 1550, Training Loss: 0.13398612669945553\n",
      "Epoch: 6 - Batch: 1551, Training Loss: 0.13407421561195879\n",
      "Epoch: 6 - Batch: 1552, Training Loss: 0.134155794533331\n",
      "Epoch: 6 - Batch: 1553, Training Loss: 0.1342356718434129\n",
      "Epoch: 6 - Batch: 1554, Training Loss: 0.13433029026929813\n",
      "Epoch: 6 - Batch: 1555, Training Loss: 0.13440753636915687\n",
      "Epoch: 6 - Batch: 1556, Training Loss: 0.134495678825758\n",
      "Epoch: 6 - Batch: 1557, Training Loss: 0.1345835267990877\n",
      "Epoch: 6 - Batch: 1558, Training Loss: 0.13467379737873972\n",
      "Epoch: 6 - Batch: 1559, Training Loss: 0.13476175549216135\n",
      "Epoch: 6 - Batch: 1560, Training Loss: 0.13484351855284143\n",
      "Epoch: 6 - Batch: 1561, Training Loss: 0.13493140191334002\n",
      "Epoch: 6 - Batch: 1562, Training Loss: 0.13502150484875067\n",
      "Epoch: 6 - Batch: 1563, Training Loss: 0.13511455272190012\n",
      "Epoch: 6 - Batch: 1564, Training Loss: 0.1352050716974842\n",
      "Epoch: 6 - Batch: 1565, Training Loss: 0.13528756013334686\n",
      "Epoch: 6 - Batch: 1566, Training Loss: 0.13537679875816278\n",
      "Epoch: 6 - Batch: 1567, Training Loss: 0.1354599002050622\n",
      "Epoch: 6 - Batch: 1568, Training Loss: 0.13554247555818724\n",
      "Epoch: 6 - Batch: 1569, Training Loss: 0.1356319033696778\n",
      "Epoch: 6 - Batch: 1570, Training Loss: 0.13572156292758572\n",
      "Epoch: 6 - Batch: 1571, Training Loss: 0.135807298253573\n",
      "Epoch: 6 - Batch: 1572, Training Loss: 0.135891340740927\n",
      "Epoch: 6 - Batch: 1573, Training Loss: 0.13598084100097368\n",
      "Epoch: 6 - Batch: 1574, Training Loss: 0.13606489019137907\n",
      "Epoch: 6 - Batch: 1575, Training Loss: 0.1361455138692413\n",
      "Epoch: 6 - Batch: 1576, Training Loss: 0.13623616911755074\n",
      "Epoch: 6 - Batch: 1577, Training Loss: 0.13631775254263215\n",
      "Epoch: 6 - Batch: 1578, Training Loss: 0.1363955085439172\n",
      "Epoch: 6 - Batch: 1579, Training Loss: 0.13647686691945465\n",
      "Epoch: 6 - Batch: 1580, Training Loss: 0.13656309242074566\n",
      "Epoch: 6 - Batch: 1581, Training Loss: 0.13664842737996163\n",
      "Epoch: 6 - Batch: 1582, Training Loss: 0.1367338765011004\n",
      "Epoch: 6 - Batch: 1583, Training Loss: 0.13681602775185658\n",
      "Epoch: 6 - Batch: 1584, Training Loss: 0.1369023027939484\n",
      "Epoch: 6 - Batch: 1585, Training Loss: 0.13699090689618393\n",
      "Epoch: 6 - Batch: 1586, Training Loss: 0.1370793150963376\n",
      "Epoch: 6 - Batch: 1587, Training Loss: 0.13716200640916232\n",
      "Epoch: 6 - Batch: 1588, Training Loss: 0.13724419771142266\n",
      "Epoch: 6 - Batch: 1589, Training Loss: 0.13732722491816698\n",
      "Epoch: 6 - Batch: 1590, Training Loss: 0.13741440910407363\n",
      "Epoch: 6 - Batch: 1591, Training Loss: 0.13750753808105565\n",
      "Epoch: 6 - Batch: 1592, Training Loss: 0.13758986329607306\n",
      "Epoch: 6 - Batch: 1593, Training Loss: 0.13767085413434613\n",
      "Epoch: 6 - Batch: 1594, Training Loss: 0.13775263868803606\n",
      "Epoch: 6 - Batch: 1595, Training Loss: 0.13783739007403997\n",
      "Epoch: 6 - Batch: 1596, Training Loss: 0.13792505216588627\n",
      "Epoch: 6 - Batch: 1597, Training Loss: 0.13801280950284123\n",
      "Epoch: 6 - Batch: 1598, Training Loss: 0.13809718182356798\n",
      "Epoch: 6 - Batch: 1599, Training Loss: 0.1381776732628915\n",
      "Epoch: 6 - Batch: 1600, Training Loss: 0.13825983074099862\n",
      "Epoch: 6 - Batch: 1601, Training Loss: 0.1383478000364691\n",
      "Epoch: 6 - Batch: 1602, Training Loss: 0.1384309508511874\n",
      "Epoch: 6 - Batch: 1603, Training Loss: 0.13850928975229043\n",
      "Epoch: 6 - Batch: 1604, Training Loss: 0.13859872745464294\n",
      "Epoch: 6 - Batch: 1605, Training Loss: 0.13869529848865805\n",
      "Epoch: 6 - Batch: 1606, Training Loss: 0.13877849772597228\n",
      "Epoch: 6 - Batch: 1607, Training Loss: 0.13887382218394903\n",
      "Epoch: 6 - Batch: 1608, Training Loss: 0.13896418251653217\n",
      "Epoch: 6 - Batch: 1609, Training Loss: 0.13904995520044716\n",
      "Epoch: 6 - Batch: 1610, Training Loss: 0.13914452858924076\n",
      "Epoch: 6 - Batch: 1611, Training Loss: 0.1392304872905536\n",
      "Epoch: 6 - Batch: 1612, Training Loss: 0.1393155122475442\n",
      "Epoch: 6 - Batch: 1613, Training Loss: 0.13939764074499333\n",
      "Epoch: 6 - Batch: 1614, Training Loss: 0.13949023584401232\n",
      "Epoch: 6 - Batch: 1615, Training Loss: 0.13956924762026984\n",
      "Epoch: 6 - Batch: 1616, Training Loss: 0.13965896640596895\n",
      "Epoch: 6 - Batch: 1617, Training Loss: 0.1397513069002387\n",
      "Epoch: 6 - Batch: 1618, Training Loss: 0.13984711361959404\n",
      "Epoch: 6 - Batch: 1619, Training Loss: 0.1399289583277643\n",
      "Epoch: 6 - Batch: 1620, Training Loss: 0.14000970164119309\n",
      "Epoch: 6 - Batch: 1621, Training Loss: 0.14009255705830667\n",
      "Epoch: 6 - Batch: 1622, Training Loss: 0.14017674502349808\n",
      "Epoch: 6 - Batch: 1623, Training Loss: 0.14026655863420684\n",
      "Epoch: 6 - Batch: 1624, Training Loss: 0.14034580853250292\n",
      "Epoch: 6 - Batch: 1625, Training Loss: 0.1404274178554565\n",
      "Epoch: 6 - Batch: 1626, Training Loss: 0.14052132565287215\n",
      "Epoch: 6 - Batch: 1627, Training Loss: 0.14060140827401954\n",
      "Epoch: 6 - Batch: 1628, Training Loss: 0.14068106361116542\n",
      "Epoch: 6 - Batch: 1629, Training Loss: 0.1407604827167185\n",
      "Epoch: 6 - Batch: 1630, Training Loss: 0.1408575720788531\n",
      "Epoch: 6 - Batch: 1631, Training Loss: 0.1409459089610114\n",
      "Epoch: 6 - Batch: 1632, Training Loss: 0.14102549311607632\n",
      "Epoch: 6 - Batch: 1633, Training Loss: 0.14111226333759316\n",
      "Epoch: 6 - Batch: 1634, Training Loss: 0.14119895170774824\n",
      "Epoch: 6 - Batch: 1635, Training Loss: 0.14129785689869725\n",
      "Epoch: 6 - Batch: 1636, Training Loss: 0.14138491855776725\n",
      "Epoch: 6 - Batch: 1637, Training Loss: 0.14146289184911928\n",
      "Epoch: 6 - Batch: 1638, Training Loss: 0.14155207649738238\n",
      "Epoch: 6 - Batch: 1639, Training Loss: 0.14163555164711789\n",
      "Epoch: 6 - Batch: 1640, Training Loss: 0.141715681856841\n",
      "Epoch: 6 - Batch: 1641, Training Loss: 0.14180726441207217\n",
      "Epoch: 6 - Batch: 1642, Training Loss: 0.14189374564867313\n",
      "Epoch: 6 - Batch: 1643, Training Loss: 0.14197559471846022\n",
      "Epoch: 6 - Batch: 1644, Training Loss: 0.14205644507683926\n",
      "Epoch: 6 - Batch: 1645, Training Loss: 0.1421445820412628\n",
      "Epoch: 6 - Batch: 1646, Training Loss: 0.14222683793993338\n",
      "Epoch: 6 - Batch: 1647, Training Loss: 0.14231772167889237\n",
      "Epoch: 6 - Batch: 1648, Training Loss: 0.14240477992156844\n",
      "Epoch: 6 - Batch: 1649, Training Loss: 0.1424861310256852\n",
      "Epoch: 6 - Batch: 1650, Training Loss: 0.14256640503284942\n",
      "Epoch: 6 - Batch: 1651, Training Loss: 0.14265241295371087\n",
      "Epoch: 6 - Batch: 1652, Training Loss: 0.14273379427407115\n",
      "Epoch: 6 - Batch: 1653, Training Loss: 0.14281997258217377\n",
      "Epoch: 6 - Batch: 1654, Training Loss: 0.14290668981560625\n",
      "Epoch: 6 - Batch: 1655, Training Loss: 0.14299175325490746\n",
      "Epoch: 6 - Batch: 1656, Training Loss: 0.14308093428908297\n",
      "Epoch: 6 - Batch: 1657, Training Loss: 0.14316209212291497\n",
      "Epoch: 6 - Batch: 1658, Training Loss: 0.14325048219702927\n",
      "Epoch: 6 - Batch: 1659, Training Loss: 0.14333694960742843\n",
      "Epoch: 6 - Batch: 1660, Training Loss: 0.1434210976570301\n",
      "Epoch: 6 - Batch: 1661, Training Loss: 0.1434997729092194\n",
      "Epoch: 6 - Batch: 1662, Training Loss: 0.14358017763515216\n",
      "Epoch: 6 - Batch: 1663, Training Loss: 0.14367168692016286\n",
      "Epoch: 6 - Batch: 1664, Training Loss: 0.14375752302050393\n",
      "Epoch: 6 - Batch: 1665, Training Loss: 0.1438431062706272\n",
      "Epoch: 6 - Batch: 1666, Training Loss: 0.1439247764894124\n",
      "Epoch: 6 - Batch: 1667, Training Loss: 0.14401814080836564\n",
      "Epoch: 6 - Batch: 1668, Training Loss: 0.14410263701507306\n",
      "Epoch: 6 - Batch: 1669, Training Loss: 0.1441866311663221\n",
      "Epoch: 6 - Batch: 1670, Training Loss: 0.14427354348017207\n",
      "Epoch: 6 - Batch: 1671, Training Loss: 0.14435962351598156\n",
      "Epoch: 6 - Batch: 1672, Training Loss: 0.14444680419282532\n",
      "Epoch: 6 - Batch: 1673, Training Loss: 0.14453838333784053\n",
      "Epoch: 6 - Batch: 1674, Training Loss: 0.14463752192471355\n",
      "Epoch: 6 - Batch: 1675, Training Loss: 0.14472857631656463\n",
      "Epoch: 6 - Batch: 1676, Training Loss: 0.14481668021054211\n",
      "Epoch: 6 - Batch: 1677, Training Loss: 0.14490270293371793\n",
      "Epoch: 6 - Batch: 1678, Training Loss: 0.14499175218034344\n",
      "Epoch: 6 - Batch: 1679, Training Loss: 0.1450798378865023\n",
      "Epoch: 6 - Batch: 1680, Training Loss: 0.14517073947349393\n",
      "Epoch: 6 - Batch: 1681, Training Loss: 0.14526054076541517\n",
      "Epoch: 6 - Batch: 1682, Training Loss: 0.14533926758476554\n",
      "Epoch: 6 - Batch: 1683, Training Loss: 0.14542043812659447\n",
      "Epoch: 6 - Batch: 1684, Training Loss: 0.1455040271849577\n",
      "Epoch: 6 - Batch: 1685, Training Loss: 0.14559713492840282\n",
      "Epoch: 6 - Batch: 1686, Training Loss: 0.1456806256959391\n",
      "Epoch: 6 - Batch: 1687, Training Loss: 0.1457659559290405\n",
      "Epoch: 6 - Batch: 1688, Training Loss: 0.14584983212511932\n",
      "Epoch: 6 - Batch: 1689, Training Loss: 0.14594018312285392\n",
      "Epoch: 6 - Batch: 1690, Training Loss: 0.14601743176184087\n",
      "Epoch: 6 - Batch: 1691, Training Loss: 0.14610371288919133\n",
      "Epoch: 6 - Batch: 1692, Training Loss: 0.14619235511541762\n",
      "Epoch: 6 - Batch: 1693, Training Loss: 0.14628129705761994\n",
      "Epoch: 6 - Batch: 1694, Training Loss: 0.14637077167926738\n",
      "Epoch: 6 - Batch: 1695, Training Loss: 0.14645335080710611\n",
      "Epoch: 6 - Batch: 1696, Training Loss: 0.1465396338928596\n",
      "Epoch: 6 - Batch: 1697, Training Loss: 0.14662548216989582\n",
      "Epoch: 6 - Batch: 1698, Training Loss: 0.1467118724921153\n",
      "Epoch: 6 - Batch: 1699, Training Loss: 0.14680696138571547\n",
      "Epoch: 6 - Batch: 1700, Training Loss: 0.1469066610581444\n",
      "Epoch: 6 - Batch: 1701, Training Loss: 0.14699693009331452\n",
      "Epoch: 6 - Batch: 1702, Training Loss: 0.14708449782349578\n",
      "Epoch: 6 - Batch: 1703, Training Loss: 0.147177096993769\n",
      "Epoch: 6 - Batch: 1704, Training Loss: 0.147263756469757\n",
      "Epoch: 6 - Batch: 1705, Training Loss: 0.1473501989089731\n",
      "Epoch: 6 - Batch: 1706, Training Loss: 0.1474388991768285\n",
      "Epoch: 6 - Batch: 1707, Training Loss: 0.14752511530620344\n",
      "Epoch: 6 - Batch: 1708, Training Loss: 0.14761013979984952\n",
      "Epoch: 6 - Batch: 1709, Training Loss: 0.1476954582701769\n",
      "Epoch: 6 - Batch: 1710, Training Loss: 0.1477765302246384\n",
      "Epoch: 6 - Batch: 1711, Training Loss: 0.1478724317010461\n",
      "Epoch: 6 - Batch: 1712, Training Loss: 0.1479590195933879\n",
      "Epoch: 6 - Batch: 1713, Training Loss: 0.14805096151248534\n",
      "Epoch: 6 - Batch: 1714, Training Loss: 0.1481389128487205\n",
      "Epoch: 6 - Batch: 1715, Training Loss: 0.1482255678766007\n",
      "Epoch: 6 - Batch: 1716, Training Loss: 0.14831141644399953\n",
      "Epoch: 6 - Batch: 1717, Training Loss: 0.14840184126179018\n",
      "Epoch: 6 - Batch: 1718, Training Loss: 0.14848631789064526\n",
      "Epoch: 6 - Batch: 1719, Training Loss: 0.14857601097616588\n",
      "Epoch: 6 - Batch: 1720, Training Loss: 0.1486596846474245\n",
      "Epoch: 6 - Batch: 1721, Training Loss: 0.1487522929301408\n",
      "Epoch: 6 - Batch: 1722, Training Loss: 0.1488417813903459\n",
      "Epoch: 6 - Batch: 1723, Training Loss: 0.14892964368411163\n",
      "Epoch: 6 - Batch: 1724, Training Loss: 0.14901653861327352\n",
      "Epoch: 6 - Batch: 1725, Training Loss: 0.14910204808040836\n",
      "Epoch: 6 - Batch: 1726, Training Loss: 0.14918808735425199\n",
      "Epoch: 6 - Batch: 1727, Training Loss: 0.14926738065866688\n",
      "Epoch: 6 - Batch: 1728, Training Loss: 0.1493574262381984\n",
      "Epoch: 6 - Batch: 1729, Training Loss: 0.149449381186545\n",
      "Epoch: 6 - Batch: 1730, Training Loss: 0.14953592019294626\n",
      "Epoch: 6 - Batch: 1731, Training Loss: 0.14962477305561156\n",
      "Epoch: 6 - Batch: 1732, Training Loss: 0.1497138951152909\n",
      "Epoch: 6 - Batch: 1733, Training Loss: 0.14979832842451818\n",
      "Epoch: 6 - Batch: 1734, Training Loss: 0.1498931841631335\n",
      "Epoch: 6 - Batch: 1735, Training Loss: 0.1499776805860684\n",
      "Epoch: 6 - Batch: 1736, Training Loss: 0.1500585028225213\n",
      "Epoch: 6 - Batch: 1737, Training Loss: 0.15014607751522688\n",
      "Epoch: 6 - Batch: 1738, Training Loss: 0.15023348723873373\n",
      "Epoch: 6 - Batch: 1739, Training Loss: 0.1503182287400536\n",
      "Epoch: 6 - Batch: 1740, Training Loss: 0.15040924569366385\n",
      "Epoch: 6 - Batch: 1741, Training Loss: 0.15048973692911577\n",
      "Epoch: 6 - Batch: 1742, Training Loss: 0.15057331145699346\n",
      "Epoch: 6 - Batch: 1743, Training Loss: 0.15065890440646293\n",
      "Epoch: 6 - Batch: 1744, Training Loss: 0.15075046444314827\n",
      "Epoch: 6 - Batch: 1745, Training Loss: 0.15083575630514182\n",
      "Epoch: 6 - Batch: 1746, Training Loss: 0.15092539186180132\n",
      "Epoch: 6 - Batch: 1747, Training Loss: 0.15100965625413418\n",
      "Epoch: 6 - Batch: 1748, Training Loss: 0.15111091285696868\n",
      "Epoch: 6 - Batch: 1749, Training Loss: 0.15119254603907836\n",
      "Epoch: 6 - Batch: 1750, Training Loss: 0.15128093342579418\n",
      "Epoch: 6 - Batch: 1751, Training Loss: 0.15137493947696923\n",
      "Epoch: 6 - Batch: 1752, Training Loss: 0.15146673598618648\n",
      "Epoch: 6 - Batch: 1753, Training Loss: 0.15154942901029714\n",
      "Epoch: 6 - Batch: 1754, Training Loss: 0.1516333580338342\n",
      "Epoch: 6 - Batch: 1755, Training Loss: 0.15171932249915343\n",
      "Epoch: 6 - Batch: 1756, Training Loss: 0.15180868186538493\n",
      "Epoch: 6 - Batch: 1757, Training Loss: 0.15189258567062183\n",
      "Epoch: 6 - Batch: 1758, Training Loss: 0.1519777823245743\n",
      "Epoch: 6 - Batch: 1759, Training Loss: 0.15206222110744536\n",
      "Epoch: 6 - Batch: 1760, Training Loss: 0.15214459203912648\n",
      "Epoch: 6 - Batch: 1761, Training Loss: 0.15223855765576583\n",
      "Epoch: 6 - Batch: 1762, Training Loss: 0.15232667293566376\n",
      "Epoch: 6 - Batch: 1763, Training Loss: 0.15241126737141886\n",
      "Epoch: 6 - Batch: 1764, Training Loss: 0.15249770219325032\n",
      "Epoch: 6 - Batch: 1765, Training Loss: 0.15258129608275287\n",
      "Epoch: 6 - Batch: 1766, Training Loss: 0.1526670522283559\n",
      "Epoch: 6 - Batch: 1767, Training Loss: 0.15275650821119596\n",
      "Epoch: 6 - Batch: 1768, Training Loss: 0.15284054232128028\n",
      "Epoch: 6 - Batch: 1769, Training Loss: 0.15292604126122658\n",
      "Epoch: 6 - Batch: 1770, Training Loss: 0.1530103659748438\n",
      "Epoch: 6 - Batch: 1771, Training Loss: 0.15309611276706456\n",
      "Epoch: 6 - Batch: 1772, Training Loss: 0.15318525896390675\n",
      "Epoch: 6 - Batch: 1773, Training Loss: 0.15327116255810605\n",
      "Epoch: 6 - Batch: 1774, Training Loss: 0.15336430856145633\n",
      "Epoch: 6 - Batch: 1775, Training Loss: 0.15345274395066905\n",
      "Epoch: 6 - Batch: 1776, Training Loss: 0.15353755486100468\n",
      "Epoch: 6 - Batch: 1777, Training Loss: 0.15362166932121438\n",
      "Epoch: 6 - Batch: 1778, Training Loss: 0.15370758255298062\n",
      "Epoch: 6 - Batch: 1779, Training Loss: 0.15379796360015474\n",
      "Epoch: 6 - Batch: 1780, Training Loss: 0.15387979223335757\n",
      "Epoch: 6 - Batch: 1781, Training Loss: 0.15397268953805737\n",
      "Epoch: 6 - Batch: 1782, Training Loss: 0.1540614931441065\n",
      "Epoch: 6 - Batch: 1783, Training Loss: 0.15415499141288436\n",
      "Epoch: 6 - Batch: 1784, Training Loss: 0.15423722192645073\n",
      "Epoch: 6 - Batch: 1785, Training Loss: 0.15433097003061774\n",
      "Epoch: 6 - Batch: 1786, Training Loss: 0.15441543780650271\n",
      "Epoch: 6 - Batch: 1787, Training Loss: 0.15450078601737324\n",
      "Epoch: 6 - Batch: 1788, Training Loss: 0.15458601776813197\n",
      "Epoch: 6 - Batch: 1789, Training Loss: 0.15468102414933207\n",
      "Epoch: 6 - Batch: 1790, Training Loss: 0.15476628668470366\n",
      "Epoch: 6 - Batch: 1791, Training Loss: 0.1548567970544347\n",
      "Epoch: 6 - Batch: 1792, Training Loss: 0.15495610426166165\n",
      "Epoch: 6 - Batch: 1793, Training Loss: 0.15503557565719334\n",
      "Epoch: 6 - Batch: 1794, Training Loss: 0.15513199279903378\n",
      "Epoch: 6 - Batch: 1795, Training Loss: 0.15521634446077087\n",
      "Epoch: 6 - Batch: 1796, Training Loss: 0.1553012273933856\n",
      "Epoch: 6 - Batch: 1797, Training Loss: 0.155392179972991\n",
      "Epoch: 6 - Batch: 1798, Training Loss: 0.15548150395205365\n",
      "Epoch: 6 - Batch: 1799, Training Loss: 0.15556681444667664\n",
      "Epoch: 6 - Batch: 1800, Training Loss: 0.15565274760225914\n",
      "Epoch: 6 - Batch: 1801, Training Loss: 0.15574146778157497\n",
      "Epoch: 6 - Batch: 1802, Training Loss: 0.15582922914653868\n",
      "Epoch: 6 - Batch: 1803, Training Loss: 0.15591615187637445\n",
      "Epoch: 6 - Batch: 1804, Training Loss: 0.15600442176508666\n",
      "Epoch: 6 - Batch: 1805, Training Loss: 0.1560879930495524\n",
      "Epoch: 6 - Batch: 1806, Training Loss: 0.15617307622115412\n",
      "Epoch: 6 - Batch: 1807, Training Loss: 0.15626075365897238\n",
      "Epoch: 6 - Batch: 1808, Training Loss: 0.15634811035732724\n",
      "Epoch: 6 - Batch: 1809, Training Loss: 0.15644218597814416\n",
      "Epoch: 6 - Batch: 1810, Training Loss: 0.15652813782195743\n",
      "Epoch: 6 - Batch: 1811, Training Loss: 0.1566113475061471\n",
      "Epoch: 6 - Batch: 1812, Training Loss: 0.1566980971304536\n",
      "Epoch: 6 - Batch: 1813, Training Loss: 0.1567801633420374\n",
      "Epoch: 6 - Batch: 1814, Training Loss: 0.15687152358454654\n",
      "Epoch: 6 - Batch: 1815, Training Loss: 0.15696321988777934\n",
      "Epoch: 6 - Batch: 1816, Training Loss: 0.1570519561976639\n",
      "Epoch: 6 - Batch: 1817, Training Loss: 0.15714356318659845\n",
      "Epoch: 6 - Batch: 1818, Training Loss: 0.15723285656638605\n",
      "Epoch: 6 - Batch: 1819, Training Loss: 0.15732193063592437\n",
      "Epoch: 6 - Batch: 1820, Training Loss: 0.15740361623774912\n",
      "Epoch: 6 - Batch: 1821, Training Loss: 0.15749228782766495\n",
      "Epoch: 6 - Batch: 1822, Training Loss: 0.15758073538739487\n",
      "Epoch: 6 - Batch: 1823, Training Loss: 0.15766771970499016\n",
      "Epoch: 6 - Batch: 1824, Training Loss: 0.15775362468304524\n",
      "Epoch: 6 - Batch: 1825, Training Loss: 0.1578510305898304\n",
      "Epoch: 6 - Batch: 1826, Training Loss: 0.15793841916629134\n",
      "Epoch: 6 - Batch: 1827, Training Loss: 0.1580230794649318\n",
      "Epoch: 6 - Batch: 1828, Training Loss: 0.15811374919430335\n",
      "Epoch: 6 - Batch: 1829, Training Loss: 0.15819132632605273\n",
      "Epoch: 6 - Batch: 1830, Training Loss: 0.15827995840812203\n",
      "Epoch: 6 - Batch: 1831, Training Loss: 0.15837022792517053\n",
      "Epoch: 6 - Batch: 1832, Training Loss: 0.15846349989028516\n",
      "Epoch: 6 - Batch: 1833, Training Loss: 0.15854628756642342\n",
      "Epoch: 6 - Batch: 1834, Training Loss: 0.1586340075639844\n",
      "Epoch: 6 - Batch: 1835, Training Loss: 0.15871885553570728\n",
      "Epoch: 6 - Batch: 1836, Training Loss: 0.15880302500418367\n",
      "Epoch: 6 - Batch: 1837, Training Loss: 0.158879941672235\n",
      "Epoch: 6 - Batch: 1838, Training Loss: 0.15896238686260497\n",
      "Epoch: 6 - Batch: 1839, Training Loss: 0.15904835948278853\n",
      "Epoch: 6 - Batch: 1840, Training Loss: 0.15913160165669907\n",
      "Epoch: 6 - Batch: 1841, Training Loss: 0.15921198547652507\n",
      "Epoch: 6 - Batch: 1842, Training Loss: 0.1592886198071105\n",
      "Epoch: 6 - Batch: 1843, Training Loss: 0.15936862411684855\n",
      "Epoch: 6 - Batch: 1844, Training Loss: 0.1594612688802269\n",
      "Epoch: 6 - Batch: 1845, Training Loss: 0.15954764328810508\n",
      "Epoch: 6 - Batch: 1846, Training Loss: 0.15964057037041554\n",
      "Epoch: 6 - Batch: 1847, Training Loss: 0.1597264989110862\n",
      "Epoch: 6 - Batch: 1848, Training Loss: 0.159804506403445\n",
      "Epoch: 6 - Batch: 1849, Training Loss: 0.1598942348081773\n",
      "Epoch: 6 - Batch: 1850, Training Loss: 0.15997967258046317\n",
      "Epoch: 6 - Batch: 1851, Training Loss: 0.16005762995129597\n",
      "Epoch: 6 - Batch: 1852, Training Loss: 0.16014636594610626\n",
      "Epoch: 6 - Batch: 1853, Training Loss: 0.16023266320402546\n",
      "Epoch: 6 - Batch: 1854, Training Loss: 0.1603099723681684\n",
      "Epoch: 6 - Batch: 1855, Training Loss: 0.16039599636399726\n",
      "Epoch: 6 - Batch: 1856, Training Loss: 0.16048711400547036\n",
      "Epoch: 6 - Batch: 1857, Training Loss: 0.16057290589656206\n",
      "Epoch: 6 - Batch: 1858, Training Loss: 0.16066102408008592\n",
      "Epoch: 6 - Batch: 1859, Training Loss: 0.16075347210736218\n",
      "Epoch: 6 - Batch: 1860, Training Loss: 0.16083895381706864\n",
      "Epoch: 6 - Batch: 1861, Training Loss: 0.16092332552618055\n",
      "Epoch: 6 - Batch: 1862, Training Loss: 0.1610106923130911\n",
      "Epoch: 6 - Batch: 1863, Training Loss: 0.1610941338734346\n",
      "Epoch: 6 - Batch: 1864, Training Loss: 0.16117692197584987\n",
      "Epoch: 6 - Batch: 1865, Training Loss: 0.16125946490721124\n",
      "Epoch: 6 - Batch: 1866, Training Loss: 0.161341852834371\n",
      "Epoch: 6 - Batch: 1867, Training Loss: 0.16143496506299151\n",
      "Epoch: 6 - Batch: 1868, Training Loss: 0.1615245347011643\n",
      "Epoch: 6 - Batch: 1869, Training Loss: 0.16160742795808397\n",
      "Epoch: 6 - Batch: 1870, Training Loss: 0.16169268668785222\n",
      "Epoch: 6 - Batch: 1871, Training Loss: 0.1617786985255197\n",
      "Epoch: 6 - Batch: 1872, Training Loss: 0.1618611954251431\n",
      "Epoch: 6 - Batch: 1873, Training Loss: 0.16194610573833262\n",
      "Epoch: 6 - Batch: 1874, Training Loss: 0.16203751582436102\n",
      "Epoch: 6 - Batch: 1875, Training Loss: 0.16212372633182193\n",
      "Epoch: 6 - Batch: 1876, Training Loss: 0.1622117734815351\n",
      "Epoch: 6 - Batch: 1877, Training Loss: 0.16229794198526673\n",
      "Epoch: 6 - Batch: 1878, Training Loss: 0.16238703372367777\n",
      "Epoch: 6 - Batch: 1879, Training Loss: 0.16247657619750322\n",
      "Epoch: 6 - Batch: 1880, Training Loss: 0.16255926209846341\n",
      "Epoch: 6 - Batch: 1881, Training Loss: 0.16265321781188496\n",
      "Epoch: 6 - Batch: 1882, Training Loss: 0.16273707187862738\n",
      "Epoch: 6 - Batch: 1883, Training Loss: 0.1628170022571067\n",
      "Epoch: 6 - Batch: 1884, Training Loss: 0.16289874672222493\n",
      "Epoch: 6 - Batch: 1885, Training Loss: 0.1629868906985962\n",
      "Epoch: 6 - Batch: 1886, Training Loss: 0.16307276302012638\n",
      "Epoch: 6 - Batch: 1887, Training Loss: 0.1631565700728403\n",
      "Epoch: 6 - Batch: 1888, Training Loss: 0.1632395595448033\n",
      "Epoch: 6 - Batch: 1889, Training Loss: 0.16333837269698803\n",
      "Epoch: 6 - Batch: 1890, Training Loss: 0.16342017732894243\n",
      "Epoch: 6 - Batch: 1891, Training Loss: 0.1635109742680197\n",
      "Epoch: 6 - Batch: 1892, Training Loss: 0.16358787729520702\n",
      "Epoch: 6 - Batch: 1893, Training Loss: 0.16366592501205196\n",
      "Epoch: 6 - Batch: 1894, Training Loss: 0.1637569735781174\n",
      "Epoch: 6 - Batch: 1895, Training Loss: 0.16384166631087735\n",
      "Epoch: 6 - Batch: 1896, Training Loss: 0.16392927572353563\n",
      "Epoch: 6 - Batch: 1897, Training Loss: 0.1640235543634069\n",
      "Epoch: 6 - Batch: 1898, Training Loss: 0.16410991675521605\n",
      "Epoch: 6 - Batch: 1899, Training Loss: 0.16419894365009977\n",
      "Epoch: 6 - Batch: 1900, Training Loss: 0.16428292160282285\n",
      "Epoch: 6 - Batch: 1901, Training Loss: 0.16437386530474643\n",
      "Epoch: 6 - Batch: 1902, Training Loss: 0.16446370320088827\n",
      "Epoch: 6 - Batch: 1903, Training Loss: 0.16454618796088408\n",
      "Epoch: 6 - Batch: 1904, Training Loss: 0.1646231563011212\n",
      "Epoch: 6 - Batch: 1905, Training Loss: 0.16470611200453233\n",
      "Epoch: 6 - Batch: 1906, Training Loss: 0.16479615873933628\n",
      "Epoch: 6 - Batch: 1907, Training Loss: 0.16487647451546852\n",
      "Epoch: 6 - Batch: 1908, Training Loss: 0.16496115660365937\n",
      "Epoch: 6 - Batch: 1909, Training Loss: 0.16504580143276928\n",
      "Epoch: 6 - Batch: 1910, Training Loss: 0.16512528334313364\n",
      "Epoch: 6 - Batch: 1911, Training Loss: 0.16521165144952574\n",
      "Epoch: 6 - Batch: 1912, Training Loss: 0.16529813855515785\n",
      "Epoch: 6 - Batch: 1913, Training Loss: 0.16537931525638053\n",
      "Epoch: 6 - Batch: 1914, Training Loss: 0.16546005061263863\n",
      "Epoch: 6 - Batch: 1915, Training Loss: 0.16555283161785275\n",
      "Epoch: 6 - Batch: 1916, Training Loss: 0.16563064284980988\n",
      "Epoch: 6 - Batch: 1917, Training Loss: 0.1657227954177022\n",
      "Epoch: 6 - Batch: 1918, Training Loss: 0.1658005375742517\n",
      "Epoch: 6 - Batch: 1919, Training Loss: 0.16588758473372578\n",
      "Epoch: 6 - Batch: 1920, Training Loss: 0.16596880288884217\n",
      "Epoch: 6 - Batch: 1921, Training Loss: 0.1660712358741025\n",
      "Epoch: 6 - Batch: 1922, Training Loss: 0.16615702207532293\n",
      "Epoch: 6 - Batch: 1923, Training Loss: 0.16623869552516424\n",
      "Epoch: 6 - Batch: 1924, Training Loss: 0.16632759697722954\n",
      "Epoch: 6 - Batch: 1925, Training Loss: 0.16641877667598465\n",
      "Epoch: 6 - Batch: 1926, Training Loss: 0.16650753901965581\n",
      "Epoch: 6 - Batch: 1927, Training Loss: 0.16659630891275445\n",
      "Epoch: 6 - Batch: 1928, Training Loss: 0.1666876537166227\n",
      "Epoch: 6 - Batch: 1929, Training Loss: 0.1667723135519186\n",
      "Epoch: 6 - Batch: 1930, Training Loss: 0.16685871818839615\n",
      "Epoch: 6 - Batch: 1931, Training Loss: 0.1669483733970431\n",
      "Epoch: 6 - Batch: 1932, Training Loss: 0.16703130453083645\n",
      "Epoch: 6 - Batch: 1933, Training Loss: 0.16711137232495776\n",
      "Epoch: 6 - Batch: 1934, Training Loss: 0.16718942116618551\n",
      "Epoch: 6 - Batch: 1935, Training Loss: 0.16726728991464793\n",
      "Epoch: 6 - Batch: 1936, Training Loss: 0.1673462747242518\n",
      "Epoch: 6 - Batch: 1937, Training Loss: 0.16742909716358825\n",
      "Epoch: 6 - Batch: 1938, Training Loss: 0.1675161926097439\n",
      "Epoch: 6 - Batch: 1939, Training Loss: 0.16760060580981706\n",
      "Epoch: 6 - Batch: 1940, Training Loss: 0.16769262633068643\n",
      "Epoch: 6 - Batch: 1941, Training Loss: 0.16778146845611372\n",
      "Epoch: 6 - Batch: 1942, Training Loss: 0.16786356539670902\n",
      "Epoch: 6 - Batch: 1943, Training Loss: 0.1679395275502458\n",
      "Epoch: 6 - Batch: 1944, Training Loss: 0.16802523282061566\n",
      "Epoch: 6 - Batch: 1945, Training Loss: 0.16812380307522382\n",
      "Epoch: 6 - Batch: 1946, Training Loss: 0.16821183340861826\n",
      "Epoch: 6 - Batch: 1947, Training Loss: 0.16830079868485284\n",
      "Epoch: 6 - Batch: 1948, Training Loss: 0.16838312768693983\n",
      "Epoch: 6 - Batch: 1949, Training Loss: 0.16847188356849882\n",
      "Epoch: 6 - Batch: 1950, Training Loss: 0.16856055594918937\n",
      "Epoch: 6 - Batch: 1951, Training Loss: 0.16864481117347777\n",
      "Epoch: 6 - Batch: 1952, Training Loss: 0.16873103624849178\n",
      "Epoch: 6 - Batch: 1953, Training Loss: 0.16881630392389907\n",
      "Epoch: 6 - Batch: 1954, Training Loss: 0.168899795488388\n",
      "Epoch: 6 - Batch: 1955, Training Loss: 0.16898472174063053\n",
      "Epoch: 6 - Batch: 1956, Training Loss: 0.16907477462865028\n",
      "Epoch: 6 - Batch: 1957, Training Loss: 0.1691683974012016\n",
      "Epoch: 6 - Batch: 1958, Training Loss: 0.16925747648103914\n",
      "Epoch: 6 - Batch: 1959, Training Loss: 0.16934241507060294\n",
      "Epoch: 6 - Batch: 1960, Training Loss: 0.16943194482407561\n",
      "Epoch: 6 - Batch: 1961, Training Loss: 0.16951811862846908\n",
      "Epoch: 6 - Batch: 1962, Training Loss: 0.16960777000680097\n",
      "Epoch: 6 - Batch: 1963, Training Loss: 0.1696864612721784\n",
      "Epoch: 6 - Batch: 1964, Training Loss: 0.16976923184781328\n",
      "Epoch: 6 - Batch: 1965, Training Loss: 0.1698526390712356\n",
      "Epoch: 6 - Batch: 1966, Training Loss: 0.16993389925479296\n",
      "Epoch: 6 - Batch: 1967, Training Loss: 0.1700215862251534\n",
      "Epoch: 6 - Batch: 1968, Training Loss: 0.1701156078632396\n",
      "Epoch: 6 - Batch: 1969, Training Loss: 0.17020303827044778\n",
      "Epoch: 6 - Batch: 1970, Training Loss: 0.17028861919149238\n",
      "Epoch: 6 - Batch: 1971, Training Loss: 0.17037254460094184\n",
      "Epoch: 6 - Batch: 1972, Training Loss: 0.1704557261817392\n",
      "Epoch: 6 - Batch: 1973, Training Loss: 0.17054087377676916\n",
      "Epoch: 6 - Batch: 1974, Training Loss: 0.17062526505137754\n",
      "Epoch: 6 - Batch: 1975, Training Loss: 0.17071155352428383\n",
      "Epoch: 6 - Batch: 1976, Training Loss: 0.17079969283631982\n",
      "Epoch: 6 - Batch: 1977, Training Loss: 0.17088740989936524\n",
      "Epoch: 6 - Batch: 1978, Training Loss: 0.17096852173852684\n",
      "Epoch: 6 - Batch: 1979, Training Loss: 0.17105575145575933\n",
      "Epoch: 6 - Batch: 1980, Training Loss: 0.17114421902579652\n",
      "Epoch: 6 - Batch: 1981, Training Loss: 0.17123527355973994\n",
      "Epoch: 6 - Batch: 1982, Training Loss: 0.17132434868570387\n",
      "Epoch: 6 - Batch: 1983, Training Loss: 0.17139939247178004\n",
      "Epoch: 6 - Batch: 1984, Training Loss: 0.17149122536899042\n",
      "Epoch: 6 - Batch: 1985, Training Loss: 0.17157545515257328\n",
      "Epoch: 6 - Batch: 1986, Training Loss: 0.17166222641198195\n",
      "Epoch: 6 - Batch: 1987, Training Loss: 0.17175295224929132\n",
      "Epoch: 6 - Batch: 1988, Training Loss: 0.17185107981832468\n",
      "Epoch: 6 - Batch: 1989, Training Loss: 0.17193515221302585\n",
      "Epoch: 6 - Batch: 1990, Training Loss: 0.1720200868425381\n",
      "Epoch: 6 - Batch: 1991, Training Loss: 0.1720980220840345\n",
      "Epoch: 6 - Batch: 1992, Training Loss: 0.17218037410507944\n",
      "Epoch: 6 - Batch: 1993, Training Loss: 0.172264957420565\n",
      "Epoch: 6 - Batch: 1994, Training Loss: 0.1723505869310096\n",
      "Epoch: 6 - Batch: 1995, Training Loss: 0.17243385773961423\n",
      "Epoch: 6 - Batch: 1996, Training Loss: 0.17252033696838873\n",
      "Epoch: 6 - Batch: 1997, Training Loss: 0.17260649797211636\n",
      "Epoch: 6 - Batch: 1998, Training Loss: 0.17269360283343352\n",
      "Epoch: 6 - Batch: 1999, Training Loss: 0.1727723148010461\n",
      "Epoch: 6 - Batch: 2000, Training Loss: 0.17285634174473447\n",
      "Epoch: 6 - Batch: 2001, Training Loss: 0.17294177323406806\n",
      "Epoch: 6 - Batch: 2002, Training Loss: 0.17302369517251034\n",
      "Epoch: 6 - Batch: 2003, Training Loss: 0.17311030196931035\n",
      "Epoch: 6 - Batch: 2004, Training Loss: 0.17319108493267799\n",
      "Epoch: 6 - Batch: 2005, Training Loss: 0.17328585775732797\n",
      "Epoch: 6 - Batch: 2006, Training Loss: 0.17336983457949032\n",
      "Epoch: 6 - Batch: 2007, Training Loss: 0.17345229772909562\n",
      "Epoch: 6 - Batch: 2008, Training Loss: 0.17353557039725642\n",
      "Epoch: 6 - Batch: 2009, Training Loss: 0.17362479218721982\n",
      "Epoch: 6 - Batch: 2010, Training Loss: 0.17372092608961695\n",
      "Epoch: 6 - Batch: 2011, Training Loss: 0.17381056036126752\n",
      "Epoch: 6 - Batch: 2012, Training Loss: 0.17389703897224928\n",
      "Epoch: 6 - Batch: 2013, Training Loss: 0.17398046715120177\n",
      "Epoch: 6 - Batch: 2014, Training Loss: 0.17406708596407083\n",
      "Epoch: 6 - Batch: 2015, Training Loss: 0.17414901408661854\n",
      "Epoch: 6 - Batch: 2016, Training Loss: 0.17422822209238809\n",
      "Epoch: 6 - Batch: 2017, Training Loss: 0.17430972074394796\n",
      "Epoch: 6 - Batch: 2018, Training Loss: 0.17439872348313507\n",
      "Epoch: 6 - Batch: 2019, Training Loss: 0.17449596768266723\n",
      "Epoch: 6 - Batch: 2020, Training Loss: 0.1745857192916953\n",
      "Epoch: 6 - Batch: 2021, Training Loss: 0.17466881157055028\n",
      "Epoch: 6 - Batch: 2022, Training Loss: 0.17475148982323618\n",
      "Epoch: 6 - Batch: 2023, Training Loss: 0.17483975297800738\n",
      "Epoch: 6 - Batch: 2024, Training Loss: 0.17492351787254387\n",
      "Epoch: 6 - Batch: 2025, Training Loss: 0.1750141571570886\n",
      "Epoch: 6 - Batch: 2026, Training Loss: 0.17510099020195047\n",
      "Epoch: 6 - Batch: 2027, Training Loss: 0.17519066092337343\n",
      "Epoch: 6 - Batch: 2028, Training Loss: 0.17527287854360507\n",
      "Epoch: 6 - Batch: 2029, Training Loss: 0.17535909392545077\n",
      "Epoch: 6 - Batch: 2030, Training Loss: 0.17544210757164416\n",
      "Epoch: 6 - Batch: 2031, Training Loss: 0.17553077154417537\n",
      "Epoch: 6 - Batch: 2032, Training Loss: 0.1756121295799268\n",
      "Epoch: 6 - Batch: 2033, Training Loss: 0.17569457193462806\n",
      "Epoch: 6 - Batch: 2034, Training Loss: 0.17577833434563767\n",
      "Epoch: 6 - Batch: 2035, Training Loss: 0.17586985775956862\n",
      "Epoch: 6 - Batch: 2036, Training Loss: 0.17595617227243943\n",
      "Epoch: 6 - Batch: 2037, Training Loss: 0.17604197922531842\n",
      "Epoch: 6 - Batch: 2038, Training Loss: 0.1761250468357088\n",
      "Epoch: 6 - Batch: 2039, Training Loss: 0.17621019505470942\n",
      "Epoch: 6 - Batch: 2040, Training Loss: 0.17629427854496843\n",
      "Epoch: 6 - Batch: 2041, Training Loss: 0.17638884190698564\n",
      "Epoch: 6 - Batch: 2042, Training Loss: 0.17648279601414604\n",
      "Epoch: 6 - Batch: 2043, Training Loss: 0.17656362412580803\n",
      "Epoch: 6 - Batch: 2044, Training Loss: 0.176640093548974\n",
      "Epoch: 6 - Batch: 2045, Training Loss: 0.17672687087510752\n",
      "Epoch: 6 - Batch: 2046, Training Loss: 0.17680929753787283\n",
      "Epoch: 6 - Batch: 2047, Training Loss: 0.17690575978476214\n",
      "Epoch: 6 - Batch: 2048, Training Loss: 0.1769978206351424\n",
      "Epoch: 6 - Batch: 2049, Training Loss: 0.17708061314242002\n",
      "Epoch: 6 - Batch: 2050, Training Loss: 0.17716620459683102\n",
      "Epoch: 6 - Batch: 2051, Training Loss: 0.17724928122699557\n",
      "Epoch: 6 - Batch: 2052, Training Loss: 0.17732652557850082\n",
      "Epoch: 6 - Batch: 2053, Training Loss: 0.17740880461968792\n",
      "Epoch: 6 - Batch: 2054, Training Loss: 0.17749710474317743\n",
      "Epoch: 6 - Batch: 2055, Training Loss: 0.17757664361403355\n",
      "Epoch: 6 - Batch: 2056, Training Loss: 0.17766781048123317\n",
      "Epoch: 6 - Batch: 2057, Training Loss: 0.17774628643348048\n",
      "Epoch: 6 - Batch: 2058, Training Loss: 0.1778306074897646\n",
      "Epoch: 6 - Batch: 2059, Training Loss: 0.1779192773189711\n",
      "Epoch: 6 - Batch: 2060, Training Loss: 0.1780051412632315\n",
      "Epoch: 6 - Batch: 2061, Training Loss: 0.1780820793810472\n",
      "Epoch: 6 - Batch: 2062, Training Loss: 0.1781700660982733\n",
      "Epoch: 6 - Batch: 2063, Training Loss: 0.17825839800719995\n",
      "Epoch: 6 - Batch: 2064, Training Loss: 0.1783454009697805\n",
      "Epoch: 6 - Batch: 2065, Training Loss: 0.1784312545783682\n",
      "Epoch: 6 - Batch: 2066, Training Loss: 0.17851398066674695\n",
      "Epoch: 6 - Batch: 2067, Training Loss: 0.1785969815060966\n",
      "Epoch: 6 - Batch: 2068, Training Loss: 0.17868412950788168\n",
      "Epoch: 6 - Batch: 2069, Training Loss: 0.178769114085938\n",
      "Epoch: 6 - Batch: 2070, Training Loss: 0.17885623676439819\n",
      "Epoch: 6 - Batch: 2071, Training Loss: 0.17894512532930668\n",
      "Epoch: 6 - Batch: 2072, Training Loss: 0.17903083911903858\n",
      "Epoch: 6 - Batch: 2073, Training Loss: 0.17911606083066506\n",
      "Epoch: 6 - Batch: 2074, Training Loss: 0.17920509046237068\n",
      "Epoch: 6 - Batch: 2075, Training Loss: 0.17929013247661924\n",
      "Epoch: 6 - Batch: 2076, Training Loss: 0.17937592414530554\n",
      "Epoch: 6 - Batch: 2077, Training Loss: 0.17946075358942373\n",
      "Epoch: 6 - Batch: 2078, Training Loss: 0.17955564645120556\n",
      "Epoch: 6 - Batch: 2079, Training Loss: 0.17964066906058374\n",
      "Epoch: 6 - Batch: 2080, Training Loss: 0.17972345930673986\n",
      "Epoch: 6 - Batch: 2081, Training Loss: 0.1798011702400436\n",
      "Epoch: 6 - Batch: 2082, Training Loss: 0.17988544156401115\n",
      "Epoch: 6 - Batch: 2083, Training Loss: 0.17996463773904947\n",
      "Epoch: 6 - Batch: 2084, Training Loss: 0.18005667856058868\n",
      "Epoch: 6 - Batch: 2085, Training Loss: 0.18013577695756805\n",
      "Epoch: 6 - Batch: 2086, Training Loss: 0.1802202470007524\n",
      "Epoch: 6 - Batch: 2087, Training Loss: 0.1803085921675115\n",
      "Epoch: 6 - Batch: 2088, Training Loss: 0.18039148546149877\n",
      "Epoch: 6 - Batch: 2089, Training Loss: 0.18047796038425779\n",
      "Epoch: 6 - Batch: 2090, Training Loss: 0.180557767340101\n",
      "Epoch: 6 - Batch: 2091, Training Loss: 0.18063573592387228\n",
      "Epoch: 6 - Batch: 2092, Training Loss: 0.1807280156335131\n",
      "Epoch: 6 - Batch: 2093, Training Loss: 0.18081461577050723\n",
      "Epoch: 6 - Batch: 2094, Training Loss: 0.18090106605569126\n",
      "Epoch: 6 - Batch: 2095, Training Loss: 0.1809890191649916\n",
      "Epoch: 6 - Batch: 2096, Training Loss: 0.18107556429371904\n",
      "Epoch: 6 - Batch: 2097, Training Loss: 0.1811638724361585\n",
      "Epoch: 6 - Batch: 2098, Training Loss: 0.18125345444872012\n",
      "Epoch: 6 - Batch: 2099, Training Loss: 0.18133791470606728\n",
      "Epoch: 6 - Batch: 2100, Training Loss: 0.18142691726609447\n",
      "Epoch: 6 - Batch: 2101, Training Loss: 0.18151343718887758\n",
      "Epoch: 6 - Batch: 2102, Training Loss: 0.1815941320117532\n",
      "Epoch: 6 - Batch: 2103, Training Loss: 0.18168286588753435\n",
      "Epoch: 6 - Batch: 2104, Training Loss: 0.18176846951864054\n",
      "Epoch: 6 - Batch: 2105, Training Loss: 0.18185085301004833\n",
      "Epoch: 6 - Batch: 2106, Training Loss: 0.18193804809999703\n",
      "Epoch: 6 - Batch: 2107, Training Loss: 0.18201758544490507\n",
      "Epoch: 6 - Batch: 2108, Training Loss: 0.18209937072141252\n",
      "Epoch: 6 - Batch: 2109, Training Loss: 0.18218721833642246\n",
      "Epoch: 6 - Batch: 2110, Training Loss: 0.18226984164262094\n",
      "Epoch: 6 - Batch: 2111, Training Loss: 0.18234895767140546\n",
      "Epoch: 6 - Batch: 2112, Training Loss: 0.18243577079245107\n",
      "Epoch: 6 - Batch: 2113, Training Loss: 0.18251388240760041\n",
      "Epoch: 6 - Batch: 2114, Training Loss: 0.18259667588481263\n",
      "Epoch: 6 - Batch: 2115, Training Loss: 0.18268612709211474\n",
      "Epoch: 6 - Batch: 2116, Training Loss: 0.18277047706727761\n",
      "Epoch: 6 - Batch: 2117, Training Loss: 0.18285741640338257\n",
      "Epoch: 6 - Batch: 2118, Training Loss: 0.18294340398032866\n",
      "Epoch: 6 - Batch: 2119, Training Loss: 0.1830305341279032\n",
      "Epoch: 6 - Batch: 2120, Training Loss: 0.18311737037293155\n",
      "Epoch: 6 - Batch: 2121, Training Loss: 0.18322653482184678\n",
      "Epoch: 6 - Batch: 2122, Training Loss: 0.1833103356780994\n",
      "Epoch: 6 - Batch: 2123, Training Loss: 0.18339330145771032\n",
      "Epoch: 6 - Batch: 2124, Training Loss: 0.18348320380199212\n",
      "Epoch: 6 - Batch: 2125, Training Loss: 0.1835732542744422\n",
      "Epoch: 6 - Batch: 2126, Training Loss: 0.1836570239754063\n",
      "Epoch: 6 - Batch: 2127, Training Loss: 0.18374571217159133\n",
      "Epoch: 6 - Batch: 2128, Training Loss: 0.18383724563701037\n",
      "Epoch: 6 - Batch: 2129, Training Loss: 0.18392699145114244\n",
      "Epoch: 6 - Batch: 2130, Training Loss: 0.18400686082876538\n",
      "Epoch: 6 - Batch: 2131, Training Loss: 0.18409907099965397\n",
      "Epoch: 6 - Batch: 2132, Training Loss: 0.18418684302154267\n",
      "Epoch: 6 - Batch: 2133, Training Loss: 0.18426938662012024\n",
      "Epoch: 6 - Batch: 2134, Training Loss: 0.1843550397364259\n",
      "Epoch: 6 - Batch: 2135, Training Loss: 0.18444843589023965\n",
      "Epoch: 6 - Batch: 2136, Training Loss: 0.18453227878111117\n",
      "Epoch: 6 - Batch: 2137, Training Loss: 0.1846162582721382\n",
      "Epoch: 6 - Batch: 2138, Training Loss: 0.18470909234574975\n",
      "Epoch: 6 - Batch: 2139, Training Loss: 0.18479280564841347\n",
      "Epoch: 6 - Batch: 2140, Training Loss: 0.18487961886212798\n",
      "Epoch: 6 - Batch: 2141, Training Loss: 0.18495990805463808\n",
      "Epoch: 6 - Batch: 2142, Training Loss: 0.1850462240996349\n",
      "Epoch: 6 - Batch: 2143, Training Loss: 0.18513531826597146\n",
      "Epoch: 6 - Batch: 2144, Training Loss: 0.1852186420255929\n",
      "Epoch: 6 - Batch: 2145, Training Loss: 0.18530705907871672\n",
      "Epoch: 6 - Batch: 2146, Training Loss: 0.18540589682523093\n",
      "Epoch: 6 - Batch: 2147, Training Loss: 0.1854852475349484\n",
      "Epoch: 6 - Batch: 2148, Training Loss: 0.18557187090095004\n",
      "Epoch: 6 - Batch: 2149, Training Loss: 0.18564951914064523\n",
      "Epoch: 6 - Batch: 2150, Training Loss: 0.1857320022326006\n",
      "Epoch: 6 - Batch: 2151, Training Loss: 0.18582715263982516\n",
      "Epoch: 6 - Batch: 2152, Training Loss: 0.18591118999949932\n",
      "Epoch: 6 - Batch: 2153, Training Loss: 0.18599569105587987\n",
      "Epoch: 6 - Batch: 2154, Training Loss: 0.18608969738506165\n",
      "Epoch: 6 - Batch: 2155, Training Loss: 0.18618998874232148\n",
      "Epoch: 6 - Batch: 2156, Training Loss: 0.18626966277387605\n",
      "Epoch: 6 - Batch: 2157, Training Loss: 0.18635434685135954\n",
      "Epoch: 6 - Batch: 2158, Training Loss: 0.1864419237372294\n",
      "Epoch: 6 - Batch: 2159, Training Loss: 0.1865290653633933\n",
      "Epoch: 6 - Batch: 2160, Training Loss: 0.18661648680025072\n",
      "Epoch: 6 - Batch: 2161, Training Loss: 0.18670186603049535\n",
      "Epoch: 6 - Batch: 2162, Training Loss: 0.18679070642610293\n",
      "Epoch: 6 - Batch: 2163, Training Loss: 0.18688220736473354\n",
      "Epoch: 6 - Batch: 2164, Training Loss: 0.1869704431945313\n",
      "Epoch: 6 - Batch: 2165, Training Loss: 0.18705096695677162\n",
      "Epoch: 6 - Batch: 2166, Training Loss: 0.18714287134495936\n",
      "Epoch: 6 - Batch: 2167, Training Loss: 0.187230775926639\n",
      "Epoch: 6 - Batch: 2168, Training Loss: 0.18731711702313195\n",
      "Epoch: 6 - Batch: 2169, Training Loss: 0.18740116062869083\n",
      "Epoch: 6 - Batch: 2170, Training Loss: 0.1874862329209622\n",
      "Epoch: 6 - Batch: 2171, Training Loss: 0.1875587109210677\n",
      "Epoch: 6 - Batch: 2172, Training Loss: 0.18763824101308882\n",
      "Epoch: 6 - Batch: 2173, Training Loss: 0.18773661498258362\n",
      "Epoch: 6 - Batch: 2174, Training Loss: 0.1878230466227824\n",
      "Epoch: 6 - Batch: 2175, Training Loss: 0.18791718373251198\n",
      "Epoch: 6 - Batch: 2176, Training Loss: 0.1879965395699093\n",
      "Epoch: 6 - Batch: 2177, Training Loss: 0.18807637884866935\n",
      "Epoch: 6 - Batch: 2178, Training Loss: 0.1881598471285494\n",
      "Epoch: 6 - Batch: 2179, Training Loss: 0.1882447239450159\n",
      "Epoch: 6 - Batch: 2180, Training Loss: 0.1883368479338155\n",
      "Epoch: 6 - Batch: 2181, Training Loss: 0.18842319166828347\n",
      "Epoch: 6 - Batch: 2182, Training Loss: 0.18850790999001926\n",
      "Epoch: 6 - Batch: 2183, Training Loss: 0.1885984285084367\n",
      "Epoch: 6 - Batch: 2184, Training Loss: 0.1886811800993003\n",
      "Epoch: 6 - Batch: 2185, Training Loss: 0.18876691952743144\n",
      "Epoch: 6 - Batch: 2186, Training Loss: 0.18884876424795755\n",
      "Epoch: 6 - Batch: 2187, Training Loss: 0.18893212168346193\n",
      "Epoch: 6 - Batch: 2188, Training Loss: 0.1890228983960045\n",
      "Epoch: 6 - Batch: 2189, Training Loss: 0.1891071155333697\n",
      "Epoch: 6 - Batch: 2190, Training Loss: 0.18920494196550366\n",
      "Epoch: 6 - Batch: 2191, Training Loss: 0.18929126764806745\n",
      "Epoch: 6 - Batch: 2192, Training Loss: 0.18938101206598787\n",
      "Epoch: 6 - Batch: 2193, Training Loss: 0.1894712093692415\n",
      "Epoch: 6 - Batch: 2194, Training Loss: 0.18955849736286434\n",
      "Epoch: 6 - Batch: 2195, Training Loss: 0.18964584340429425\n",
      "Epoch: 6 - Batch: 2196, Training Loss: 0.18973510393604118\n",
      "Epoch: 6 - Batch: 2197, Training Loss: 0.18982384906800628\n",
      "Epoch: 6 - Batch: 2198, Training Loss: 0.18991344601879664\n",
      "Epoch: 6 - Batch: 2199, Training Loss: 0.18999727581246179\n",
      "Epoch: 6 - Batch: 2200, Training Loss: 0.19008295268289882\n",
      "Epoch: 6 - Batch: 2201, Training Loss: 0.1901634524624245\n",
      "Epoch: 6 - Batch: 2202, Training Loss: 0.1902473394142751\n",
      "Epoch: 6 - Batch: 2203, Training Loss: 0.19033636694548536\n",
      "Epoch: 6 - Batch: 2204, Training Loss: 0.19042377252360285\n",
      "Epoch: 6 - Batch: 2205, Training Loss: 0.19049975502105496\n",
      "Epoch: 6 - Batch: 2206, Training Loss: 0.190588758445992\n",
      "Epoch: 6 - Batch: 2207, Training Loss: 0.19067045923316261\n",
      "Epoch: 6 - Batch: 2208, Training Loss: 0.19075650043451964\n",
      "Epoch: 6 - Batch: 2209, Training Loss: 0.19084507823113975\n",
      "Epoch: 6 - Batch: 2210, Training Loss: 0.19092755558997837\n",
      "Epoch: 6 - Batch: 2211, Training Loss: 0.19101474979030553\n",
      "Epoch: 6 - Batch: 2212, Training Loss: 0.19110635563014555\n",
      "Epoch: 6 - Batch: 2213, Training Loss: 0.19119210618472415\n",
      "Epoch: 6 - Batch: 2214, Training Loss: 0.19128717322699465\n",
      "Epoch: 6 - Batch: 2215, Training Loss: 0.19137673918312265\n",
      "Epoch: 6 - Batch: 2216, Training Loss: 0.19145693507657122\n",
      "Epoch: 6 - Batch: 2217, Training Loss: 0.19154458986610717\n",
      "Epoch: 6 - Batch: 2218, Training Loss: 0.1916269123381248\n",
      "Epoch: 6 - Batch: 2219, Training Loss: 0.19171871912123553\n",
      "Epoch: 6 - Batch: 2220, Training Loss: 0.19181178154958223\n",
      "Epoch: 6 - Batch: 2221, Training Loss: 0.1919027301135348\n",
      "Epoch: 6 - Batch: 2222, Training Loss: 0.19198042441585764\n",
      "Epoch: 6 - Batch: 2223, Training Loss: 0.19207425139980333\n",
      "Epoch: 6 - Batch: 2224, Training Loss: 0.19215283111034343\n",
      "Epoch: 6 - Batch: 2225, Training Loss: 0.192241483369524\n",
      "Epoch: 6 - Batch: 2226, Training Loss: 0.19232445871874468\n",
      "Epoch: 6 - Batch: 2227, Training Loss: 0.1924128023842674\n",
      "Epoch: 6 - Batch: 2228, Training Loss: 0.19249817514622192\n",
      "Epoch: 6 - Batch: 2229, Training Loss: 0.19258055053218878\n",
      "Epoch: 6 - Batch: 2230, Training Loss: 0.19266014832193973\n",
      "Epoch: 6 - Batch: 2231, Training Loss: 0.19274694048745517\n",
      "Epoch: 6 - Batch: 2232, Training Loss: 0.19283563276107235\n",
      "Epoch: 6 - Batch: 2233, Training Loss: 0.19291569983855408\n",
      "Epoch: 6 - Batch: 2234, Training Loss: 0.1930084009260977\n",
      "Epoch: 6 - Batch: 2235, Training Loss: 0.1931027262958128\n",
      "Epoch: 6 - Batch: 2236, Training Loss: 0.19318270230446485\n",
      "Epoch: 6 - Batch: 2237, Training Loss: 0.19327665818469045\n",
      "Epoch: 6 - Batch: 2238, Training Loss: 0.1933689857732696\n",
      "Epoch: 6 - Batch: 2239, Training Loss: 0.19345222416011057\n",
      "Epoch: 6 - Batch: 2240, Training Loss: 0.19353733028271305\n",
      "Epoch: 6 - Batch: 2241, Training Loss: 0.19363067836607273\n",
      "Epoch: 6 - Batch: 2242, Training Loss: 0.1937184006494669\n",
      "Epoch: 6 - Batch: 2243, Training Loss: 0.19380356433182017\n",
      "Epoch: 6 - Batch: 2244, Training Loss: 0.19388144337270982\n",
      "Epoch: 6 - Batch: 2245, Training Loss: 0.19396140824873648\n",
      "Epoch: 6 - Batch: 2246, Training Loss: 0.1940432813815513\n",
      "Epoch: 6 - Batch: 2247, Training Loss: 0.19413164993794405\n",
      "Epoch: 6 - Batch: 2248, Training Loss: 0.19422323892439775\n",
      "Epoch: 6 - Batch: 2249, Training Loss: 0.1943069731702358\n",
      "Epoch: 6 - Batch: 2250, Training Loss: 0.1943894429672911\n",
      "Epoch: 6 - Batch: 2251, Training Loss: 0.19447123254981405\n",
      "Epoch: 6 - Batch: 2252, Training Loss: 0.19455429803499732\n",
      "Epoch: 6 - Batch: 2253, Training Loss: 0.19464104775815066\n",
      "Epoch: 6 - Batch: 2254, Training Loss: 0.19472615013075112\n",
      "Epoch: 6 - Batch: 2255, Training Loss: 0.19482752220522903\n",
      "Epoch: 6 - Batch: 2256, Training Loss: 0.19490843633835392\n",
      "Epoch: 6 - Batch: 2257, Training Loss: 0.19499158899409458\n",
      "Epoch: 6 - Batch: 2258, Training Loss: 0.195077933717031\n",
      "Epoch: 6 - Batch: 2259, Training Loss: 0.19516257188015712\n",
      "Epoch: 6 - Batch: 2260, Training Loss: 0.19524929906623084\n",
      "Epoch: 6 - Batch: 2261, Training Loss: 0.19533420750133038\n",
      "Epoch: 6 - Batch: 2262, Training Loss: 0.1954228286793576\n",
      "Epoch: 6 - Batch: 2263, Training Loss: 0.19551037342764846\n",
      "Epoch: 6 - Batch: 2264, Training Loss: 0.1955965253745343\n",
      "Epoch: 6 - Batch: 2265, Training Loss: 0.1956806517952117\n",
      "Epoch: 6 - Batch: 2266, Training Loss: 0.19576557262941183\n",
      "Epoch: 6 - Batch: 2267, Training Loss: 0.19585770136285974\n",
      "Epoch: 6 - Batch: 2268, Training Loss: 0.19594560451743812\n",
      "Epoch: 6 - Batch: 2269, Training Loss: 0.196031065561977\n",
      "Epoch: 6 - Batch: 2270, Training Loss: 0.19611744216919735\n",
      "Epoch: 6 - Batch: 2271, Training Loss: 0.196196214569297\n",
      "Epoch: 6 - Batch: 2272, Training Loss: 0.19628025935484006\n",
      "Epoch: 6 - Batch: 2273, Training Loss: 0.1963737469358033\n",
      "Epoch: 6 - Batch: 2274, Training Loss: 0.19647223729127478\n",
      "Epoch: 6 - Batch: 2275, Training Loss: 0.19655335128035514\n",
      "Epoch: 6 - Batch: 2276, Training Loss: 0.19664095715306093\n",
      "Epoch: 6 - Batch: 2277, Training Loss: 0.1967207945475531\n",
      "Epoch: 6 - Batch: 2278, Training Loss: 0.19680368613643234\n",
      "Epoch: 6 - Batch: 2279, Training Loss: 0.196892877815177\n",
      "Epoch: 6 - Batch: 2280, Training Loss: 0.19697643930144967\n",
      "Epoch: 6 - Batch: 2281, Training Loss: 0.1970667106718764\n",
      "Epoch: 6 - Batch: 2282, Training Loss: 0.1971499279425611\n",
      "Epoch: 6 - Batch: 2283, Training Loss: 0.19723437940871735\n",
      "Epoch: 6 - Batch: 2284, Training Loss: 0.1973256088521449\n",
      "Epoch: 6 - Batch: 2285, Training Loss: 0.19741990755174685\n",
      "Epoch: 6 - Batch: 2286, Training Loss: 0.1975064464222337\n",
      "Epoch: 6 - Batch: 2287, Training Loss: 0.197598401994551\n",
      "Epoch: 6 - Batch: 2288, Training Loss: 0.19767804347460544\n",
      "Epoch: 6 - Batch: 2289, Training Loss: 0.19775985401883647\n",
      "Epoch: 6 - Batch: 2290, Training Loss: 0.19783845684100343\n",
      "Epoch: 6 - Batch: 2291, Training Loss: 0.19793254330408316\n",
      "Epoch: 6 - Batch: 2292, Training Loss: 0.19801931950583387\n",
      "Epoch: 6 - Batch: 2293, Training Loss: 0.1981094831208487\n",
      "Epoch: 6 - Batch: 2294, Training Loss: 0.19818887530917156\n",
      "Epoch: 6 - Batch: 2295, Training Loss: 0.19827147006716697\n",
      "Epoch: 6 - Batch: 2296, Training Loss: 0.19835949618721482\n",
      "Epoch: 6 - Batch: 2297, Training Loss: 0.1984475504291888\n",
      "Epoch: 6 - Batch: 2298, Training Loss: 0.1985374195609322\n",
      "Epoch: 6 - Batch: 2299, Training Loss: 0.19861825701979854\n",
      "Epoch: 6 - Batch: 2300, Training Loss: 0.1987047158984798\n",
      "Epoch: 6 - Batch: 2301, Training Loss: 0.1987924752432612\n",
      "Epoch: 6 - Batch: 2302, Training Loss: 0.19886754634195497\n",
      "Epoch: 6 - Batch: 2303, Training Loss: 0.19895550269466727\n",
      "Epoch: 6 - Batch: 2304, Training Loss: 0.19903848449364428\n",
      "Epoch: 6 - Batch: 2305, Training Loss: 0.19912294323122126\n",
      "Epoch: 6 - Batch: 2306, Training Loss: 0.19921348679495687\n",
      "Epoch: 6 - Batch: 2307, Training Loss: 0.1993017815992508\n",
      "Epoch: 6 - Batch: 2308, Training Loss: 0.19939419673524092\n",
      "Epoch: 6 - Batch: 2309, Training Loss: 0.1994851775541531\n",
      "Epoch: 6 - Batch: 2310, Training Loss: 0.19957324718534453\n",
      "Epoch: 6 - Batch: 2311, Training Loss: 0.1996643568758249\n",
      "Epoch: 6 - Batch: 2312, Training Loss: 0.19974466761076826\n",
      "Epoch: 6 - Batch: 2313, Training Loss: 0.19983050719546047\n",
      "Epoch: 6 - Batch: 2314, Training Loss: 0.19992041175761824\n",
      "Epoch: 6 - Batch: 2315, Training Loss: 0.20001159756016573\n",
      "Epoch: 6 - Batch: 2316, Training Loss: 0.20009299027262437\n",
      "Epoch: 6 - Batch: 2317, Training Loss: 0.20017967599244854\n",
      "Epoch: 6 - Batch: 2318, Training Loss: 0.20026236797248942\n",
      "Epoch: 6 - Batch: 2319, Training Loss: 0.20035119006907565\n",
      "Epoch: 6 - Batch: 2320, Training Loss: 0.20043632321393312\n",
      "Epoch: 6 - Batch: 2321, Training Loss: 0.20052108332934862\n",
      "Epoch: 6 - Batch: 2322, Training Loss: 0.20060396497176455\n",
      "Epoch: 6 - Batch: 2323, Training Loss: 0.20069425357243117\n",
      "Epoch: 6 - Batch: 2324, Training Loss: 0.20079055927681488\n",
      "Epoch: 6 - Batch: 2325, Training Loss: 0.2008818406519012\n",
      "Epoch: 6 - Batch: 2326, Training Loss: 0.2009698476759751\n",
      "Epoch: 6 - Batch: 2327, Training Loss: 0.20105467488986145\n",
      "Epoch: 6 - Batch: 2328, Training Loss: 0.2011444105598365\n",
      "Epoch: 6 - Batch: 2329, Training Loss: 0.20123016799662638\n",
      "Epoch: 6 - Batch: 2330, Training Loss: 0.20131766591004865\n",
      "Epoch: 6 - Batch: 2331, Training Loss: 0.20140234455713388\n",
      "Epoch: 6 - Batch: 2332, Training Loss: 0.20148115108410516\n",
      "Epoch: 6 - Batch: 2333, Training Loss: 0.20156586762659784\n",
      "Epoch: 6 - Batch: 2334, Training Loss: 0.20165552940238174\n",
      "Epoch: 6 - Batch: 2335, Training Loss: 0.20174046397629267\n",
      "Epoch: 6 - Batch: 2336, Training Loss: 0.2018384881021075\n",
      "Epoch: 6 - Batch: 2337, Training Loss: 0.2019200520049379\n",
      "Epoch: 6 - Batch: 2338, Training Loss: 0.20200672739526723\n",
      "Epoch: 6 - Batch: 2339, Training Loss: 0.2020846417121231\n",
      "Epoch: 6 - Batch: 2340, Training Loss: 0.20216692799384123\n",
      "Epoch: 6 - Batch: 2341, Training Loss: 0.2022535640740177\n",
      "Epoch: 6 - Batch: 2342, Training Loss: 0.20234158493664925\n",
      "Epoch: 6 - Batch: 2343, Training Loss: 0.20242875162666513\n",
      "Epoch: 6 - Batch: 2344, Training Loss: 0.20250979663300672\n",
      "Epoch: 6 - Batch: 2345, Training Loss: 0.20258896209138938\n",
      "Epoch: 6 - Batch: 2346, Training Loss: 0.20267266264281067\n",
      "Epoch: 6 - Batch: 2347, Training Loss: 0.20275045043571077\n",
      "Epoch: 6 - Batch: 2348, Training Loss: 0.20283804609012446\n",
      "Epoch: 6 - Batch: 2349, Training Loss: 0.20292668658653104\n",
      "Epoch: 6 - Batch: 2350, Training Loss: 0.20301070886588413\n",
      "Epoch: 6 - Batch: 2351, Training Loss: 0.20309373216817825\n",
      "Epoch: 6 - Batch: 2352, Training Loss: 0.2031835880235554\n",
      "Epoch: 6 - Batch: 2353, Training Loss: 0.20326833337372413\n",
      "Epoch: 6 - Batch: 2354, Training Loss: 0.2033564329802199\n",
      "Epoch: 6 - Batch: 2355, Training Loss: 0.2034476589084067\n",
      "Epoch: 6 - Batch: 2356, Training Loss: 0.2035387799848075\n",
      "Epoch: 6 - Batch: 2357, Training Loss: 0.20363145327785517\n",
      "Epoch: 6 - Batch: 2358, Training Loss: 0.20371722128507905\n",
      "Epoch: 6 - Batch: 2359, Training Loss: 0.20381458216712842\n",
      "Epoch: 6 - Batch: 2360, Training Loss: 0.20390145927914735\n",
      "Epoch: 6 - Batch: 2361, Training Loss: 0.20398870419408155\n",
      "Epoch: 6 - Batch: 2362, Training Loss: 0.20407860390038832\n",
      "Epoch: 6 - Batch: 2363, Training Loss: 0.20416116522343994\n",
      "Epoch: 6 - Batch: 2364, Training Loss: 0.2042494269819995\n",
      "Epoch: 6 - Batch: 2365, Training Loss: 0.20432830669887822\n",
      "Epoch: 6 - Batch: 2366, Training Loss: 0.2044088571559434\n",
      "Epoch: 6 - Batch: 2367, Training Loss: 0.20449131208272717\n",
      "Epoch: 6 - Batch: 2368, Training Loss: 0.204578021272498\n",
      "Epoch: 6 - Batch: 2369, Training Loss: 0.20466583385248088\n",
      "Epoch: 6 - Batch: 2370, Training Loss: 0.2047539867250678\n",
      "Epoch: 6 - Batch: 2371, Training Loss: 0.20484624064803914\n",
      "Epoch: 6 - Batch: 2372, Training Loss: 0.20493842202978546\n",
      "Epoch: 6 - Batch: 2373, Training Loss: 0.205020307629658\n",
      "Epoch: 6 - Batch: 2374, Training Loss: 0.20510602279088982\n",
      "Epoch: 6 - Batch: 2375, Training Loss: 0.2052065136952088\n",
      "Epoch: 6 - Batch: 2376, Training Loss: 0.20529927462215844\n",
      "Epoch: 6 - Batch: 2377, Training Loss: 0.20538435451945855\n",
      "Epoch: 6 - Batch: 2378, Training Loss: 0.2054687581313882\n",
      "Epoch: 6 - Batch: 2379, Training Loss: 0.20555447641865135\n",
      "Epoch: 6 - Batch: 2380, Training Loss: 0.2056416470748967\n",
      "Epoch: 6 - Batch: 2381, Training Loss: 0.20573618246943598\n",
      "Epoch: 6 - Batch: 2382, Training Loss: 0.2058284267177728\n",
      "Epoch: 6 - Batch: 2383, Training Loss: 0.2059164971706286\n",
      "Epoch: 6 - Batch: 2384, Training Loss: 0.20599599622474182\n",
      "Epoch: 6 - Batch: 2385, Training Loss: 0.20608213759501579\n",
      "Epoch: 6 - Batch: 2386, Training Loss: 0.2061679241483781\n",
      "Epoch: 6 - Batch: 2387, Training Loss: 0.20624960305950732\n",
      "Epoch: 6 - Batch: 2388, Training Loss: 0.20633600767127316\n",
      "Epoch: 6 - Batch: 2389, Training Loss: 0.20642345918845972\n",
      "Epoch: 6 - Batch: 2390, Training Loss: 0.20651409907844134\n",
      "Epoch: 6 - Batch: 2391, Training Loss: 0.20660592896700103\n",
      "Epoch: 6 - Batch: 2392, Training Loss: 0.20669367169933534\n",
      "Epoch: 6 - Batch: 2393, Training Loss: 0.20679213497077253\n",
      "Epoch: 6 - Batch: 2394, Training Loss: 0.20687354765029295\n",
      "Epoch: 6 - Batch: 2395, Training Loss: 0.2069595370806173\n",
      "Epoch: 6 - Batch: 2396, Training Loss: 0.20703884956131924\n",
      "Epoch: 6 - Batch: 2397, Training Loss: 0.2071293300484148\n",
      "Epoch: 6 - Batch: 2398, Training Loss: 0.20721401414443208\n",
      "Epoch: 6 - Batch: 2399, Training Loss: 0.2073033281523197\n",
      "Epoch: 6 - Batch: 2400, Training Loss: 0.2073912690416794\n",
      "Epoch: 6 - Batch: 2401, Training Loss: 0.20748428601283536\n",
      "Epoch: 6 - Batch: 2402, Training Loss: 0.2075728521146387\n",
      "Epoch: 6 - Batch: 2403, Training Loss: 0.20765769746692025\n",
      "Epoch: 6 - Batch: 2404, Training Loss: 0.20774459685036792\n",
      "Epoch: 6 - Batch: 2405, Training Loss: 0.2078272096479117\n",
      "Epoch: 6 - Batch: 2406, Training Loss: 0.20790314544640964\n",
      "Epoch: 6 - Batch: 2407, Training Loss: 0.20799528128447423\n",
      "Epoch: 6 - Batch: 2408, Training Loss: 0.20807770382038396\n",
      "Epoch: 6 - Batch: 2409, Training Loss: 0.2081593088249662\n",
      "Epoch: 6 - Batch: 2410, Training Loss: 0.20825109056275876\n",
      "Epoch: 6 - Batch: 2411, Training Loss: 0.20833793305367182\n",
      "Epoch: 6 - Batch: 2412, Training Loss: 0.2084319158820173\n",
      "Epoch 6 - Batch 2412, Training Loss: 0.2084319158820173, Validation Loss: 0.20801444121854223\n",
      "Validation loss decreased (0.208022 --> 0.208014). Saving model...\n",
      "Epoch: 7 - Batch: 1, Training Loss: 8.931024552973151e-05\n",
      "Epoch: 7 - Batch: 2, Training Loss: 0.00018359223383773815\n",
      "Epoch: 7 - Batch: 3, Training Loss: 0.0002665895517686904\n",
      "Epoch: 7 - Batch: 4, Training Loss: 0.0003581082642968021\n",
      "Epoch: 7 - Batch: 5, Training Loss: 0.0004400985059058093\n",
      "Epoch: 7 - Batch: 6, Training Loss: 0.0005234499179308687\n",
      "Epoch: 7 - Batch: 7, Training Loss: 0.0006169958741312992\n",
      "Epoch: 7 - Batch: 8, Training Loss: 0.0007066194517893182\n",
      "Epoch: 7 - Batch: 9, Training Loss: 0.0007910845773433571\n",
      "Epoch: 7 - Batch: 10, Training Loss: 0.0008695025868083707\n",
      "Epoch: 7 - Batch: 11, Training Loss: 0.000958909065915182\n",
      "Epoch: 7 - Batch: 12, Training Loss: 0.0010454553931605558\n",
      "Epoch: 7 - Batch: 13, Training Loss: 0.00113696765593233\n",
      "Epoch: 7 - Batch: 14, Training Loss: 0.001224208382231679\n",
      "Epoch: 7 - Batch: 15, Training Loss: 0.0013126199184662075\n",
      "Epoch: 7 - Batch: 16, Training Loss: 0.0014022666757083058\n",
      "Epoch: 7 - Batch: 17, Training Loss: 0.001486398026371872\n",
      "Epoch: 7 - Batch: 18, Training Loss: 0.0015759700183350452\n",
      "Epoch: 7 - Batch: 19, Training Loss: 0.0016582128383330445\n",
      "Epoch: 7 - Batch: 20, Training Loss: 0.0017390037588713378\n",
      "Epoch: 7 - Batch: 21, Training Loss: 0.0018316196898619335\n",
      "Epoch: 7 - Batch: 22, Training Loss: 0.0019204024639394549\n",
      "Epoch: 7 - Batch: 23, Training Loss: 0.002019289343511287\n",
      "Epoch: 7 - Batch: 24, Training Loss: 0.0021024296619306946\n",
      "Epoch: 7 - Batch: 25, Training Loss: 0.0021904782263892602\n",
      "Epoch: 7 - Batch: 26, Training Loss: 0.002269295496776527\n",
      "Epoch: 7 - Batch: 27, Training Loss: 0.0023551305159802854\n",
      "Epoch: 7 - Batch: 28, Training Loss: 0.0024404862886341057\n",
      "Epoch: 7 - Batch: 29, Training Loss: 0.0025246147912730823\n",
      "Epoch: 7 - Batch: 30, Training Loss: 0.002601234325722082\n",
      "Epoch: 7 - Batch: 31, Training Loss: 0.0026824926806128836\n",
      "Epoch: 7 - Batch: 32, Training Loss: 0.002763058706055431\n",
      "Epoch: 7 - Batch: 33, Training Loss: 0.002857832265878791\n",
      "Epoch: 7 - Batch: 34, Training Loss: 0.0029462741542712573\n",
      "Epoch: 7 - Batch: 35, Training Loss: 0.003035403645997419\n",
      "Epoch: 7 - Batch: 36, Training Loss: 0.0031168169247768017\n",
      "Epoch: 7 - Batch: 37, Training Loss: 0.003212310837127676\n",
      "Epoch: 7 - Batch: 38, Training Loss: 0.003294492236170207\n",
      "Epoch: 7 - Batch: 39, Training Loss: 0.003383312343463771\n",
      "Epoch: 7 - Batch: 40, Training Loss: 0.003467144843061172\n",
      "Epoch: 7 - Batch: 41, Training Loss: 0.003545671250442565\n",
      "Epoch: 7 - Batch: 42, Training Loss: 0.003629785580915796\n",
      "Epoch: 7 - Batch: 43, Training Loss: 0.00370150672929797\n",
      "Epoch: 7 - Batch: 44, Training Loss: 0.0037873818803189404\n",
      "Epoch: 7 - Batch: 45, Training Loss: 0.0038757229881796673\n",
      "Epoch: 7 - Batch: 46, Training Loss: 0.003963644435601448\n",
      "Epoch: 7 - Batch: 47, Training Loss: 0.00405183979379597\n",
      "Epoch: 7 - Batch: 48, Training Loss: 0.0041376492919811166\n",
      "Epoch: 7 - Batch: 49, Training Loss: 0.004224637279265358\n",
      "Epoch: 7 - Batch: 50, Training Loss: 0.004309579254333455\n",
      "Epoch: 7 - Batch: 51, Training Loss: 0.004389953220365059\n",
      "Epoch: 7 - Batch: 52, Training Loss: 0.004481539426751398\n",
      "Epoch: 7 - Batch: 53, Training Loss: 0.004554185188478894\n",
      "Epoch: 7 - Batch: 54, Training Loss: 0.004645433184222796\n",
      "Epoch: 7 - Batch: 55, Training Loss: 0.004730066516209597\n",
      "Epoch: 7 - Batch: 56, Training Loss: 0.004812583370538888\n",
      "Epoch: 7 - Batch: 57, Training Loss: 0.004890509623150722\n",
      "Epoch: 7 - Batch: 58, Training Loss: 0.004974263626000972\n",
      "Epoch: 7 - Batch: 59, Training Loss: 0.005061316073781024\n",
      "Epoch: 7 - Batch: 60, Training Loss: 0.0051561517403098086\n",
      "Epoch: 7 - Batch: 61, Training Loss: 0.005238894719142424\n",
      "Epoch: 7 - Batch: 62, Training Loss: 0.005327820617266951\n",
      "Epoch: 7 - Batch: 63, Training Loss: 0.00540842160360137\n",
      "Epoch: 7 - Batch: 64, Training Loss: 0.005497950424206989\n",
      "Epoch: 7 - Batch: 65, Training Loss: 0.005582450955463681\n",
      "Epoch: 7 - Batch: 66, Training Loss: 0.005667453956099885\n",
      "Epoch: 7 - Batch: 67, Training Loss: 0.005755528060110845\n",
      "Epoch: 7 - Batch: 68, Training Loss: 0.0058438049360590785\n",
      "Epoch: 7 - Batch: 69, Training Loss: 0.005926234984328696\n",
      "Epoch: 7 - Batch: 70, Training Loss: 0.006017024349266814\n",
      "Epoch: 7 - Batch: 71, Training Loss: 0.006102716726301915\n",
      "Epoch: 7 - Batch: 72, Training Loss: 0.006185979979301171\n",
      "Epoch: 7 - Batch: 73, Training Loss: 0.006266646772919602\n",
      "Epoch: 7 - Batch: 74, Training Loss: 0.00635250829543245\n",
      "Epoch: 7 - Batch: 75, Training Loss: 0.006435625211862387\n",
      "Epoch: 7 - Batch: 76, Training Loss: 0.006520242926048402\n",
      "Epoch: 7 - Batch: 77, Training Loss: 0.006606775297355494\n",
      "Epoch: 7 - Batch: 78, Training Loss: 0.006682514433886479\n",
      "Epoch: 7 - Batch: 79, Training Loss: 0.006772104236814711\n",
      "Epoch: 7 - Batch: 80, Training Loss: 0.00685908061577313\n",
      "Epoch: 7 - Batch: 81, Training Loss: 0.006946426119723328\n",
      "Epoch: 7 - Batch: 82, Training Loss: 0.007035918607937163\n",
      "Epoch: 7 - Batch: 83, Training Loss: 0.00711564840764351\n",
      "Epoch: 7 - Batch: 84, Training Loss: 0.007208598397709244\n",
      "Epoch: 7 - Batch: 85, Training Loss: 0.00728648435787775\n",
      "Epoch: 7 - Batch: 86, Training Loss: 0.007370818566969576\n",
      "Epoch: 7 - Batch: 87, Training Loss: 0.0074588029489390684\n",
      "Epoch: 7 - Batch: 88, Training Loss: 0.007550338576632748\n",
      "Epoch: 7 - Batch: 89, Training Loss: 0.007633917219010159\n",
      "Epoch: 7 - Batch: 90, Training Loss: 0.007717840775081372\n",
      "Epoch: 7 - Batch: 91, Training Loss: 0.007801584744977319\n",
      "Epoch: 7 - Batch: 92, Training Loss: 0.007880848642457184\n",
      "Epoch: 7 - Batch: 93, Training Loss: 0.007961750537107634\n",
      "Epoch: 7 - Batch: 94, Training Loss: 0.008053205530491239\n",
      "Epoch: 7 - Batch: 95, Training Loss: 0.008137475470603006\n",
      "Epoch: 7 - Batch: 96, Training Loss: 0.008220667430318608\n",
      "Epoch: 7 - Batch: 97, Training Loss: 0.008299800849969114\n",
      "Epoch: 7 - Batch: 98, Training Loss: 0.008385157388685947\n",
      "Epoch: 7 - Batch: 99, Training Loss: 0.00846747195913424\n",
      "Epoch: 7 - Batch: 100, Training Loss: 0.008559887583180645\n",
      "Epoch: 7 - Batch: 101, Training Loss: 0.00864557247874551\n",
      "Epoch: 7 - Batch: 102, Training Loss: 0.008728180581064367\n",
      "Epoch: 7 - Batch: 103, Training Loss: 0.008815211226938178\n",
      "Epoch: 7 - Batch: 104, Training Loss: 0.008898964834401066\n",
      "Epoch: 7 - Batch: 105, Training Loss: 0.008976343673210634\n",
      "Epoch: 7 - Batch: 106, Training Loss: 0.00907134060835957\n",
      "Epoch: 7 - Batch: 107, Training Loss: 0.009149022449802601\n",
      "Epoch: 7 - Batch: 108, Training Loss: 0.009239881884794725\n",
      "Epoch: 7 - Batch: 109, Training Loss: 0.00932671378674001\n",
      "Epoch: 7 - Batch: 110, Training Loss: 0.009419210279857737\n",
      "Epoch: 7 - Batch: 111, Training Loss: 0.009506468236396956\n",
      "Epoch: 7 - Batch: 112, Training Loss: 0.009594197148707375\n",
      "Epoch: 7 - Batch: 113, Training Loss: 0.009682107988627594\n",
      "Epoch: 7 - Batch: 114, Training Loss: 0.0097700845184512\n",
      "Epoch: 7 - Batch: 115, Training Loss: 0.009863319805210108\n",
      "Epoch: 7 - Batch: 116, Training Loss: 0.009949035337117577\n",
      "Epoch: 7 - Batch: 117, Training Loss: 0.01003634109228207\n",
      "Epoch: 7 - Batch: 118, Training Loss: 0.010118251966056143\n",
      "Epoch: 7 - Batch: 119, Training Loss: 0.010198678160287057\n",
      "Epoch: 7 - Batch: 120, Training Loss: 0.01028787493582191\n",
      "Epoch: 7 - Batch: 121, Training Loss: 0.010366998100112722\n",
      "Epoch: 7 - Batch: 122, Training Loss: 0.01044885289683864\n",
      "Epoch: 7 - Batch: 123, Training Loss: 0.010529030515977597\n",
      "Epoch: 7 - Batch: 124, Training Loss: 0.01061709684815573\n",
      "Epoch: 7 - Batch: 125, Training Loss: 0.010703982720475885\n",
      "Epoch: 7 - Batch: 126, Training Loss: 0.01078244098531666\n",
      "Epoch: 7 - Batch: 127, Training Loss: 0.01086878111805291\n",
      "Epoch: 7 - Batch: 128, Training Loss: 0.010955800093821626\n",
      "Epoch: 7 - Batch: 129, Training Loss: 0.011037534754568862\n",
      "Epoch: 7 - Batch: 130, Training Loss: 0.011121858770080269\n",
      "Epoch: 7 - Batch: 131, Training Loss: 0.011214488916482104\n",
      "Epoch: 7 - Batch: 132, Training Loss: 0.01129941590389802\n",
      "Epoch: 7 - Batch: 133, Training Loss: 0.011378304862956304\n",
      "Epoch: 7 - Batch: 134, Training Loss: 0.011464371276435568\n",
      "Epoch: 7 - Batch: 135, Training Loss: 0.011542858392197892\n",
      "Epoch: 7 - Batch: 136, Training Loss: 0.01163629520913064\n",
      "Epoch: 7 - Batch: 137, Training Loss: 0.011726081636958256\n",
      "Epoch: 7 - Batch: 138, Training Loss: 0.0118067852633806\n",
      "Epoch: 7 - Batch: 139, Training Loss: 0.01189836395122914\n",
      "Epoch: 7 - Batch: 140, Training Loss: 0.011982346525042013\n",
      "Epoch: 7 - Batch: 141, Training Loss: 0.01207924471738722\n",
      "Epoch: 7 - Batch: 142, Training Loss: 0.012162099609376977\n",
      "Epoch: 7 - Batch: 143, Training Loss: 0.012250853897030674\n",
      "Epoch: 7 - Batch: 144, Training Loss: 0.012338894782325324\n",
      "Epoch: 7 - Batch: 145, Training Loss: 0.012422764794298666\n",
      "Epoch: 7 - Batch: 146, Training Loss: 0.012505567266573361\n",
      "Epoch: 7 - Batch: 147, Training Loss: 0.012588740834598716\n",
      "Epoch: 7 - Batch: 148, Training Loss: 0.012686181374845616\n",
      "Epoch: 7 - Batch: 149, Training Loss: 0.0127725387069321\n",
      "Epoch: 7 - Batch: 150, Training Loss: 0.012861003330097863\n",
      "Epoch: 7 - Batch: 151, Training Loss: 0.012945261056446911\n",
      "Epoch: 7 - Batch: 152, Training Loss: 0.013027467315469807\n",
      "Epoch: 7 - Batch: 153, Training Loss: 0.013113476706678001\n",
      "Epoch: 7 - Batch: 154, Training Loss: 0.013197953835945225\n",
      "Epoch: 7 - Batch: 155, Training Loss: 0.01328767844743001\n",
      "Epoch: 7 - Batch: 156, Training Loss: 0.0133679787725655\n",
      "Epoch: 7 - Batch: 157, Training Loss: 0.013451340161943515\n",
      "Epoch: 7 - Batch: 158, Training Loss: 0.013553461390447063\n",
      "Epoch: 7 - Batch: 159, Training Loss: 0.013640054256020493\n",
      "Epoch: 7 - Batch: 160, Training Loss: 0.01373125141359878\n",
      "Epoch: 7 - Batch: 161, Training Loss: 0.013819861434288878\n",
      "Epoch: 7 - Batch: 162, Training Loss: 0.013904330692876433\n",
      "Epoch: 7 - Batch: 163, Training Loss: 0.01398683846565226\n",
      "Epoch: 7 - Batch: 164, Training Loss: 0.01407341956845168\n",
      "Epoch: 7 - Batch: 165, Training Loss: 0.01414967100344487\n",
      "Epoch: 7 - Batch: 166, Training Loss: 0.014252810026974623\n",
      "Epoch: 7 - Batch: 167, Training Loss: 0.014341357817400747\n",
      "Epoch: 7 - Batch: 168, Training Loss: 0.014434159747255383\n",
      "Epoch: 7 - Batch: 169, Training Loss: 0.014523534576839475\n",
      "Epoch: 7 - Batch: 170, Training Loss: 0.014614243461915708\n",
      "Epoch: 7 - Batch: 171, Training Loss: 0.014707428812486418\n",
      "Epoch: 7 - Batch: 172, Training Loss: 0.014804720779754234\n",
      "Epoch: 7 - Batch: 173, Training Loss: 0.014887935177702611\n",
      "Epoch: 7 - Batch: 174, Training Loss: 0.014973895083711317\n",
      "Epoch: 7 - Batch: 175, Training Loss: 0.015058060320307366\n",
      "Epoch: 7 - Batch: 176, Training Loss: 0.015151960166730295\n",
      "Epoch: 7 - Batch: 177, Training Loss: 0.015232379892671089\n",
      "Epoch: 7 - Batch: 178, Training Loss: 0.015327263536985044\n",
      "Epoch: 7 - Batch: 179, Training Loss: 0.015423049078404805\n",
      "Epoch: 7 - Batch: 180, Training Loss: 0.01550583181837898\n",
      "Epoch: 7 - Batch: 181, Training Loss: 0.015592639218663695\n",
      "Epoch: 7 - Batch: 182, Training Loss: 0.015678354750571163\n",
      "Epoch: 7 - Batch: 183, Training Loss: 0.01576200785550905\n",
      "Epoch: 7 - Batch: 184, Training Loss: 0.01584878717093523\n",
      "Epoch: 7 - Batch: 185, Training Loss: 0.01592963431679194\n",
      "Epoch: 7 - Batch: 186, Training Loss: 0.016017960399932925\n",
      "Epoch: 7 - Batch: 187, Training Loss: 0.01609565253998114\n",
      "Epoch: 7 - Batch: 188, Training Loss: 0.01618501788644648\n",
      "Epoch: 7 - Batch: 189, Training Loss: 0.016271084874473006\n",
      "Epoch: 7 - Batch: 190, Training Loss: 0.016358924649692887\n",
      "Epoch: 7 - Batch: 191, Training Loss: 0.01644499218137703\n",
      "Epoch: 7 - Batch: 192, Training Loss: 0.016527819210913643\n",
      "Epoch: 7 - Batch: 193, Training Loss: 0.01661355785444799\n",
      "Epoch: 7 - Batch: 194, Training Loss: 0.01669791455119601\n",
      "Epoch: 7 - Batch: 195, Training Loss: 0.016782571680559646\n",
      "Epoch: 7 - Batch: 196, Training Loss: 0.01687186203934067\n",
      "Epoch: 7 - Batch: 197, Training Loss: 0.01695916846789926\n",
      "Epoch: 7 - Batch: 198, Training Loss: 0.01703921366933962\n",
      "Epoch: 7 - Batch: 199, Training Loss: 0.017125547111380358\n",
      "Epoch: 7 - Batch: 200, Training Loss: 0.017209896376081564\n",
      "Epoch: 7 - Batch: 201, Training Loss: 0.017298163756555192\n",
      "Epoch: 7 - Batch: 202, Training Loss: 0.01739083439927196\n",
      "Epoch: 7 - Batch: 203, Training Loss: 0.017470077538915336\n",
      "Epoch: 7 - Batch: 204, Training Loss: 0.01755026828615029\n",
      "Epoch: 7 - Batch: 205, Training Loss: 0.017642827269004947\n",
      "Epoch: 7 - Batch: 206, Training Loss: 0.01773021266383318\n",
      "Epoch: 7 - Batch: 207, Training Loss: 0.017810143697172848\n",
      "Epoch: 7 - Batch: 208, Training Loss: 0.01789467305463938\n",
      "Epoch: 7 - Batch: 209, Training Loss: 0.017982100984498636\n",
      "Epoch: 7 - Batch: 210, Training Loss: 0.018063366165999354\n",
      "Epoch: 7 - Batch: 211, Training Loss: 0.01814900928024036\n",
      "Epoch: 7 - Batch: 212, Training Loss: 0.01822926543319403\n",
      "Epoch: 7 - Batch: 213, Training Loss: 0.018321281975478082\n",
      "Epoch: 7 - Batch: 214, Training Loss: 0.018411825409477226\n",
      "Epoch: 7 - Batch: 215, Training Loss: 0.018494234829646832\n",
      "Epoch: 7 - Batch: 216, Training Loss: 0.01857483631639338\n",
      "Epoch: 7 - Batch: 217, Training Loss: 0.01866676932074142\n",
      "Epoch: 7 - Batch: 218, Training Loss: 0.01874992167376365\n",
      "Epoch: 7 - Batch: 219, Training Loss: 0.018834672108366715\n",
      "Epoch: 7 - Batch: 220, Training Loss: 0.018929442295041645\n",
      "Epoch: 7 - Batch: 221, Training Loss: 0.019015594519934252\n",
      "Epoch: 7 - Batch: 222, Training Loss: 0.019100258024919093\n",
      "Epoch: 7 - Batch: 223, Training Loss: 0.019183628959195134\n",
      "Epoch: 7 - Batch: 224, Training Loss: 0.019272737223562316\n",
      "Epoch: 7 - Batch: 225, Training Loss: 0.019354078405927466\n",
      "Epoch: 7 - Batch: 226, Training Loss: 0.019443152376618946\n",
      "Epoch: 7 - Batch: 227, Training Loss: 0.019540469370671172\n",
      "Epoch: 7 - Batch: 228, Training Loss: 0.019627128253578152\n",
      "Epoch: 7 - Batch: 229, Training Loss: 0.019713818897210543\n",
      "Epoch: 7 - Batch: 230, Training Loss: 0.019799639126455805\n",
      "Epoch: 7 - Batch: 231, Training Loss: 0.019881781165922065\n",
      "Epoch: 7 - Batch: 232, Training Loss: 0.01996603125017476\n",
      "Epoch: 7 - Batch: 233, Training Loss: 0.020052979389826458\n",
      "Epoch: 7 - Batch: 234, Training Loss: 0.02012598033914123\n",
      "Epoch: 7 - Batch: 235, Training Loss: 0.020215282405094917\n",
      "Epoch: 7 - Batch: 236, Training Loss: 0.020307219808127355\n",
      "Epoch: 7 - Batch: 237, Training Loss: 0.020391051016537903\n",
      "Epoch: 7 - Batch: 238, Training Loss: 0.020480514190485622\n",
      "Epoch: 7 - Batch: 239, Training Loss: 0.020570193258288686\n",
      "Epoch: 7 - Batch: 240, Training Loss: 0.020654290914535522\n",
      "Epoch: 7 - Batch: 241, Training Loss: 0.020741011045415998\n",
      "Epoch: 7 - Batch: 242, Training Loss: 0.020821076652550972\n",
      "Epoch: 7 - Batch: 243, Training Loss: 0.02090940741856102\n",
      "Epoch: 7 - Batch: 244, Training Loss: 0.021005369370701302\n",
      "Epoch: 7 - Batch: 245, Training Loss: 0.02109363734425597\n",
      "Epoch: 7 - Batch: 246, Training Loss: 0.021185334376484205\n",
      "Epoch: 7 - Batch: 247, Training Loss: 0.021274573118680744\n",
      "Epoch: 7 - Batch: 248, Training Loss: 0.02135645781657589\n",
      "Epoch: 7 - Batch: 249, Training Loss: 0.021444135754806287\n",
      "Epoch: 7 - Batch: 250, Training Loss: 0.021527614543341088\n",
      "Epoch: 7 - Batch: 251, Training Loss: 0.02161687101618963\n",
      "Epoch: 7 - Batch: 252, Training Loss: 0.021701164889592634\n",
      "Epoch: 7 - Batch: 253, Training Loss: 0.021787395129354042\n",
      "Epoch: 7 - Batch: 254, Training Loss: 0.0218761814124944\n",
      "Epoch: 7 - Batch: 255, Training Loss: 0.02196316365430604\n",
      "Epoch: 7 - Batch: 256, Training Loss: 0.022046404981840507\n",
      "Epoch: 7 - Batch: 257, Training Loss: 0.022129277079358425\n",
      "Epoch: 7 - Batch: 258, Training Loss: 0.022212475433229018\n",
      "Epoch: 7 - Batch: 259, Training Loss: 0.022299198875478647\n",
      "Epoch: 7 - Batch: 260, Training Loss: 0.022373489887263644\n",
      "Epoch: 7 - Batch: 261, Training Loss: 0.022459129344171554\n",
      "Epoch: 7 - Batch: 262, Training Loss: 0.022541309414959666\n",
      "Epoch: 7 - Batch: 263, Training Loss: 0.022630020166758084\n",
      "Epoch: 7 - Batch: 264, Training Loss: 0.02272976289316394\n",
      "Epoch: 7 - Batch: 265, Training Loss: 0.022815893835096218\n",
      "Epoch: 7 - Batch: 266, Training Loss: 0.022902258222375935\n",
      "Epoch: 7 - Batch: 267, Training Loss: 0.022991710294587894\n",
      "Epoch: 7 - Batch: 268, Training Loss: 0.023080506573614986\n",
      "Epoch: 7 - Batch: 269, Training Loss: 0.023175899279157124\n",
      "Epoch: 7 - Batch: 270, Training Loss: 0.023266399764204103\n",
      "Epoch: 7 - Batch: 271, Training Loss: 0.023356871583667362\n",
      "Epoch: 7 - Batch: 272, Training Loss: 0.0234468264774007\n",
      "Epoch: 7 - Batch: 273, Training Loss: 0.023527214399370586\n",
      "Epoch: 7 - Batch: 274, Training Loss: 0.023611748964830023\n",
      "Epoch: 7 - Batch: 275, Training Loss: 0.02369506020179238\n",
      "Epoch: 7 - Batch: 276, Training Loss: 0.02378295219285571\n",
      "Epoch: 7 - Batch: 277, Training Loss: 0.023865497972241682\n",
      "Epoch: 7 - Batch: 278, Training Loss: 0.023948098821673622\n",
      "Epoch: 7 - Batch: 279, Training Loss: 0.0240377977329799\n",
      "Epoch: 7 - Batch: 280, Training Loss: 0.02412388196360215\n",
      "Epoch: 7 - Batch: 281, Training Loss: 0.024207788678642924\n",
      "Epoch: 7 - Batch: 282, Training Loss: 0.024300983808872908\n",
      "Epoch: 7 - Batch: 283, Training Loss: 0.024385364834299532\n",
      "Epoch: 7 - Batch: 284, Training Loss: 0.024471107956831332\n",
      "Epoch: 7 - Batch: 285, Training Loss: 0.02455475954199905\n",
      "Epoch: 7 - Batch: 286, Training Loss: 0.024644321606032686\n",
      "Epoch: 7 - Batch: 287, Training Loss: 0.024727378726302096\n",
      "Epoch: 7 - Batch: 288, Training Loss: 0.024809003537319983\n",
      "Epoch: 7 - Batch: 289, Training Loss: 0.024893222948162513\n",
      "Epoch: 7 - Batch: 290, Training Loss: 0.02497183063853637\n",
      "Epoch: 7 - Batch: 291, Training Loss: 0.025050568392818443\n",
      "Epoch: 7 - Batch: 292, Training Loss: 0.025127875277306113\n",
      "Epoch: 7 - Batch: 293, Training Loss: 0.02521665663663823\n",
      "Epoch: 7 - Batch: 294, Training Loss: 0.02530164930777961\n",
      "Epoch: 7 - Batch: 295, Training Loss: 0.0253870293349769\n",
      "Epoch: 7 - Batch: 296, Training Loss: 0.025475503095297475\n",
      "Epoch: 7 - Batch: 297, Training Loss: 0.025563421608203678\n",
      "Epoch: 7 - Batch: 298, Training Loss: 0.02565499324703691\n",
      "Epoch: 7 - Batch: 299, Training Loss: 0.02575089378439965\n",
      "Epoch: 7 - Batch: 300, Training Loss: 0.025828521667823665\n",
      "Epoch: 7 - Batch: 301, Training Loss: 0.025914656341224168\n",
      "Epoch: 7 - Batch: 302, Training Loss: 0.02600865358885248\n",
      "Epoch: 7 - Batch: 303, Training Loss: 0.026096932923615868\n",
      "Epoch: 7 - Batch: 304, Training Loss: 0.026179068229141126\n",
      "Epoch: 7 - Batch: 305, Training Loss: 0.02626174966963765\n",
      "Epoch: 7 - Batch: 306, Training Loss: 0.02634913217937373\n",
      "Epoch: 7 - Batch: 307, Training Loss: 0.02644208874893228\n",
      "Epoch: 7 - Batch: 308, Training Loss: 0.026535255374204657\n",
      "Epoch: 7 - Batch: 309, Training Loss: 0.026622986411722144\n",
      "Epoch: 7 - Batch: 310, Training Loss: 0.026702690912815272\n",
      "Epoch: 7 - Batch: 311, Training Loss: 0.026790355364629877\n",
      "Epoch: 7 - Batch: 312, Training Loss: 0.026876106092190467\n",
      "Epoch: 7 - Batch: 313, Training Loss: 0.0269557871418707\n",
      "Epoch: 7 - Batch: 314, Training Loss: 0.027034761492243257\n",
      "Epoch: 7 - Batch: 315, Training Loss: 0.02711863259770977\n",
      "Epoch: 7 - Batch: 316, Training Loss: 0.02719951494539753\n",
      "Epoch: 7 - Batch: 317, Training Loss: 0.027285294820420185\n",
      "Epoch: 7 - Batch: 318, Training Loss: 0.027367248569011293\n",
      "Epoch: 7 - Batch: 319, Training Loss: 0.027451375255339576\n",
      "Epoch: 7 - Batch: 320, Training Loss: 0.02753165730616544\n",
      "Epoch: 7 - Batch: 321, Training Loss: 0.027614674473777537\n",
      "Epoch: 7 - Batch: 322, Training Loss: 0.027699557801779625\n",
      "Epoch: 7 - Batch: 323, Training Loss: 0.027782968311859405\n",
      "Epoch: 7 - Batch: 324, Training Loss: 0.02787046417420974\n",
      "Epoch: 7 - Batch: 325, Training Loss: 0.027951771841318057\n",
      "Epoch: 7 - Batch: 326, Training Loss: 0.028035115364129667\n",
      "Epoch: 7 - Batch: 327, Training Loss: 0.028126491842232336\n",
      "Epoch: 7 - Batch: 328, Training Loss: 0.028205891567626797\n",
      "Epoch: 7 - Batch: 329, Training Loss: 0.02829884881057945\n",
      "Epoch: 7 - Batch: 330, Training Loss: 0.02838757131897395\n",
      "Epoch: 7 - Batch: 331, Training Loss: 0.028466714987806223\n",
      "Epoch: 7 - Batch: 332, Training Loss: 0.028559419714149156\n",
      "Epoch: 7 - Batch: 333, Training Loss: 0.028633772912952635\n",
      "Epoch: 7 - Batch: 334, Training Loss: 0.028725161184718954\n",
      "Epoch: 7 - Batch: 335, Training Loss: 0.028815487260693936\n",
      "Epoch: 7 - Batch: 336, Training Loss: 0.028903817816654447\n",
      "Epoch: 7 - Batch: 337, Training Loss: 0.028983366430102298\n",
      "Epoch: 7 - Batch: 338, Training Loss: 0.029070107763630042\n",
      "Epoch: 7 - Batch: 339, Training Loss: 0.02915937973689875\n",
      "Epoch: 7 - Batch: 340, Training Loss: 0.029255715669881843\n",
      "Epoch: 7 - Batch: 341, Training Loss: 0.02934409616820848\n",
      "Epoch: 7 - Batch: 342, Training Loss: 0.029425454444899093\n",
      "Epoch: 7 - Batch: 343, Training Loss: 0.02951865684037185\n",
      "Epoch: 7 - Batch: 344, Training Loss: 0.02960715321190717\n",
      "Epoch: 7 - Batch: 345, Training Loss: 0.029689382384863263\n",
      "Epoch: 7 - Batch: 346, Training Loss: 0.02976807207486918\n",
      "Epoch: 7 - Batch: 347, Training Loss: 0.02985362641846956\n",
      "Epoch: 7 - Batch: 348, Training Loss: 0.02993399568927624\n",
      "Epoch: 7 - Batch: 349, Training Loss: 0.030014905479318073\n",
      "Epoch: 7 - Batch: 350, Training Loss: 0.030101115158936674\n",
      "Epoch: 7 - Batch: 351, Training Loss: 0.03018073266245437\n",
      "Epoch: 7 - Batch: 352, Training Loss: 0.030266707203973387\n",
      "Epoch: 7 - Batch: 353, Training Loss: 0.030349010691219695\n",
      "Epoch: 7 - Batch: 354, Training Loss: 0.030431885964192364\n",
      "Epoch: 7 - Batch: 355, Training Loss: 0.030513787645210278\n",
      "Epoch: 7 - Batch: 356, Training Loss: 0.03059158597765475\n",
      "Epoch: 7 - Batch: 357, Training Loss: 0.03066437024851739\n",
      "Epoch: 7 - Batch: 358, Training Loss: 0.03075391052300064\n",
      "Epoch: 7 - Batch: 359, Training Loss: 0.030839128787343576\n",
      "Epoch: 7 - Batch: 360, Training Loss: 0.03091731186875854\n",
      "Epoch: 7 - Batch: 361, Training Loss: 0.03100594653937947\n",
      "Epoch: 7 - Batch: 362, Training Loss: 0.03110222334341821\n",
      "Epoch: 7 - Batch: 363, Training Loss: 0.031186660223487598\n",
      "Epoch: 7 - Batch: 364, Training Loss: 0.03127516818481496\n",
      "Epoch: 7 - Batch: 365, Training Loss: 0.03135604416923737\n",
      "Epoch: 7 - Batch: 366, Training Loss: 0.031446416029833245\n",
      "Epoch: 7 - Batch: 367, Training Loss: 0.0315253674662726\n",
      "Epoch: 7 - Batch: 368, Training Loss: 0.03161171133665501\n",
      "Epoch: 7 - Batch: 369, Training Loss: 0.03169596667832403\n",
      "Epoch: 7 - Batch: 370, Training Loss: 0.03178183541665623\n",
      "Epoch: 7 - Batch: 371, Training Loss: 0.03187240663279546\n",
      "Epoch: 7 - Batch: 372, Training Loss: 0.031951811127549974\n",
      "Epoch: 7 - Batch: 373, Training Loss: 0.03204049815610669\n",
      "Epoch: 7 - Batch: 374, Training Loss: 0.03212545472961753\n",
      "Epoch: 7 - Batch: 375, Training Loss: 0.032212920511638744\n",
      "Epoch: 7 - Batch: 376, Training Loss: 0.032299615788716776\n",
      "Epoch: 7 - Batch: 377, Training Loss: 0.032388011670082956\n",
      "Epoch: 7 - Batch: 378, Training Loss: 0.03247828530780909\n",
      "Epoch: 7 - Batch: 379, Training Loss: 0.03256874737850272\n",
      "Epoch: 7 - Batch: 380, Training Loss: 0.032646815161256254\n",
      "Epoch: 7 - Batch: 381, Training Loss: 0.03273225848749898\n",
      "Epoch: 7 - Batch: 382, Training Loss: 0.03281704259798499\n",
      "Epoch: 7 - Batch: 383, Training Loss: 0.03290042255821315\n",
      "Epoch: 7 - Batch: 384, Training Loss: 0.03298472726770103\n",
      "Epoch: 7 - Batch: 385, Training Loss: 0.03307331412116291\n",
      "Epoch: 7 - Batch: 386, Training Loss: 0.03316109408786641\n",
      "Epoch: 7 - Batch: 387, Training Loss: 0.033242777280299424\n",
      "Epoch: 7 - Batch: 388, Training Loss: 0.03333682898635888\n",
      "Epoch: 7 - Batch: 389, Training Loss: 0.03342077068082531\n",
      "Epoch: 7 - Batch: 390, Training Loss: 0.03350612102925876\n",
      "Epoch: 7 - Batch: 391, Training Loss: 0.033590967067290305\n",
      "Epoch: 7 - Batch: 392, Training Loss: 0.03367022658461955\n",
      "Epoch: 7 - Batch: 393, Training Loss: 0.033760348393598795\n",
      "Epoch: 7 - Batch: 394, Training Loss: 0.03385289878295626\n",
      "Epoch: 7 - Batch: 395, Training Loss: 0.03394037351061653\n",
      "Epoch: 7 - Batch: 396, Training Loss: 0.034021744581795056\n",
      "Epoch: 7 - Batch: 397, Training Loss: 0.034121919590145794\n",
      "Epoch: 7 - Batch: 398, Training Loss: 0.03420244383426448\n",
      "Epoch: 7 - Batch: 399, Training Loss: 0.0342879439418391\n",
      "Epoch: 7 - Batch: 400, Training Loss: 0.03437483058022227\n",
      "Epoch: 7 - Batch: 401, Training Loss: 0.034460619709780364\n",
      "Epoch: 7 - Batch: 402, Training Loss: 0.03455276838557835\n",
      "Epoch: 7 - Batch: 403, Training Loss: 0.034638715472387435\n",
      "Epoch: 7 - Batch: 404, Training Loss: 0.03472498669034213\n",
      "Epoch: 7 - Batch: 405, Training Loss: 0.0348176924360431\n",
      "Epoch: 7 - Batch: 406, Training Loss: 0.03491154689941042\n",
      "Epoch: 7 - Batch: 407, Training Loss: 0.0350033801734744\n",
      "Epoch: 7 - Batch: 408, Training Loss: 0.03509664967977388\n",
      "Epoch: 7 - Batch: 409, Training Loss: 0.035185109144370154\n",
      "Epoch: 7 - Batch: 410, Training Loss: 0.03527694139907609\n",
      "Epoch: 7 - Batch: 411, Training Loss: 0.03536959595446958\n",
      "Epoch: 7 - Batch: 412, Training Loss: 0.03545386329985179\n",
      "Epoch: 7 - Batch: 413, Training Loss: 0.03553792188070702\n",
      "Epoch: 7 - Batch: 414, Training Loss: 0.03562801575927592\n",
      "Epoch: 7 - Batch: 415, Training Loss: 0.03571978367704459\n",
      "Epoch: 7 - Batch: 416, Training Loss: 0.03580528797325408\n",
      "Epoch: 7 - Batch: 417, Training Loss: 0.03589460117183317\n",
      "Epoch: 7 - Batch: 418, Training Loss: 0.035976070756244025\n",
      "Epoch: 7 - Batch: 419, Training Loss: 0.036071599777757035\n",
      "Epoch: 7 - Batch: 420, Training Loss: 0.03615737065153928\n",
      "Epoch: 7 - Batch: 421, Training Loss: 0.03623922098780153\n",
      "Epoch: 7 - Batch: 422, Training Loss: 0.03631652364040884\n",
      "Epoch: 7 - Batch: 423, Training Loss: 0.0363988441972689\n",
      "Epoch: 7 - Batch: 424, Training Loss: 0.03648119760834755\n",
      "Epoch: 7 - Batch: 425, Training Loss: 0.03656661228136241\n",
      "Epoch: 7 - Batch: 426, Training Loss: 0.036656317976033116\n",
      "Epoch: 7 - Batch: 427, Training Loss: 0.0367438687230916\n",
      "Epoch: 7 - Batch: 428, Training Loss: 0.03683390339858101\n",
      "Epoch: 7 - Batch: 429, Training Loss: 0.03692344817677343\n",
      "Epoch: 7 - Batch: 430, Training Loss: 0.03700537233309169\n",
      "Epoch: 7 - Batch: 431, Training Loss: 0.0370895473246353\n",
      "Epoch: 7 - Batch: 432, Training Loss: 0.03717314286982242\n",
      "Epoch: 7 - Batch: 433, Training Loss: 0.03726416306066671\n",
      "Epoch: 7 - Batch: 434, Training Loss: 0.03735260247171024\n",
      "Epoch: 7 - Batch: 435, Training Loss: 0.037441376924119384\n",
      "Epoch: 7 - Batch: 436, Training Loss: 0.03752336968014489\n",
      "Epoch: 7 - Batch: 437, Training Loss: 0.03761217096947121\n",
      "Epoch: 7 - Batch: 438, Training Loss: 0.03769097436423326\n",
      "Epoch: 7 - Batch: 439, Training Loss: 0.03777088704912817\n",
      "Epoch: 7 - Batch: 440, Training Loss: 0.037852013591757265\n",
      "Epoch: 7 - Batch: 441, Training Loss: 0.03794092825840955\n",
      "Epoch: 7 - Batch: 442, Training Loss: 0.038029779094714625\n",
      "Epoch: 7 - Batch: 443, Training Loss: 0.038114757093278134\n",
      "Epoch: 7 - Batch: 444, Training Loss: 0.03820030086644451\n",
      "Epoch: 7 - Batch: 445, Training Loss: 0.03827890514660237\n",
      "Epoch: 7 - Batch: 446, Training Loss: 0.038364120896528804\n",
      "Epoch: 7 - Batch: 447, Training Loss: 0.038442954767007334\n",
      "Epoch: 7 - Batch: 448, Training Loss: 0.03853379815792168\n",
      "Epoch: 7 - Batch: 449, Training Loss: 0.03862605200057995\n",
      "Epoch: 7 - Batch: 450, Training Loss: 0.038713986193066215\n",
      "Epoch: 7 - Batch: 451, Training Loss: 0.03880581201654959\n",
      "Epoch: 7 - Batch: 452, Training Loss: 0.03888519564226492\n",
      "Epoch: 7 - Batch: 453, Training Loss: 0.038985447170426006\n",
      "Epoch: 7 - Batch: 454, Training Loss: 0.03906777090891875\n",
      "Epoch: 7 - Batch: 455, Training Loss: 0.03915286195120013\n",
      "Epoch: 7 - Batch: 456, Training Loss: 0.03923557792631152\n",
      "Epoch: 7 - Batch: 457, Training Loss: 0.03932045014639992\n",
      "Epoch: 7 - Batch: 458, Training Loss: 0.03940959867849872\n",
      "Epoch: 7 - Batch: 459, Training Loss: 0.03949487042516025\n",
      "Epoch: 7 - Batch: 460, Training Loss: 0.03958266631238298\n",
      "Epoch: 7 - Batch: 461, Training Loss: 0.03966717003762821\n",
      "Epoch: 7 - Batch: 462, Training Loss: 0.039757476616063915\n",
      "Epoch: 7 - Batch: 463, Training Loss: 0.039838944606569476\n",
      "Epoch: 7 - Batch: 464, Training Loss: 0.03993020213485555\n",
      "Epoch: 7 - Batch: 465, Training Loss: 0.04001909559886056\n",
      "Epoch: 7 - Batch: 466, Training Loss: 0.04010286476234496\n",
      "Epoch: 7 - Batch: 467, Training Loss: 0.040175493145562326\n",
      "Epoch: 7 - Batch: 468, Training Loss: 0.04026058480563646\n",
      "Epoch: 7 - Batch: 469, Training Loss: 0.040341999307645496\n",
      "Epoch: 7 - Batch: 470, Training Loss: 0.040428552059935494\n",
      "Epoch: 7 - Batch: 471, Training Loss: 0.04051822698852711\n",
      "Epoch: 7 - Batch: 472, Training Loss: 0.040624305167255514\n",
      "Epoch: 7 - Batch: 473, Training Loss: 0.040702809346453664\n",
      "Epoch: 7 - Batch: 474, Training Loss: 0.04079180186734864\n",
      "Epoch: 7 - Batch: 475, Training Loss: 0.04087885527270746\n",
      "Epoch: 7 - Batch: 476, Training Loss: 0.04097346711909988\n",
      "Epoch: 7 - Batch: 477, Training Loss: 0.041060192532108394\n",
      "Epoch: 7 - Batch: 478, Training Loss: 0.041142112981670136\n",
      "Epoch: 7 - Batch: 479, Training Loss: 0.041228423868097476\n",
      "Epoch: 7 - Batch: 480, Training Loss: 0.0413200500209632\n",
      "Epoch: 7 - Batch: 481, Training Loss: 0.04141073473798695\n",
      "Epoch: 7 - Batch: 482, Training Loss: 0.04150604136646486\n",
      "Epoch: 7 - Batch: 483, Training Loss: 0.04160494104670252\n",
      "Epoch: 7 - Batch: 484, Training Loss: 0.041691604204735354\n",
      "Epoch: 7 - Batch: 485, Training Loss: 0.04178475053551581\n",
      "Epoch: 7 - Batch: 486, Training Loss: 0.041871672171858415\n",
      "Epoch: 7 - Batch: 487, Training Loss: 0.041949776583544256\n",
      "Epoch: 7 - Batch: 488, Training Loss: 0.042032761162588646\n",
      "Epoch: 7 - Batch: 489, Training Loss: 0.042115002067429116\n",
      "Epoch: 7 - Batch: 490, Training Loss: 0.042206348897657585\n",
      "Epoch: 7 - Batch: 491, Training Loss: 0.042296009629371745\n",
      "Epoch: 7 - Batch: 492, Training Loss: 0.04239085836839518\n",
      "Epoch: 7 - Batch: 493, Training Loss: 0.04248073838564689\n",
      "Epoch: 7 - Batch: 494, Training Loss: 0.04256917397255328\n",
      "Epoch: 7 - Batch: 495, Training Loss: 0.04265633077788511\n",
      "Epoch: 7 - Batch: 496, Training Loss: 0.04274034528540537\n",
      "Epoch: 7 - Batch: 497, Training Loss: 0.04282969304948897\n",
      "Epoch: 7 - Batch: 498, Training Loss: 0.04292022926149083\n",
      "Epoch: 7 - Batch: 499, Training Loss: 0.043006014560733864\n",
      "Epoch: 7 - Batch: 500, Training Loss: 0.04309318771286193\n",
      "Epoch: 7 - Batch: 501, Training Loss: 0.0431791673388153\n",
      "Epoch: 7 - Batch: 502, Training Loss: 0.04326765459172959\n",
      "Epoch: 7 - Batch: 503, Training Loss: 0.043348977666589156\n",
      "Epoch: 7 - Batch: 504, Training Loss: 0.04344241316144542\n",
      "Epoch: 7 - Batch: 505, Training Loss: 0.04352633486413837\n",
      "Epoch: 7 - Batch: 506, Training Loss: 0.043616736712394466\n",
      "Epoch: 7 - Batch: 507, Training Loss: 0.043712556467176866\n",
      "Epoch: 7 - Batch: 508, Training Loss: 0.04379269498003458\n",
      "Epoch: 7 - Batch: 509, Training Loss: 0.04388352257398823\n",
      "Epoch: 7 - Batch: 510, Training Loss: 0.043962831990438114\n",
      "Epoch: 7 - Batch: 511, Training Loss: 0.04404978830348793\n",
      "Epoch: 7 - Batch: 512, Training Loss: 0.04413715750596812\n",
      "Epoch: 7 - Batch: 513, Training Loss: 0.04423046390685079\n",
      "Epoch: 7 - Batch: 514, Training Loss: 0.044320085007159865\n",
      "Epoch: 7 - Batch: 515, Training Loss: 0.04440878171652901\n",
      "Epoch: 7 - Batch: 516, Training Loss: 0.044496184088302096\n",
      "Epoch: 7 - Batch: 517, Training Loss: 0.04457820584351941\n",
      "Epoch: 7 - Batch: 518, Training Loss: 0.044665830904868115\n",
      "Epoch: 7 - Batch: 519, Training Loss: 0.044752145176799735\n",
      "Epoch: 7 - Batch: 520, Training Loss: 0.044838193995541405\n",
      "Epoch: 7 - Batch: 521, Training Loss: 0.044920002383675744\n",
      "Epoch: 7 - Batch: 522, Training Loss: 0.04501261161161497\n",
      "Epoch: 7 - Batch: 523, Training Loss: 0.04510379359620325\n",
      "Epoch: 7 - Batch: 524, Training Loss: 0.04518500494942143\n",
      "Epoch: 7 - Batch: 525, Training Loss: 0.04526859044312047\n",
      "Epoch: 7 - Batch: 526, Training Loss: 0.04536170433360348\n",
      "Epoch: 7 - Batch: 527, Training Loss: 0.045449938044372085\n",
      "Epoch: 7 - Batch: 528, Training Loss: 0.04553209996440913\n",
      "Epoch: 7 - Batch: 529, Training Loss: 0.045613962773907044\n",
      "Epoch: 7 - Batch: 530, Training Loss: 0.04570150679320245\n",
      "Epoch: 7 - Batch: 531, Training Loss: 0.04578860122617798\n",
      "Epoch: 7 - Batch: 532, Training Loss: 0.04587832019721493\n",
      "Epoch: 7 - Batch: 533, Training Loss: 0.04596345002452532\n",
      "Epoch: 7 - Batch: 534, Training Loss: 0.046051907987885214\n",
      "Epoch: 7 - Batch: 535, Training Loss: 0.04613820255840596\n",
      "Epoch: 7 - Batch: 536, Training Loss: 0.046221412440289315\n",
      "Epoch: 7 - Batch: 537, Training Loss: 0.04631153289630243\n",
      "Epoch: 7 - Batch: 538, Training Loss: 0.04639074384276547\n",
      "Epoch: 7 - Batch: 539, Training Loss: 0.0464764755484872\n",
      "Epoch: 7 - Batch: 540, Training Loss: 0.046564937694304025\n",
      "Epoch: 7 - Batch: 541, Training Loss: 0.04665561306412343\n",
      "Epoch: 7 - Batch: 542, Training Loss: 0.04673865415884883\n",
      "Epoch: 7 - Batch: 543, Training Loss: 0.04682026096120798\n",
      "Epoch: 7 - Batch: 544, Training Loss: 0.04690459428685617\n",
      "Epoch: 7 - Batch: 545, Training Loss: 0.0469856564245908\n",
      "Epoch: 7 - Batch: 546, Training Loss: 0.04707042910591089\n",
      "Epoch: 7 - Batch: 547, Training Loss: 0.04716014955347252\n",
      "Epoch: 7 - Batch: 548, Training Loss: 0.047242703302385\n",
      "Epoch: 7 - Batch: 549, Training Loss: 0.047331528148148984\n",
      "Epoch: 7 - Batch: 550, Training Loss: 0.04741881769243164\n",
      "Epoch: 7 - Batch: 551, Training Loss: 0.0475058455211705\n",
      "Epoch: 7 - Batch: 552, Training Loss: 0.04758631952020462\n",
      "Epoch: 7 - Batch: 553, Training Loss: 0.04766343816941848\n",
      "Epoch: 7 - Batch: 554, Training Loss: 0.04775067935288447\n",
      "Epoch: 7 - Batch: 555, Training Loss: 0.04782945534853793\n",
      "Epoch: 7 - Batch: 556, Training Loss: 0.047924162804536756\n",
      "Epoch: 7 - Batch: 557, Training Loss: 0.04801069934594493\n",
      "Epoch: 7 - Batch: 558, Training Loss: 0.04809804687625535\n",
      "Epoch: 7 - Batch: 559, Training Loss: 0.04818824684837367\n",
      "Epoch: 7 - Batch: 560, Training Loss: 0.04827477194826006\n",
      "Epoch: 7 - Batch: 561, Training Loss: 0.048359352743151174\n",
      "Epoch: 7 - Batch: 562, Training Loss: 0.048443038269852724\n",
      "Epoch: 7 - Batch: 563, Training Loss: 0.04853017678851907\n",
      "Epoch: 7 - Batch: 564, Training Loss: 0.04861470841328501\n",
      "Epoch: 7 - Batch: 565, Training Loss: 0.04870275101604351\n",
      "Epoch: 7 - Batch: 566, Training Loss: 0.048789954501795726\n",
      "Epoch: 7 - Batch: 567, Training Loss: 0.04886907036995413\n",
      "Epoch: 7 - Batch: 568, Training Loss: 0.04895320638495298\n",
      "Epoch: 7 - Batch: 569, Training Loss: 0.049037912573239105\n",
      "Epoch: 7 - Batch: 570, Training Loss: 0.04912695026714015\n",
      "Epoch: 7 - Batch: 571, Training Loss: 0.04921152275271874\n",
      "Epoch: 7 - Batch: 572, Training Loss: 0.049286735375376284\n",
      "Epoch: 7 - Batch: 573, Training Loss: 0.04937501184975923\n",
      "Epoch: 7 - Batch: 574, Training Loss: 0.0494660081999812\n",
      "Epoch: 7 - Batch: 575, Training Loss: 0.049551381480881625\n",
      "Epoch: 7 - Batch: 576, Training Loss: 0.04963936514621153\n",
      "Epoch: 7 - Batch: 577, Training Loss: 0.04972471558113596\n",
      "Epoch: 7 - Batch: 578, Training Loss: 0.04980185886174687\n",
      "Epoch: 7 - Batch: 579, Training Loss: 0.04988853575331259\n",
      "Epoch: 7 - Batch: 580, Training Loss: 0.04997683110126413\n",
      "Epoch: 7 - Batch: 581, Training Loss: 0.050064870009621974\n",
      "Epoch: 7 - Batch: 582, Training Loss: 0.0501408708925864\n",
      "Epoch: 7 - Batch: 583, Training Loss: 0.050224493274690696\n",
      "Epoch: 7 - Batch: 584, Training Loss: 0.050306615569500585\n",
      "Epoch: 7 - Batch: 585, Training Loss: 0.05039185052660369\n",
      "Epoch: 7 - Batch: 586, Training Loss: 0.05047043178162567\n",
      "Epoch: 7 - Batch: 587, Training Loss: 0.05056036954396598\n",
      "Epoch: 7 - Batch: 588, Training Loss: 0.05064987307831423\n",
      "Epoch: 7 - Batch: 589, Training Loss: 0.050740843919873435\n",
      "Epoch: 7 - Batch: 590, Training Loss: 0.05082518168127359\n",
      "Epoch: 7 - Batch: 591, Training Loss: 0.050913381969454274\n",
      "Epoch: 7 - Batch: 592, Training Loss: 0.0510019795169087\n",
      "Epoch: 7 - Batch: 593, Training Loss: 0.051086076184687135\n",
      "Epoch: 7 - Batch: 594, Training Loss: 0.05117390630380629\n",
      "Epoch: 7 - Batch: 595, Training Loss: 0.05125847696689626\n",
      "Epoch: 7 - Batch: 596, Training Loss: 0.05134045393213901\n",
      "Epoch: 7 - Batch: 597, Training Loss: 0.05143156949165053\n",
      "Epoch: 7 - Batch: 598, Training Loss: 0.05151782383768515\n",
      "Epoch: 7 - Batch: 599, Training Loss: 0.05160687510449297\n",
      "Epoch: 7 - Batch: 600, Training Loss: 0.05168782646580913\n",
      "Epoch: 7 - Batch: 601, Training Loss: 0.05176876840578581\n",
      "Epoch: 7 - Batch: 602, Training Loss: 0.0518551700212568\n",
      "Epoch: 7 - Batch: 603, Training Loss: 0.05194368451883148\n",
      "Epoch: 7 - Batch: 604, Training Loss: 0.05202741008468133\n",
      "Epoch: 7 - Batch: 605, Training Loss: 0.052111248249438274\n",
      "Epoch: 7 - Batch: 606, Training Loss: 0.052204540441087625\n",
      "Epoch: 7 - Batch: 607, Training Loss: 0.052298719671927084\n",
      "Epoch: 7 - Batch: 608, Training Loss: 0.052386475377909185\n",
      "Epoch: 7 - Batch: 609, Training Loss: 0.052474442906492384\n",
      "Epoch: 7 - Batch: 610, Training Loss: 0.052568730288131124\n",
      "Epoch: 7 - Batch: 611, Training Loss: 0.05265760534809003\n",
      "Epoch: 7 - Batch: 612, Training Loss: 0.05274356839866385\n",
      "Epoch: 7 - Batch: 613, Training Loss: 0.0528280336168868\n",
      "Epoch: 7 - Batch: 614, Training Loss: 0.05291564392534457\n",
      "Epoch: 7 - Batch: 615, Training Loss: 0.05300593593622717\n",
      "Epoch: 7 - Batch: 616, Training Loss: 0.05309268437437157\n",
      "Epoch: 7 - Batch: 617, Training Loss: 0.05317838907637209\n",
      "Epoch: 7 - Batch: 618, Training Loss: 0.053267003835532595\n",
      "Epoch: 7 - Batch: 619, Training Loss: 0.05335899736503661\n",
      "Epoch: 7 - Batch: 620, Training Loss: 0.05345226600642623\n",
      "Epoch: 7 - Batch: 621, Training Loss: 0.05353713274669292\n",
      "Epoch: 7 - Batch: 622, Training Loss: 0.053620045723557275\n",
      "Epoch: 7 - Batch: 623, Training Loss: 0.05370714186164079\n",
      "Epoch: 7 - Batch: 624, Training Loss: 0.05378852243447185\n",
      "Epoch: 7 - Batch: 625, Training Loss: 0.053868063078393195\n",
      "Epoch: 7 - Batch: 626, Training Loss: 0.05395163304078243\n",
      "Epoch: 7 - Batch: 627, Training Loss: 0.05402786120970055\n",
      "Epoch: 7 - Batch: 628, Training Loss: 0.05411310292545638\n",
      "Epoch: 7 - Batch: 629, Training Loss: 0.05421011340781231\n",
      "Epoch: 7 - Batch: 630, Training Loss: 0.05429671927174525\n",
      "Epoch: 7 - Batch: 631, Training Loss: 0.05438181895076932\n",
      "Epoch: 7 - Batch: 632, Training Loss: 0.054469632988743126\n",
      "Epoch: 7 - Batch: 633, Training Loss: 0.05456055896001471\n",
      "Epoch: 7 - Batch: 634, Training Loss: 0.05465312425053337\n",
      "Epoch: 7 - Batch: 635, Training Loss: 0.054741039149351974\n",
      "Epoch: 7 - Batch: 636, Training Loss: 0.054832797089767696\n",
      "Epoch: 7 - Batch: 637, Training Loss: 0.05491294346094922\n",
      "Epoch: 7 - Batch: 638, Training Loss: 0.0550001669570087\n",
      "Epoch: 7 - Batch: 639, Training Loss: 0.0550859171038441\n",
      "Epoch: 7 - Batch: 640, Training Loss: 0.05516846875226122\n",
      "Epoch: 7 - Batch: 641, Training Loss: 0.05524801444972728\n",
      "Epoch: 7 - Batch: 642, Training Loss: 0.05533637580265651\n",
      "Epoch: 7 - Batch: 643, Training Loss: 0.05543117317839048\n",
      "Epoch: 7 - Batch: 644, Training Loss: 0.055518046663965946\n",
      "Epoch: 7 - Batch: 645, Training Loss: 0.055604599354476675\n",
      "Epoch: 7 - Batch: 646, Training Loss: 0.05569024999343341\n",
      "Epoch: 7 - Batch: 647, Training Loss: 0.05578135604511446\n",
      "Epoch: 7 - Batch: 648, Training Loss: 0.05586616710989827\n",
      "Epoch: 7 - Batch: 649, Training Loss: 0.055950620392365244\n",
      "Epoch: 7 - Batch: 650, Training Loss: 0.056034373789778595\n",
      "Epoch: 7 - Batch: 651, Training Loss: 0.056129849767605856\n",
      "Epoch: 7 - Batch: 652, Training Loss: 0.056214168908732445\n",
      "Epoch: 7 - Batch: 653, Training Loss: 0.05630055509174048\n",
      "Epoch: 7 - Batch: 654, Training Loss: 0.05639241934193308\n",
      "Epoch: 7 - Batch: 655, Training Loss: 0.05647497375806173\n",
      "Epoch: 7 - Batch: 656, Training Loss: 0.05655689155111463\n",
      "Epoch: 7 - Batch: 657, Training Loss: 0.05664703889964627\n",
      "Epoch: 7 - Batch: 658, Training Loss: 0.056729792028813814\n",
      "Epoch: 7 - Batch: 659, Training Loss: 0.05681307873940389\n",
      "Epoch: 7 - Batch: 660, Training Loss: 0.056891644178931396\n",
      "Epoch: 7 - Batch: 661, Training Loss: 0.05697260571529419\n",
      "Epoch: 7 - Batch: 662, Training Loss: 0.05705070662409512\n",
      "Epoch: 7 - Batch: 663, Training Loss: 0.057132538290660374\n",
      "Epoch: 7 - Batch: 664, Training Loss: 0.057224004620540996\n",
      "Epoch: 7 - Batch: 665, Training Loss: 0.05731179772151841\n",
      "Epoch: 7 - Batch: 666, Training Loss: 0.05739852763823609\n",
      "Epoch: 7 - Batch: 667, Training Loss: 0.05748128505488533\n",
      "Epoch: 7 - Batch: 668, Training Loss: 0.0575586879072992\n",
      "Epoch: 7 - Batch: 669, Training Loss: 0.057640619619222816\n",
      "Epoch: 7 - Batch: 670, Training Loss: 0.05773312051102494\n",
      "Epoch: 7 - Batch: 671, Training Loss: 0.057815933238659335\n",
      "Epoch: 7 - Batch: 672, Training Loss: 0.05790014883154265\n",
      "Epoch: 7 - Batch: 673, Training Loss: 0.057981517067052436\n",
      "Epoch: 7 - Batch: 674, Training Loss: 0.05805921342044723\n",
      "Epoch: 7 - Batch: 675, Training Loss: 0.05814317342011292\n",
      "Epoch: 7 - Batch: 676, Training Loss: 0.05823297403864006\n",
      "Epoch: 7 - Batch: 677, Training Loss: 0.05832090188021684\n",
      "Epoch: 7 - Batch: 678, Training Loss: 0.05839740775933313\n",
      "Epoch: 7 - Batch: 679, Training Loss: 0.058486994658635424\n",
      "Epoch: 7 - Batch: 680, Training Loss: 0.05857525706587739\n",
      "Epoch: 7 - Batch: 681, Training Loss: 0.05865926878097441\n",
      "Epoch: 7 - Batch: 682, Training Loss: 0.05874312233494882\n",
      "Epoch: 7 - Batch: 683, Training Loss: 0.05882493207604929\n",
      "Epoch: 7 - Batch: 684, Training Loss: 0.05890685415040597\n",
      "Epoch: 7 - Batch: 685, Training Loss: 0.05900229016939799\n",
      "Epoch: 7 - Batch: 686, Training Loss: 0.05908537443548095\n",
      "Epoch: 7 - Batch: 687, Training Loss: 0.05917706781655402\n",
      "Epoch: 7 - Batch: 688, Training Loss: 0.05926220532920626\n",
      "Epoch: 7 - Batch: 689, Training Loss: 0.059347437290014515\n",
      "Epoch: 7 - Batch: 690, Training Loss: 0.059432001175917994\n",
      "Epoch: 7 - Batch: 691, Training Loss: 0.05950679158665252\n",
      "Epoch: 7 - Batch: 692, Training Loss: 0.05959208028554719\n",
      "Epoch: 7 - Batch: 693, Training Loss: 0.05967110084681764\n",
      "Epoch: 7 - Batch: 694, Training Loss: 0.059755017162357794\n",
      "Epoch: 7 - Batch: 695, Training Loss: 0.05983683996977498\n",
      "Epoch: 7 - Batch: 696, Training Loss: 0.05992823095982941\n",
      "Epoch: 7 - Batch: 697, Training Loss: 0.06001540177052294\n",
      "Epoch: 7 - Batch: 698, Training Loss: 0.06010172796585469\n",
      "Epoch: 7 - Batch: 699, Training Loss: 0.06018830631947636\n",
      "Epoch: 7 - Batch: 700, Training Loss: 0.06028054095495795\n",
      "Epoch: 7 - Batch: 701, Training Loss: 0.0603702190775381\n",
      "Epoch: 7 - Batch: 702, Training Loss: 0.060457728080340285\n",
      "Epoch: 7 - Batch: 703, Training Loss: 0.06055292673717882\n",
      "Epoch: 7 - Batch: 704, Training Loss: 0.06063185582805431\n",
      "Epoch: 7 - Batch: 705, Training Loss: 0.060720797467538175\n",
      "Epoch: 7 - Batch: 706, Training Loss: 0.06080905899751443\n",
      "Epoch: 7 - Batch: 707, Training Loss: 0.06090167755412423\n",
      "Epoch: 7 - Batch: 708, Training Loss: 0.06098574677337066\n",
      "Epoch: 7 - Batch: 709, Training Loss: 0.06106639886352158\n",
      "Epoch: 7 - Batch: 710, Training Loss: 0.061151151089723625\n",
      "Epoch: 7 - Batch: 711, Training Loss: 0.06122944585571242\n",
      "Epoch: 7 - Batch: 712, Training Loss: 0.061311434367501715\n",
      "Epoch: 7 - Batch: 713, Training Loss: 0.06138836694989434\n",
      "Epoch: 7 - Batch: 714, Training Loss: 0.06147184277302392\n",
      "Epoch: 7 - Batch: 715, Training Loss: 0.061555482113539284\n",
      "Epoch: 7 - Batch: 716, Training Loss: 0.061649245959065645\n",
      "Epoch: 7 - Batch: 717, Training Loss: 0.061741476980459634\n",
      "Epoch: 7 - Batch: 718, Training Loss: 0.06182258726278348\n",
      "Epoch: 7 - Batch: 719, Training Loss: 0.061902620170148054\n",
      "Epoch: 7 - Batch: 720, Training Loss: 0.061986304504509586\n",
      "Epoch: 7 - Batch: 721, Training Loss: 0.06207808157178893\n",
      "Epoch: 7 - Batch: 722, Training Loss: 0.06216312369106223\n",
      "Epoch: 7 - Batch: 723, Training Loss: 0.06223888816945193\n",
      "Epoch: 7 - Batch: 724, Training Loss: 0.06231961712537713\n",
      "Epoch: 7 - Batch: 725, Training Loss: 0.06241152345961204\n",
      "Epoch: 7 - Batch: 726, Training Loss: 0.0625008767158732\n",
      "Epoch: 7 - Batch: 727, Training Loss: 0.06259029259160778\n",
      "Epoch: 7 - Batch: 728, Training Loss: 0.06267228046089263\n",
      "Epoch: 7 - Batch: 729, Training Loss: 0.06275155149387879\n",
      "Epoch: 7 - Batch: 730, Training Loss: 0.06284498286805738\n",
      "Epoch: 7 - Batch: 731, Training Loss: 0.0629276312010403\n",
      "Epoch: 7 - Batch: 732, Training Loss: 0.06302357969765442\n",
      "Epoch: 7 - Batch: 733, Training Loss: 0.06311238998204322\n",
      "Epoch: 7 - Batch: 734, Training Loss: 0.0631970253527461\n",
      "Epoch: 7 - Batch: 735, Training Loss: 0.06328938638244695\n",
      "Epoch: 7 - Batch: 736, Training Loss: 0.06337521483451375\n",
      "Epoch: 7 - Batch: 737, Training Loss: 0.06345600353099813\n",
      "Epoch: 7 - Batch: 738, Training Loss: 0.06354851845287367\n",
      "Epoch: 7 - Batch: 739, Training Loss: 0.06363260582287118\n",
      "Epoch: 7 - Batch: 740, Training Loss: 0.06372628178737848\n",
      "Epoch: 7 - Batch: 741, Training Loss: 0.06382449000307774\n",
      "Epoch: 7 - Batch: 742, Training Loss: 0.06390556187104823\n",
      "Epoch: 7 - Batch: 743, Training Loss: 0.0639964437813407\n",
      "Epoch: 7 - Batch: 744, Training Loss: 0.06407883120191038\n",
      "Epoch: 7 - Batch: 745, Training Loss: 0.06416761193727182\n",
      "Epoch: 7 - Batch: 746, Training Loss: 0.06425353263197452\n",
      "Epoch: 7 - Batch: 747, Training Loss: 0.06434604025771765\n",
      "Epoch: 7 - Batch: 748, Training Loss: 0.06442417882098685\n",
      "Epoch: 7 - Batch: 749, Training Loss: 0.0645069008442893\n",
      "Epoch: 7 - Batch: 750, Training Loss: 0.06459294271904042\n",
      "Epoch: 7 - Batch: 751, Training Loss: 0.0646780502316766\n",
      "Epoch: 7 - Batch: 752, Training Loss: 0.06477148636285938\n",
      "Epoch: 7 - Batch: 753, Training Loss: 0.0648553966549202\n",
      "Epoch: 7 - Batch: 754, Training Loss: 0.06494375642629999\n",
      "Epoch: 7 - Batch: 755, Training Loss: 0.06502484718019491\n",
      "Epoch: 7 - Batch: 756, Training Loss: 0.06510576233267784\n",
      "Epoch: 7 - Batch: 757, Training Loss: 0.06519564447513662\n",
      "Epoch: 7 - Batch: 758, Training Loss: 0.06528364019978106\n",
      "Epoch: 7 - Batch: 759, Training Loss: 0.06537865487175992\n",
      "Epoch: 7 - Batch: 760, Training Loss: 0.06546116353415732\n",
      "Epoch: 7 - Batch: 761, Training Loss: 0.06553852485854235\n",
      "Epoch: 7 - Batch: 762, Training Loss: 0.06562283656144419\n",
      "Epoch: 7 - Batch: 763, Training Loss: 0.06571522730697645\n",
      "Epoch: 7 - Batch: 764, Training Loss: 0.0658082987674532\n",
      "Epoch: 7 - Batch: 765, Training Loss: 0.0658920557418866\n",
      "Epoch: 7 - Batch: 766, Training Loss: 0.06597493078009802\n",
      "Epoch: 7 - Batch: 767, Training Loss: 0.0660646668145708\n",
      "Epoch: 7 - Batch: 768, Training Loss: 0.06614953689709627\n",
      "Epoch: 7 - Batch: 769, Training Loss: 0.06624118444412502\n",
      "Epoch: 7 - Batch: 770, Training Loss: 0.06633026912734283\n",
      "Epoch: 7 - Batch: 771, Training Loss: 0.06641636154989698\n",
      "Epoch: 7 - Batch: 772, Training Loss: 0.06650364392778371\n",
      "Epoch: 7 - Batch: 773, Training Loss: 0.0665957302559371\n",
      "Epoch: 7 - Batch: 774, Training Loss: 0.06668118902082072\n",
      "Epoch: 7 - Batch: 775, Training Loss: 0.06677084249347004\n",
      "Epoch: 7 - Batch: 776, Training Loss: 0.06686007080732491\n",
      "Epoch: 7 - Batch: 777, Training Loss: 0.06694566667895412\n",
      "Epoch: 7 - Batch: 778, Training Loss: 0.06703340592075936\n",
      "Epoch: 7 - Batch: 779, Training Loss: 0.06712122097809121\n",
      "Epoch: 7 - Batch: 780, Training Loss: 0.06720651300071089\n",
      "Epoch: 7 - Batch: 781, Training Loss: 0.06728953600028656\n",
      "Epoch: 7 - Batch: 782, Training Loss: 0.0673695112861211\n",
      "Epoch: 7 - Batch: 783, Training Loss: 0.06745739611696644\n",
      "Epoch: 7 - Batch: 784, Training Loss: 0.06755032053659013\n",
      "Epoch: 7 - Batch: 785, Training Loss: 0.0676368267999755\n",
      "Epoch: 7 - Batch: 786, Training Loss: 0.0677324958009704\n",
      "Epoch: 7 - Batch: 787, Training Loss: 0.06781018556251653\n",
      "Epoch: 7 - Batch: 788, Training Loss: 0.06788910911796896\n",
      "Epoch: 7 - Batch: 789, Training Loss: 0.0679775142600485\n",
      "Epoch: 7 - Batch: 790, Training Loss: 0.06805766247225242\n",
      "Epoch: 7 - Batch: 791, Training Loss: 0.06814550854278045\n",
      "Epoch: 7 - Batch: 792, Training Loss: 0.06822735005078426\n",
      "Epoch: 7 - Batch: 793, Training Loss: 0.06831706371498147\n",
      "Epoch: 7 - Batch: 794, Training Loss: 0.06841164953994316\n",
      "Epoch: 7 - Batch: 795, Training Loss: 0.06849836980673804\n",
      "Epoch: 7 - Batch: 796, Training Loss: 0.06859332951987362\n",
      "Epoch: 7 - Batch: 797, Training Loss: 0.06867668424944577\n",
      "Epoch: 7 - Batch: 798, Training Loss: 0.06876236257169575\n",
      "Epoch: 7 - Batch: 799, Training Loss: 0.06885659433963089\n",
      "Epoch: 7 - Batch: 800, Training Loss: 0.06893464917344834\n",
      "Epoch: 7 - Batch: 801, Training Loss: 0.06902836955725455\n",
      "Epoch: 7 - Batch: 802, Training Loss: 0.06911643853358566\n",
      "Epoch: 7 - Batch: 803, Training Loss: 0.06920638112706529\n",
      "Epoch: 7 - Batch: 804, Training Loss: 0.0692897565403388\n",
      "Epoch: 7 - Batch: 805, Training Loss: 0.06937719930957996\n",
      "Epoch: 7 - Batch: 806, Training Loss: 0.06946960560495581\n",
      "Epoch: 7 - Batch: 807, Training Loss: 0.06955628672222396\n",
      "Epoch: 7 - Batch: 808, Training Loss: 0.06965213991812806\n",
      "Epoch: 7 - Batch: 809, Training Loss: 0.06974028374622908\n",
      "Epoch: 7 - Batch: 810, Training Loss: 0.06982404762140752\n",
      "Epoch: 7 - Batch: 811, Training Loss: 0.0699171501939273\n",
      "Epoch: 7 - Batch: 812, Training Loss: 0.07000597013441683\n",
      "Epoch: 7 - Batch: 813, Training Loss: 0.0701099082666942\n",
      "Epoch: 7 - Batch: 814, Training Loss: 0.07019521316411484\n",
      "Epoch: 7 - Batch: 815, Training Loss: 0.07027829759700184\n",
      "Epoch: 7 - Batch: 816, Training Loss: 0.07036483727061926\n",
      "Epoch: 7 - Batch: 817, Training Loss: 0.07044866209105274\n",
      "Epoch: 7 - Batch: 818, Training Loss: 0.07052768088543593\n",
      "Epoch: 7 - Batch: 819, Training Loss: 0.07060692004649398\n",
      "Epoch: 7 - Batch: 820, Training Loss: 0.07068331565578186\n",
      "Epoch: 7 - Batch: 821, Training Loss: 0.07077238014335459\n",
      "Epoch: 7 - Batch: 822, Training Loss: 0.07086154630108062\n",
      "Epoch: 7 - Batch: 823, Training Loss: 0.07094755619887887\n",
      "Epoch: 7 - Batch: 824, Training Loss: 0.07103894761521029\n",
      "Epoch: 7 - Batch: 825, Training Loss: 0.07112546675957455\n",
      "Epoch: 7 - Batch: 826, Training Loss: 0.0712125563737666\n",
      "Epoch: 7 - Batch: 827, Training Loss: 0.07130553841492035\n",
      "Epoch: 7 - Batch: 828, Training Loss: 0.07138512495466529\n",
      "Epoch: 7 - Batch: 829, Training Loss: 0.0714757994533969\n",
      "Epoch: 7 - Batch: 830, Training Loss: 0.07155231845443126\n",
      "Epoch: 7 - Batch: 831, Training Loss: 0.07163589839417345\n",
      "Epoch: 7 - Batch: 832, Training Loss: 0.07172326850480899\n",
      "Epoch: 7 - Batch: 833, Training Loss: 0.0718008235133999\n",
      "Epoch: 7 - Batch: 834, Training Loss: 0.07188732971500598\n",
      "Epoch: 7 - Batch: 835, Training Loss: 0.07196825999723343\n",
      "Epoch: 7 - Batch: 836, Training Loss: 0.07204427274799663\n",
      "Epoch: 7 - Batch: 837, Training Loss: 0.07212395814693785\n",
      "Epoch: 7 - Batch: 838, Training Loss: 0.07221496246050839\n",
      "Epoch: 7 - Batch: 839, Training Loss: 0.07230203060640229\n",
      "Epoch: 7 - Batch: 840, Training Loss: 0.07238607746155108\n",
      "Epoch: 7 - Batch: 841, Training Loss: 0.07248109524720542\n",
      "Epoch: 7 - Batch: 842, Training Loss: 0.07257499306726811\n",
      "Epoch: 7 - Batch: 843, Training Loss: 0.07267163393039806\n",
      "Epoch: 7 - Batch: 844, Training Loss: 0.07274911224001873\n",
      "Epoch: 7 - Batch: 845, Training Loss: 0.07283456116964172\n",
      "Epoch: 7 - Batch: 846, Training Loss: 0.07291635631847737\n",
      "Epoch: 7 - Batch: 847, Training Loss: 0.07300753226105254\n",
      "Epoch: 7 - Batch: 848, Training Loss: 0.07309894557813705\n",
      "Epoch: 7 - Batch: 849, Training Loss: 0.07319235545930577\n",
      "Epoch: 7 - Batch: 850, Training Loss: 0.07328453828668713\n",
      "Epoch: 7 - Batch: 851, Training Loss: 0.07337231145149242\n",
      "Epoch: 7 - Batch: 852, Training Loss: 0.0734545198604341\n",
      "Epoch: 7 - Batch: 853, Training Loss: 0.07354272656748148\n",
      "Epoch: 7 - Batch: 854, Training Loss: 0.0736265814249986\n",
      "Epoch: 7 - Batch: 855, Training Loss: 0.07371970686820609\n",
      "Epoch: 7 - Batch: 856, Training Loss: 0.07380599191854051\n",
      "Epoch: 7 - Batch: 857, Training Loss: 0.0739012048402137\n",
      "Epoch: 7 - Batch: 858, Training Loss: 0.07398468317775979\n",
      "Epoch: 7 - Batch: 859, Training Loss: 0.07407315137176766\n",
      "Epoch: 7 - Batch: 860, Training Loss: 0.07415956260627182\n",
      "Epoch: 7 - Batch: 861, Training Loss: 0.07424835976256462\n",
      "Epoch: 7 - Batch: 862, Training Loss: 0.07433236891382171\n",
      "Epoch: 7 - Batch: 863, Training Loss: 0.07442511699686002\n",
      "Epoch: 7 - Batch: 864, Training Loss: 0.07451157288542434\n",
      "Epoch: 7 - Batch: 865, Training Loss: 0.07459615349497764\n",
      "Epoch: 7 - Batch: 866, Training Loss: 0.07468015889623272\n",
      "Epoch: 7 - Batch: 867, Training Loss: 0.07476755998299688\n",
      "Epoch: 7 - Batch: 868, Training Loss: 0.07485774480065896\n",
      "Epoch: 7 - Batch: 869, Training Loss: 0.07493873232756286\n",
      "Epoch: 7 - Batch: 870, Training Loss: 0.07502427419792755\n",
      "Epoch: 7 - Batch: 871, Training Loss: 0.07510778577890166\n",
      "Epoch: 7 - Batch: 872, Training Loss: 0.07519215581997314\n",
      "Epoch: 7 - Batch: 873, Training Loss: 0.0752719403622953\n",
      "Epoch: 7 - Batch: 874, Training Loss: 0.0753555694103834\n",
      "Epoch: 7 - Batch: 875, Training Loss: 0.07544246381442148\n",
      "Epoch: 7 - Batch: 876, Training Loss: 0.07552767114383269\n",
      "Epoch: 7 - Batch: 877, Training Loss: 0.07560758631225446\n",
      "Epoch: 7 - Batch: 878, Training Loss: 0.07569334032029457\n",
      "Epoch: 7 - Batch: 879, Training Loss: 0.07578179817245177\n",
      "Epoch: 7 - Batch: 880, Training Loss: 0.07585266891057614\n",
      "Epoch: 7 - Batch: 881, Training Loss: 0.07593623134824964\n",
      "Epoch: 7 - Batch: 882, Training Loss: 0.07602265171990862\n",
      "Epoch: 7 - Batch: 883, Training Loss: 0.07611248306126935\n",
      "Epoch: 7 - Batch: 884, Training Loss: 0.07621084752911161\n",
      "Epoch: 7 - Batch: 885, Training Loss: 0.07630106934020373\n",
      "Epoch: 7 - Batch: 886, Training Loss: 0.07637738818256416\n",
      "Epoch: 7 - Batch: 887, Training Loss: 0.07645941555969553\n",
      "Epoch: 7 - Batch: 888, Training Loss: 0.07654435548986961\n",
      "Epoch: 7 - Batch: 889, Training Loss: 0.07663190203593738\n",
      "Epoch: 7 - Batch: 890, Training Loss: 0.07671418229417619\n",
      "Epoch: 7 - Batch: 891, Training Loss: 0.0767961788434492\n",
      "Epoch: 7 - Batch: 892, Training Loss: 0.07688429366618049\n",
      "Epoch: 7 - Batch: 893, Training Loss: 0.07696482332216369\n",
      "Epoch: 7 - Batch: 894, Training Loss: 0.07704292232816294\n",
      "Epoch: 7 - Batch: 895, Training Loss: 0.07713072606135364\n",
      "Epoch: 7 - Batch: 896, Training Loss: 0.07720914849421476\n",
      "Epoch: 7 - Batch: 897, Training Loss: 0.07729114858961816\n",
      "Epoch: 7 - Batch: 898, Training Loss: 0.07737574579926272\n",
      "Epoch: 7 - Batch: 899, Training Loss: 0.07745906153375631\n",
      "Epoch: 7 - Batch: 900, Training Loss: 0.07754334578812616\n",
      "Epoch: 7 - Batch: 901, Training Loss: 0.07763489900437952\n",
      "Epoch: 7 - Batch: 902, Training Loss: 0.07771111574413171\n",
      "Epoch: 7 - Batch: 903, Training Loss: 0.07779692836217027\n",
      "Epoch: 7 - Batch: 904, Training Loss: 0.07789530876906554\n",
      "Epoch: 7 - Batch: 905, Training Loss: 0.07798265555802467\n",
      "Epoch: 7 - Batch: 906, Training Loss: 0.0780617111728559\n",
      "Epoch: 7 - Batch: 907, Training Loss: 0.07814692041742466\n",
      "Epoch: 7 - Batch: 908, Training Loss: 0.07822907123572594\n",
      "Epoch: 7 - Batch: 909, Training Loss: 0.07832103915772035\n",
      "Epoch: 7 - Batch: 910, Training Loss: 0.07840639281752296\n",
      "Epoch: 7 - Batch: 911, Training Loss: 0.07848808376325501\n",
      "Epoch: 7 - Batch: 912, Training Loss: 0.07858534435282892\n",
      "Epoch: 7 - Batch: 913, Training Loss: 0.0786720350582406\n",
      "Epoch: 7 - Batch: 914, Training Loss: 0.07876202971929341\n",
      "Epoch: 7 - Batch: 915, Training Loss: 0.07884838256002659\n",
      "Epoch: 7 - Batch: 916, Training Loss: 0.07893639598443337\n",
      "Epoch: 7 - Batch: 917, Training Loss: 0.07903025734236782\n",
      "Epoch: 7 - Batch: 918, Training Loss: 0.07911511401261263\n",
      "Epoch: 7 - Batch: 919, Training Loss: 0.07920294750488022\n",
      "Epoch: 7 - Batch: 920, Training Loss: 0.07929232124097114\n",
      "Epoch: 7 - Batch: 921, Training Loss: 0.0793837805094806\n",
      "Epoch: 7 - Batch: 922, Training Loss: 0.07947420302908219\n",
      "Epoch: 7 - Batch: 923, Training Loss: 0.07955812251389917\n",
      "Epoch: 7 - Batch: 924, Training Loss: 0.07965005276289153\n",
      "Epoch: 7 - Batch: 925, Training Loss: 0.07974101685197595\n",
      "Epoch: 7 - Batch: 926, Training Loss: 0.07982034212098786\n",
      "Epoch: 7 - Batch: 927, Training Loss: 0.0799065903199846\n",
      "Epoch: 7 - Batch: 928, Training Loss: 0.07999126588428396\n",
      "Epoch: 7 - Batch: 929, Training Loss: 0.08008535523863376\n",
      "Epoch: 7 - Batch: 930, Training Loss: 0.08016838355122712\n",
      "Epoch: 7 - Batch: 931, Training Loss: 0.08025437527974051\n",
      "Epoch: 7 - Batch: 932, Training Loss: 0.08034605368551725\n",
      "Epoch: 7 - Batch: 933, Training Loss: 0.08042617451096847\n",
      "Epoch: 7 - Batch: 934, Training Loss: 0.08051539708552867\n",
      "Epoch: 7 - Batch: 935, Training Loss: 0.08059632126396371\n",
      "Epoch: 7 - Batch: 936, Training Loss: 0.08068805831062853\n",
      "Epoch: 7 - Batch: 937, Training Loss: 0.08077960734442494\n",
      "Epoch: 7 - Batch: 938, Training Loss: 0.08087239544807777\n",
      "Epoch: 7 - Batch: 939, Training Loss: 0.08095687839695273\n",
      "Epoch: 7 - Batch: 940, Training Loss: 0.08104252607668218\n",
      "Epoch: 7 - Batch: 941, Training Loss: 0.08113217086934332\n",
      "Epoch: 7 - Batch: 942, Training Loss: 0.08121420016781014\n",
      "Epoch: 7 - Batch: 943, Training Loss: 0.08130473550872423\n",
      "Epoch: 7 - Batch: 944, Training Loss: 0.08138893278405243\n",
      "Epoch: 7 - Batch: 945, Training Loss: 0.0814783295143896\n",
      "Epoch: 7 - Batch: 946, Training Loss: 0.08156045791916981\n",
      "Epoch: 7 - Batch: 947, Training Loss: 0.08165070418867701\n",
      "Epoch: 7 - Batch: 948, Training Loss: 0.08173344135407982\n",
      "Epoch: 7 - Batch: 949, Training Loss: 0.08181998620480052\n",
      "Epoch: 7 - Batch: 950, Training Loss: 0.08191074627400631\n",
      "Epoch: 7 - Batch: 951, Training Loss: 0.0819926735131104\n",
      "Epoch: 7 - Batch: 952, Training Loss: 0.08208363807789525\n",
      "Epoch: 7 - Batch: 953, Training Loss: 0.08216744768827115\n",
      "Epoch: 7 - Batch: 954, Training Loss: 0.08225430910835417\n",
      "Epoch: 7 - Batch: 955, Training Loss: 0.0823445491690442\n",
      "Epoch: 7 - Batch: 956, Training Loss: 0.08243006968849136\n",
      "Epoch: 7 - Batch: 957, Training Loss: 0.08251938247932723\n",
      "Epoch: 7 - Batch: 958, Training Loss: 0.08260215796023657\n",
      "Epoch: 7 - Batch: 959, Training Loss: 0.08269335753956245\n",
      "Epoch: 7 - Batch: 960, Training Loss: 0.08278748261468921\n",
      "Epoch: 7 - Batch: 961, Training Loss: 0.08287218849407896\n",
      "Epoch: 7 - Batch: 962, Training Loss: 0.08295922975363225\n",
      "Epoch: 7 - Batch: 963, Training Loss: 0.083049259882167\n",
      "Epoch: 7 - Batch: 964, Training Loss: 0.08314022652891342\n",
      "Epoch: 7 - Batch: 965, Training Loss: 0.083227902576698\n",
      "Epoch: 7 - Batch: 966, Training Loss: 0.08330982349578223\n",
      "Epoch: 7 - Batch: 967, Training Loss: 0.08339390148150783\n",
      "Epoch: 7 - Batch: 968, Training Loss: 0.08348404119412105\n",
      "Epoch: 7 - Batch: 969, Training Loss: 0.08356673579978112\n",
      "Epoch: 7 - Batch: 970, Training Loss: 0.08365297178501513\n",
      "Epoch: 7 - Batch: 971, Training Loss: 0.08373297131921521\n",
      "Epoch: 7 - Batch: 972, Training Loss: 0.08381228481927519\n",
      "Epoch: 7 - Batch: 973, Training Loss: 0.0839025008354713\n",
      "Epoch: 7 - Batch: 974, Training Loss: 0.08398268533064358\n",
      "Epoch: 7 - Batch: 975, Training Loss: 0.08407321306652889\n",
      "Epoch: 7 - Batch: 976, Training Loss: 0.08416245405749104\n",
      "Epoch: 7 - Batch: 977, Training Loss: 0.08424342719600172\n",
      "Epoch: 7 - Batch: 978, Training Loss: 0.08433570859839469\n",
      "Epoch: 7 - Batch: 979, Training Loss: 0.08442166036189493\n",
      "Epoch: 7 - Batch: 980, Training Loss: 0.08450834771269194\n",
      "Epoch: 7 - Batch: 981, Training Loss: 0.08459043207502681\n",
      "Epoch: 7 - Batch: 982, Training Loss: 0.08467958871874445\n",
      "Epoch: 7 - Batch: 983, Training Loss: 0.08477080987139325\n",
      "Epoch: 7 - Batch: 984, Training Loss: 0.08485437646681199\n",
      "Epoch: 7 - Batch: 985, Training Loss: 0.08494446741086531\n",
      "Epoch: 7 - Batch: 986, Training Loss: 0.08503161827920878\n",
      "Epoch: 7 - Batch: 987, Training Loss: 0.08511294832277061\n",
      "Epoch: 7 - Batch: 988, Training Loss: 0.0852050808618219\n",
      "Epoch: 7 - Batch: 989, Training Loss: 0.08529309156176265\n",
      "Epoch: 7 - Batch: 990, Training Loss: 0.08538031232100024\n",
      "Epoch: 7 - Batch: 991, Training Loss: 0.08546439145582034\n",
      "Epoch: 7 - Batch: 992, Training Loss: 0.08555783445685855\n",
      "Epoch: 7 - Batch: 993, Training Loss: 0.08563796434532944\n",
      "Epoch: 7 - Batch: 994, Training Loss: 0.08572535554740955\n",
      "Epoch: 7 - Batch: 995, Training Loss: 0.08581506833434105\n",
      "Epoch: 7 - Batch: 996, Training Loss: 0.08589838909766764\n",
      "Epoch: 7 - Batch: 997, Training Loss: 0.0860019403186999\n",
      "Epoch: 7 - Batch: 998, Training Loss: 0.08608934391370263\n",
      "Epoch: 7 - Batch: 999, Training Loss: 0.08618128202719673\n",
      "Epoch: 7 - Batch: 1000, Training Loss: 0.08626252921239456\n",
      "Epoch: 7 - Batch: 1001, Training Loss: 0.08635103685246968\n",
      "Epoch: 7 - Batch: 1002, Training Loss: 0.086438554794733\n",
      "Epoch: 7 - Batch: 1003, Training Loss: 0.08652051190411669\n",
      "Epoch: 7 - Batch: 1004, Training Loss: 0.08660352372164355\n",
      "Epoch: 7 - Batch: 1005, Training Loss: 0.08668485868283568\n",
      "Epoch: 7 - Batch: 1006, Training Loss: 0.08677021391183187\n",
      "Epoch: 7 - Batch: 1007, Training Loss: 0.08685188440862382\n",
      "Epoch: 7 - Batch: 1008, Training Loss: 0.08694469748345972\n",
      "Epoch: 7 - Batch: 1009, Training Loss: 0.08703365040383924\n",
      "Epoch: 7 - Batch: 1010, Training Loss: 0.08711905311020848\n",
      "Epoch: 7 - Batch: 1011, Training Loss: 0.08720088480148545\n",
      "Epoch: 7 - Batch: 1012, Training Loss: 0.08728588948869587\n",
      "Epoch: 7 - Batch: 1013, Training Loss: 0.0873754780127931\n",
      "Epoch: 7 - Batch: 1014, Training Loss: 0.08745942054745767\n",
      "Epoch: 7 - Batch: 1015, Training Loss: 0.08753982773838352\n",
      "Epoch: 7 - Batch: 1016, Training Loss: 0.08762720376747363\n",
      "Epoch: 7 - Batch: 1017, Training Loss: 0.0877130981318492\n",
      "Epoch: 7 - Batch: 1018, Training Loss: 0.08780485686303964\n",
      "Epoch: 7 - Batch: 1019, Training Loss: 0.08788756186615175\n",
      "Epoch: 7 - Batch: 1020, Training Loss: 0.08797541963149659\n",
      "Epoch: 7 - Batch: 1021, Training Loss: 0.08806437958235765\n",
      "Epoch: 7 - Batch: 1022, Training Loss: 0.08815895313795526\n",
      "Epoch: 7 - Batch: 1023, Training Loss: 0.08824173441376061\n",
      "Epoch: 7 - Batch: 1024, Training Loss: 0.08833068591321681\n",
      "Epoch: 7 - Batch: 1025, Training Loss: 0.08841779613697509\n",
      "Epoch: 7 - Batch: 1026, Training Loss: 0.08850940208183988\n",
      "Epoch: 7 - Batch: 1027, Training Loss: 0.08859431876447268\n",
      "Epoch: 7 - Batch: 1028, Training Loss: 0.08868785940147751\n",
      "Epoch: 7 - Batch: 1029, Training Loss: 0.08878020388668845\n",
      "Epoch: 7 - Batch: 1030, Training Loss: 0.08887061409985841\n",
      "Epoch: 7 - Batch: 1031, Training Loss: 0.088965331378762\n",
      "Epoch: 7 - Batch: 1032, Training Loss: 0.08905062611349186\n",
      "Epoch: 7 - Batch: 1033, Training Loss: 0.08913870473356784\n",
      "Epoch: 7 - Batch: 1034, Training Loss: 0.08922260441194917\n",
      "Epoch: 7 - Batch: 1035, Training Loss: 0.0893009617603438\n",
      "Epoch: 7 - Batch: 1036, Training Loss: 0.08938524669428567\n",
      "Epoch: 7 - Batch: 1037, Training Loss: 0.08947460755557562\n",
      "Epoch: 7 - Batch: 1038, Training Loss: 0.08955964290384036\n",
      "Epoch: 7 - Batch: 1039, Training Loss: 0.08964497638653167\n",
      "Epoch: 7 - Batch: 1040, Training Loss: 0.08973198832564093\n",
      "Epoch: 7 - Batch: 1041, Training Loss: 0.08982374795263086\n",
      "Epoch: 7 - Batch: 1042, Training Loss: 0.08991093719208221\n",
      "Epoch: 7 - Batch: 1043, Training Loss: 0.08998956572060561\n",
      "Epoch: 7 - Batch: 1044, Training Loss: 0.09007137708650102\n",
      "Epoch: 7 - Batch: 1045, Training Loss: 0.09016448431717816\n",
      "Epoch: 7 - Batch: 1046, Training Loss: 0.09025106810119812\n",
      "Epoch: 7 - Batch: 1047, Training Loss: 0.09032609449023038\n",
      "Epoch: 7 - Batch: 1048, Training Loss: 0.09040281111698839\n",
      "Epoch: 7 - Batch: 1049, Training Loss: 0.09048731498432594\n",
      "Epoch: 7 - Batch: 1050, Training Loss: 0.0905832072000203\n",
      "Epoch: 7 - Batch: 1051, Training Loss: 0.09066946275075079\n",
      "Epoch: 7 - Batch: 1052, Training Loss: 0.09076284802523418\n",
      "Epoch: 7 - Batch: 1053, Training Loss: 0.09085384662422177\n",
      "Epoch: 7 - Batch: 1054, Training Loss: 0.09094746048202364\n",
      "Epoch: 7 - Batch: 1055, Training Loss: 0.09103221837338524\n",
      "Epoch: 7 - Batch: 1056, Training Loss: 0.09112170733400245\n",
      "Epoch: 7 - Batch: 1057, Training Loss: 0.09120466837514297\n",
      "Epoch: 7 - Batch: 1058, Training Loss: 0.09128291553402224\n",
      "Epoch: 7 - Batch: 1059, Training Loss: 0.09137078999830518\n",
      "Epoch: 7 - Batch: 1060, Training Loss: 0.0914594687237273\n",
      "Epoch: 7 - Batch: 1061, Training Loss: 0.09154212341997556\n",
      "Epoch: 7 - Batch: 1062, Training Loss: 0.09162546944484781\n",
      "Epoch: 7 - Batch: 1063, Training Loss: 0.09171059065394932\n",
      "Epoch: 7 - Batch: 1064, Training Loss: 0.09180016323899352\n",
      "Epoch: 7 - Batch: 1065, Training Loss: 0.09188638888855478\n",
      "Epoch: 7 - Batch: 1066, Training Loss: 0.0919801805780005\n",
      "Epoch: 7 - Batch: 1067, Training Loss: 0.09206401384366092\n",
      "Epoch: 7 - Batch: 1068, Training Loss: 0.09214484331446698\n",
      "Epoch: 7 - Batch: 1069, Training Loss: 0.09222774961916962\n",
      "Epoch: 7 - Batch: 1070, Training Loss: 0.09230972645467589\n",
      "Epoch: 7 - Batch: 1071, Training Loss: 0.09239117791922531\n",
      "Epoch: 7 - Batch: 1072, Training Loss: 0.09248057066479923\n",
      "Epoch: 7 - Batch: 1073, Training Loss: 0.0925652785705492\n",
      "Epoch: 7 - Batch: 1074, Training Loss: 0.09264534616079892\n",
      "Epoch: 7 - Batch: 1075, Training Loss: 0.09272700668641584\n",
      "Epoch: 7 - Batch: 1076, Training Loss: 0.09280336794024874\n",
      "Epoch: 7 - Batch: 1077, Training Loss: 0.09289121976860522\n",
      "Epoch: 7 - Batch: 1078, Training Loss: 0.09297463265100918\n",
      "Epoch: 7 - Batch: 1079, Training Loss: 0.09305747927075397\n",
      "Epoch: 7 - Batch: 1080, Training Loss: 0.0931491876333013\n",
      "Epoch: 7 - Batch: 1081, Training Loss: 0.09323599285895552\n",
      "Epoch: 7 - Batch: 1082, Training Loss: 0.09331431985519222\n",
      "Epoch: 7 - Batch: 1083, Training Loss: 0.09339159364452211\n",
      "Epoch: 7 - Batch: 1084, Training Loss: 0.09347957800795782\n",
      "Epoch: 7 - Batch: 1085, Training Loss: 0.09357919207902295\n",
      "Epoch: 7 - Batch: 1086, Training Loss: 0.09366441789897124\n",
      "Epoch: 7 - Batch: 1087, Training Loss: 0.0937474500480576\n",
      "Epoch: 7 - Batch: 1088, Training Loss: 0.09382652885482877\n",
      "Epoch: 7 - Batch: 1089, Training Loss: 0.09390964333715526\n",
      "Epoch: 7 - Batch: 1090, Training Loss: 0.09400030894584917\n",
      "Epoch: 7 - Batch: 1091, Training Loss: 0.09408415725476311\n",
      "Epoch: 7 - Batch: 1092, Training Loss: 0.09416964635327088\n",
      "Epoch: 7 - Batch: 1093, Training Loss: 0.09425967444308955\n",
      "Epoch: 7 - Batch: 1094, Training Loss: 0.09434553308051023\n",
      "Epoch: 7 - Batch: 1095, Training Loss: 0.09442273218487428\n",
      "Epoch: 7 - Batch: 1096, Training Loss: 0.09450774077035697\n",
      "Epoch: 7 - Batch: 1097, Training Loss: 0.09459123365074446\n",
      "Epoch: 7 - Batch: 1098, Training Loss: 0.09468428251236233\n",
      "Epoch: 7 - Batch: 1099, Training Loss: 0.09476700263904102\n",
      "Epoch: 7 - Batch: 1100, Training Loss: 0.09485697459621019\n",
      "Epoch: 7 - Batch: 1101, Training Loss: 0.09494859327996152\n",
      "Epoch: 7 - Batch: 1102, Training Loss: 0.09503491975947795\n",
      "Epoch: 7 - Batch: 1103, Training Loss: 0.09511813737612657\n",
      "Epoch: 7 - Batch: 1104, Training Loss: 0.09519524859329362\n",
      "Epoch: 7 - Batch: 1105, Training Loss: 0.09528432370690168\n",
      "Epoch: 7 - Batch: 1106, Training Loss: 0.09536754456696225\n",
      "Epoch: 7 - Batch: 1107, Training Loss: 0.09545230214324954\n",
      "Epoch: 7 - Batch: 1108, Training Loss: 0.09553793700377937\n",
      "Epoch: 7 - Batch: 1109, Training Loss: 0.09562160012931571\n",
      "Epoch: 7 - Batch: 1110, Training Loss: 0.09570921459551869\n",
      "Epoch: 7 - Batch: 1111, Training Loss: 0.09579865332958512\n",
      "Epoch: 7 - Batch: 1112, Training Loss: 0.0958926036187863\n",
      "Epoch: 7 - Batch: 1113, Training Loss: 0.0959802664519839\n",
      "Epoch: 7 - Batch: 1114, Training Loss: 0.09606504396444332\n",
      "Epoch: 7 - Batch: 1115, Training Loss: 0.0961452416556688\n",
      "Epoch: 7 - Batch: 1116, Training Loss: 0.09623468651823934\n",
      "Epoch: 7 - Batch: 1117, Training Loss: 0.09632596571869519\n",
      "Epoch: 7 - Batch: 1118, Training Loss: 0.09641089511550284\n",
      "Epoch: 7 - Batch: 1119, Training Loss: 0.09650468307965826\n",
      "Epoch: 7 - Batch: 1120, Training Loss: 0.09659458505944234\n",
      "Epoch: 7 - Batch: 1121, Training Loss: 0.09667656953086702\n",
      "Epoch: 7 - Batch: 1122, Training Loss: 0.09676980589876326\n",
      "Epoch: 7 - Batch: 1123, Training Loss: 0.09685514250748588\n",
      "Epoch: 7 - Batch: 1124, Training Loss: 0.09694067752035103\n",
      "Epoch: 7 - Batch: 1125, Training Loss: 0.0970259705067274\n",
      "Epoch: 7 - Batch: 1126, Training Loss: 0.09710902911587733\n",
      "Epoch: 7 - Batch: 1127, Training Loss: 0.0971916239974313\n",
      "Epoch: 7 - Batch: 1128, Training Loss: 0.09726788168448713\n",
      "Epoch: 7 - Batch: 1129, Training Loss: 0.09735241674418078\n",
      "Epoch: 7 - Batch: 1130, Training Loss: 0.09743913407646008\n",
      "Epoch: 7 - Batch: 1131, Training Loss: 0.09752113658125523\n",
      "Epoch: 7 - Batch: 1132, Training Loss: 0.09761067351965762\n",
      "Epoch: 7 - Batch: 1133, Training Loss: 0.09769834363663177\n",
      "Epoch: 7 - Batch: 1134, Training Loss: 0.09778523431537954\n",
      "Epoch: 7 - Batch: 1135, Training Loss: 0.0978649352332747\n",
      "Epoch: 7 - Batch: 1136, Training Loss: 0.09794744148628036\n",
      "Epoch: 7 - Batch: 1137, Training Loss: 0.0980227975480592\n",
      "Epoch: 7 - Batch: 1138, Training Loss: 0.09810388935220182\n",
      "Epoch: 7 - Batch: 1139, Training Loss: 0.09818953619173312\n",
      "Epoch: 7 - Batch: 1140, Training Loss: 0.09827870554344768\n",
      "Epoch: 7 - Batch: 1141, Training Loss: 0.09836513781379506\n",
      "Epoch: 7 - Batch: 1142, Training Loss: 0.09844281392212136\n",
      "Epoch: 7 - Batch: 1143, Training Loss: 0.09853303085265072\n",
      "Epoch: 7 - Batch: 1144, Training Loss: 0.09862513115527619\n",
      "Epoch: 7 - Batch: 1145, Training Loss: 0.0987108047761233\n",
      "Epoch: 7 - Batch: 1146, Training Loss: 0.09879362279206366\n",
      "Epoch: 7 - Batch: 1147, Training Loss: 0.09888767822341342\n",
      "Epoch: 7 - Batch: 1148, Training Loss: 0.09897715496408999\n",
      "Epoch: 7 - Batch: 1149, Training Loss: 0.09907332109673501\n",
      "Epoch: 7 - Batch: 1150, Training Loss: 0.09915656763844032\n",
      "Epoch: 7 - Batch: 1151, Training Loss: 0.09923388233182838\n",
      "Epoch: 7 - Batch: 1152, Training Loss: 0.09932742437740068\n",
      "Epoch: 7 - Batch: 1153, Training Loss: 0.09941252073887767\n",
      "Epoch: 7 - Batch: 1154, Training Loss: 0.09949817242190415\n",
      "Epoch: 7 - Batch: 1155, Training Loss: 0.09958233997522302\n",
      "Epoch: 7 - Batch: 1156, Training Loss: 0.09966803285884818\n",
      "Epoch: 7 - Batch: 1157, Training Loss: 0.09976226282282848\n",
      "Epoch: 7 - Batch: 1158, Training Loss: 0.09984476381223989\n",
      "Epoch: 7 - Batch: 1159, Training Loss: 0.09993183799396897\n",
      "Epoch: 7 - Batch: 1160, Training Loss: 0.10001540637480877\n",
      "Epoch: 7 - Batch: 1161, Training Loss: 0.10010874299193497\n",
      "Epoch: 7 - Batch: 1162, Training Loss: 0.10019077246956169\n",
      "Epoch: 7 - Batch: 1163, Training Loss: 0.100278490990598\n",
      "Epoch: 7 - Batch: 1164, Training Loss: 0.10036706051497317\n",
      "Epoch: 7 - Batch: 1165, Training Loss: 0.10045612576243099\n",
      "Epoch: 7 - Batch: 1166, Training Loss: 0.10053379369735915\n",
      "Epoch: 7 - Batch: 1167, Training Loss: 0.10061862977657152\n",
      "Epoch: 7 - Batch: 1168, Training Loss: 0.10070379896963612\n",
      "Epoch: 7 - Batch: 1169, Training Loss: 0.1007852859758026\n",
      "Epoch: 7 - Batch: 1170, Training Loss: 0.10086625757600932\n",
      "Epoch: 7 - Batch: 1171, Training Loss: 0.10095385926625819\n",
      "Epoch: 7 - Batch: 1172, Training Loss: 0.10103801267212303\n",
      "Epoch: 7 - Batch: 1173, Training Loss: 0.10112995644459875\n",
      "Epoch: 7 - Batch: 1174, Training Loss: 0.10122452511345569\n",
      "Epoch: 7 - Batch: 1175, Training Loss: 0.10131329983769365\n",
      "Epoch: 7 - Batch: 1176, Training Loss: 0.10139659624762994\n",
      "Epoch: 7 - Batch: 1177, Training Loss: 0.1014987785017016\n",
      "Epoch: 7 - Batch: 1178, Training Loss: 0.10159044128967755\n",
      "Epoch: 7 - Batch: 1179, Training Loss: 0.10167073481909276\n",
      "Epoch: 7 - Batch: 1180, Training Loss: 0.10175378915929478\n",
      "Epoch: 7 - Batch: 1181, Training Loss: 0.10184331016482208\n",
      "Epoch: 7 - Batch: 1182, Training Loss: 0.10193151906503374\n",
      "Epoch: 7 - Batch: 1183, Training Loss: 0.10201780382500557\n",
      "Epoch: 7 - Batch: 1184, Training Loss: 0.10210635044780339\n",
      "Epoch: 7 - Batch: 1185, Training Loss: 0.10219390198563659\n",
      "Epoch: 7 - Batch: 1186, Training Loss: 0.10228044806576486\n",
      "Epoch: 7 - Batch: 1187, Training Loss: 0.10237335485976133\n",
      "Epoch: 7 - Batch: 1188, Training Loss: 0.10246113031657773\n",
      "Epoch: 7 - Batch: 1189, Training Loss: 0.10254402096641202\n",
      "Epoch: 7 - Batch: 1190, Training Loss: 0.10262627960190449\n",
      "Epoch: 7 - Batch: 1191, Training Loss: 0.10271120269104814\n",
      "Epoch: 7 - Batch: 1192, Training Loss: 0.10279325318598431\n",
      "Epoch: 7 - Batch: 1193, Training Loss: 0.10287932627780323\n",
      "Epoch: 7 - Batch: 1194, Training Loss: 0.10296144647211776\n",
      "Epoch: 7 - Batch: 1195, Training Loss: 0.10304310396437226\n",
      "Epoch: 7 - Batch: 1196, Training Loss: 0.10313518659194706\n",
      "Epoch: 7 - Batch: 1197, Training Loss: 0.1032160692052857\n",
      "Epoch: 7 - Batch: 1198, Training Loss: 0.10330235642407269\n",
      "Epoch: 7 - Batch: 1199, Training Loss: 0.10338581379991069\n",
      "Epoch: 7 - Batch: 1200, Training Loss: 0.10347047309154894\n",
      "Epoch: 7 - Batch: 1201, Training Loss: 0.10355843826634176\n",
      "Epoch: 7 - Batch: 1202, Training Loss: 0.10363800423976596\n",
      "Epoch: 7 - Batch: 1203, Training Loss: 0.10371732611709567\n",
      "Epoch: 7 - Batch: 1204, Training Loss: 0.10380363384042411\n",
      "Epoch: 7 - Batch: 1205, Training Loss: 0.10389612225899056\n",
      "Epoch: 7 - Batch: 1206, Training Loss: 0.10398581169953394\n",
      "Epoch: 7 - Batch: 1207, Training Loss: 0.10407528804482315\n",
      "Epoch: 7 - Batch: 1208, Training Loss: 0.10416650392769976\n",
      "Epoch: 7 - Batch: 1209, Training Loss: 0.10425268949486723\n",
      "Epoch: 7 - Batch: 1210, Training Loss: 0.10433133935577439\n",
      "Epoch: 7 - Batch: 1211, Training Loss: 0.1044173575505293\n",
      "Epoch: 7 - Batch: 1212, Training Loss: 0.10450742601928228\n",
      "Epoch: 7 - Batch: 1213, Training Loss: 0.10460199403945684\n",
      "Epoch: 7 - Batch: 1214, Training Loss: 0.10468905096005642\n",
      "Epoch: 7 - Batch: 1215, Training Loss: 0.104780169497329\n",
      "Epoch: 7 - Batch: 1216, Training Loss: 0.1048735529987472\n",
      "Epoch: 7 - Batch: 1217, Training Loss: 0.10495400027824477\n",
      "Epoch: 7 - Batch: 1218, Training Loss: 0.10504368996590524\n",
      "Epoch: 7 - Batch: 1219, Training Loss: 0.10513248778941421\n",
      "Epoch: 7 - Batch: 1220, Training Loss: 0.10521454874975962\n",
      "Epoch: 7 - Batch: 1221, Training Loss: 0.10530583129247426\n",
      "Epoch: 7 - Batch: 1222, Training Loss: 0.10538669104130312\n",
      "Epoch: 7 - Batch: 1223, Training Loss: 0.10546999812669818\n",
      "Epoch: 7 - Batch: 1224, Training Loss: 0.10555292633215389\n",
      "Epoch: 7 - Batch: 1225, Training Loss: 0.10564656983165203\n",
      "Epoch: 7 - Batch: 1226, Training Loss: 0.10573343611994193\n",
      "Epoch: 7 - Batch: 1227, Training Loss: 0.10581978333876105\n",
      "Epoch: 7 - Batch: 1228, Training Loss: 0.10590059488122143\n",
      "Epoch: 7 - Batch: 1229, Training Loss: 0.10598609099167694\n",
      "Epoch: 7 - Batch: 1230, Training Loss: 0.10607343889884095\n",
      "Epoch: 7 - Batch: 1231, Training Loss: 0.10616582246562142\n",
      "Epoch: 7 - Batch: 1232, Training Loss: 0.10625018831805803\n",
      "Epoch: 7 - Batch: 1233, Training Loss: 0.10633751921120962\n",
      "Epoch: 7 - Batch: 1234, Training Loss: 0.106425406766521\n",
      "Epoch: 7 - Batch: 1235, Training Loss: 0.1065179164742257\n",
      "Epoch: 7 - Batch: 1236, Training Loss: 0.10659877354183403\n",
      "Epoch: 7 - Batch: 1237, Training Loss: 0.10668650666749102\n",
      "Epoch: 7 - Batch: 1238, Training Loss: 0.10676648562301451\n",
      "Epoch: 7 - Batch: 1239, Training Loss: 0.1068463665904295\n",
      "Epoch: 7 - Batch: 1240, Training Loss: 0.1069269900829539\n",
      "Epoch: 7 - Batch: 1241, Training Loss: 0.10701868468107868\n",
      "Epoch: 7 - Batch: 1242, Training Loss: 0.10710681365126401\n",
      "Epoch: 7 - Batch: 1243, Training Loss: 0.1071935144822989\n",
      "Epoch: 7 - Batch: 1244, Training Loss: 0.10727677433126008\n",
      "Epoch: 7 - Batch: 1245, Training Loss: 0.10735837849564418\n",
      "Epoch: 7 - Batch: 1246, Training Loss: 0.10744034893699546\n",
      "Epoch: 7 - Batch: 1247, Training Loss: 0.10753673156893273\n",
      "Epoch: 7 - Batch: 1248, Training Loss: 0.10762291683955967\n",
      "Epoch: 7 - Batch: 1249, Training Loss: 0.10770698242618472\n",
      "Epoch: 7 - Batch: 1250, Training Loss: 0.1077937614944938\n",
      "Epoch: 7 - Batch: 1251, Training Loss: 0.10787095310503175\n",
      "Epoch: 7 - Batch: 1252, Training Loss: 0.10795649045933144\n",
      "Epoch: 7 - Batch: 1253, Training Loss: 0.10804099590204051\n",
      "Epoch: 7 - Batch: 1254, Training Loss: 0.10813629547532518\n",
      "Epoch: 7 - Batch: 1255, Training Loss: 0.10822959857719455\n",
      "Epoch: 7 - Batch: 1256, Training Loss: 0.10832305980311895\n",
      "Epoch: 7 - Batch: 1257, Training Loss: 0.10841436695869684\n",
      "Epoch: 7 - Batch: 1258, Training Loss: 0.10849679440605897\n",
      "Epoch: 7 - Batch: 1259, Training Loss: 0.10858445357574555\n",
      "Epoch: 7 - Batch: 1260, Training Loss: 0.10867718777080278\n",
      "Epoch: 7 - Batch: 1261, Training Loss: 0.10876523086161756\n",
      "Epoch: 7 - Batch: 1262, Training Loss: 0.10884513966059606\n",
      "Epoch: 7 - Batch: 1263, Training Loss: 0.10893092626955972\n",
      "Epoch: 7 - Batch: 1264, Training Loss: 0.10902138382418831\n",
      "Epoch: 7 - Batch: 1265, Training Loss: 0.10911581386652949\n",
      "Epoch: 7 - Batch: 1266, Training Loss: 0.10919567279109907\n",
      "Epoch: 7 - Batch: 1267, Training Loss: 0.10928264587104419\n",
      "Epoch: 7 - Batch: 1268, Training Loss: 0.10936616653974969\n",
      "Epoch: 7 - Batch: 1269, Training Loss: 0.10945179679772352\n",
      "Epoch: 7 - Batch: 1270, Training Loss: 0.10952984271679155\n",
      "Epoch: 7 - Batch: 1271, Training Loss: 0.10961359995069789\n",
      "Epoch: 7 - Batch: 1272, Training Loss: 0.10969058566944516\n",
      "Epoch: 7 - Batch: 1273, Training Loss: 0.10978279031726654\n",
      "Epoch: 7 - Batch: 1274, Training Loss: 0.109870695936838\n",
      "Epoch: 7 - Batch: 1275, Training Loss: 0.1099622499747557\n",
      "Epoch: 7 - Batch: 1276, Training Loss: 0.11005689694884405\n",
      "Epoch: 7 - Batch: 1277, Training Loss: 0.1101477471416566\n",
      "Epoch: 7 - Batch: 1278, Training Loss: 0.11022741429717782\n",
      "Epoch: 7 - Batch: 1279, Training Loss: 0.11031021010964664\n",
      "Epoch: 7 - Batch: 1280, Training Loss: 0.11039713056600509\n",
      "Epoch: 7 - Batch: 1281, Training Loss: 0.11048765317421054\n",
      "Epoch: 7 - Batch: 1282, Training Loss: 0.11057815934912878\n",
      "Epoch: 7 - Batch: 1283, Training Loss: 0.11065988195056148\n",
      "Epoch: 7 - Batch: 1284, Training Loss: 0.11074105311018317\n",
      "Epoch: 7 - Batch: 1285, Training Loss: 0.11082288844025946\n",
      "Epoch: 7 - Batch: 1286, Training Loss: 0.11090592969561097\n",
      "Epoch: 7 - Batch: 1287, Training Loss: 0.11099295281059113\n",
      "Epoch: 7 - Batch: 1288, Training Loss: 0.11107966956214525\n",
      "Epoch: 7 - Batch: 1289, Training Loss: 0.11116475621809808\n",
      "Epoch: 7 - Batch: 1290, Training Loss: 0.11124940026261122\n",
      "Epoch: 7 - Batch: 1291, Training Loss: 0.11133530801455575\n",
      "Epoch: 7 - Batch: 1292, Training Loss: 0.11142409830099315\n",
      "Epoch: 7 - Batch: 1293, Training Loss: 0.11151111271127342\n",
      "Epoch: 7 - Batch: 1294, Training Loss: 0.11160269723108554\n",
      "Epoch: 7 - Batch: 1295, Training Loss: 0.11169014205757659\n",
      "Epoch: 7 - Batch: 1296, Training Loss: 0.11176948925823121\n",
      "Epoch: 7 - Batch: 1297, Training Loss: 0.11186055404135639\n",
      "Epoch: 7 - Batch: 1298, Training Loss: 0.11194218249117359\n",
      "Epoch: 7 - Batch: 1299, Training Loss: 0.11203436407061358\n",
      "Epoch: 7 - Batch: 1300, Training Loss: 0.11211698815523095\n",
      "Epoch: 7 - Batch: 1301, Training Loss: 0.11220714071793343\n",
      "Epoch: 7 - Batch: 1302, Training Loss: 0.11228787231183368\n",
      "Epoch: 7 - Batch: 1303, Training Loss: 0.1123670012463088\n",
      "Epoch: 7 - Batch: 1304, Training Loss: 0.11245844830723346\n",
      "Epoch: 7 - Batch: 1305, Training Loss: 0.11253767435255138\n",
      "Epoch: 7 - Batch: 1306, Training Loss: 0.11262999597325254\n",
      "Epoch: 7 - Batch: 1307, Training Loss: 0.11272064362271113\n",
      "Epoch: 7 - Batch: 1308, Training Loss: 0.11280742935700401\n",
      "Epoch: 7 - Batch: 1309, Training Loss: 0.11289695140042313\n",
      "Epoch: 7 - Batch: 1310, Training Loss: 0.11297688214957813\n",
      "Epoch: 7 - Batch: 1311, Training Loss: 0.11306751117876315\n",
      "Epoch: 7 - Batch: 1312, Training Loss: 0.11314613085431642\n",
      "Epoch: 7 - Batch: 1313, Training Loss: 0.11323573551516035\n",
      "Epoch: 7 - Batch: 1314, Training Loss: 0.11331792673092972\n",
      "Epoch: 7 - Batch: 1315, Training Loss: 0.1134059766236427\n",
      "Epoch: 7 - Batch: 1316, Training Loss: 0.11349603560309901\n",
      "Epoch: 7 - Batch: 1317, Training Loss: 0.11358166344550316\n",
      "Epoch: 7 - Batch: 1318, Training Loss: 0.11367582173934623\n",
      "Epoch: 7 - Batch: 1319, Training Loss: 0.11376312933553313\n",
      "Epoch: 7 - Batch: 1320, Training Loss: 0.113846292629665\n",
      "Epoch: 7 - Batch: 1321, Training Loss: 0.11393957497369789\n",
      "Epoch: 7 - Batch: 1322, Training Loss: 0.11402011084467617\n",
      "Epoch: 7 - Batch: 1323, Training Loss: 0.11410756508015084\n",
      "Epoch: 7 - Batch: 1324, Training Loss: 0.1141871294287801\n",
      "Epoch: 7 - Batch: 1325, Training Loss: 0.11428488434522503\n",
      "Epoch: 7 - Batch: 1326, Training Loss: 0.11437006937849581\n",
      "Epoch: 7 - Batch: 1327, Training Loss: 0.11445416877074029\n",
      "Epoch: 7 - Batch: 1328, Training Loss: 0.114534637944813\n",
      "Epoch: 7 - Batch: 1329, Training Loss: 0.11462826425731676\n",
      "Epoch: 7 - Batch: 1330, Training Loss: 0.11471066756133812\n",
      "Epoch: 7 - Batch: 1331, Training Loss: 0.11479035279965322\n",
      "Epoch: 7 - Batch: 1332, Training Loss: 0.11488369563795243\n",
      "Epoch: 7 - Batch: 1333, Training Loss: 0.11496918589791058\n",
      "Epoch: 7 - Batch: 1334, Training Loss: 0.11505038089197667\n",
      "Epoch: 7 - Batch: 1335, Training Loss: 0.1151358136045399\n",
      "Epoch: 7 - Batch: 1336, Training Loss: 0.11522425100775698\n",
      "Epoch: 7 - Batch: 1337, Training Loss: 0.11531531481476961\n",
      "Epoch: 7 - Batch: 1338, Training Loss: 0.11540272359923145\n",
      "Epoch: 7 - Batch: 1339, Training Loss: 0.11549007298909807\n",
      "Epoch: 7 - Batch: 1340, Training Loss: 0.1155746389075397\n",
      "Epoch: 7 - Batch: 1341, Training Loss: 0.11566295630451459\n",
      "Epoch: 7 - Batch: 1342, Training Loss: 0.11575093046201403\n",
      "Epoch: 7 - Batch: 1343, Training Loss: 0.11583190845019782\n",
      "Epoch: 7 - Batch: 1344, Training Loss: 0.11591839529635696\n",
      "Epoch: 7 - Batch: 1345, Training Loss: 0.11599493555563994\n",
      "Epoch: 7 - Batch: 1346, Training Loss: 0.11607961010675921\n",
      "Epoch: 7 - Batch: 1347, Training Loss: 0.11616145168272021\n",
      "Epoch: 7 - Batch: 1348, Training Loss: 0.11625231336647796\n",
      "Epoch: 7 - Batch: 1349, Training Loss: 0.11634373018026944\n",
      "Epoch: 7 - Batch: 1350, Training Loss: 0.11642958404215216\n",
      "Epoch: 7 - Batch: 1351, Training Loss: 0.1165227822366342\n",
      "Epoch: 7 - Batch: 1352, Training Loss: 0.11660796329131964\n",
      "Epoch: 7 - Batch: 1353, Training Loss: 0.11669570698741064\n",
      "Epoch: 7 - Batch: 1354, Training Loss: 0.11679176965830337\n",
      "Epoch: 7 - Batch: 1355, Training Loss: 0.11688114932520473\n",
      "Epoch: 7 - Batch: 1356, Training Loss: 0.11696216617255266\n",
      "Epoch: 7 - Batch: 1357, Training Loss: 0.11704231770230368\n",
      "Epoch: 7 - Batch: 1358, Training Loss: 0.1171176820610392\n",
      "Epoch: 7 - Batch: 1359, Training Loss: 0.11720473272957611\n",
      "Epoch: 7 - Batch: 1360, Training Loss: 0.11727933822876185\n",
      "Epoch: 7 - Batch: 1361, Training Loss: 0.11737288601362883\n",
      "Epoch: 7 - Batch: 1362, Training Loss: 0.11745624307845758\n",
      "Epoch: 7 - Batch: 1363, Training Loss: 0.11754169088987569\n",
      "Epoch: 7 - Batch: 1364, Training Loss: 0.1176282410659699\n",
      "Epoch: 7 - Batch: 1365, Training Loss: 0.11770009497801463\n",
      "Epoch: 7 - Batch: 1366, Training Loss: 0.11779007756739707\n",
      "Epoch: 7 - Batch: 1367, Training Loss: 0.11788056362080535\n",
      "Epoch: 7 - Batch: 1368, Training Loss: 0.11796853607319678\n",
      "Epoch: 7 - Batch: 1369, Training Loss: 0.11805189762937884\n",
      "Epoch: 7 - Batch: 1370, Training Loss: 0.118134071472816\n",
      "Epoch: 7 - Batch: 1371, Training Loss: 0.11821693300607786\n",
      "Epoch: 7 - Batch: 1372, Training Loss: 0.11830057083300097\n",
      "Epoch: 7 - Batch: 1373, Training Loss: 0.1183927875895603\n",
      "Epoch: 7 - Batch: 1374, Training Loss: 0.11847212716047444\n",
      "Epoch: 7 - Batch: 1375, Training Loss: 0.11855490165291536\n",
      "Epoch: 7 - Batch: 1376, Training Loss: 0.11864021809688255\n",
      "Epoch: 7 - Batch: 1377, Training Loss: 0.11872686442006286\n",
      "Epoch: 7 - Batch: 1378, Training Loss: 0.11881297567911804\n",
      "Epoch: 7 - Batch: 1379, Training Loss: 0.11890026072586947\n",
      "Epoch: 7 - Batch: 1380, Training Loss: 0.11898731948131352\n",
      "Epoch: 7 - Batch: 1381, Training Loss: 0.11907094124562508\n",
      "Epoch: 7 - Batch: 1382, Training Loss: 0.11915569902578396\n",
      "Epoch: 7 - Batch: 1383, Training Loss: 0.11923894369557722\n",
      "Epoch: 7 - Batch: 1384, Training Loss: 0.11932751279367539\n",
      "Epoch: 7 - Batch: 1385, Training Loss: 0.11941978053667059\n",
      "Epoch: 7 - Batch: 1386, Training Loss: 0.11949799861604499\n",
      "Epoch: 7 - Batch: 1387, Training Loss: 0.11958383430864285\n",
      "Epoch: 7 - Batch: 1388, Training Loss: 0.11967574541841573\n",
      "Epoch: 7 - Batch: 1389, Training Loss: 0.11976202041108415\n",
      "Epoch: 7 - Batch: 1390, Training Loss: 0.1198469455883673\n",
      "Epoch: 7 - Batch: 1391, Training Loss: 0.11993155187338739\n",
      "Epoch: 7 - Batch: 1392, Training Loss: 0.1200134905849029\n",
      "Epoch: 7 - Batch: 1393, Training Loss: 0.12009562044767401\n",
      "Epoch: 7 - Batch: 1394, Training Loss: 0.12018614479805502\n",
      "Epoch: 7 - Batch: 1395, Training Loss: 0.12027637229901839\n",
      "Epoch: 7 - Batch: 1396, Training Loss: 0.12036886734650107\n",
      "Epoch: 7 - Batch: 1397, Training Loss: 0.12045204478190906\n",
      "Epoch: 7 - Batch: 1398, Training Loss: 0.12053644481681867\n",
      "Epoch: 7 - Batch: 1399, Training Loss: 0.12062049227122644\n",
      "Epoch: 7 - Batch: 1400, Training Loss: 0.12070073568529355\n",
      "Epoch: 7 - Batch: 1401, Training Loss: 0.12078310992834382\n",
      "Epoch: 7 - Batch: 1402, Training Loss: 0.12086701015656069\n",
      "Epoch: 7 - Batch: 1403, Training Loss: 0.12094686453417561\n",
      "Epoch: 7 - Batch: 1404, Training Loss: 0.12102217935211029\n",
      "Epoch: 7 - Batch: 1405, Training Loss: 0.1211112027193974\n",
      "Epoch: 7 - Batch: 1406, Training Loss: 0.12119918125704747\n",
      "Epoch: 7 - Batch: 1407, Training Loss: 0.12128124998304776\n",
      "Epoch: 7 - Batch: 1408, Training Loss: 0.12136873302619848\n",
      "Epoch: 7 - Batch: 1409, Training Loss: 0.12145759050995358\n",
      "Epoch: 7 - Batch: 1410, Training Loss: 0.12154093463820209\n",
      "Epoch: 7 - Batch: 1411, Training Loss: 0.12163117799790542\n",
      "Epoch: 7 - Batch: 1412, Training Loss: 0.12172600599762615\n",
      "Epoch: 7 - Batch: 1413, Training Loss: 0.12181407068647555\n",
      "Epoch: 7 - Batch: 1414, Training Loss: 0.1218988345148255\n",
      "Epoch: 7 - Batch: 1415, Training Loss: 0.12198473498299348\n",
      "Epoch: 7 - Batch: 1416, Training Loss: 0.12206587327604072\n",
      "Epoch: 7 - Batch: 1417, Training Loss: 0.12215605545572776\n",
      "Epoch: 7 - Batch: 1418, Training Loss: 0.12224197291937436\n",
      "Epoch: 7 - Batch: 1419, Training Loss: 0.12233675392977832\n",
      "Epoch: 7 - Batch: 1420, Training Loss: 0.1224245687029255\n",
      "Epoch: 7 - Batch: 1421, Training Loss: 0.12250845093483949\n",
      "Epoch: 7 - Batch: 1422, Training Loss: 0.12259569492308457\n",
      "Epoch: 7 - Batch: 1423, Training Loss: 0.12268698493491358\n",
      "Epoch: 7 - Batch: 1424, Training Loss: 0.12276895254677406\n",
      "Epoch: 7 - Batch: 1425, Training Loss: 0.12286196426051371\n",
      "Epoch: 7 - Batch: 1426, Training Loss: 0.12294768327059437\n",
      "Epoch: 7 - Batch: 1427, Training Loss: 0.1230285931718389\n",
      "Epoch: 7 - Batch: 1428, Training Loss: 0.1231116012949651\n",
      "Epoch: 7 - Batch: 1429, Training Loss: 0.1231964525780571\n",
      "Epoch: 7 - Batch: 1430, Training Loss: 0.12328356091754748\n",
      "Epoch: 7 - Batch: 1431, Training Loss: 0.12337214911161963\n",
      "Epoch: 7 - Batch: 1432, Training Loss: 0.12345898236035312\n",
      "Epoch: 7 - Batch: 1433, Training Loss: 0.12354876484048505\n",
      "Epoch: 7 - Batch: 1434, Training Loss: 0.12363224328923383\n",
      "Epoch: 7 - Batch: 1435, Training Loss: 0.12372743875826177\n",
      "Epoch: 7 - Batch: 1436, Training Loss: 0.12381584716228704\n",
      "Epoch: 7 - Batch: 1437, Training Loss: 0.12390233880869587\n",
      "Epoch: 7 - Batch: 1438, Training Loss: 0.12399547821401957\n",
      "Epoch: 7 - Batch: 1439, Training Loss: 0.12408516961296596\n",
      "Epoch: 7 - Batch: 1440, Training Loss: 0.12417066207844424\n",
      "Epoch: 7 - Batch: 1441, Training Loss: 0.12425034861412412\n",
      "Epoch: 7 - Batch: 1442, Training Loss: 0.12433666574643619\n",
      "Epoch: 7 - Batch: 1443, Training Loss: 0.12443147145001647\n",
      "Epoch: 7 - Batch: 1444, Training Loss: 0.12451697142785462\n",
      "Epoch: 7 - Batch: 1445, Training Loss: 0.12460783091227016\n",
      "Epoch: 7 - Batch: 1446, Training Loss: 0.12468378216176484\n",
      "Epoch: 7 - Batch: 1447, Training Loss: 0.12476869252437778\n",
      "Epoch: 7 - Batch: 1448, Training Loss: 0.12486046686101315\n",
      "Epoch: 7 - Batch: 1449, Training Loss: 0.12494804728683548\n",
      "Epoch: 7 - Batch: 1450, Training Loss: 0.12503259518426252\n",
      "Epoch: 7 - Batch: 1451, Training Loss: 0.12512427821730698\n",
      "Epoch: 7 - Batch: 1452, Training Loss: 0.12520645356815846\n",
      "Epoch: 7 - Batch: 1453, Training Loss: 0.12529778803635394\n",
      "Epoch: 7 - Batch: 1454, Training Loss: 0.12538028869215725\n",
      "Epoch: 7 - Batch: 1455, Training Loss: 0.1254673385946312\n",
      "Epoch: 7 - Batch: 1456, Training Loss: 0.1255531925491828\n",
      "Epoch: 7 - Batch: 1457, Training Loss: 0.1256323310779794\n",
      "Epoch: 7 - Batch: 1458, Training Loss: 0.12571389647586825\n",
      "Epoch: 7 - Batch: 1459, Training Loss: 0.1258019451515295\n",
      "Epoch: 7 - Batch: 1460, Training Loss: 0.1259071497589498\n",
      "Epoch: 7 - Batch: 1461, Training Loss: 0.12599426030396033\n",
      "Epoch: 7 - Batch: 1462, Training Loss: 0.12607548438988714\n",
      "Epoch: 7 - Batch: 1463, Training Loss: 0.12616820289300845\n",
      "Epoch: 7 - Batch: 1464, Training Loss: 0.12626260701894365\n",
      "Epoch: 7 - Batch: 1465, Training Loss: 0.12634414999713353\n",
      "Epoch: 7 - Batch: 1466, Training Loss: 0.1264317261540079\n",
      "Epoch: 7 - Batch: 1467, Training Loss: 0.12651548183725445\n",
      "Epoch: 7 - Batch: 1468, Training Loss: 0.12661064100478023\n",
      "Epoch: 7 - Batch: 1469, Training Loss: 0.1266977815436289\n",
      "Epoch: 7 - Batch: 1470, Training Loss: 0.126778451432389\n",
      "Epoch: 7 - Batch: 1471, Training Loss: 0.12686507868430705\n",
      "Epoch: 7 - Batch: 1472, Training Loss: 0.1269519738729419\n",
      "Epoch: 7 - Batch: 1473, Training Loss: 0.12703505422839675\n",
      "Epoch: 7 - Batch: 1474, Training Loss: 0.12711785597785394\n",
      "Epoch: 7 - Batch: 1475, Training Loss: 0.1272035118124477\n",
      "Epoch: 7 - Batch: 1476, Training Loss: 0.12729339152080305\n",
      "Epoch: 7 - Batch: 1477, Training Loss: 0.12738677610335855\n",
      "Epoch: 7 - Batch: 1478, Training Loss: 0.1274684799918488\n",
      "Epoch: 7 - Batch: 1479, Training Loss: 0.1275485440545019\n",
      "Epoch: 7 - Batch: 1480, Training Loss: 0.12763071661910805\n",
      "Epoch: 7 - Batch: 1481, Training Loss: 0.127719657739646\n",
      "Epoch: 7 - Batch: 1482, Training Loss: 0.12780909470682517\n",
      "Epoch: 7 - Batch: 1483, Training Loss: 0.1278920701610706\n",
      "Epoch: 7 - Batch: 1484, Training Loss: 0.12798895182128173\n",
      "Epoch: 7 - Batch: 1485, Training Loss: 0.12806900334274196\n",
      "Epoch: 7 - Batch: 1486, Training Loss: 0.12814696448828847\n",
      "Epoch: 7 - Batch: 1487, Training Loss: 0.12824071073907722\n",
      "Epoch: 7 - Batch: 1488, Training Loss: 0.12833413365567303\n",
      "Epoch: 7 - Batch: 1489, Training Loss: 0.12842662882671427\n",
      "Epoch: 7 - Batch: 1490, Training Loss: 0.12850522264764083\n",
      "Epoch: 7 - Batch: 1491, Training Loss: 0.12859539012386037\n",
      "Epoch: 7 - Batch: 1492, Training Loss: 0.12868396510334554\n",
      "Epoch: 7 - Batch: 1493, Training Loss: 0.12877155684713107\n",
      "Epoch: 7 - Batch: 1494, Training Loss: 0.12885475559638904\n",
      "Epoch: 7 - Batch: 1495, Training Loss: 0.1289454735135953\n",
      "Epoch: 7 - Batch: 1496, Training Loss: 0.1290263504617744\n",
      "Epoch: 7 - Batch: 1497, Training Loss: 0.1291165167209422\n",
      "Epoch: 7 - Batch: 1498, Training Loss: 0.12919618456839133\n",
      "Epoch: 7 - Batch: 1499, Training Loss: 0.1292869746005456\n",
      "Epoch: 7 - Batch: 1500, Training Loss: 0.12937453786408526\n",
      "Epoch: 7 - Batch: 1501, Training Loss: 0.12945927782710118\n",
      "Epoch: 7 - Batch: 1502, Training Loss: 0.12954039017903074\n",
      "Epoch: 7 - Batch: 1503, Training Loss: 0.12962978995508617\n",
      "Epoch: 7 - Batch: 1504, Training Loss: 0.12971174715713876\n",
      "Epoch: 7 - Batch: 1505, Training Loss: 0.12980394167933693\n",
      "Epoch: 7 - Batch: 1506, Training Loss: 0.12988255817325753\n",
      "Epoch: 7 - Batch: 1507, Training Loss: 0.1299623424066833\n",
      "Epoch: 7 - Batch: 1508, Training Loss: 0.13004750676243065\n",
      "Epoch: 7 - Batch: 1509, Training Loss: 0.13013478343809026\n",
      "Epoch: 7 - Batch: 1510, Training Loss: 0.13022804092832072\n",
      "Epoch: 7 - Batch: 1511, Training Loss: 0.13031548710777788\n",
      "Epoch: 7 - Batch: 1512, Training Loss: 0.13040267340653572\n",
      "Epoch: 7 - Batch: 1513, Training Loss: 0.130486090267525\n",
      "Epoch: 7 - Batch: 1514, Training Loss: 0.1305778396762228\n",
      "Epoch: 7 - Batch: 1515, Training Loss: 0.13065870500989815\n",
      "Epoch: 7 - Batch: 1516, Training Loss: 0.13075785198021883\n",
      "Epoch: 7 - Batch: 1517, Training Loss: 0.13084075687017607\n",
      "Epoch: 7 - Batch: 1518, Training Loss: 0.1309312770689898\n",
      "Epoch: 7 - Batch: 1519, Training Loss: 0.131016756499041\n",
      "Epoch: 7 - Batch: 1520, Training Loss: 0.13110362994137095\n",
      "Epoch: 7 - Batch: 1521, Training Loss: 0.13118745214236316\n",
      "Epoch: 7 - Batch: 1522, Training Loss: 0.13127596624455048\n",
      "Epoch: 7 - Batch: 1523, Training Loss: 0.13136255372297112\n",
      "Epoch: 7 - Batch: 1524, Training Loss: 0.13144803448745465\n",
      "Epoch: 7 - Batch: 1525, Training Loss: 0.13154014195029812\n",
      "Epoch: 7 - Batch: 1526, Training Loss: 0.13162568874447106\n",
      "Epoch: 7 - Batch: 1527, Training Loss: 0.13171031589235238\n",
      "Epoch: 7 - Batch: 1528, Training Loss: 0.13180145223441211\n",
      "Epoch: 7 - Batch: 1529, Training Loss: 0.13188205414125773\n",
      "Epoch: 7 - Batch: 1530, Training Loss: 0.13196423654730244\n",
      "Epoch: 7 - Batch: 1531, Training Loss: 0.1320485867634045\n",
      "Epoch: 7 - Batch: 1532, Training Loss: 0.13212746678300163\n",
      "Epoch: 7 - Batch: 1533, Training Loss: 0.1322102049677625\n",
      "Epoch: 7 - Batch: 1534, Training Loss: 0.13229547608427542\n",
      "Epoch: 7 - Batch: 1535, Training Loss: 0.1323749240407501\n",
      "Epoch: 7 - Batch: 1536, Training Loss: 0.13246260847198232\n",
      "Epoch: 7 - Batch: 1537, Training Loss: 0.13254530117484073\n",
      "Epoch: 7 - Batch: 1538, Training Loss: 0.13262584407012262\n",
      "Epoch: 7 - Batch: 1539, Training Loss: 0.13271812883330814\n",
      "Epoch: 7 - Batch: 1540, Training Loss: 0.13281040879006606\n",
      "Epoch: 7 - Batch: 1541, Training Loss: 0.13289452798256826\n",
      "Epoch: 7 - Batch: 1542, Training Loss: 0.1329798974949329\n",
      "Epoch: 7 - Batch: 1543, Training Loss: 0.1330650941365295\n",
      "Epoch: 7 - Batch: 1544, Training Loss: 0.13314352048001873\n",
      "Epoch: 7 - Batch: 1545, Training Loss: 0.13322799033786528\n",
      "Epoch: 7 - Batch: 1546, Training Loss: 0.13330939692594915\n",
      "Epoch: 7 - Batch: 1547, Training Loss: 0.13339004935588608\n",
      "Epoch: 7 - Batch: 1548, Training Loss: 0.13349233568950278\n",
      "Epoch: 7 - Batch: 1549, Training Loss: 0.13356935606642348\n",
      "Epoch: 7 - Batch: 1550, Training Loss: 0.13364723082600938\n",
      "Epoch: 7 - Batch: 1551, Training Loss: 0.13373495947884684\n",
      "Epoch: 7 - Batch: 1552, Training Loss: 0.13382723538906222\n",
      "Epoch: 7 - Batch: 1553, Training Loss: 0.1339083867237145\n",
      "Epoch: 7 - Batch: 1554, Training Loss: 0.13399255425849957\n",
      "Epoch: 7 - Batch: 1555, Training Loss: 0.13407574024415925\n",
      "Epoch: 7 - Batch: 1556, Training Loss: 0.13416082118552913\n",
      "Epoch: 7 - Batch: 1557, Training Loss: 0.13425940141131235\n",
      "Epoch: 7 - Batch: 1558, Training Loss: 0.13433889110586536\n",
      "Epoch: 7 - Batch: 1559, Training Loss: 0.13442760941326914\n",
      "Epoch: 7 - Batch: 1560, Training Loss: 0.1345230996003297\n",
      "Epoch: 7 - Batch: 1561, Training Loss: 0.13461611838161847\n",
      "Epoch: 7 - Batch: 1562, Training Loss: 0.13470478609179581\n",
      "Epoch: 7 - Batch: 1563, Training Loss: 0.13479044670192755\n",
      "Epoch: 7 - Batch: 1564, Training Loss: 0.1348784640184287\n",
      "Epoch: 7 - Batch: 1565, Training Loss: 0.13496373127991484\n",
      "Epoch: 7 - Batch: 1566, Training Loss: 0.1350420781752561\n",
      "Epoch: 7 - Batch: 1567, Training Loss: 0.13512566100626838\n",
      "Epoch: 7 - Batch: 1568, Training Loss: 0.13521395795430316\n",
      "Epoch: 7 - Batch: 1569, Training Loss: 0.13530537295178394\n",
      "Epoch: 7 - Batch: 1570, Training Loss: 0.13539371878575923\n",
      "Epoch: 7 - Batch: 1571, Training Loss: 0.13548896715638056\n",
      "Epoch: 7 - Batch: 1572, Training Loss: 0.13557250922542702\n",
      "Epoch: 7 - Batch: 1573, Training Loss: 0.13566187423210635\n",
      "Epoch: 7 - Batch: 1574, Training Loss: 0.1357438146857974\n",
      "Epoch: 7 - Batch: 1575, Training Loss: 0.13583529433528976\n",
      "Epoch: 7 - Batch: 1576, Training Loss: 0.1359299401355719\n",
      "Epoch: 7 - Batch: 1577, Training Loss: 0.1360147391286853\n",
      "Epoch: 7 - Batch: 1578, Training Loss: 0.13610433586324824\n",
      "Epoch: 7 - Batch: 1579, Training Loss: 0.13619383855739833\n",
      "Epoch: 7 - Batch: 1580, Training Loss: 0.13628088756407275\n",
      "Epoch: 7 - Batch: 1581, Training Loss: 0.136370062655082\n",
      "Epoch: 7 - Batch: 1582, Training Loss: 0.13646050407914181\n",
      "Epoch: 7 - Batch: 1583, Training Loss: 0.1365497470470408\n",
      "Epoch: 7 - Batch: 1584, Training Loss: 0.1366348570617376\n",
      "Epoch: 7 - Batch: 1585, Training Loss: 0.1367155936041578\n",
      "Epoch: 7 - Batch: 1586, Training Loss: 0.13679291458049816\n",
      "Epoch: 7 - Batch: 1587, Training Loss: 0.1368753499047179\n",
      "Epoch: 7 - Batch: 1588, Training Loss: 0.1369581054062096\n",
      "Epoch: 7 - Batch: 1589, Training Loss: 0.13704323640732624\n",
      "Epoch: 7 - Batch: 1590, Training Loss: 0.13712452286560936\n",
      "Epoch: 7 - Batch: 1591, Training Loss: 0.1372056422665542\n",
      "Epoch: 7 - Batch: 1592, Training Loss: 0.13729078262105313\n",
      "Epoch: 7 - Batch: 1593, Training Loss: 0.1373707246140362\n",
      "Epoch: 7 - Batch: 1594, Training Loss: 0.13745628056389775\n",
      "Epoch: 7 - Batch: 1595, Training Loss: 0.1375440480141201\n",
      "Epoch: 7 - Batch: 1596, Training Loss: 0.13761554677564508\n",
      "Epoch: 7 - Batch: 1597, Training Loss: 0.13771065332576213\n",
      "Epoch: 7 - Batch: 1598, Training Loss: 0.13779518179360709\n",
      "Epoch: 7 - Batch: 1599, Training Loss: 0.13787462310263174\n",
      "Epoch: 7 - Batch: 1600, Training Loss: 0.13795715414766055\n",
      "Epoch: 7 - Batch: 1601, Training Loss: 0.13804636410689275\n",
      "Epoch: 7 - Batch: 1602, Training Loss: 0.1381427915102708\n",
      "Epoch: 7 - Batch: 1603, Training Loss: 0.1382285066220792\n",
      "Epoch: 7 - Batch: 1604, Training Loss: 0.13830670367796621\n",
      "Epoch: 7 - Batch: 1605, Training Loss: 0.13839546512583792\n",
      "Epoch: 7 - Batch: 1606, Training Loss: 0.13847343357369477\n",
      "Epoch: 7 - Batch: 1607, Training Loss: 0.13856265239207505\n",
      "Epoch: 7 - Batch: 1608, Training Loss: 0.13865299005784207\n",
      "Epoch: 7 - Batch: 1609, Training Loss: 0.13874737324678088\n",
      "Epoch: 7 - Batch: 1610, Training Loss: 0.1388321396266149\n",
      "Epoch: 7 - Batch: 1611, Training Loss: 0.13893127052196816\n",
      "Epoch: 7 - Batch: 1612, Training Loss: 0.1390177555023932\n",
      "Epoch: 7 - Batch: 1613, Training Loss: 0.13909712806344032\n",
      "Epoch: 7 - Batch: 1614, Training Loss: 0.13917717610054942\n",
      "Epoch: 7 - Batch: 1615, Training Loss: 0.13926887598080223\n",
      "Epoch: 7 - Batch: 1616, Training Loss: 0.1393552417457598\n",
      "Epoch: 7 - Batch: 1617, Training Loss: 0.13945295480056782\n",
      "Epoch: 7 - Batch: 1618, Training Loss: 0.13953875633540438\n",
      "Epoch: 7 - Batch: 1619, Training Loss: 0.13961763559511645\n",
      "Epoch: 7 - Batch: 1620, Training Loss: 0.13969881180210494\n",
      "Epoch: 7 - Batch: 1621, Training Loss: 0.13978420108383766\n",
      "Epoch: 7 - Batch: 1622, Training Loss: 0.1398706973822259\n",
      "Epoch: 7 - Batch: 1623, Training Loss: 0.1399630065791148\n",
      "Epoch: 7 - Batch: 1624, Training Loss: 0.1400564199446347\n",
      "Epoch: 7 - Batch: 1625, Training Loss: 0.14013960745624246\n",
      "Epoch: 7 - Batch: 1626, Training Loss: 0.14022198906338235\n",
      "Epoch: 7 - Batch: 1627, Training Loss: 0.14031231489841817\n",
      "Epoch: 7 - Batch: 1628, Training Loss: 0.14039821999385385\n",
      "Epoch: 7 - Batch: 1629, Training Loss: 0.14048680299228894\n",
      "Epoch: 7 - Batch: 1630, Training Loss: 0.1405730155137542\n",
      "Epoch: 7 - Batch: 1631, Training Loss: 0.14065581674426547\n",
      "Epoch: 7 - Batch: 1632, Training Loss: 0.1407432640604613\n",
      "Epoch: 7 - Batch: 1633, Training Loss: 0.1408288107990329\n",
      "Epoch: 7 - Batch: 1634, Training Loss: 0.14090876600042504\n",
      "Epoch: 7 - Batch: 1635, Training Loss: 0.14099510366199028\n",
      "Epoch: 7 - Batch: 1636, Training Loss: 0.1410898953663868\n",
      "Epoch: 7 - Batch: 1637, Training Loss: 0.14118128445363953\n",
      "Epoch: 7 - Batch: 1638, Training Loss: 0.14126425983992777\n",
      "Epoch: 7 - Batch: 1639, Training Loss: 0.141346885129241\n",
      "Epoch: 7 - Batch: 1640, Training Loss: 0.14144041012991126\n",
      "Epoch: 7 - Batch: 1641, Training Loss: 0.14152567654502135\n",
      "Epoch: 7 - Batch: 1642, Training Loss: 0.14160177542845012\n",
      "Epoch: 7 - Batch: 1643, Training Loss: 0.14168216458600552\n",
      "Epoch: 7 - Batch: 1644, Training Loss: 0.14176085557337623\n",
      "Epoch: 7 - Batch: 1645, Training Loss: 0.14185099220938152\n",
      "Epoch: 7 - Batch: 1646, Training Loss: 0.1419360483957958\n",
      "Epoch: 7 - Batch: 1647, Training Loss: 0.14201681455520057\n",
      "Epoch: 7 - Batch: 1648, Training Loss: 0.1421107271713997\n",
      "Epoch: 7 - Batch: 1649, Training Loss: 0.14220270085122258\n",
      "Epoch: 7 - Batch: 1650, Training Loss: 0.14228444133157753\n",
      "Epoch: 7 - Batch: 1651, Training Loss: 0.1423813495877667\n",
      "Epoch: 7 - Batch: 1652, Training Loss: 0.14246161799772858\n",
      "Epoch: 7 - Batch: 1653, Training Loss: 0.1425562985179634\n",
      "Epoch: 7 - Batch: 1654, Training Loss: 0.142640337885464\n",
      "Epoch: 7 - Batch: 1655, Training Loss: 0.14272797673594695\n",
      "Epoch: 7 - Batch: 1656, Training Loss: 0.14282419710461772\n",
      "Epoch: 7 - Batch: 1657, Training Loss: 0.1429109460987756\n",
      "Epoch: 7 - Batch: 1658, Training Loss: 0.1429945908758672\n",
      "Epoch: 7 - Batch: 1659, Training Loss: 0.14308517734530948\n",
      "Epoch: 7 - Batch: 1660, Training Loss: 0.1431748868579394\n",
      "Epoch: 7 - Batch: 1661, Training Loss: 0.14325714199054695\n",
      "Epoch: 7 - Batch: 1662, Training Loss: 0.143348047976223\n",
      "Epoch: 7 - Batch: 1663, Training Loss: 0.1434449408740843\n",
      "Epoch: 7 - Batch: 1664, Training Loss: 0.14353049408094604\n",
      "Epoch: 7 - Batch: 1665, Training Loss: 0.14361993216015212\n",
      "Epoch: 7 - Batch: 1666, Training Loss: 0.1436980880401522\n",
      "Epoch: 7 - Batch: 1667, Training Loss: 0.1437770712682066\n",
      "Epoch: 7 - Batch: 1668, Training Loss: 0.1438623427553952\n",
      "Epoch: 7 - Batch: 1669, Training Loss: 0.14394665655879238\n",
      "Epoch: 7 - Batch: 1670, Training Loss: 0.14403353494346438\n",
      "Epoch: 7 - Batch: 1671, Training Loss: 0.14411648074983563\n",
      "Epoch: 7 - Batch: 1672, Training Loss: 0.14420742345711288\n",
      "Epoch: 7 - Batch: 1673, Training Loss: 0.14429219678711536\n",
      "Epoch: 7 - Batch: 1674, Training Loss: 0.1443741059496035\n",
      "Epoch: 7 - Batch: 1675, Training Loss: 0.14445728270543945\n",
      "Epoch: 7 - Batch: 1676, Training Loss: 0.1445366762776181\n",
      "Epoch: 7 - Batch: 1677, Training Loss: 0.14462136705197506\n",
      "Epoch: 7 - Batch: 1678, Training Loss: 0.1447004281898735\n",
      "Epoch: 7 - Batch: 1679, Training Loss: 0.14478867929768602\n",
      "Epoch: 7 - Batch: 1680, Training Loss: 0.1448772029694831\n",
      "Epoch: 7 - Batch: 1681, Training Loss: 0.14496270001280565\n",
      "Epoch: 7 - Batch: 1682, Training Loss: 0.14504421005646387\n",
      "Epoch: 7 - Batch: 1683, Training Loss: 0.14513023717214615\n",
      "Epoch: 7 - Batch: 1684, Training Loss: 0.14522506866857385\n",
      "Epoch: 7 - Batch: 1685, Training Loss: 0.14531196474683028\n",
      "Epoch: 7 - Batch: 1686, Training Loss: 0.14539606274286312\n",
      "Epoch: 7 - Batch: 1687, Training Loss: 0.14548057031077927\n",
      "Epoch: 7 - Batch: 1688, Training Loss: 0.14556246051320784\n",
      "Epoch: 7 - Batch: 1689, Training Loss: 0.14565748991954386\n",
      "Epoch: 7 - Batch: 1690, Training Loss: 0.14574711792677592\n",
      "Epoch: 7 - Batch: 1691, Training Loss: 0.14583532875450098\n",
      "Epoch: 7 - Batch: 1692, Training Loss: 0.1459189520880061\n",
      "Epoch: 7 - Batch: 1693, Training Loss: 0.14600466297411208\n",
      "Epoch: 7 - Batch: 1694, Training Loss: 0.1460907771429711\n",
      "Epoch: 7 - Batch: 1695, Training Loss: 0.14617293062395914\n",
      "Epoch: 7 - Batch: 1696, Training Loss: 0.14626009455861935\n",
      "Epoch: 7 - Batch: 1697, Training Loss: 0.14634275215849354\n",
      "Epoch: 7 - Batch: 1698, Training Loss: 0.1464277347719689\n",
      "Epoch: 7 - Batch: 1699, Training Loss: 0.1465108152695161\n",
      "Epoch: 7 - Batch: 1700, Training Loss: 0.14659873822435218\n",
      "Epoch: 7 - Batch: 1701, Training Loss: 0.14667954777134196\n",
      "Epoch: 7 - Batch: 1702, Training Loss: 0.14676571393363907\n",
      "Epoch: 7 - Batch: 1703, Training Loss: 0.14685304684697298\n",
      "Epoch: 7 - Batch: 1704, Training Loss: 0.14694634550691243\n",
      "Epoch: 7 - Batch: 1705, Training Loss: 0.1470255056708112\n",
      "Epoch: 7 - Batch: 1706, Training Loss: 0.14711532861636845\n",
      "Epoch: 7 - Batch: 1707, Training Loss: 0.14719909617976962\n",
      "Epoch: 7 - Batch: 1708, Training Loss: 0.14729281145442974\n",
      "Epoch: 7 - Batch: 1709, Training Loss: 0.14738942856342835\n",
      "Epoch: 7 - Batch: 1710, Training Loss: 0.14747113554088234\n",
      "Epoch: 7 - Batch: 1711, Training Loss: 0.1475601521371609\n",
      "Epoch: 7 - Batch: 1712, Training Loss: 0.14765652482881278\n",
      "Epoch: 7 - Batch: 1713, Training Loss: 0.14774155506793143\n",
      "Epoch: 7 - Batch: 1714, Training Loss: 0.1478188110462963\n",
      "Epoch: 7 - Batch: 1715, Training Loss: 0.14790540376359945\n",
      "Epoch: 7 - Batch: 1716, Training Loss: 0.14799070921703358\n",
      "Epoch: 7 - Batch: 1717, Training Loss: 0.14808501696729937\n",
      "Epoch: 7 - Batch: 1718, Training Loss: 0.1481676756297771\n",
      "Epoch: 7 - Batch: 1719, Training Loss: 0.14824887660407704\n",
      "Epoch: 7 - Batch: 1720, Training Loss: 0.14833776284608477\n",
      "Epoch: 7 - Batch: 1721, Training Loss: 0.14843321337085064\n",
      "Epoch: 7 - Batch: 1722, Training Loss: 0.14851459669285944\n",
      "Epoch: 7 - Batch: 1723, Training Loss: 0.1486163583103796\n",
      "Epoch: 7 - Batch: 1724, Training Loss: 0.14870577850448552\n",
      "Epoch: 7 - Batch: 1725, Training Loss: 0.14879756667967856\n",
      "Epoch: 7 - Batch: 1726, Training Loss: 0.1488865032903294\n",
      "Epoch: 7 - Batch: 1727, Training Loss: 0.14897414800366557\n",
      "Epoch: 7 - Batch: 1728, Training Loss: 0.1490561984244666\n",
      "Epoch: 7 - Batch: 1729, Training Loss: 0.1491351834008745\n",
      "Epoch: 7 - Batch: 1730, Training Loss: 0.14922001552621325\n",
      "Epoch: 7 - Batch: 1731, Training Loss: 0.1493057799996626\n",
      "Epoch: 7 - Batch: 1732, Training Loss: 0.14939627545587655\n",
      "Epoch: 7 - Batch: 1733, Training Loss: 0.14948156650856162\n",
      "Epoch: 7 - Batch: 1734, Training Loss: 0.1495711504671704\n",
      "Epoch: 7 - Batch: 1735, Training Loss: 0.149648485658922\n",
      "Epoch: 7 - Batch: 1736, Training Loss: 0.1497355554852122\n",
      "Epoch: 7 - Batch: 1737, Training Loss: 0.1498299661659285\n",
      "Epoch: 7 - Batch: 1738, Training Loss: 0.1499083186152266\n",
      "Epoch: 7 - Batch: 1739, Training Loss: 0.14999359119590835\n",
      "Epoch: 7 - Batch: 1740, Training Loss: 0.15009006811537554\n",
      "Epoch: 7 - Batch: 1741, Training Loss: 0.15017178660848643\n",
      "Epoch: 7 - Batch: 1742, Training Loss: 0.15025573766251307\n",
      "Epoch: 7 - Batch: 1743, Training Loss: 0.15034122555632495\n",
      "Epoch: 7 - Batch: 1744, Training Loss: 0.15042035816666696\n",
      "Epoch: 7 - Batch: 1745, Training Loss: 0.1505134683318597\n",
      "Epoch: 7 - Batch: 1746, Training Loss: 0.15061205786160173\n",
      "Epoch: 7 - Batch: 1747, Training Loss: 0.15069072290785474\n",
      "Epoch: 7 - Batch: 1748, Training Loss: 0.1507811324229189\n",
      "Epoch: 7 - Batch: 1749, Training Loss: 0.1508537499082721\n",
      "Epoch: 7 - Batch: 1750, Training Loss: 0.15093198215363432\n",
      "Epoch: 7 - Batch: 1751, Training Loss: 0.1510215602493899\n",
      "Epoch: 7 - Batch: 1752, Training Loss: 0.15111073471025052\n",
      "Epoch: 7 - Batch: 1753, Training Loss: 0.15120162365102452\n",
      "Epoch: 7 - Batch: 1754, Training Loss: 0.1512977740361323\n",
      "Epoch: 7 - Batch: 1755, Training Loss: 0.15137220605293514\n",
      "Epoch: 7 - Batch: 1756, Training Loss: 0.15146021290402706\n",
      "Epoch: 7 - Batch: 1757, Training Loss: 0.15154895204340246\n",
      "Epoch: 7 - Batch: 1758, Training Loss: 0.15163188175009457\n",
      "Epoch: 7 - Batch: 1759, Training Loss: 0.15172084132410202\n",
      "Epoch: 7 - Batch: 1760, Training Loss: 0.15181021918704854\n",
      "Epoch: 7 - Batch: 1761, Training Loss: 0.15189754594716662\n",
      "Epoch: 7 - Batch: 1762, Training Loss: 0.1519905661740904\n",
      "Epoch: 7 - Batch: 1763, Training Loss: 0.15207104368218735\n",
      "Epoch: 7 - Batch: 1764, Training Loss: 0.15215314972904784\n",
      "Epoch: 7 - Batch: 1765, Training Loss: 0.1522347393382347\n",
      "Epoch: 7 - Batch: 1766, Training Loss: 0.15232193166047187\n",
      "Epoch: 7 - Batch: 1767, Training Loss: 0.1524039230079299\n",
      "Epoch: 7 - Batch: 1768, Training Loss: 0.15249104458054105\n",
      "Epoch: 7 - Batch: 1769, Training Loss: 0.1525719844323782\n",
      "Epoch: 7 - Batch: 1770, Training Loss: 0.1526570771550559\n",
      "Epoch: 7 - Batch: 1771, Training Loss: 0.15273526936744775\n",
      "Epoch: 7 - Batch: 1772, Training Loss: 0.1528215246216377\n",
      "Epoch: 7 - Batch: 1773, Training Loss: 0.1529085518943631\n",
      "Epoch: 7 - Batch: 1774, Training Loss: 0.15299493113338059\n",
      "Epoch: 7 - Batch: 1775, Training Loss: 0.15308383506279483\n",
      "Epoch: 7 - Batch: 1776, Training Loss: 0.15317589840905782\n",
      "Epoch: 7 - Batch: 1777, Training Loss: 0.15325873963547187\n",
      "Epoch: 7 - Batch: 1778, Training Loss: 0.15335410995838258\n",
      "Epoch: 7 - Batch: 1779, Training Loss: 0.15344494617672308\n",
      "Epoch: 7 - Batch: 1780, Training Loss: 0.15353527678111892\n",
      "Epoch: 7 - Batch: 1781, Training Loss: 0.1536204565199058\n",
      "Epoch: 7 - Batch: 1782, Training Loss: 0.15371031964052573\n",
      "Epoch: 7 - Batch: 1783, Training Loss: 0.15379740772259176\n",
      "Epoch: 7 - Batch: 1784, Training Loss: 0.15388125108916367\n",
      "Epoch: 7 - Batch: 1785, Training Loss: 0.15396238186367314\n",
      "Epoch: 7 - Batch: 1786, Training Loss: 0.15405045889602173\n",
      "Epoch: 7 - Batch: 1787, Training Loss: 0.15412855344155732\n",
      "Epoch: 7 - Batch: 1788, Training Loss: 0.15421636319822735\n",
      "Epoch: 7 - Batch: 1789, Training Loss: 0.154307950912028\n",
      "Epoch: 7 - Batch: 1790, Training Loss: 0.15439074194278093\n",
      "Epoch: 7 - Batch: 1791, Training Loss: 0.15447751666182902\n",
      "Epoch: 7 - Batch: 1792, Training Loss: 0.15455832731466784\n",
      "Epoch: 7 - Batch: 1793, Training Loss: 0.15464621870029427\n",
      "Epoch: 7 - Batch: 1794, Training Loss: 0.15473187428777097\n",
      "Epoch: 7 - Batch: 1795, Training Loss: 0.15481268702874926\n",
      "Epoch: 7 - Batch: 1796, Training Loss: 0.15489725905716123\n",
      "Epoch: 7 - Batch: 1797, Training Loss: 0.15499092466128406\n",
      "Epoch: 7 - Batch: 1798, Training Loss: 0.15507773907351652\n",
      "Epoch: 7 - Batch: 1799, Training Loss: 0.1551612415111579\n",
      "Epoch: 7 - Batch: 1800, Training Loss: 0.15524399569057312\n",
      "Epoch: 7 - Batch: 1801, Training Loss: 0.1553284657893588\n",
      "Epoch: 7 - Batch: 1802, Training Loss: 0.15541489715772877\n",
      "Epoch: 7 - Batch: 1803, Training Loss: 0.15549271288103922\n",
      "Epoch: 7 - Batch: 1804, Training Loss: 0.1555699124672816\n",
      "Epoch: 7 - Batch: 1805, Training Loss: 0.15564835394051538\n",
      "Epoch: 7 - Batch: 1806, Training Loss: 0.15573792462893585\n",
      "Epoch: 7 - Batch: 1807, Training Loss: 0.15582601228114187\n",
      "Epoch: 7 - Batch: 1808, Training Loss: 0.15591595037709025\n",
      "Epoch: 7 - Batch: 1809, Training Loss: 0.1560076415093977\n",
      "Epoch: 7 - Batch: 1810, Training Loss: 0.156094929379462\n",
      "Epoch: 7 - Batch: 1811, Training Loss: 0.15617827392780959\n",
      "Epoch: 7 - Batch: 1812, Training Loss: 0.15626330762656768\n",
      "Epoch: 7 - Batch: 1813, Training Loss: 0.15635342403603825\n",
      "Epoch: 7 - Batch: 1814, Training Loss: 0.1564429131387478\n",
      "Epoch: 7 - Batch: 1815, Training Loss: 0.15652313303344484\n",
      "Epoch: 7 - Batch: 1816, Training Loss: 0.15661237608165685\n",
      "Epoch: 7 - Batch: 1817, Training Loss: 0.15669530713513716\n",
      "Epoch: 7 - Batch: 1818, Training Loss: 0.1567714206911438\n",
      "Epoch: 7 - Batch: 1819, Training Loss: 0.1568518078038052\n",
      "Epoch: 7 - Batch: 1820, Training Loss: 0.15694955388581378\n",
      "Epoch: 7 - Batch: 1821, Training Loss: 0.1570370387946986\n",
      "Epoch: 7 - Batch: 1822, Training Loss: 0.15712758211749506\n",
      "Epoch: 7 - Batch: 1823, Training Loss: 0.15721044977308307\n",
      "Epoch: 7 - Batch: 1824, Training Loss: 0.157310779569408\n",
      "Epoch: 7 - Batch: 1825, Training Loss: 0.15739460366084604\n",
      "Epoch: 7 - Batch: 1826, Training Loss: 0.15747251981254637\n",
      "Epoch: 7 - Batch: 1827, Training Loss: 0.15755649809887753\n",
      "Epoch: 7 - Batch: 1828, Training Loss: 0.1576398753655293\n",
      "Epoch: 7 - Batch: 1829, Training Loss: 0.15772558273021656\n",
      "Epoch: 7 - Batch: 1830, Training Loss: 0.15780897303847333\n",
      "Epoch: 7 - Batch: 1831, Training Loss: 0.15789257671999102\n",
      "Epoch: 7 - Batch: 1832, Training Loss: 0.15798412764423325\n",
      "Epoch: 7 - Batch: 1833, Training Loss: 0.15806868994240936\n",
      "Epoch: 7 - Batch: 1834, Training Loss: 0.1581509024534949\n",
      "Epoch: 7 - Batch: 1835, Training Loss: 0.1582322850341524\n",
      "Epoch: 7 - Batch: 1836, Training Loss: 0.15832351010978518\n",
      "Epoch: 7 - Batch: 1837, Training Loss: 0.15842045167118163\n",
      "Epoch: 7 - Batch: 1838, Training Loss: 0.15850595305758725\n",
      "Epoch: 7 - Batch: 1839, Training Loss: 0.15857989651152546\n",
      "Epoch: 7 - Batch: 1840, Training Loss: 0.15866691624717927\n",
      "Epoch: 7 - Batch: 1841, Training Loss: 0.1587515661298339\n",
      "Epoch: 7 - Batch: 1842, Training Loss: 0.1588432229561098\n",
      "Epoch: 7 - Batch: 1843, Training Loss: 0.1589281528595075\n",
      "Epoch: 7 - Batch: 1844, Training Loss: 0.15902262066134173\n",
      "Epoch: 7 - Batch: 1845, Training Loss: 0.15910652332984\n",
      "Epoch: 7 - Batch: 1846, Training Loss: 0.15919170896236973\n",
      "Epoch: 7 - Batch: 1847, Training Loss: 0.15927750483822467\n",
      "Epoch: 7 - Batch: 1848, Training Loss: 0.15936765333585082\n",
      "Epoch: 7 - Batch: 1849, Training Loss: 0.15945999439231198\n",
      "Epoch: 7 - Batch: 1850, Training Loss: 0.15954413947033053\n",
      "Epoch: 7 - Batch: 1851, Training Loss: 0.15964578003753874\n",
      "Epoch: 7 - Batch: 1852, Training Loss: 0.15972022644891273\n",
      "Epoch: 7 - Batch: 1853, Training Loss: 0.15980574201366202\n",
      "Epoch: 7 - Batch: 1854, Training Loss: 0.15988839771073454\n",
      "Epoch: 7 - Batch: 1855, Training Loss: 0.15998018040610584\n",
      "Epoch: 7 - Batch: 1856, Training Loss: 0.16006757091008014\n",
      "Epoch: 7 - Batch: 1857, Training Loss: 0.16015923862186435\n",
      "Epoch: 7 - Batch: 1858, Training Loss: 0.1602453131593183\n",
      "Epoch: 7 - Batch: 1859, Training Loss: 0.16033111480535758\n",
      "Epoch: 7 - Batch: 1860, Training Loss: 0.16042171386541618\n",
      "Epoch: 7 - Batch: 1861, Training Loss: 0.16050782606351632\n",
      "Epoch: 7 - Batch: 1862, Training Loss: 0.16060354722475334\n",
      "Epoch: 7 - Batch: 1863, Training Loss: 0.1606891100938937\n",
      "Epoch: 7 - Batch: 1864, Training Loss: 0.16077559810150321\n",
      "Epoch: 7 - Batch: 1865, Training Loss: 0.16085818047275394\n",
      "Epoch: 7 - Batch: 1866, Training Loss: 0.16094688929086104\n",
      "Epoch: 7 - Batch: 1867, Training Loss: 0.16103767628347498\n",
      "Epoch: 7 - Batch: 1868, Training Loss: 0.16112416185698106\n",
      "Epoch: 7 - Batch: 1869, Training Loss: 0.16120441307994857\n",
      "Epoch: 7 - Batch: 1870, Training Loss: 0.16129783053525645\n",
      "Epoch: 7 - Batch: 1871, Training Loss: 0.16137653798680401\n",
      "Epoch: 7 - Batch: 1872, Training Loss: 0.16146009138616954\n",
      "Epoch: 7 - Batch: 1873, Training Loss: 0.1615473376355361\n",
      "Epoch: 7 - Batch: 1874, Training Loss: 0.16163500782046744\n",
      "Epoch: 7 - Batch: 1875, Training Loss: 0.16171421252722368\n",
      "Epoch: 7 - Batch: 1876, Training Loss: 0.16179947199834321\n",
      "Epoch: 7 - Batch: 1877, Training Loss: 0.1618814546287455\n",
      "Epoch: 7 - Batch: 1878, Training Loss: 0.16198283117604295\n",
      "Epoch: 7 - Batch: 1879, Training Loss: 0.1620704648494226\n",
      "Epoch: 7 - Batch: 1880, Training Loss: 0.16215999527011146\n",
      "Epoch: 7 - Batch: 1881, Training Loss: 0.16224089327266758\n",
      "Epoch: 7 - Batch: 1882, Training Loss: 0.1623263755198537\n",
      "Epoch: 7 - Batch: 1883, Training Loss: 0.16241613819355\n",
      "Epoch: 7 - Batch: 1884, Training Loss: 0.16250182221184917\n",
      "Epoch: 7 - Batch: 1885, Training Loss: 0.16259232297490286\n",
      "Epoch: 7 - Batch: 1886, Training Loss: 0.16267774149058867\n",
      "Epoch: 7 - Batch: 1887, Training Loss: 0.1627550984904244\n",
      "Epoch: 7 - Batch: 1888, Training Loss: 0.16283762591518772\n",
      "Epoch: 7 - Batch: 1889, Training Loss: 0.16292466690291219\n",
      "Epoch: 7 - Batch: 1890, Training Loss: 0.16300974364329135\n",
      "Epoch: 7 - Batch: 1891, Training Loss: 0.16309603841150577\n",
      "Epoch: 7 - Batch: 1892, Training Loss: 0.1631801687242775\n",
      "Epoch: 7 - Batch: 1893, Training Loss: 0.16326984242346157\n",
      "Epoch: 7 - Batch: 1894, Training Loss: 0.16336360002310318\n",
      "Epoch: 7 - Batch: 1895, Training Loss: 0.16344310397631295\n",
      "Epoch: 7 - Batch: 1896, Training Loss: 0.16352480020417307\n",
      "Epoch: 7 - Batch: 1897, Training Loss: 0.16360846208176802\n",
      "Epoch: 7 - Batch: 1898, Training Loss: 0.16369967233037475\n",
      "Epoch: 7 - Batch: 1899, Training Loss: 0.16378960567549686\n",
      "Epoch: 7 - Batch: 1900, Training Loss: 0.16387967894449953\n",
      "Epoch: 7 - Batch: 1901, Training Loss: 0.16396804344199387\n",
      "Epoch: 7 - Batch: 1902, Training Loss: 0.16406044981768278\n",
      "Epoch: 7 - Batch: 1903, Training Loss: 0.16414339648896387\n",
      "Epoch: 7 - Batch: 1904, Training Loss: 0.16422885066982526\n",
      "Epoch: 7 - Batch: 1905, Training Loss: 0.1643118706731061\n",
      "Epoch: 7 - Batch: 1906, Training Loss: 0.16439617325120898\n",
      "Epoch: 7 - Batch: 1907, Training Loss: 0.1644855392463567\n",
      "Epoch: 7 - Batch: 1908, Training Loss: 0.16457518801142526\n",
      "Epoch: 7 - Batch: 1909, Training Loss: 0.16466204006577012\n",
      "Epoch: 7 - Batch: 1910, Training Loss: 0.1647612657614213\n",
      "Epoch: 7 - Batch: 1911, Training Loss: 0.16484448586159678\n",
      "Epoch: 7 - Batch: 1912, Training Loss: 0.1649406548669781\n",
      "Epoch: 7 - Batch: 1913, Training Loss: 0.16502971193485988\n",
      "Epoch: 7 - Batch: 1914, Training Loss: 0.1651110398655705\n",
      "Epoch: 7 - Batch: 1915, Training Loss: 0.16518990166645936\n",
      "Epoch: 7 - Batch: 1916, Training Loss: 0.16527726774908616\n",
      "Epoch: 7 - Batch: 1917, Training Loss: 0.16535607711172617\n",
      "Epoch: 7 - Batch: 1918, Training Loss: 0.16543914514839353\n",
      "Epoch: 7 - Batch: 1919, Training Loss: 0.16552065522911932\n",
      "Epoch: 7 - Batch: 1920, Training Loss: 0.1655957563834305\n",
      "Epoch: 7 - Batch: 1921, Training Loss: 0.16569001762625787\n",
      "Epoch: 7 - Batch: 1922, Training Loss: 0.16577400771243062\n",
      "Epoch: 7 - Batch: 1923, Training Loss: 0.165856712437536\n",
      "Epoch: 7 - Batch: 1924, Training Loss: 0.16594027738344808\n",
      "Epoch: 7 - Batch: 1925, Training Loss: 0.1660290729273018\n",
      "Epoch: 7 - Batch: 1926, Training Loss: 0.16611589468509008\n",
      "Epoch: 7 - Batch: 1927, Training Loss: 0.16619564785589627\n",
      "Epoch: 7 - Batch: 1928, Training Loss: 0.16628414769942684\n",
      "Epoch: 7 - Batch: 1929, Training Loss: 0.16636307163791078\n",
      "Epoch: 7 - Batch: 1930, Training Loss: 0.16644773144231703\n",
      "Epoch: 7 - Batch: 1931, Training Loss: 0.16653527806869786\n",
      "Epoch: 7 - Batch: 1932, Training Loss: 0.16660972331862148\n",
      "Epoch: 7 - Batch: 1933, Training Loss: 0.16668956382678912\n",
      "Epoch: 7 - Batch: 1934, Training Loss: 0.16677737851962324\n",
      "Epoch: 7 - Batch: 1935, Training Loss: 0.16686760436490203\n",
      "Epoch: 7 - Batch: 1936, Training Loss: 0.1669607211899006\n",
      "Epoch: 7 - Batch: 1937, Training Loss: 0.1670495125945429\n",
      "Epoch: 7 - Batch: 1938, Training Loss: 0.1671380006814774\n",
      "Epoch: 7 - Batch: 1939, Training Loss: 0.167224722591601\n",
      "Epoch: 7 - Batch: 1940, Training Loss: 0.16730397926090565\n",
      "Epoch: 7 - Batch: 1941, Training Loss: 0.1673906415663845\n",
      "Epoch: 7 - Batch: 1942, Training Loss: 0.16748917556273601\n",
      "Epoch: 7 - Batch: 1943, Training Loss: 0.16756984900380445\n",
      "Epoch: 7 - Batch: 1944, Training Loss: 0.167651769385409\n",
      "Epoch: 7 - Batch: 1945, Training Loss: 0.1677337072629043\n",
      "Epoch: 7 - Batch: 1946, Training Loss: 0.1678235161969219\n",
      "Epoch: 7 - Batch: 1947, Training Loss: 0.16790247413871892\n",
      "Epoch: 7 - Batch: 1948, Training Loss: 0.16798678749730536\n",
      "Epoch: 7 - Batch: 1949, Training Loss: 0.16806354626988496\n",
      "Epoch: 7 - Batch: 1950, Training Loss: 0.16815245032285775\n",
      "Epoch: 7 - Batch: 1951, Training Loss: 0.1682388021504108\n",
      "Epoch: 7 - Batch: 1952, Training Loss: 0.16832743074812898\n",
      "Epoch: 7 - Batch: 1953, Training Loss: 0.16841814959202436\n",
      "Epoch: 7 - Batch: 1954, Training Loss: 0.16849587493843304\n",
      "Epoch: 7 - Batch: 1955, Training Loss: 0.1685916701977328\n",
      "Epoch: 7 - Batch: 1956, Training Loss: 0.16868089318621415\n",
      "Epoch: 7 - Batch: 1957, Training Loss: 0.16876323763311996\n",
      "Epoch: 7 - Batch: 1958, Training Loss: 0.1688456529841\n",
      "Epoch: 7 - Batch: 1959, Training Loss: 0.168929690529112\n",
      "Epoch: 7 - Batch: 1960, Training Loss: 0.1690126015352175\n",
      "Epoch: 7 - Batch: 1961, Training Loss: 0.16909596931257256\n",
      "Epoch: 7 - Batch: 1962, Training Loss: 0.1691787962617961\n",
      "Epoch: 7 - Batch: 1963, Training Loss: 0.16926136296582261\n",
      "Epoch: 7 - Batch: 1964, Training Loss: 0.1693458176383233\n",
      "Epoch: 7 - Batch: 1965, Training Loss: 0.16943710992980754\n",
      "Epoch: 7 - Batch: 1966, Training Loss: 0.16951894629159772\n",
      "Epoch: 7 - Batch: 1967, Training Loss: 0.1696096809509876\n",
      "Epoch: 7 - Batch: 1968, Training Loss: 0.16969969530727338\n",
      "Epoch: 7 - Batch: 1969, Training Loss: 0.16979697090921117\n",
      "Epoch: 7 - Batch: 1970, Training Loss: 0.16988215750549762\n",
      "Epoch: 7 - Batch: 1971, Training Loss: 0.16997590709854515\n",
      "Epoch: 7 - Batch: 1972, Training Loss: 0.17005866591140603\n",
      "Epoch: 7 - Batch: 1973, Training Loss: 0.17014311000729476\n",
      "Epoch: 7 - Batch: 1974, Training Loss: 0.17022873177061823\n",
      "Epoch: 7 - Batch: 1975, Training Loss: 0.17031333795061357\n",
      "Epoch: 7 - Batch: 1976, Training Loss: 0.17040644384068043\n",
      "Epoch: 7 - Batch: 1977, Training Loss: 0.17049584720611177\n",
      "Epoch: 7 - Batch: 1978, Training Loss: 0.17058707465406872\n",
      "Epoch: 7 - Batch: 1979, Training Loss: 0.17066907038167736\n",
      "Epoch: 7 - Batch: 1980, Training Loss: 0.17075185041571928\n",
      "Epoch: 7 - Batch: 1981, Training Loss: 0.17084308583938068\n",
      "Epoch: 7 - Batch: 1982, Training Loss: 0.17092193118227061\n",
      "Epoch: 7 - Batch: 1983, Training Loss: 0.17100534646788837\n",
      "Epoch: 7 - Batch: 1984, Training Loss: 0.17109040291995353\n",
      "Epoch: 7 - Batch: 1985, Training Loss: 0.17118629883004857\n",
      "Epoch: 7 - Batch: 1986, Training Loss: 0.17127058202181486\n",
      "Epoch: 7 - Batch: 1987, Training Loss: 0.17136189767204313\n",
      "Epoch: 7 - Batch: 1988, Training Loss: 0.17145719925315422\n",
      "Epoch: 7 - Batch: 1989, Training Loss: 0.17154011905662853\n",
      "Epoch: 7 - Batch: 1990, Training Loss: 0.17163095051178687\n",
      "Epoch: 7 - Batch: 1991, Training Loss: 0.17172789380300302\n",
      "Epoch: 7 - Batch: 1992, Training Loss: 0.171810274804706\n",
      "Epoch: 7 - Batch: 1993, Training Loss: 0.17190128656885714\n",
      "Epoch: 7 - Batch: 1994, Training Loss: 0.1719814259157351\n",
      "Epoch: 7 - Batch: 1995, Training Loss: 0.1720659486504633\n",
      "Epoch: 7 - Batch: 1996, Training Loss: 0.17214907922011308\n",
      "Epoch: 7 - Batch: 1997, Training Loss: 0.17223504654581273\n",
      "Epoch: 7 - Batch: 1998, Training Loss: 0.17232362405207027\n",
      "Epoch: 7 - Batch: 1999, Training Loss: 0.17240478622280742\n",
      "Epoch: 7 - Batch: 2000, Training Loss: 0.17248894440421023\n",
      "Epoch: 7 - Batch: 2001, Training Loss: 0.1725816235786449\n",
      "Epoch: 7 - Batch: 2002, Training Loss: 0.1726709488056489\n",
      "Epoch: 7 - Batch: 2003, Training Loss: 0.17274370219305182\n",
      "Epoch: 7 - Batch: 2004, Training Loss: 0.17282767512929775\n",
      "Epoch: 7 - Batch: 2005, Training Loss: 0.1729140368971362\n",
      "Epoch: 7 - Batch: 2006, Training Loss: 0.17298929442714894\n",
      "Epoch: 7 - Batch: 2007, Training Loss: 0.17307680767418734\n",
      "Epoch: 7 - Batch: 2008, Training Loss: 0.1731614878410427\n",
      "Epoch: 7 - Batch: 2009, Training Loss: 0.17324370382412355\n",
      "Epoch: 7 - Batch: 2010, Training Loss: 0.1733320248104743\n",
      "Epoch: 7 - Batch: 2011, Training Loss: 0.17341972807202963\n",
      "Epoch: 7 - Batch: 2012, Training Loss: 0.173505396559019\n",
      "Epoch: 7 - Batch: 2013, Training Loss: 0.1735889969662351\n",
      "Epoch: 7 - Batch: 2014, Training Loss: 0.1736819121869247\n",
      "Epoch: 7 - Batch: 2015, Training Loss: 0.17377090535032413\n",
      "Epoch: 7 - Batch: 2016, Training Loss: 0.1738642217409156\n",
      "Epoch: 7 - Batch: 2017, Training Loss: 0.17394817715038113\n",
      "Epoch: 7 - Batch: 2018, Training Loss: 0.17403478410780726\n",
      "Epoch: 7 - Batch: 2019, Training Loss: 0.1741218030217967\n",
      "Epoch: 7 - Batch: 2020, Training Loss: 0.1742083095631888\n",
      "Epoch: 7 - Batch: 2021, Training Loss: 0.17428428213271138\n",
      "Epoch: 7 - Batch: 2022, Training Loss: 0.17437610628815434\n",
      "Epoch: 7 - Batch: 2023, Training Loss: 0.17447224874080314\n",
      "Epoch: 7 - Batch: 2024, Training Loss: 0.17456821478890938\n",
      "Epoch: 7 - Batch: 2025, Training Loss: 0.17465100542427492\n",
      "Epoch: 7 - Batch: 2026, Training Loss: 0.17473946415987576\n",
      "Epoch: 7 - Batch: 2027, Training Loss: 0.17482958134776522\n",
      "Epoch: 7 - Batch: 2028, Training Loss: 0.1749146720070151\n",
      "Epoch: 7 - Batch: 2029, Training Loss: 0.1749985836026186\n",
      "Epoch: 7 - Batch: 2030, Training Loss: 0.17508818853529137\n",
      "Epoch: 7 - Batch: 2031, Training Loss: 0.17516960605830695\n",
      "Epoch: 7 - Batch: 2032, Training Loss: 0.1752518217572031\n",
      "Epoch: 7 - Batch: 2033, Training Loss: 0.17534401634118055\n",
      "Epoch: 7 - Batch: 2034, Training Loss: 0.17542084978019223\n",
      "Epoch: 7 - Batch: 2035, Training Loss: 0.17550108351962485\n",
      "Epoch: 7 - Batch: 2036, Training Loss: 0.17559541620400612\n",
      "Epoch: 7 - Batch: 2037, Training Loss: 0.17567940194091788\n",
      "Epoch: 7 - Batch: 2038, Training Loss: 0.17576628608341835\n",
      "Epoch: 7 - Batch: 2039, Training Loss: 0.17584399200642287\n",
      "Epoch: 7 - Batch: 2040, Training Loss: 0.1759323567510343\n",
      "Epoch: 7 - Batch: 2041, Training Loss: 0.17601491363215604\n",
      "Epoch: 7 - Batch: 2042, Training Loss: 0.17610531653065983\n",
      "Epoch: 7 - Batch: 2043, Training Loss: 0.1762012601034657\n",
      "Epoch: 7 - Batch: 2044, Training Loss: 0.17629324152423176\n",
      "Epoch: 7 - Batch: 2045, Training Loss: 0.17638175576851142\n",
      "Epoch: 7 - Batch: 2046, Training Loss: 0.17646718822777963\n",
      "Epoch: 7 - Batch: 2047, Training Loss: 0.17655320908522132\n",
      "Epoch: 7 - Batch: 2048, Training Loss: 0.17664180594692577\n",
      "Epoch: 7 - Batch: 2049, Training Loss: 0.17673112045522552\n",
      "Epoch: 7 - Batch: 2050, Training Loss: 0.1768179995565371\n",
      "Epoch: 7 - Batch: 2051, Training Loss: 0.17689984031701167\n",
      "Epoch: 7 - Batch: 2052, Training Loss: 0.17698644405573755\n",
      "Epoch: 7 - Batch: 2053, Training Loss: 0.1770612212518851\n",
      "Epoch: 7 - Batch: 2054, Training Loss: 0.17713818002869042\n",
      "Epoch: 7 - Batch: 2055, Training Loss: 0.17722819835120568\n",
      "Epoch: 7 - Batch: 2056, Training Loss: 0.1773250858186687\n",
      "Epoch: 7 - Batch: 2057, Training Loss: 0.17741974619268186\n",
      "Epoch: 7 - Batch: 2058, Training Loss: 0.17750407080745223\n",
      "Epoch: 7 - Batch: 2059, Training Loss: 0.17759018441411986\n",
      "Epoch: 7 - Batch: 2060, Training Loss: 0.17767024416457955\n",
      "Epoch: 7 - Batch: 2061, Training Loss: 0.17774877697847177\n",
      "Epoch: 7 - Batch: 2062, Training Loss: 0.1778417646477175\n",
      "Epoch: 7 - Batch: 2063, Training Loss: 0.17793255424123894\n",
      "Epoch: 7 - Batch: 2064, Training Loss: 0.17802219559279445\n",
      "Epoch: 7 - Batch: 2065, Training Loss: 0.1781112663262519\n",
      "Epoch: 7 - Batch: 2066, Training Loss: 0.1781927874469342\n",
      "Epoch: 7 - Batch: 2067, Training Loss: 0.1782734783406479\n",
      "Epoch: 7 - Batch: 2068, Training Loss: 0.17835659281679647\n",
      "Epoch: 7 - Batch: 2069, Training Loss: 0.178448357336705\n",
      "Epoch: 7 - Batch: 2070, Training Loss: 0.17853685743353062\n",
      "Epoch: 7 - Batch: 2071, Training Loss: 0.178626622912006\n",
      "Epoch: 7 - Batch: 2072, Training Loss: 0.1787103385066808\n",
      "Epoch: 7 - Batch: 2073, Training Loss: 0.178792596314331\n",
      "Epoch: 7 - Batch: 2074, Training Loss: 0.17888221967576154\n",
      "Epoch: 7 - Batch: 2075, Training Loss: 0.17896260080201115\n",
      "Epoch: 7 - Batch: 2076, Training Loss: 0.17905460021290218\n",
      "Epoch: 7 - Batch: 2077, Training Loss: 0.17913618533073572\n",
      "Epoch: 7 - Batch: 2078, Training Loss: 0.17922659437627736\n",
      "Epoch: 7 - Batch: 2079, Training Loss: 0.17931359561521615\n",
      "Epoch: 7 - Batch: 2080, Training Loss: 0.17940090592969116\n",
      "Epoch: 7 - Batch: 2081, Training Loss: 0.17948623060883573\n",
      "Epoch: 7 - Batch: 2082, Training Loss: 0.17956917702064387\n",
      "Epoch: 7 - Batch: 2083, Training Loss: 0.1796607315775075\n",
      "Epoch: 7 - Batch: 2084, Training Loss: 0.17974861645159831\n",
      "Epoch: 7 - Batch: 2085, Training Loss: 0.17983977651689975\n",
      "Epoch: 7 - Batch: 2086, Training Loss: 0.17993228993136096\n",
      "Epoch: 7 - Batch: 2087, Training Loss: 0.18003096647721223\n",
      "Epoch: 7 - Batch: 2088, Training Loss: 0.1801212610148672\n",
      "Epoch: 7 - Batch: 2089, Training Loss: 0.18020819303630597\n",
      "Epoch: 7 - Batch: 2090, Training Loss: 0.18029714469638827\n",
      "Epoch: 7 - Batch: 2091, Training Loss: 0.18037948921742922\n",
      "Epoch: 7 - Batch: 2092, Training Loss: 0.1804736497600379\n",
      "Epoch: 7 - Batch: 2093, Training Loss: 0.18056507875670247\n",
      "Epoch: 7 - Batch: 2094, Training Loss: 0.1806617506330286\n",
      "Epoch: 7 - Batch: 2095, Training Loss: 0.18074666611714346\n",
      "Epoch: 7 - Batch: 2096, Training Loss: 0.1808281762967161\n",
      "Epoch: 7 - Batch: 2097, Training Loss: 0.18091560575252347\n",
      "Epoch: 7 - Batch: 2098, Training Loss: 0.1810022663096488\n",
      "Epoch: 7 - Batch: 2099, Training Loss: 0.1810871308505733\n",
      "Epoch: 7 - Batch: 2100, Training Loss: 0.1811708257739026\n",
      "Epoch: 7 - Batch: 2101, Training Loss: 0.18125481185060038\n",
      "Epoch: 7 - Batch: 2102, Training Loss: 0.18134073063014555\n",
      "Epoch: 7 - Batch: 2103, Training Loss: 0.18143131180510394\n",
      "Epoch: 7 - Batch: 2104, Training Loss: 0.18152255006443407\n",
      "Epoch: 7 - Batch: 2105, Training Loss: 0.18160807797886047\n",
      "Epoch: 7 - Batch: 2106, Training Loss: 0.18169443629323745\n",
      "Epoch: 7 - Batch: 2107, Training Loss: 0.18178148298318905\n",
      "Epoch: 7 - Batch: 2108, Training Loss: 0.18186194788831384\n",
      "Epoch: 7 - Batch: 2109, Training Loss: 0.18195050460584525\n",
      "Epoch: 7 - Batch: 2110, Training Loss: 0.1820431503762258\n",
      "Epoch: 7 - Batch: 2111, Training Loss: 0.18213412477627125\n",
      "Epoch: 7 - Batch: 2112, Training Loss: 0.1822227172145796\n",
      "Epoch: 7 - Batch: 2113, Training Loss: 0.18230985984156775\n",
      "Epoch: 7 - Batch: 2114, Training Loss: 0.18239518656560635\n",
      "Epoch: 7 - Batch: 2115, Training Loss: 0.18248775413578028\n",
      "Epoch: 7 - Batch: 2116, Training Loss: 0.1825714151546433\n",
      "Epoch: 7 - Batch: 2117, Training Loss: 0.182661781405681\n",
      "Epoch: 7 - Batch: 2118, Training Loss: 0.18275542741341771\n",
      "Epoch: 7 - Batch: 2119, Training Loss: 0.18283829889932082\n",
      "Epoch: 7 - Batch: 2120, Training Loss: 0.1829238202527882\n",
      "Epoch: 7 - Batch: 2121, Training Loss: 0.18300441450518162\n",
      "Epoch: 7 - Batch: 2122, Training Loss: 0.18308823866457685\n",
      "Epoch: 7 - Batch: 2123, Training Loss: 0.1831732524024511\n",
      "Epoch: 7 - Batch: 2124, Training Loss: 0.18326271731239646\n",
      "Epoch: 7 - Batch: 2125, Training Loss: 0.18335048845084154\n",
      "Epoch: 7 - Batch: 2126, Training Loss: 0.18344541818867274\n",
      "Epoch: 7 - Batch: 2127, Training Loss: 0.18352442283560189\n",
      "Epoch: 7 - Batch: 2128, Training Loss: 0.18360445229568292\n",
      "Epoch: 7 - Batch: 2129, Training Loss: 0.18368842970474838\n",
      "Epoch: 7 - Batch: 2130, Training Loss: 0.1837725403531768\n",
      "Epoch: 7 - Batch: 2131, Training Loss: 0.18385567679185771\n",
      "Epoch: 7 - Batch: 2132, Training Loss: 0.18395221245723775\n",
      "Epoch: 7 - Batch: 2133, Training Loss: 0.18403248338161615\n",
      "Epoch: 7 - Batch: 2134, Training Loss: 0.18411944384005532\n",
      "Epoch: 7 - Batch: 2135, Training Loss: 0.1842100970496487\n",
      "Epoch: 7 - Batch: 2136, Training Loss: 0.18430509927928151\n",
      "Epoch: 7 - Batch: 2137, Training Loss: 0.18439030978414747\n",
      "Epoch: 7 - Batch: 2138, Training Loss: 0.18448098848386982\n",
      "Epoch: 7 - Batch: 2139, Training Loss: 0.1845662939990832\n",
      "Epoch: 7 - Batch: 2140, Training Loss: 0.18464662833370973\n",
      "Epoch: 7 - Batch: 2141, Training Loss: 0.18473529338737824\n",
      "Epoch: 7 - Batch: 2142, Training Loss: 0.1848210256429355\n",
      "Epoch: 7 - Batch: 2143, Training Loss: 0.18490597821932725\n",
      "Epoch: 7 - Batch: 2144, Training Loss: 0.18500053341412426\n",
      "Epoch: 7 - Batch: 2145, Training Loss: 0.1850852981629854\n",
      "Epoch: 7 - Batch: 2146, Training Loss: 0.18517682713087322\n",
      "Epoch: 7 - Batch: 2147, Training Loss: 0.18527190223533913\n",
      "Epoch: 7 - Batch: 2148, Training Loss: 0.1853634009190262\n",
      "Epoch: 7 - Batch: 2149, Training Loss: 0.1854470056816812\n",
      "Epoch: 7 - Batch: 2150, Training Loss: 0.18553153471171757\n",
      "Epoch: 7 - Batch: 2151, Training Loss: 0.18561830163422113\n",
      "Epoch: 7 - Batch: 2152, Training Loss: 0.18570610579368882\n",
      "Epoch: 7 - Batch: 2153, Training Loss: 0.1857895931077636\n",
      "Epoch: 7 - Batch: 2154, Training Loss: 0.18588132516266298\n",
      "Epoch: 7 - Batch: 2155, Training Loss: 0.18596727942822386\n",
      "Epoch: 7 - Batch: 2156, Training Loss: 0.186053212614696\n",
      "Epoch: 7 - Batch: 2157, Training Loss: 0.1861375956047036\n",
      "Epoch: 7 - Batch: 2158, Training Loss: 0.18622643610533593\n",
      "Epoch: 7 - Batch: 2159, Training Loss: 0.18631417691015684\n",
      "Epoch: 7 - Batch: 2160, Training Loss: 0.18639945427873242\n",
      "Epoch: 7 - Batch: 2161, Training Loss: 0.18648796086855987\n",
      "Epoch: 7 - Batch: 2162, Training Loss: 0.18657942877998993\n",
      "Epoch: 7 - Batch: 2163, Training Loss: 0.18666048144696165\n",
      "Epoch: 7 - Batch: 2164, Training Loss: 0.18674561289288907\n",
      "Epoch: 7 - Batch: 2165, Training Loss: 0.18682888210141047\n",
      "Epoch: 7 - Batch: 2166, Training Loss: 0.1869159773684062\n",
      "Epoch: 7 - Batch: 2167, Training Loss: 0.18699822579796238\n",
      "Epoch: 7 - Batch: 2168, Training Loss: 0.18709214030460733\n",
      "Epoch: 7 - Batch: 2169, Training Loss: 0.18717342887574168\n",
      "Epoch: 7 - Batch: 2170, Training Loss: 0.18726554958406175\n",
      "Epoch: 7 - Batch: 2171, Training Loss: 0.18735482885964078\n",
      "Epoch: 7 - Batch: 2172, Training Loss: 0.18744294841466455\n",
      "Epoch: 7 - Batch: 2173, Training Loss: 0.18752179508580893\n",
      "Epoch: 7 - Batch: 2174, Training Loss: 0.18760624839916554\n",
      "Epoch: 7 - Batch: 2175, Training Loss: 0.18769495746438974\n",
      "Epoch: 7 - Batch: 2176, Training Loss: 0.18778456030274504\n",
      "Epoch: 7 - Batch: 2177, Training Loss: 0.18787638112828506\n",
      "Epoch: 7 - Batch: 2178, Training Loss: 0.18796112198710047\n",
      "Epoch: 7 - Batch: 2179, Training Loss: 0.1880382977944998\n",
      "Epoch: 7 - Batch: 2180, Training Loss: 0.1881240536188506\n",
      "Epoch: 7 - Batch: 2181, Training Loss: 0.18820420125650728\n",
      "Epoch: 7 - Batch: 2182, Training Loss: 0.18828635331039406\n",
      "Epoch: 7 - Batch: 2183, Training Loss: 0.18838054195021714\n",
      "Epoch: 7 - Batch: 2184, Training Loss: 0.18848004476916336\n",
      "Epoch: 7 - Batch: 2185, Training Loss: 0.18856214704748805\n",
      "Epoch: 7 - Batch: 2186, Training Loss: 0.18864867036813132\n",
      "Epoch: 7 - Batch: 2187, Training Loss: 0.18872934395747004\n",
      "Epoch: 7 - Batch: 2188, Training Loss: 0.18881232832646488\n",
      "Epoch: 7 - Batch: 2189, Training Loss: 0.18890360065088738\n",
      "Epoch: 7 - Batch: 2190, Training Loss: 0.18899337849139575\n",
      "Epoch: 7 - Batch: 2191, Training Loss: 0.1890826353966992\n",
      "Epoch: 7 - Batch: 2192, Training Loss: 0.18916725847950425\n",
      "Epoch: 7 - Batch: 2193, Training Loss: 0.18925706709844753\n",
      "Epoch: 7 - Batch: 2194, Training Loss: 0.18934255405321446\n",
      "Epoch: 7 - Batch: 2195, Training Loss: 0.1894275066296062\n",
      "Epoch: 7 - Batch: 2196, Training Loss: 0.18951361694961638\n",
      "Epoch: 7 - Batch: 2197, Training Loss: 0.18959851079862905\n",
      "Epoch: 7 - Batch: 2198, Training Loss: 0.18969071669439178\n",
      "Epoch: 7 - Batch: 2199, Training Loss: 0.18977735265718765\n",
      "Epoch: 7 - Batch: 2200, Training Loss: 0.18986166756643388\n",
      "Epoch: 7 - Batch: 2201, Training Loss: 0.18994960094343372\n",
      "Epoch: 7 - Batch: 2202, Training Loss: 0.19003375790160687\n",
      "Epoch: 7 - Batch: 2203, Training Loss: 0.1901243216918772\n",
      "Epoch: 7 - Batch: 2204, Training Loss: 0.19021913445682867\n",
      "Epoch: 7 - Batch: 2205, Training Loss: 0.19030581287434248\n",
      "Epoch: 7 - Batch: 2206, Training Loss: 0.19039002977694644\n",
      "Epoch: 7 - Batch: 2207, Training Loss: 0.19047491521779972\n",
      "Epoch: 7 - Batch: 2208, Training Loss: 0.19056312166537417\n",
      "Epoch: 7 - Batch: 2209, Training Loss: 0.1906584390496239\n",
      "Epoch: 7 - Batch: 2210, Training Loss: 0.19075251611607585\n",
      "Epoch: 7 - Batch: 2211, Training Loss: 0.19084774944962157\n",
      "Epoch: 7 - Batch: 2212, Training Loss: 0.19093963534109432\n",
      "Epoch: 7 - Batch: 2213, Training Loss: 0.19102358344824952\n",
      "Epoch: 7 - Batch: 2214, Training Loss: 0.1911115833862405\n",
      "Epoch: 7 - Batch: 2215, Training Loss: 0.19120200672750648\n",
      "Epoch: 7 - Batch: 2216, Training Loss: 0.19128827715468644\n",
      "Epoch: 7 - Batch: 2217, Training Loss: 0.19137721750298345\n",
      "Epoch: 7 - Batch: 2218, Training Loss: 0.19146118020240346\n",
      "Epoch: 7 - Batch: 2219, Training Loss: 0.19155429956653028\n",
      "Epoch: 7 - Batch: 2220, Training Loss: 0.19164461219933496\n",
      "Epoch: 7 - Batch: 2221, Training Loss: 0.19172535967668689\n",
      "Epoch: 7 - Batch: 2222, Training Loss: 0.19181514798260446\n",
      "Epoch: 7 - Batch: 2223, Training Loss: 0.1918972631975094\n",
      "Epoch: 7 - Batch: 2224, Training Loss: 0.19198207267456585\n",
      "Epoch: 7 - Batch: 2225, Training Loss: 0.19206912103255788\n",
      "Epoch: 7 - Batch: 2226, Training Loss: 0.1921610694075896\n",
      "Epoch: 7 - Batch: 2227, Training Loss: 0.1922396230658093\n",
      "Epoch: 7 - Batch: 2228, Training Loss: 0.19232956068605728\n",
      "Epoch: 7 - Batch: 2229, Training Loss: 0.19241082796187542\n",
      "Epoch: 7 - Batch: 2230, Training Loss: 0.19248587212480517\n",
      "Epoch: 7 - Batch: 2231, Training Loss: 0.19257281343166904\n",
      "Epoch: 7 - Batch: 2232, Training Loss: 0.19264946320707327\n",
      "Epoch: 7 - Batch: 2233, Training Loss: 0.19273926166332578\n",
      "Epoch: 7 - Batch: 2234, Training Loss: 0.19283317502705413\n",
      "Epoch: 7 - Batch: 2235, Training Loss: 0.1929135501916037\n",
      "Epoch: 7 - Batch: 2236, Training Loss: 0.19301156303652287\n",
      "Epoch: 7 - Batch: 2237, Training Loss: 0.1930950320268646\n",
      "Epoch: 7 - Batch: 2238, Training Loss: 0.19318272440908957\n",
      "Epoch: 7 - Batch: 2239, Training Loss: 0.19327267299515888\n",
      "Epoch: 7 - Batch: 2240, Training Loss: 0.19336729774724193\n",
      "Epoch: 7 - Batch: 2241, Training Loss: 0.19345109220933954\n",
      "Epoch: 7 - Batch: 2242, Training Loss: 0.19353812257720662\n",
      "Epoch: 7 - Batch: 2243, Training Loss: 0.19362499094745808\n",
      "Epoch: 7 - Batch: 2244, Training Loss: 0.1937161068652894\n",
      "Epoch: 7 - Batch: 2245, Training Loss: 0.19380169494037408\n",
      "Epoch: 7 - Batch: 2246, Training Loss: 0.1938882510658125\n",
      "Epoch: 7 - Batch: 2247, Training Loss: 0.19396530430930764\n",
      "Epoch: 7 - Batch: 2248, Training Loss: 0.19404622343419797\n",
      "Epoch: 7 - Batch: 2249, Training Loss: 0.19414113419343584\n",
      "Epoch: 7 - Batch: 2250, Training Loss: 0.19422596281588966\n",
      "Epoch: 7 - Batch: 2251, Training Loss: 0.1943081576828142\n",
      "Epoch: 7 - Batch: 2252, Training Loss: 0.19439504227630336\n",
      "Epoch: 7 - Batch: 2253, Training Loss: 0.1944886959137806\n",
      "Epoch: 7 - Batch: 2254, Training Loss: 0.19457675343623407\n",
      "Epoch: 7 - Batch: 2255, Training Loss: 0.19466076350800235\n",
      "Epoch: 7 - Batch: 2256, Training Loss: 0.19474223827569442\n",
      "Epoch: 7 - Batch: 2257, Training Loss: 0.19482541888655713\n",
      "Epoch: 7 - Batch: 2258, Training Loss: 0.19490975988519132\n",
      "Epoch: 7 - Batch: 2259, Training Loss: 0.19499392302746993\n",
      "Epoch: 7 - Batch: 2260, Training Loss: 0.19507862074581742\n",
      "Epoch: 7 - Batch: 2261, Training Loss: 0.1951663473599388\n",
      "Epoch: 7 - Batch: 2262, Training Loss: 0.19525209869293628\n",
      "Epoch: 7 - Batch: 2263, Training Loss: 0.19534827767567056\n",
      "Epoch: 7 - Batch: 2264, Training Loss: 0.19542693069967662\n",
      "Epoch: 7 - Batch: 2265, Training Loss: 0.19551392236900567\n",
      "Epoch: 7 - Batch: 2266, Training Loss: 0.19560360476671168\n",
      "Epoch: 7 - Batch: 2267, Training Loss: 0.19569721355861297\n",
      "Epoch: 7 - Batch: 2268, Training Loss: 0.19578109899687135\n",
      "Epoch: 7 - Batch: 2269, Training Loss: 0.19586380991843802\n",
      "Epoch: 7 - Batch: 2270, Training Loss: 0.1959473413799829\n",
      "Epoch: 7 - Batch: 2271, Training Loss: 0.19603207070460762\n",
      "Epoch: 7 - Batch: 2272, Training Loss: 0.1961142582383322\n",
      "Epoch: 7 - Batch: 2273, Training Loss: 0.19620467304788022\n",
      "Epoch: 7 - Batch: 2274, Training Loss: 0.1962874500176701\n",
      "Epoch: 7 - Batch: 2275, Training Loss: 0.1963700295841417\n",
      "Epoch: 7 - Batch: 2276, Training Loss: 0.19646015060852415\n",
      "Epoch: 7 - Batch: 2277, Training Loss: 0.19654528763312012\n",
      "Epoch: 7 - Batch: 2278, Training Loss: 0.1966244212010409\n",
      "Epoch: 7 - Batch: 2279, Training Loss: 0.19672053843538956\n",
      "Epoch: 7 - Batch: 2280, Training Loss: 0.1968129140326435\n",
      "Epoch: 7 - Batch: 2281, Training Loss: 0.19690357127642355\n",
      "Epoch: 7 - Batch: 2282, Training Loss: 0.19699276527806894\n",
      "Epoch: 7 - Batch: 2283, Training Loss: 0.19707727796130908\n",
      "Epoch: 7 - Batch: 2284, Training Loss: 0.19716432450916835\n",
      "Epoch: 7 - Batch: 2285, Training Loss: 0.1972530437741509\n",
      "Epoch: 7 - Batch: 2286, Training Loss: 0.1973371396511546\n",
      "Epoch: 7 - Batch: 2287, Training Loss: 0.1974194020613608\n",
      "Epoch: 7 - Batch: 2288, Training Loss: 0.19750550117461044\n",
      "Epoch: 7 - Batch: 2289, Training Loss: 0.19758614934795532\n",
      "Epoch: 7 - Batch: 2290, Training Loss: 0.19768021803713753\n",
      "Epoch: 7 - Batch: 2291, Training Loss: 0.19776644037532964\n",
      "Epoch: 7 - Batch: 2292, Training Loss: 0.19784794959732352\n",
      "Epoch: 7 - Batch: 2293, Training Loss: 0.19793587961353076\n",
      "Epoch: 7 - Batch: 2294, Training Loss: 0.19802377124627432\n",
      "Epoch: 7 - Batch: 2295, Training Loss: 0.19812338629345197\n",
      "Epoch: 7 - Batch: 2296, Training Loss: 0.19821198698547152\n",
      "Epoch: 7 - Batch: 2297, Training Loss: 0.19829697567802756\n",
      "Epoch: 7 - Batch: 2298, Training Loss: 0.1983784068789747\n",
      "Epoch: 7 - Batch: 2299, Training Loss: 0.19846973354444772\n",
      "Epoch: 7 - Batch: 2300, Training Loss: 0.198554807461514\n",
      "Epoch: 7 - Batch: 2301, Training Loss: 0.19864068004869506\n",
      "Epoch: 7 - Batch: 2302, Training Loss: 0.1987307858753758\n",
      "Epoch: 7 - Batch: 2303, Training Loss: 0.1988109607020915\n",
      "Epoch: 7 - Batch: 2304, Training Loss: 0.19890043112892614\n",
      "Epoch: 7 - Batch: 2305, Training Loss: 0.19898049886744612\n",
      "Epoch: 7 - Batch: 2306, Training Loss: 0.19906741565411562\n",
      "Epoch: 7 - Batch: 2307, Training Loss: 0.19915216794209695\n",
      "Epoch: 7 - Batch: 2308, Training Loss: 0.19924538022225374\n",
      "Epoch: 7 - Batch: 2309, Training Loss: 0.19933111540614867\n",
      "Epoch: 7 - Batch: 2310, Training Loss: 0.19942351804419142\n",
      "Epoch: 7 - Batch: 2311, Training Loss: 0.19950946563759056\n",
      "Epoch: 7 - Batch: 2312, Training Loss: 0.19959702021496412\n",
      "Epoch: 7 - Batch: 2313, Training Loss: 0.1996833187702481\n",
      "Epoch: 7 - Batch: 2314, Training Loss: 0.19976660858833573\n",
      "Epoch: 7 - Batch: 2315, Training Loss: 0.1998509268769083\n",
      "Epoch: 7 - Batch: 2316, Training Loss: 0.1999330196844701\n",
      "Epoch: 7 - Batch: 2317, Training Loss: 0.2000124946940894\n",
      "Epoch: 7 - Batch: 2318, Training Loss: 0.200094624346811\n",
      "Epoch: 7 - Batch: 2319, Training Loss: 0.20017426766788782\n",
      "Epoch: 7 - Batch: 2320, Training Loss: 0.20026500658386975\n",
      "Epoch: 7 - Batch: 2321, Training Loss: 0.20035193568314882\n",
      "Epoch: 7 - Batch: 2322, Training Loss: 0.20045073790657975\n",
      "Epoch: 7 - Batch: 2323, Training Loss: 0.20053909403916023\n",
      "Epoch: 7 - Batch: 2324, Training Loss: 0.200622233813922\n",
      "Epoch: 7 - Batch: 2325, Training Loss: 0.20070849649398087\n",
      "Epoch: 7 - Batch: 2326, Training Loss: 0.2007910869768504\n",
      "Epoch: 7 - Batch: 2327, Training Loss: 0.20087786091130172\n",
      "Epoch: 7 - Batch: 2328, Training Loss: 0.2009716512045358\n",
      "Epoch: 7 - Batch: 2329, Training Loss: 0.20106022063006412\n",
      "Epoch: 7 - Batch: 2330, Training Loss: 0.201155032165608\n",
      "Epoch: 7 - Batch: 2331, Training Loss: 0.20124279690372016\n",
      "Epoch: 7 - Batch: 2332, Training Loss: 0.20133037275169818\n",
      "Epoch: 7 - Batch: 2333, Training Loss: 0.20140948045881432\n",
      "Epoch: 7 - Batch: 2334, Training Loss: 0.20148925013704283\n",
      "Epoch: 7 - Batch: 2335, Training Loss: 0.20156663563565827\n",
      "Epoch: 7 - Batch: 2336, Training Loss: 0.20165100698027247\n",
      "Epoch: 7 - Batch: 2337, Training Loss: 0.20173521096482996\n",
      "Epoch: 7 - Batch: 2338, Training Loss: 0.20182235696496656\n",
      "Epoch: 7 - Batch: 2339, Training Loss: 0.20191378417621006\n",
      "Epoch: 7 - Batch: 2340, Training Loss: 0.2020035623688603\n",
      "Epoch: 7 - Batch: 2341, Training Loss: 0.20209515208430948\n",
      "Epoch: 7 - Batch: 2342, Training Loss: 0.20217700806101954\n",
      "Epoch: 7 - Batch: 2343, Training Loss: 0.20226907578743314\n",
      "Epoch: 7 - Batch: 2344, Training Loss: 0.2023561445078743\n",
      "Epoch: 7 - Batch: 2345, Training Loss: 0.20243879939563833\n",
      "Epoch: 7 - Batch: 2346, Training Loss: 0.20252405487581668\n",
      "Epoch: 7 - Batch: 2347, Training Loss: 0.20261292637728934\n",
      "Epoch: 7 - Batch: 2348, Training Loss: 0.2026993960117424\n",
      "Epoch: 7 - Batch: 2349, Training Loss: 0.20278182761684974\n",
      "Epoch: 7 - Batch: 2350, Training Loss: 0.20287317947591121\n",
      "Epoch: 7 - Batch: 2351, Training Loss: 0.20295694788568847\n",
      "Epoch: 7 - Batch: 2352, Training Loss: 0.20304964164621003\n",
      "Epoch: 7 - Batch: 2353, Training Loss: 0.20313983646593678\n",
      "Epoch: 7 - Batch: 2354, Training Loss: 0.20322939649125435\n",
      "Epoch: 7 - Batch: 2355, Training Loss: 0.2033136203996281\n",
      "Epoch: 7 - Batch: 2356, Training Loss: 0.20340326052177604\n",
      "Epoch: 7 - Batch: 2357, Training Loss: 0.20348444991246187\n",
      "Epoch: 7 - Batch: 2358, Training Loss: 0.20357387559874537\n",
      "Epoch: 7 - Batch: 2359, Training Loss: 0.20367035855404772\n",
      "Epoch: 7 - Batch: 2360, Training Loss: 0.20375293208468415\n",
      "Epoch: 7 - Batch: 2361, Training Loss: 0.20384307485537148\n",
      "Epoch: 7 - Batch: 2362, Training Loss: 0.20392895390466473\n",
      "Epoch: 7 - Batch: 2363, Training Loss: 0.20401816860854527\n",
      "Epoch: 7 - Batch: 2364, Training Loss: 0.2041021079677551\n",
      "Epoch: 7 - Batch: 2365, Training Loss: 0.20419514161313745\n",
      "Epoch: 7 - Batch: 2366, Training Loss: 0.20427535614539338\n",
      "Epoch: 7 - Batch: 2367, Training Loss: 0.2043595340529188\n",
      "Epoch: 7 - Batch: 2368, Training Loss: 0.20445031669009384\n",
      "Epoch: 7 - Batch: 2369, Training Loss: 0.20454163333496841\n",
      "Epoch: 7 - Batch: 2370, Training Loss: 0.20463006223735722\n",
      "Epoch: 7 - Batch: 2371, Training Loss: 0.20472086492190708\n",
      "Epoch: 7 - Batch: 2372, Training Loss: 0.20480420078408856\n",
      "Epoch: 7 - Batch: 2373, Training Loss: 0.2048978461678546\n",
      "Epoch: 7 - Batch: 2374, Training Loss: 0.20498789767819653\n",
      "Epoch: 7 - Batch: 2375, Training Loss: 0.20507353494811809\n",
      "Epoch: 7 - Batch: 2376, Training Loss: 0.20516661231469357\n",
      "Epoch: 7 - Batch: 2377, Training Loss: 0.20525195132663002\n",
      "Epoch: 7 - Batch: 2378, Training Loss: 0.20533969740740102\n",
      "Epoch: 7 - Batch: 2379, Training Loss: 0.20542104468738065\n",
      "Epoch: 7 - Batch: 2380, Training Loss: 0.20550987555662395\n",
      "Epoch: 7 - Batch: 2381, Training Loss: 0.20559228889359962\n",
      "Epoch: 7 - Batch: 2382, Training Loss: 0.20567411260299423\n",
      "Epoch: 7 - Batch: 2383, Training Loss: 0.2057688966714723\n",
      "Epoch: 7 - Batch: 2384, Training Loss: 0.20584741967481562\n",
      "Epoch: 7 - Batch: 2385, Training Loss: 0.2059388249729502\n",
      "Epoch: 7 - Batch: 2386, Training Loss: 0.20603103306187723\n",
      "Epoch: 7 - Batch: 2387, Training Loss: 0.20612069084054202\n",
      "Epoch: 7 - Batch: 2388, Training Loss: 0.20621242051076139\n",
      "Epoch: 7 - Batch: 2389, Training Loss: 0.20630627090287446\n",
      "Epoch: 7 - Batch: 2390, Training Loss: 0.20638857981434114\n",
      "Epoch: 7 - Batch: 2391, Training Loss: 0.20647560856976913\n",
      "Epoch: 7 - Batch: 2392, Training Loss: 0.20655001621464789\n",
      "Epoch: 7 - Batch: 2393, Training Loss: 0.20663459028177594\n",
      "Epoch: 7 - Batch: 2394, Training Loss: 0.2067165931510688\n",
      "Epoch: 7 - Batch: 2395, Training Loss: 0.20680518190733235\n",
      "Epoch: 7 - Batch: 2396, Training Loss: 0.2068871334689371\n",
      "Epoch: 7 - Batch: 2397, Training Loss: 0.20697824891724595\n",
      "Epoch: 7 - Batch: 2398, Training Loss: 0.20706155062784407\n",
      "Epoch: 7 - Batch: 2399, Training Loss: 0.20714532105162567\n",
      "Epoch: 7 - Batch: 2400, Training Loss: 0.20723962954324276\n",
      "Epoch: 7 - Batch: 2401, Training Loss: 0.20731960151770815\n",
      "Epoch: 7 - Batch: 2402, Training Loss: 0.20740758194580403\n",
      "Epoch: 7 - Batch: 2403, Training Loss: 0.20749501089502131\n",
      "Epoch: 7 - Batch: 2404, Training Loss: 0.20757580220476904\n",
      "Epoch: 7 - Batch: 2405, Training Loss: 0.2076651685397028\n",
      "Epoch: 7 - Batch: 2406, Training Loss: 0.20776478744808516\n",
      "Epoch: 7 - Batch: 2407, Training Loss: 0.20784760845414244\n",
      "Epoch: 7 - Batch: 2408, Training Loss: 0.20793377837261948\n",
      "Epoch: 7 - Batch: 2409, Training Loss: 0.20802459102834436\n",
      "Epoch: 7 - Batch: 2410, Training Loss: 0.20810516045164706\n",
      "Epoch: 7 - Batch: 2411, Training Loss: 0.208202081341698\n",
      "Epoch: 7 - Batch: 2412, Training Loss: 0.20827990055553752\n",
      "Epoch 7 - Batch 2412, Training Loss: 0.20827990055553752, Validation Loss: 0.20808773931381516\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch: 8 - Batch: 1, Training Loss: 8.332874520896482e-05\n",
      "Epoch: 8 - Batch: 2, Training Loss: 0.0001689653726260658\n",
      "Epoch: 8 - Batch: 3, Training Loss: 0.0002558644286435635\n",
      "Epoch: 8 - Batch: 4, Training Loss: 0.0003389089768304556\n",
      "Epoch: 8 - Batch: 5, Training Loss: 0.00043378549181604466\n",
      "Epoch: 8 - Batch: 6, Training Loss: 0.0005180418095976164\n",
      "Epoch: 8 - Batch: 7, Training Loss: 0.0006052292698058323\n",
      "Epoch: 8 - Batch: 8, Training Loss: 0.0006937697764554031\n",
      "Epoch: 8 - Batch: 9, Training Loss: 0.0007904203451094936\n",
      "Epoch: 8 - Batch: 10, Training Loss: 0.0008695079801391013\n",
      "Epoch: 8 - Batch: 11, Training Loss: 0.0009594628924062201\n",
      "Epoch: 8 - Batch: 12, Training Loss: 0.0010425977495376942\n",
      "Epoch: 8 - Batch: 13, Training Loss: 0.0011347576135624306\n",
      "Epoch: 8 - Batch: 14, Training Loss: 0.0012210096366962982\n",
      "Epoch: 8 - Batch: 15, Training Loss: 0.0013033205745232046\n",
      "Epoch: 8 - Batch: 16, Training Loss: 0.0013987868344704705\n",
      "Epoch: 8 - Batch: 17, Training Loss: 0.001487761000742762\n",
      "Epoch: 8 - Batch: 18, Training Loss: 0.0015681304877769096\n",
      "Epoch: 8 - Batch: 19, Training Loss: 0.0016576561143939966\n",
      "Epoch: 8 - Batch: 20, Training Loss: 0.0017426062072588633\n",
      "Epoch: 8 - Batch: 21, Training Loss: 0.001823550043859292\n",
      "Epoch: 8 - Batch: 22, Training Loss: 0.001917575363990284\n",
      "Epoch: 8 - Batch: 23, Training Loss: 0.0020048976389329823\n",
      "Epoch: 8 - Batch: 24, Training Loss: 0.0020925790924041427\n",
      "Epoch: 8 - Batch: 25, Training Loss: 0.002180771052738525\n",
      "Epoch: 8 - Batch: 26, Training Loss: 0.002267305623387816\n",
      "Epoch: 8 - Batch: 27, Training Loss: 0.002356931721640266\n",
      "Epoch: 8 - Batch: 28, Training Loss: 0.0024421479163774804\n",
      "Epoch: 8 - Batch: 29, Training Loss: 0.0025271368189830686\n",
      "Epoch: 8 - Batch: 30, Training Loss: 0.0026041803013527177\n",
      "Epoch: 8 - Batch: 31, Training Loss: 0.0026890928447740785\n",
      "Epoch: 8 - Batch: 32, Training Loss: 0.002766849383538833\n",
      "Epoch: 8 - Batch: 33, Training Loss: 0.002857679634001322\n",
      "Epoch: 8 - Batch: 34, Training Loss: 0.0029421177620120704\n",
      "Epoch: 8 - Batch: 35, Training Loss: 0.00303703815881688\n",
      "Epoch: 8 - Batch: 36, Training Loss: 0.0031186161226103358\n",
      "Epoch: 8 - Batch: 37, Training Loss: 0.003205584525864318\n",
      "Epoch: 8 - Batch: 38, Training Loss: 0.0032904922875697734\n",
      "Epoch: 8 - Batch: 39, Training Loss: 0.0033870531836058173\n",
      "Epoch: 8 - Batch: 40, Training Loss: 0.0034766976550147308\n",
      "Epoch: 8 - Batch: 41, Training Loss: 0.0035688260239649374\n",
      "Epoch: 8 - Batch: 42, Training Loss: 0.003662466613659218\n",
      "Epoch: 8 - Batch: 43, Training Loss: 0.0037520389392304778\n",
      "Epoch: 8 - Batch: 44, Training Loss: 0.0038450464828690485\n",
      "Epoch: 8 - Batch: 45, Training Loss: 0.0039294263900907875\n",
      "Epoch: 8 - Batch: 46, Training Loss: 0.004019864162995448\n",
      "Epoch: 8 - Batch: 47, Training Loss: 0.004108091856462643\n",
      "Epoch: 8 - Batch: 48, Training Loss: 0.004193104080744644\n",
      "Epoch: 8 - Batch: 49, Training Loss: 0.004280459382887899\n",
      "Epoch: 8 - Batch: 50, Training Loss: 0.0043629361610013255\n",
      "Epoch: 8 - Batch: 51, Training Loss: 0.004448007403024987\n",
      "Epoch: 8 - Batch: 52, Training Loss: 0.00453778180242771\n",
      "Epoch: 8 - Batch: 53, Training Loss: 0.00462490256573035\n",
      "Epoch: 8 - Batch: 54, Training Loss: 0.0047166944044344065\n",
      "Epoch: 8 - Batch: 55, Training Loss: 0.004810413985110041\n",
      "Epoch: 8 - Batch: 56, Training Loss: 0.004898157539108697\n",
      "Epoch: 8 - Batch: 57, Training Loss: 0.004981967946437263\n",
      "Epoch: 8 - Batch: 58, Training Loss: 0.005070386964142026\n",
      "Epoch: 8 - Batch: 59, Training Loss: 0.005150600995985825\n",
      "Epoch: 8 - Batch: 60, Training Loss: 0.005238128050692244\n",
      "Epoch: 8 - Batch: 61, Training Loss: 0.005320994668388446\n",
      "Epoch: 8 - Batch: 62, Training Loss: 0.005403397002475179\n",
      "Epoch: 8 - Batch: 63, Training Loss: 0.005492609655283774\n",
      "Epoch: 8 - Batch: 64, Training Loss: 0.005579344588478604\n",
      "Epoch: 8 - Batch: 65, Training Loss: 0.005673590806586233\n",
      "Epoch: 8 - Batch: 66, Training Loss: 0.0057633013879976065\n",
      "Epoch: 8 - Batch: 67, Training Loss: 0.005851250166570765\n",
      "Epoch: 8 - Batch: 68, Training Loss: 0.005936310467484776\n",
      "Epoch: 8 - Batch: 69, Training Loss: 0.0060227859091897115\n",
      "Epoch: 8 - Batch: 70, Training Loss: 0.0061179929062304015\n",
      "Epoch: 8 - Batch: 71, Training Loss: 0.006213592244914515\n",
      "Epoch: 8 - Batch: 72, Training Loss: 0.006308238842149279\n",
      "Epoch: 8 - Batch: 73, Training Loss: 0.006391054609324011\n",
      "Epoch: 8 - Batch: 74, Training Loss: 0.006479694722699091\n",
      "Epoch: 8 - Batch: 75, Training Loss: 0.006570681157347377\n",
      "Epoch: 8 - Batch: 76, Training Loss: 0.006648711143540308\n",
      "Epoch: 8 - Batch: 77, Training Loss: 0.006731451873607303\n",
      "Epoch: 8 - Batch: 78, Training Loss: 0.0068135493022588945\n",
      "Epoch: 8 - Batch: 79, Training Loss: 0.006892933032998991\n",
      "Epoch: 8 - Batch: 80, Training Loss: 0.006976401065761968\n",
      "Epoch: 8 - Batch: 81, Training Loss: 0.00706436314541309\n",
      "Epoch: 8 - Batch: 82, Training Loss: 0.007155448513392785\n",
      "Epoch: 8 - Batch: 83, Training Loss: 0.0072365781264518625\n",
      "Epoch: 8 - Batch: 84, Training Loss: 0.007333397222790947\n",
      "Epoch: 8 - Batch: 85, Training Loss: 0.007417773583882286\n",
      "Epoch: 8 - Batch: 86, Training Loss: 0.007496379081091873\n",
      "Epoch: 8 - Batch: 87, Training Loss: 0.007582113727507109\n",
      "Epoch: 8 - Batch: 88, Training Loss: 0.007671304751391435\n",
      "Epoch: 8 - Batch: 89, Training Loss: 0.0077738168141240896\n",
      "Epoch: 8 - Batch: 90, Training Loss: 0.007860484506755722\n",
      "Epoch: 8 - Batch: 91, Training Loss: 0.007951300751856507\n",
      "Epoch: 8 - Batch: 92, Training Loss: 0.008043361911133154\n",
      "Epoch: 8 - Batch: 93, Training Loss: 0.008131008304865602\n",
      "Epoch: 8 - Batch: 94, Training Loss: 0.008225368079459094\n",
      "Epoch: 8 - Batch: 95, Training Loss: 0.008314617330310357\n",
      "Epoch: 8 - Batch: 96, Training Loss: 0.008400419513829312\n",
      "Epoch: 8 - Batch: 97, Training Loss: 0.008488288152326596\n",
      "Epoch: 8 - Batch: 98, Training Loss: 0.008574635093138981\n",
      "Epoch: 8 - Batch: 99, Training Loss: 0.008659964930853045\n",
      "Epoch: 8 - Batch: 100, Training Loss: 0.008759374512517038\n",
      "Epoch: 8 - Batch: 101, Training Loss: 0.008843190109304726\n",
      "Epoch: 8 - Batch: 102, Training Loss: 0.008931858554655442\n",
      "Epoch: 8 - Batch: 103, Training Loss: 0.009021115793566997\n",
      "Epoch: 8 - Batch: 104, Training Loss: 0.009108456787630101\n",
      "Epoch: 8 - Batch: 105, Training Loss: 0.009196365187268945\n",
      "Epoch: 8 - Batch: 106, Training Loss: 0.009288500938842545\n",
      "Epoch: 8 - Batch: 107, Training Loss: 0.009386621057295286\n",
      "Epoch: 8 - Batch: 108, Training Loss: 0.009477628564854365\n",
      "Epoch: 8 - Batch: 109, Training Loss: 0.009567730579753816\n",
      "Epoch: 8 - Batch: 110, Training Loss: 0.009650242899484303\n",
      "Epoch: 8 - Batch: 111, Training Loss: 0.009737892184486832\n",
      "Epoch: 8 - Batch: 112, Training Loss: 0.009823080566194322\n",
      "Epoch: 8 - Batch: 113, Training Loss: 0.009907614149363283\n",
      "Epoch: 8 - Batch: 114, Training Loss: 0.010002735878648252\n",
      "Epoch: 8 - Batch: 115, Training Loss: 0.010091421918736564\n",
      "Epoch: 8 - Batch: 116, Training Loss: 0.010183786599592586\n",
      "Epoch: 8 - Batch: 117, Training Loss: 0.010267923201494548\n",
      "Epoch: 8 - Batch: 118, Training Loss: 0.010351078816462512\n",
      "Epoch: 8 - Batch: 119, Training Loss: 0.010441800446603231\n",
      "Epoch: 8 - Batch: 120, Training Loss: 0.010533032707165723\n",
      "Epoch: 8 - Batch: 121, Training Loss: 0.01061936677935507\n",
      "Epoch: 8 - Batch: 122, Training Loss: 0.010702872083554813\n",
      "Epoch: 8 - Batch: 123, Training Loss: 0.0107902173156762\n",
      "Epoch: 8 - Batch: 124, Training Loss: 0.010874517385540515\n",
      "Epoch: 8 - Batch: 125, Training Loss: 0.010956300493595415\n",
      "Epoch: 8 - Batch: 126, Training Loss: 0.011045242547000423\n",
      "Epoch: 8 - Batch: 127, Training Loss: 0.0111238743683592\n",
      "Epoch: 8 - Batch: 128, Training Loss: 0.011212660076952296\n",
      "Epoch: 8 - Batch: 129, Training Loss: 0.01129063793379276\n",
      "Epoch: 8 - Batch: 130, Training Loss: 0.01138355216601397\n",
      "Epoch: 8 - Batch: 131, Training Loss: 0.011466343134987612\n",
      "Epoch: 8 - Batch: 132, Training Loss: 0.011550167034909894\n",
      "Epoch: 8 - Batch: 133, Training Loss: 0.01163169047849293\n",
      "Epoch: 8 - Batch: 134, Training Loss: 0.0117209082651593\n",
      "Epoch: 8 - Batch: 135, Training Loss: 0.01180406226768818\n",
      "Epoch: 8 - Batch: 136, Training Loss: 0.0118933780548189\n",
      "Epoch: 8 - Batch: 137, Training Loss: 0.011972623888038679\n",
      "Epoch: 8 - Batch: 138, Training Loss: 0.012054515974735147\n",
      "Epoch: 8 - Batch: 139, Training Loss: 0.012153592059515404\n",
      "Epoch: 8 - Batch: 140, Training Loss: 0.012241868385628088\n",
      "Epoch: 8 - Batch: 141, Training Loss: 0.012326683311616604\n",
      "Epoch: 8 - Batch: 142, Training Loss: 0.012411744032629686\n",
      "Epoch: 8 - Batch: 143, Training Loss: 0.012505819554599758\n",
      "Epoch: 8 - Batch: 144, Training Loss: 0.012589366769859842\n",
      "Epoch: 8 - Batch: 145, Training Loss: 0.012675132077329392\n",
      "Epoch: 8 - Batch: 146, Training Loss: 0.012767849746430493\n",
      "Epoch: 8 - Batch: 147, Training Loss: 0.01286051607695385\n",
      "Epoch: 8 - Batch: 148, Training Loss: 0.012949949139682808\n",
      "Epoch: 8 - Batch: 149, Training Loss: 0.01303900819685724\n",
      "Epoch: 8 - Batch: 150, Training Loss: 0.013141740706577823\n",
      "Epoch: 8 - Batch: 151, Training Loss: 0.0132306795659943\n",
      "Epoch: 8 - Batch: 152, Training Loss: 0.013314010405520696\n",
      "Epoch: 8 - Batch: 153, Training Loss: 0.013401936375185428\n",
      "Epoch: 8 - Batch: 154, Training Loss: 0.013490517211345891\n",
      "Epoch: 8 - Batch: 155, Training Loss: 0.013567702643956316\n",
      "Epoch: 8 - Batch: 156, Training Loss: 0.013650224125553324\n",
      "Epoch: 8 - Batch: 157, Training Loss: 0.01373893295601628\n",
      "Epoch: 8 - Batch: 158, Training Loss: 0.01382671683952583\n",
      "Epoch: 8 - Batch: 159, Training Loss: 0.013911343239877947\n",
      "Epoch: 8 - Batch: 160, Training Loss: 0.013997763735095462\n",
      "Epoch: 8 - Batch: 161, Training Loss: 0.014079668696592895\n",
      "Epoch: 8 - Batch: 162, Training Loss: 0.014174579820328487\n",
      "Epoch: 8 - Batch: 163, Training Loss: 0.01425459051186568\n",
      "Epoch: 8 - Batch: 164, Training Loss: 0.014347723140002879\n",
      "Epoch: 8 - Batch: 165, Training Loss: 0.014438752789254211\n",
      "Epoch: 8 - Batch: 166, Training Loss: 0.01453252520974398\n",
      "Epoch: 8 - Batch: 167, Training Loss: 0.014621774355570475\n",
      "Epoch: 8 - Batch: 168, Training Loss: 0.014714006149205403\n",
      "Epoch: 8 - Batch: 169, Training Loss: 0.014801454824545292\n",
      "Epoch: 8 - Batch: 170, Training Loss: 0.014888091436023538\n",
      "Epoch: 8 - Batch: 171, Training Loss: 0.014971331478549078\n",
      "Epoch: 8 - Batch: 172, Training Loss: 0.015063595724837302\n",
      "Epoch: 8 - Batch: 173, Training Loss: 0.0151531807398717\n",
      "Epoch: 8 - Batch: 174, Training Loss: 0.015234521675119747\n",
      "Epoch: 8 - Batch: 175, Training Loss: 0.015320761330042707\n",
      "Epoch: 8 - Batch: 176, Training Loss: 0.015405474122533356\n",
      "Epoch: 8 - Batch: 177, Training Loss: 0.015488535900960118\n",
      "Epoch: 8 - Batch: 178, Training Loss: 0.015563962409646555\n",
      "Epoch: 8 - Batch: 179, Training Loss: 0.01564689083132973\n",
      "Epoch: 8 - Batch: 180, Training Loss: 0.015748566513877998\n",
      "Epoch: 8 - Batch: 181, Training Loss: 0.015838082305234463\n",
      "Epoch: 8 - Batch: 182, Training Loss: 0.015923085503564346\n",
      "Epoch: 8 - Batch: 183, Training Loss: 0.016006661483254993\n",
      "Epoch: 8 - Batch: 184, Training Loss: 0.016096305942308053\n",
      "Epoch: 8 - Batch: 185, Training Loss: 0.01618197552279058\n",
      "Epoch: 8 - Batch: 186, Training Loss: 0.01627201155124612\n",
      "Epoch: 8 - Batch: 187, Training Loss: 0.016358590813023138\n",
      "Epoch: 8 - Batch: 188, Training Loss: 0.01644522351387326\n",
      "Epoch: 8 - Batch: 189, Training Loss: 0.016531617901169048\n",
      "Epoch: 8 - Batch: 190, Training Loss: 0.016614863800369882\n",
      "Epoch: 8 - Batch: 191, Training Loss: 0.016698553985228785\n",
      "Epoch: 8 - Batch: 192, Training Loss: 0.01678350599942911\n",
      "Epoch: 8 - Batch: 193, Training Loss: 0.016865934095473628\n",
      "Epoch: 8 - Batch: 194, Training Loss: 0.016953043386364854\n",
      "Epoch: 8 - Batch: 195, Training Loss: 0.01703956565058251\n",
      "Epoch: 8 - Batch: 196, Training Loss: 0.01712651952952888\n",
      "Epoch: 8 - Batch: 197, Training Loss: 0.017211394968317514\n",
      "Epoch: 8 - Batch: 198, Training Loss: 0.017298713363521727\n",
      "Epoch: 8 - Batch: 199, Training Loss: 0.017380769659531847\n",
      "Epoch: 8 - Batch: 200, Training Loss: 0.017464024899759102\n",
      "Epoch: 8 - Batch: 201, Training Loss: 0.017551556668224225\n",
      "Epoch: 8 - Batch: 202, Training Loss: 0.017637510340704057\n",
      "Epoch: 8 - Batch: 203, Training Loss: 0.017721444627835384\n",
      "Epoch: 8 - Batch: 204, Training Loss: 0.017820455844376613\n",
      "Epoch: 8 - Batch: 205, Training Loss: 0.017907354943639604\n",
      "Epoch: 8 - Batch: 206, Training Loss: 0.01799066243677788\n",
      "Epoch: 8 - Batch: 207, Training Loss: 0.018081723747907785\n",
      "Epoch: 8 - Batch: 208, Training Loss: 0.018165679299465658\n",
      "Epoch: 8 - Batch: 209, Training Loss: 0.01825514757020359\n",
      "Epoch: 8 - Batch: 210, Training Loss: 0.018341636547747734\n",
      "Epoch: 8 - Batch: 211, Training Loss: 0.018421680365332323\n",
      "Epoch: 8 - Batch: 212, Training Loss: 0.018517805649520547\n",
      "Epoch: 8 - Batch: 213, Training Loss: 0.018609134291930382\n",
      "Epoch: 8 - Batch: 214, Training Loss: 0.01869649504918364\n",
      "Epoch: 8 - Batch: 215, Training Loss: 0.01877341347176637\n",
      "Epoch: 8 - Batch: 216, Training Loss: 0.018858381109945414\n",
      "Epoch: 8 - Batch: 217, Training Loss: 0.01894506821980326\n",
      "Epoch: 8 - Batch: 218, Training Loss: 0.01902824212761463\n",
      "Epoch: 8 - Batch: 219, Training Loss: 0.019117666182925253\n",
      "Epoch: 8 - Batch: 220, Training Loss: 0.019197445060087873\n",
      "Epoch: 8 - Batch: 221, Training Loss: 0.019285652798849157\n",
      "Epoch: 8 - Batch: 222, Training Loss: 0.019375687381669657\n",
      "Epoch: 8 - Batch: 223, Training Loss: 0.01946321834082627\n",
      "Epoch: 8 - Batch: 224, Training Loss: 0.019550189913357077\n",
      "Epoch: 8 - Batch: 225, Training Loss: 0.01963508444194177\n",
      "Epoch: 8 - Batch: 226, Training Loss: 0.019715778325772405\n",
      "Epoch: 8 - Batch: 227, Training Loss: 0.01980416444601309\n",
      "Epoch: 8 - Batch: 228, Training Loss: 0.019887048170390612\n",
      "Epoch: 8 - Batch: 229, Training Loss: 0.019971530069017886\n",
      "Epoch: 8 - Batch: 230, Training Loss: 0.020046988208693255\n",
      "Epoch: 8 - Batch: 231, Training Loss: 0.0201336754297538\n",
      "Epoch: 8 - Batch: 232, Training Loss: 0.02023078870664584\n",
      "Epoch: 8 - Batch: 233, Training Loss: 0.02032044974107845\n",
      "Epoch: 8 - Batch: 234, Training Loss: 0.02040277383789099\n",
      "Epoch: 8 - Batch: 235, Training Loss: 0.02048348645937581\n",
      "Epoch: 8 - Batch: 236, Training Loss: 0.02055972261635423\n",
      "Epoch: 8 - Batch: 237, Training Loss: 0.020645874013404546\n",
      "Epoch: 8 - Batch: 238, Training Loss: 0.02073165686618827\n",
      "Epoch: 8 - Batch: 239, Training Loss: 0.020822391296994824\n",
      "Epoch: 8 - Batch: 240, Training Loss: 0.020910146471675158\n",
      "Epoch: 8 - Batch: 241, Training Loss: 0.02099320453098955\n",
      "Epoch: 8 - Batch: 242, Training Loss: 0.02108227221255081\n",
      "Epoch: 8 - Batch: 243, Training Loss: 0.02116688174716078\n",
      "Epoch: 8 - Batch: 244, Training Loss: 0.021246797070030746\n",
      "Epoch: 8 - Batch: 245, Training Loss: 0.02133484454099614\n",
      "Epoch: 8 - Batch: 246, Training Loss: 0.021425057993352314\n",
      "Epoch: 8 - Batch: 247, Training Loss: 0.021505492731656996\n",
      "Epoch: 8 - Batch: 248, Training Loss: 0.021591237614898146\n",
      "Epoch: 8 - Batch: 249, Training Loss: 0.021673191777410398\n",
      "Epoch: 8 - Batch: 250, Training Loss: 0.021767158765549684\n",
      "Epoch: 8 - Batch: 251, Training Loss: 0.021853075778207574\n",
      "Epoch: 8 - Batch: 252, Training Loss: 0.02193767068348516\n",
      "Epoch: 8 - Batch: 253, Training Loss: 0.02201869931831882\n",
      "Epoch: 8 - Batch: 254, Training Loss: 0.022099384448026147\n",
      "Epoch: 8 - Batch: 255, Training Loss: 0.022187076478109233\n",
      "Epoch: 8 - Batch: 256, Training Loss: 0.022276318235134405\n",
      "Epoch: 8 - Batch: 257, Training Loss: 0.022365840136461194\n",
      "Epoch: 8 - Batch: 258, Training Loss: 0.022451562136658783\n",
      "Epoch: 8 - Batch: 259, Training Loss: 0.022536753811201647\n",
      "Epoch: 8 - Batch: 260, Training Loss: 0.02262528254272135\n",
      "Epoch: 8 - Batch: 261, Training Loss: 0.022707947982385582\n",
      "Epoch: 8 - Batch: 262, Training Loss: 0.022797003240134586\n",
      "Epoch: 8 - Batch: 263, Training Loss: 0.022885278849607678\n",
      "Epoch: 8 - Batch: 264, Training Loss: 0.022963099867402026\n",
      "Epoch: 8 - Batch: 265, Training Loss: 0.023061730097330625\n",
      "Epoch: 8 - Batch: 266, Training Loss: 0.02314734977251459\n",
      "Epoch: 8 - Batch: 267, Training Loss: 0.023242061472749632\n",
      "Epoch: 8 - Batch: 268, Training Loss: 0.02332437147770948\n",
      "Epoch: 8 - Batch: 269, Training Loss: 0.02340581968031317\n",
      "Epoch: 8 - Batch: 270, Training Loss: 0.023495409106387824\n",
      "Epoch: 8 - Batch: 271, Training Loss: 0.023580584940724507\n",
      "Epoch: 8 - Batch: 272, Training Loss: 0.023663944143116178\n",
      "Epoch: 8 - Batch: 273, Training Loss: 0.023751408559815404\n",
      "Epoch: 8 - Batch: 274, Training Loss: 0.02383927010605189\n",
      "Epoch: 8 - Batch: 275, Training Loss: 0.02392976941729264\n",
      "Epoch: 8 - Batch: 276, Training Loss: 0.024013100479224427\n",
      "Epoch: 8 - Batch: 277, Training Loss: 0.024098973097295112\n",
      "Epoch: 8 - Batch: 278, Training Loss: 0.024175864905366058\n",
      "Epoch: 8 - Batch: 279, Training Loss: 0.024263954824871488\n",
      "Epoch: 8 - Batch: 280, Training Loss: 0.024349871584234347\n",
      "Epoch: 8 - Batch: 281, Training Loss: 0.024436103924491116\n",
      "Epoch: 8 - Batch: 282, Training Loss: 0.02452572549432269\n",
      "Epoch: 8 - Batch: 283, Training Loss: 0.024613545148032618\n",
      "Epoch: 8 - Batch: 284, Training Loss: 0.02469815579466954\n",
      "Epoch: 8 - Batch: 285, Training Loss: 0.02478839573797895\n",
      "Epoch: 8 - Batch: 286, Training Loss: 0.024881270690936947\n",
      "Epoch: 8 - Batch: 287, Training Loss: 0.02496230927223392\n",
      "Epoch: 8 - Batch: 288, Training Loss: 0.025052583669845144\n",
      "Epoch: 8 - Batch: 289, Training Loss: 0.025147561960008804\n",
      "Epoch: 8 - Batch: 290, Training Loss: 0.025231312910962856\n",
      "Epoch: 8 - Batch: 291, Training Loss: 0.025316235858014173\n",
      "Epoch: 8 - Batch: 292, Training Loss: 0.025401517835323687\n",
      "Epoch: 8 - Batch: 293, Training Loss: 0.025490257963167492\n",
      "Epoch: 8 - Batch: 294, Training Loss: 0.02557424852504066\n",
      "Epoch: 8 - Batch: 295, Training Loss: 0.025655778282465628\n",
      "Epoch: 8 - Batch: 296, Training Loss: 0.02573807011814062\n",
      "Epoch: 8 - Batch: 297, Training Loss: 0.025820423127653983\n",
      "Epoch: 8 - Batch: 298, Training Loss: 0.025910642405795815\n",
      "Epoch: 8 - Batch: 299, Training Loss: 0.02599755100671134\n",
      "Epoch: 8 - Batch: 300, Training Loss: 0.02608566247482798\n",
      "Epoch: 8 - Batch: 301, Training Loss: 0.026167063842563093\n",
      "Epoch: 8 - Batch: 302, Training Loss: 0.026251829240106626\n",
      "Epoch: 8 - Batch: 303, Training Loss: 0.026347206247534918\n",
      "Epoch: 8 - Batch: 304, Training Loss: 0.026426238664248293\n",
      "Epoch: 8 - Batch: 305, Training Loss: 0.026521642452992412\n",
      "Epoch: 8 - Batch: 306, Training Loss: 0.02660622861650255\n",
      "Epoch: 8 - Batch: 307, Training Loss: 0.02669442992404128\n",
      "Epoch: 8 - Batch: 308, Training Loss: 0.026776856870991277\n",
      "Epoch: 8 - Batch: 309, Training Loss: 0.02686639077603125\n",
      "Epoch: 8 - Batch: 310, Training Loss: 0.026952183964487727\n",
      "Epoch: 8 - Batch: 311, Training Loss: 0.02703860711621408\n",
      "Epoch: 8 - Batch: 312, Training Loss: 0.027128924617225655\n",
      "Epoch: 8 - Batch: 313, Training Loss: 0.027213445683913446\n",
      "Epoch: 8 - Batch: 314, Training Loss: 0.027293727839764078\n",
      "Epoch: 8 - Batch: 315, Training Loss: 0.027378244050600832\n",
      "Epoch: 8 - Batch: 316, Training Loss: 0.0274696106749091\n",
      "Epoch: 8 - Batch: 317, Training Loss: 0.027549687402313623\n",
      "Epoch: 8 - Batch: 318, Training Loss: 0.027627710703988968\n",
      "Epoch: 8 - Batch: 319, Training Loss: 0.027717490033377857\n",
      "Epoch: 8 - Batch: 320, Training Loss: 0.027808072301829436\n",
      "Epoch: 8 - Batch: 321, Training Loss: 0.027889146362964193\n",
      "Epoch: 8 - Batch: 322, Training Loss: 0.027968107801468217\n",
      "Epoch: 8 - Batch: 323, Training Loss: 0.02805541508258081\n",
      "Epoch: 8 - Batch: 324, Training Loss: 0.028141177498780277\n",
      "Epoch: 8 - Batch: 325, Training Loss: 0.028233631969634968\n",
      "Epoch: 8 - Batch: 326, Training Loss: 0.028316495263606162\n",
      "Epoch: 8 - Batch: 327, Training Loss: 0.02840072208796172\n",
      "Epoch: 8 - Batch: 328, Training Loss: 0.028487113559275717\n",
      "Epoch: 8 - Batch: 329, Training Loss: 0.02856418007913711\n",
      "Epoch: 8 - Batch: 330, Training Loss: 0.028647563980883035\n",
      "Epoch: 8 - Batch: 331, Training Loss: 0.02872334087690706\n",
      "Epoch: 8 - Batch: 332, Training Loss: 0.028807155967104692\n",
      "Epoch: 8 - Batch: 333, Training Loss: 0.028897554899378994\n",
      "Epoch: 8 - Batch: 334, Training Loss: 0.02897727700756557\n",
      "Epoch: 8 - Batch: 335, Training Loss: 0.029062402213786174\n",
      "Epoch: 8 - Batch: 336, Training Loss: 0.02914546758158883\n",
      "Epoch: 8 - Batch: 337, Training Loss: 0.029226610310388045\n",
      "Epoch: 8 - Batch: 338, Training Loss: 0.02931337778272716\n",
      "Epoch: 8 - Batch: 339, Training Loss: 0.029399436134010998\n",
      "Epoch: 8 - Batch: 340, Training Loss: 0.029487342952100395\n",
      "Epoch: 8 - Batch: 341, Training Loss: 0.029567907297146655\n",
      "Epoch: 8 - Batch: 342, Training Loss: 0.02965311255695215\n",
      "Epoch: 8 - Batch: 343, Training Loss: 0.02974754147491052\n",
      "Epoch: 8 - Batch: 344, Training Loss: 0.0298371771860182\n",
      "Epoch: 8 - Batch: 345, Training Loss: 0.029925996564316316\n",
      "Epoch: 8 - Batch: 346, Training Loss: 0.03001872524248427\n",
      "Epoch: 8 - Batch: 347, Training Loss: 0.030106424253625457\n",
      "Epoch: 8 - Batch: 348, Training Loss: 0.030193456839368513\n",
      "Epoch: 8 - Batch: 349, Training Loss: 0.03027686188198243\n",
      "Epoch: 8 - Batch: 350, Training Loss: 0.030378232893856803\n",
      "Epoch: 8 - Batch: 351, Training Loss: 0.03045921883921125\n",
      "Epoch: 8 - Batch: 352, Training Loss: 0.03053813438806961\n",
      "Epoch: 8 - Batch: 353, Training Loss: 0.030627108010684276\n",
      "Epoch: 8 - Batch: 354, Training Loss: 0.030712229312454684\n",
      "Epoch: 8 - Batch: 355, Training Loss: 0.03080810997542457\n",
      "Epoch: 8 - Batch: 356, Training Loss: 0.03089855437724547\n",
      "Epoch: 8 - Batch: 357, Training Loss: 0.030983557371703743\n",
      "Epoch: 8 - Batch: 358, Training Loss: 0.031057751476517563\n",
      "Epoch: 8 - Batch: 359, Training Loss: 0.03114489310268146\n",
      "Epoch: 8 - Batch: 360, Training Loss: 0.03122596123918372\n",
      "Epoch: 8 - Batch: 361, Training Loss: 0.0313128924080685\n",
      "Epoch: 8 - Batch: 362, Training Loss: 0.031401903030291126\n",
      "Epoch: 8 - Batch: 363, Training Loss: 0.031485356619059546\n",
      "Epoch: 8 - Batch: 364, Training Loss: 0.03157182511677394\n",
      "Epoch: 8 - Batch: 365, Training Loss: 0.03165773236212841\n",
      "Epoch: 8 - Batch: 366, Training Loss: 0.031734315551829775\n",
      "Epoch: 8 - Batch: 367, Training Loss: 0.031817896269990834\n",
      "Epoch: 8 - Batch: 368, Training Loss: 0.03190901611080613\n",
      "Epoch: 8 - Batch: 369, Training Loss: 0.03199900704656865\n",
      "Epoch: 8 - Batch: 370, Training Loss: 0.032091536770758544\n",
      "Epoch: 8 - Batch: 371, Training Loss: 0.03217436929247273\n",
      "Epoch: 8 - Batch: 372, Training Loss: 0.032259722124433045\n",
      "Epoch: 8 - Batch: 373, Training Loss: 0.03234786222106584\n",
      "Epoch: 8 - Batch: 374, Training Loss: 0.0324332114265827\n",
      "Epoch: 8 - Batch: 375, Training Loss: 0.03251976824394901\n",
      "Epoch: 8 - Batch: 376, Training Loss: 0.03260434606725699\n",
      "Epoch: 8 - Batch: 377, Training Loss: 0.032704159339417276\n",
      "Epoch: 8 - Batch: 378, Training Loss: 0.032797775402739274\n",
      "Epoch: 8 - Batch: 379, Training Loss: 0.03289878137940989\n",
      "Epoch: 8 - Batch: 380, Training Loss: 0.032980893647443395\n",
      "Epoch: 8 - Batch: 381, Training Loss: 0.03306535079111507\n",
      "Epoch: 8 - Batch: 382, Training Loss: 0.033149641019540835\n",
      "Epoch: 8 - Batch: 383, Training Loss: 0.03323896310197971\n",
      "Epoch: 8 - Batch: 384, Training Loss: 0.033325288358266474\n",
      "Epoch: 8 - Batch: 385, Training Loss: 0.03341474111005046\n",
      "Epoch: 8 - Batch: 386, Training Loss: 0.033505695030266174\n",
      "Epoch: 8 - Batch: 387, Training Loss: 0.03358980339286142\n",
      "Epoch: 8 - Batch: 388, Training Loss: 0.033676738478552244\n",
      "Epoch: 8 - Batch: 389, Training Loss: 0.03375705001044827\n",
      "Epoch: 8 - Batch: 390, Training Loss: 0.0338525812374814\n",
      "Epoch: 8 - Batch: 391, Training Loss: 0.03393815960704193\n",
      "Epoch: 8 - Batch: 392, Training Loss: 0.034023656712143774\n",
      "Epoch: 8 - Batch: 393, Training Loss: 0.034109403973887016\n",
      "Epoch: 8 - Batch: 394, Training Loss: 0.034201765318662175\n",
      "Epoch: 8 - Batch: 395, Training Loss: 0.034292074121151796\n",
      "Epoch: 8 - Batch: 396, Training Loss: 0.034376888237831804\n",
      "Epoch: 8 - Batch: 397, Training Loss: 0.034462154739432864\n",
      "Epoch: 8 - Batch: 398, Training Loss: 0.03455178731833129\n",
      "Epoch: 8 - Batch: 399, Training Loss: 0.034645566664277816\n",
      "Epoch: 8 - Batch: 400, Training Loss: 0.034733650189628255\n",
      "Epoch: 8 - Batch: 401, Training Loss: 0.03481831979222756\n",
      "Epoch: 8 - Batch: 402, Training Loss: 0.03490260898893943\n",
      "Epoch: 8 - Batch: 403, Training Loss: 0.03498148698835428\n",
      "Epoch: 8 - Batch: 404, Training Loss: 0.03507581849275141\n",
      "Epoch: 8 - Batch: 405, Training Loss: 0.035159757160033354\n",
      "Epoch: 8 - Batch: 406, Training Loss: 0.035247896329977026\n",
      "Epoch: 8 - Batch: 407, Training Loss: 0.03533820282192175\n",
      "Epoch: 8 - Batch: 408, Training Loss: 0.03542775845473283\n",
      "Epoch: 8 - Batch: 409, Training Loss: 0.03552300968573461\n",
      "Epoch: 8 - Batch: 410, Training Loss: 0.03561409333625045\n",
      "Epoch: 8 - Batch: 411, Training Loss: 0.035694693185846206\n",
      "Epoch: 8 - Batch: 412, Training Loss: 0.03578226931183097\n",
      "Epoch: 8 - Batch: 413, Training Loss: 0.0358722096380112\n",
      "Epoch: 8 - Batch: 414, Training Loss: 0.035969967309811814\n",
      "Epoch: 8 - Batch: 415, Training Loss: 0.0360528814357707\n",
      "Epoch: 8 - Batch: 416, Training Loss: 0.03613157899645628\n",
      "Epoch: 8 - Batch: 417, Training Loss: 0.036214899976010344\n",
      "Epoch: 8 - Batch: 418, Training Loss: 0.036290772852316425\n",
      "Epoch: 8 - Batch: 419, Training Loss: 0.03637475662464724\n",
      "Epoch: 8 - Batch: 420, Training Loss: 0.036455943043749925\n",
      "Epoch: 8 - Batch: 421, Training Loss: 0.036540546320119306\n",
      "Epoch: 8 - Batch: 422, Training Loss: 0.03663304130582272\n",
      "Epoch: 8 - Batch: 423, Training Loss: 0.03671904981358728\n",
      "Epoch: 8 - Batch: 424, Training Loss: 0.03680843349614151\n",
      "Epoch: 8 - Batch: 425, Training Loss: 0.03689234518441395\n",
      "Epoch: 8 - Batch: 426, Training Loss: 0.03697382274452924\n",
      "Epoch: 8 - Batch: 427, Training Loss: 0.037057143983556265\n",
      "Epoch: 8 - Batch: 428, Training Loss: 0.0371420710883528\n",
      "Epoch: 8 - Batch: 429, Training Loss: 0.037232337318743834\n",
      "Epoch: 8 - Batch: 430, Training Loss: 0.03732807320198214\n",
      "Epoch: 8 - Batch: 431, Training Loss: 0.03740695471045983\n",
      "Epoch: 8 - Batch: 432, Training Loss: 0.037490645420442566\n",
      "Epoch: 8 - Batch: 433, Training Loss: 0.03756901902296452\n",
      "Epoch: 8 - Batch: 434, Training Loss: 0.03765386274427324\n",
      "Epoch: 8 - Batch: 435, Training Loss: 0.037734892991545976\n",
      "Epoch: 8 - Batch: 436, Training Loss: 0.03782606451072503\n",
      "Epoch: 8 - Batch: 437, Training Loss: 0.03791706148491768\n",
      "Epoch: 8 - Batch: 438, Training Loss: 0.038009126777227835\n",
      "Epoch: 8 - Batch: 439, Training Loss: 0.03809078957632209\n",
      "Epoch: 8 - Batch: 440, Training Loss: 0.03817591535708995\n",
      "Epoch: 8 - Batch: 441, Training Loss: 0.038263086278986176\n",
      "Epoch: 8 - Batch: 442, Training Loss: 0.03834944548916263\n",
      "Epoch: 8 - Batch: 443, Training Loss: 0.03843435945968525\n",
      "Epoch: 8 - Batch: 444, Training Loss: 0.03851919153560058\n",
      "Epoch: 8 - Batch: 445, Training Loss: 0.03860928923212869\n",
      "Epoch: 8 - Batch: 446, Training Loss: 0.03869747457590269\n",
      "Epoch: 8 - Batch: 447, Training Loss: 0.03878096904401755\n",
      "Epoch: 8 - Batch: 448, Training Loss: 0.03887005986809533\n",
      "Epoch: 8 - Batch: 449, Training Loss: 0.03895851890641461\n",
      "Epoch: 8 - Batch: 450, Training Loss: 0.039032310574851424\n",
      "Epoch: 8 - Batch: 451, Training Loss: 0.03911357277859107\n",
      "Epoch: 8 - Batch: 452, Training Loss: 0.03920032210635704\n",
      "Epoch: 8 - Batch: 453, Training Loss: 0.03929304093073059\n",
      "Epoch: 8 - Batch: 454, Training Loss: 0.039375676710164766\n",
      "Epoch: 8 - Batch: 455, Training Loss: 0.03946335165210031\n",
      "Epoch: 8 - Batch: 456, Training Loss: 0.03954185473780529\n",
      "Epoch: 8 - Batch: 457, Training Loss: 0.03962368038089121\n",
      "Epoch: 8 - Batch: 458, Training Loss: 0.039693338873720486\n",
      "Epoch: 8 - Batch: 459, Training Loss: 0.039780625699715036\n",
      "Epoch: 8 - Batch: 460, Training Loss: 0.0398728040851665\n",
      "Epoch: 8 - Batch: 461, Training Loss: 0.03996035945331477\n",
      "Epoch: 8 - Batch: 462, Training Loss: 0.040047379868540595\n",
      "Epoch: 8 - Batch: 463, Training Loss: 0.040134282668382176\n",
      "Epoch: 8 - Batch: 464, Training Loss: 0.04021748512439664\n",
      "Epoch: 8 - Batch: 465, Training Loss: 0.040303536457554814\n",
      "Epoch: 8 - Batch: 466, Training Loss: 0.040393545123969345\n",
      "Epoch: 8 - Batch: 467, Training Loss: 0.04047687209611311\n",
      "Epoch: 8 - Batch: 468, Training Loss: 0.04056337585743782\n",
      "Epoch: 8 - Batch: 469, Training Loss: 0.040646404392436566\n",
      "Epoch: 8 - Batch: 470, Training Loss: 0.0407293041670698\n",
      "Epoch: 8 - Batch: 471, Training Loss: 0.040812862385218814\n",
      "Epoch: 8 - Batch: 472, Training Loss: 0.040895520250743896\n",
      "Epoch: 8 - Batch: 473, Training Loss: 0.04097464054229841\n",
      "Epoch: 8 - Batch: 474, Training Loss: 0.04105426616361287\n",
      "Epoch: 8 - Batch: 475, Training Loss: 0.041141706597597444\n",
      "Epoch: 8 - Batch: 476, Training Loss: 0.041239983017616604\n",
      "Epoch: 8 - Batch: 477, Training Loss: 0.04132796407586109\n",
      "Epoch: 8 - Batch: 478, Training Loss: 0.04141129389602944\n",
      "Epoch: 8 - Batch: 479, Training Loss: 0.04149186545689505\n",
      "Epoch: 8 - Batch: 480, Training Loss: 0.04158545119277083\n",
      "Epoch: 8 - Batch: 481, Training Loss: 0.04167592114649997\n",
      "Epoch: 8 - Batch: 482, Training Loss: 0.04175730362213271\n",
      "Epoch: 8 - Batch: 483, Training Loss: 0.04184359957033129\n",
      "Epoch: 8 - Batch: 484, Training Loss: 0.04192796124031097\n",
      "Epoch: 8 - Batch: 485, Training Loss: 0.042015421913186114\n",
      "Epoch: 8 - Batch: 486, Training Loss: 0.042105783493710594\n",
      "Epoch: 8 - Batch: 487, Training Loss: 0.042187057219285076\n",
      "Epoch: 8 - Batch: 488, Training Loss: 0.04226796124532053\n",
      "Epoch: 8 - Batch: 489, Training Loss: 0.04234976549424342\n",
      "Epoch: 8 - Batch: 490, Training Loss: 0.04243696999935368\n",
      "Epoch: 8 - Batch: 491, Training Loss: 0.04252558991708368\n",
      "Epoch: 8 - Batch: 492, Training Loss: 0.042613114506797015\n",
      "Epoch: 8 - Batch: 493, Training Loss: 0.042694377458065896\n",
      "Epoch: 8 - Batch: 494, Training Loss: 0.042780184478902104\n",
      "Epoch: 8 - Batch: 495, Training Loss: 0.04286682122011683\n",
      "Epoch: 8 - Batch: 496, Training Loss: 0.04295594576950097\n",
      "Epoch: 8 - Batch: 497, Training Loss: 0.043038256404609426\n",
      "Epoch: 8 - Batch: 498, Training Loss: 0.043121330254706575\n",
      "Epoch: 8 - Batch: 499, Training Loss: 0.04321859999996908\n",
      "Epoch: 8 - Batch: 500, Training Loss: 0.04330258158531355\n",
      "Epoch: 8 - Batch: 501, Training Loss: 0.04339005497236355\n",
      "Epoch: 8 - Batch: 502, Training Loss: 0.043475664602237354\n",
      "Epoch: 8 - Batch: 503, Training Loss: 0.04355491399394339\n",
      "Epoch: 8 - Batch: 504, Training Loss: 0.04363577570117528\n",
      "Epoch: 8 - Batch: 505, Training Loss: 0.043715429981895544\n",
      "Epoch: 8 - Batch: 506, Training Loss: 0.043804689426327224\n",
      "Epoch: 8 - Batch: 507, Training Loss: 0.0438846781862217\n",
      "Epoch: 8 - Batch: 508, Training Loss: 0.04396722374320228\n",
      "Epoch: 8 - Batch: 509, Training Loss: 0.04405242650712505\n",
      "Epoch: 8 - Batch: 510, Training Loss: 0.04413934056932851\n",
      "Epoch: 8 - Batch: 511, Training Loss: 0.044229674658075493\n",
      "Epoch: 8 - Batch: 512, Training Loss: 0.04431665022154748\n",
      "Epoch: 8 - Batch: 513, Training Loss: 0.04439902101113626\n",
      "Epoch: 8 - Batch: 514, Training Loss: 0.04448835346631545\n",
      "Epoch: 8 - Batch: 515, Training Loss: 0.04457160480209251\n",
      "Epoch: 8 - Batch: 516, Training Loss: 0.04465708687629668\n",
      "Epoch: 8 - Batch: 517, Training Loss: 0.044735581195522504\n",
      "Epoch: 8 - Batch: 518, Training Loss: 0.04481834011958606\n",
      "Epoch: 8 - Batch: 519, Training Loss: 0.04491166124247002\n",
      "Epoch: 8 - Batch: 520, Training Loss: 0.04499639340894139\n",
      "Epoch: 8 - Batch: 521, Training Loss: 0.04507333785913279\n",
      "Epoch: 8 - Batch: 522, Training Loss: 0.0451592529010318\n",
      "Epoch: 8 - Batch: 523, Training Loss: 0.04524761493353306\n",
      "Epoch: 8 - Batch: 524, Training Loss: 0.04533249031672035\n",
      "Epoch: 8 - Batch: 525, Training Loss: 0.04542476748723296\n",
      "Epoch: 8 - Batch: 526, Training Loss: 0.04550441740015846\n",
      "Epoch: 8 - Batch: 527, Training Loss: 0.04559830589772852\n",
      "Epoch: 8 - Batch: 528, Training Loss: 0.04569041061139423\n",
      "Epoch: 8 - Batch: 529, Training Loss: 0.045781097583361526\n",
      "Epoch: 8 - Batch: 530, Training Loss: 0.04586321496671903\n",
      "Epoch: 8 - Batch: 531, Training Loss: 0.045939587847411534\n",
      "Epoch: 8 - Batch: 532, Training Loss: 0.046016743094668064\n",
      "Epoch: 8 - Batch: 533, Training Loss: 0.046098230681559736\n",
      "Epoch: 8 - Batch: 534, Training Loss: 0.046187628449788735\n",
      "Epoch: 8 - Batch: 535, Training Loss: 0.04627621134937699\n",
      "Epoch: 8 - Batch: 536, Training Loss: 0.04636342476305875\n",
      "Epoch: 8 - Batch: 537, Training Loss: 0.04645794463246616\n",
      "Epoch: 8 - Batch: 538, Training Loss: 0.046536039425118844\n",
      "Epoch: 8 - Batch: 539, Training Loss: 0.046610469699746145\n",
      "Epoch: 8 - Batch: 540, Training Loss: 0.04669428090402736\n",
      "Epoch: 8 - Batch: 541, Training Loss: 0.04678130712032714\n",
      "Epoch: 8 - Batch: 542, Training Loss: 0.04687165512176691\n",
      "Epoch: 8 - Batch: 543, Training Loss: 0.046948968647526665\n",
      "Epoch: 8 - Batch: 544, Training Loss: 0.04703264360997214\n",
      "Epoch: 8 - Batch: 545, Training Loss: 0.04711565672486378\n",
      "Epoch: 8 - Batch: 546, Training Loss: 0.047202497010256715\n",
      "Epoch: 8 - Batch: 547, Training Loss: 0.04728651515657629\n",
      "Epoch: 8 - Batch: 548, Training Loss: 0.04736892378597117\n",
      "Epoch: 8 - Batch: 549, Training Loss: 0.047452928983354646\n",
      "Epoch: 8 - Batch: 550, Training Loss: 0.0475442411121642\n",
      "Epoch: 8 - Batch: 551, Training Loss: 0.04763418896923809\n",
      "Epoch: 8 - Batch: 552, Training Loss: 0.04771190878640162\n",
      "Epoch: 8 - Batch: 553, Training Loss: 0.04779223130265279\n",
      "Epoch: 8 - Batch: 554, Training Loss: 0.04787749340556945\n",
      "Epoch: 8 - Batch: 555, Training Loss: 0.04796744744674879\n",
      "Epoch: 8 - Batch: 556, Training Loss: 0.04805236676735664\n",
      "Epoch: 8 - Batch: 557, Training Loss: 0.04813057041509235\n",
      "Epoch: 8 - Batch: 558, Training Loss: 0.048225987208373906\n",
      "Epoch: 8 - Batch: 559, Training Loss: 0.048308719239918946\n",
      "Epoch: 8 - Batch: 560, Training Loss: 0.04839554358625887\n",
      "Epoch: 8 - Batch: 561, Training Loss: 0.04847569043314081\n",
      "Epoch: 8 - Batch: 562, Training Loss: 0.04856978789910946\n",
      "Epoch: 8 - Batch: 563, Training Loss: 0.04865544086714487\n",
      "Epoch: 8 - Batch: 564, Training Loss: 0.04874449648321367\n",
      "Epoch: 8 - Batch: 565, Training Loss: 0.04882366092841621\n",
      "Epoch: 8 - Batch: 566, Training Loss: 0.0489112635885997\n",
      "Epoch: 8 - Batch: 567, Training Loss: 0.049003427517700746\n",
      "Epoch: 8 - Batch: 568, Training Loss: 0.04909138936259062\n",
      "Epoch: 8 - Batch: 569, Training Loss: 0.04918761653315962\n",
      "Epoch: 8 - Batch: 570, Training Loss: 0.04927442810354541\n",
      "Epoch: 8 - Batch: 571, Training Loss: 0.049354666019257025\n",
      "Epoch: 8 - Batch: 572, Training Loss: 0.04944315668856524\n",
      "Epoch: 8 - Batch: 573, Training Loss: 0.04952757873543064\n",
      "Epoch: 8 - Batch: 574, Training Loss: 0.04961570347991354\n",
      "Epoch: 8 - Batch: 575, Training Loss: 0.049712434373981325\n",
      "Epoch: 8 - Batch: 576, Training Loss: 0.0497948041751017\n",
      "Epoch: 8 - Batch: 577, Training Loss: 0.049877032106294364\n",
      "Epoch: 8 - Batch: 578, Training Loss: 0.04996064154564049\n",
      "Epoch: 8 - Batch: 579, Training Loss: 0.05004186377818905\n",
      "Epoch: 8 - Batch: 580, Training Loss: 0.050126741842596884\n",
      "Epoch: 8 - Batch: 581, Training Loss: 0.05021196574738765\n",
      "Epoch: 8 - Batch: 582, Training Loss: 0.05030442893529808\n",
      "Epoch: 8 - Batch: 583, Training Loss: 0.05038953940716154\n",
      "Epoch: 8 - Batch: 584, Training Loss: 0.05047600217175919\n",
      "Epoch: 8 - Batch: 585, Training Loss: 0.05056561330707117\n",
      "Epoch: 8 - Batch: 586, Training Loss: 0.05065247242896513\n",
      "Epoch: 8 - Batch: 587, Training Loss: 0.050731065471472825\n",
      "Epoch: 8 - Batch: 588, Training Loss: 0.050823995840440735\n",
      "Epoch: 8 - Batch: 589, Training Loss: 0.05090811842462514\n",
      "Epoch: 8 - Batch: 590, Training Loss: 0.05099907240044221\n",
      "Epoch: 8 - Batch: 591, Training Loss: 0.05108848401958472\n",
      "Epoch: 8 - Batch: 592, Training Loss: 0.051175898487739895\n",
      "Epoch: 8 - Batch: 593, Training Loss: 0.051259071240278814\n",
      "Epoch: 8 - Batch: 594, Training Loss: 0.05134145726463688\n",
      "Epoch: 8 - Batch: 595, Training Loss: 0.05142071270453396\n",
      "Epoch: 8 - Batch: 596, Training Loss: 0.05151134874566673\n",
      "Epoch: 8 - Batch: 597, Training Loss: 0.051596282948902\n",
      "Epoch: 8 - Batch: 598, Training Loss: 0.05167834875274258\n",
      "Epoch: 8 - Batch: 599, Training Loss: 0.051771863077954075\n",
      "Epoch: 8 - Batch: 600, Training Loss: 0.05185682978326606\n",
      "Epoch: 8 - Batch: 601, Training Loss: 0.05194108668795075\n",
      "Epoch: 8 - Batch: 602, Training Loss: 0.05202850932950404\n",
      "Epoch: 8 - Batch: 603, Training Loss: 0.052109617857296474\n",
      "Epoch: 8 - Batch: 604, Training Loss: 0.05219572936346875\n",
      "Epoch: 8 - Batch: 605, Training Loss: 0.05228058443363033\n",
      "Epoch: 8 - Batch: 606, Training Loss: 0.05236621042883416\n",
      "Epoch: 8 - Batch: 607, Training Loss: 0.05245281110330798\n",
      "Epoch: 8 - Batch: 608, Training Loss: 0.05253566135567417\n",
      "Epoch: 8 - Batch: 609, Training Loss: 0.05261747023804271\n",
      "Epoch: 8 - Batch: 610, Training Loss: 0.05270201444106908\n",
      "Epoch: 8 - Batch: 611, Training Loss: 0.052794053193002594\n",
      "Epoch: 8 - Batch: 612, Training Loss: 0.0528839949957075\n",
      "Epoch: 8 - Batch: 613, Training Loss: 0.0529711964798112\n",
      "Epoch: 8 - Batch: 614, Training Loss: 0.053052640981836305\n",
      "Epoch: 8 - Batch: 615, Training Loss: 0.053142044081616754\n",
      "Epoch: 8 - Batch: 616, Training Loss: 0.053226285417924075\n",
      "Epoch: 8 - Batch: 617, Training Loss: 0.05331245833269597\n",
      "Epoch: 8 - Batch: 618, Training Loss: 0.05340085938498748\n",
      "Epoch: 8 - Batch: 619, Training Loss: 0.05348499745105827\n",
      "Epoch: 8 - Batch: 620, Training Loss: 0.05356987558960124\n",
      "Epoch: 8 - Batch: 621, Training Loss: 0.05366497328674813\n",
      "Epoch: 8 - Batch: 622, Training Loss: 0.05374682128157584\n",
      "Epoch: 8 - Batch: 623, Training Loss: 0.053832577643406335\n",
      "Epoch: 8 - Batch: 624, Training Loss: 0.05391151012596404\n",
      "Epoch: 8 - Batch: 625, Training Loss: 0.05400151714904985\n",
      "Epoch: 8 - Batch: 626, Training Loss: 0.05408930041476664\n",
      "Epoch: 8 - Batch: 627, Training Loss: 0.054176299781793384\n",
      "Epoch: 8 - Batch: 628, Training Loss: 0.054256772996230704\n",
      "Epoch: 8 - Batch: 629, Training Loss: 0.05434773465738961\n",
      "Epoch: 8 - Batch: 630, Training Loss: 0.05443561118510035\n",
      "Epoch: 8 - Batch: 631, Training Loss: 0.05452317354666258\n",
      "Epoch: 8 - Batch: 632, Training Loss: 0.0546013472376277\n",
      "Epoch: 8 - Batch: 633, Training Loss: 0.05468603522573933\n",
      "Epoch: 8 - Batch: 634, Training Loss: 0.05477130582948427\n",
      "Epoch: 8 - Batch: 635, Training Loss: 0.054865351919807605\n",
      "Epoch: 8 - Batch: 636, Training Loss: 0.05495925025881622\n",
      "Epoch: 8 - Batch: 637, Training Loss: 0.0550439558540213\n",
      "Epoch: 8 - Batch: 638, Training Loss: 0.05513027645832864\n",
      "Epoch: 8 - Batch: 639, Training Loss: 0.055221027334778267\n",
      "Epoch: 8 - Batch: 640, Training Loss: 0.05530921320367611\n",
      "Epoch: 8 - Batch: 641, Training Loss: 0.0553864104114164\n",
      "Epoch: 8 - Batch: 642, Training Loss: 0.055476147187240483\n",
      "Epoch: 8 - Batch: 643, Training Loss: 0.05557161109339736\n",
      "Epoch: 8 - Batch: 644, Training Loss: 0.05565299366169901\n",
      "Epoch: 8 - Batch: 645, Training Loss: 0.05575074215927725\n",
      "Epoch: 8 - Batch: 646, Training Loss: 0.055834931465078945\n",
      "Epoch: 8 - Batch: 647, Training Loss: 0.055915501469106815\n",
      "Epoch: 8 - Batch: 648, Training Loss: 0.05600289052744608\n",
      "Epoch: 8 - Batch: 649, Training Loss: 0.05610325719364247\n",
      "Epoch: 8 - Batch: 650, Training Loss: 0.056192783098266294\n",
      "Epoch: 8 - Batch: 651, Training Loss: 0.0562734912716433\n",
      "Epoch: 8 - Batch: 652, Training Loss: 0.05635101773203111\n",
      "Epoch: 8 - Batch: 653, Training Loss: 0.0564368180374601\n",
      "Epoch: 8 - Batch: 654, Training Loss: 0.05653087530365433\n",
      "Epoch: 8 - Batch: 655, Training Loss: 0.05662047985329557\n",
      "Epoch: 8 - Batch: 656, Training Loss: 0.05670365510171721\n",
      "Epoch: 8 - Batch: 657, Training Loss: 0.056792288505862994\n",
      "Epoch: 8 - Batch: 658, Training Loss: 0.05688229617145327\n",
      "Epoch: 8 - Batch: 659, Training Loss: 0.0569606101384408\n",
      "Epoch: 8 - Batch: 660, Training Loss: 0.05704832668254031\n",
      "Epoch: 8 - Batch: 661, Training Loss: 0.057128411682189795\n",
      "Epoch: 8 - Batch: 662, Training Loss: 0.05721327610573365\n",
      "Epoch: 8 - Batch: 663, Training Loss: 0.05729535234409383\n",
      "Epoch: 8 - Batch: 664, Training Loss: 0.05738227387541167\n",
      "Epoch: 8 - Batch: 665, Training Loss: 0.05746599608664687\n",
      "Epoch: 8 - Batch: 666, Training Loss: 0.057554357359263036\n",
      "Epoch: 8 - Batch: 667, Training Loss: 0.057631555524582094\n",
      "Epoch: 8 - Batch: 668, Training Loss: 0.057716844316145675\n",
      "Epoch: 8 - Batch: 669, Training Loss: 0.05780010185456197\n",
      "Epoch: 8 - Batch: 670, Training Loss: 0.05788527777538964\n",
      "Epoch: 8 - Batch: 671, Training Loss: 0.05797096424632602\n",
      "Epoch: 8 - Batch: 672, Training Loss: 0.0580561848768152\n",
      "Epoch: 8 - Batch: 673, Training Loss: 0.05813997345134789\n",
      "Epoch: 8 - Batch: 674, Training Loss: 0.05822345970281914\n",
      "Epoch: 8 - Batch: 675, Training Loss: 0.058309102186911536\n",
      "Epoch: 8 - Batch: 676, Training Loss: 0.058393692020110625\n",
      "Epoch: 8 - Batch: 677, Training Loss: 0.05847903491796348\n",
      "Epoch: 8 - Batch: 678, Training Loss: 0.05856233432419462\n",
      "Epoch: 8 - Batch: 679, Training Loss: 0.05865247157181476\n",
      "Epoch: 8 - Batch: 680, Training Loss: 0.05874508647109145\n",
      "Epoch: 8 - Batch: 681, Training Loss: 0.058832216420972326\n",
      "Epoch: 8 - Batch: 682, Training Loss: 0.05891653951597253\n",
      "Epoch: 8 - Batch: 683, Training Loss: 0.059000515547360154\n",
      "Epoch: 8 - Batch: 684, Training Loss: 0.05908179679765037\n",
      "Epoch: 8 - Batch: 685, Training Loss: 0.05916839104707957\n",
      "Epoch: 8 - Batch: 686, Training Loss: 0.059254714036066934\n",
      "Epoch: 8 - Batch: 687, Training Loss: 0.05933625658797981\n",
      "Epoch: 8 - Batch: 688, Training Loss: 0.0594295610428153\n",
      "Epoch: 8 - Batch: 689, Training Loss: 0.05951901543792801\n",
      "Epoch: 8 - Batch: 690, Training Loss: 0.05960117872946496\n",
      "Epoch: 8 - Batch: 691, Training Loss: 0.05969453418927011\n",
      "Epoch: 8 - Batch: 692, Training Loss: 0.05978074973791986\n",
      "Epoch: 8 - Batch: 693, Training Loss: 0.05986890499518681\n",
      "Epoch: 8 - Batch: 694, Training Loss: 0.0599564151656173\n",
      "Epoch: 8 - Batch: 695, Training Loss: 0.06003900909577041\n",
      "Epoch: 8 - Batch: 696, Training Loss: 0.06012390098020212\n",
      "Epoch: 8 - Batch: 697, Training Loss: 0.06020848125614733\n",
      "Epoch: 8 - Batch: 698, Training Loss: 0.06028577940504547\n",
      "Epoch: 8 - Batch: 699, Training Loss: 0.060372418549474\n",
      "Epoch: 8 - Batch: 700, Training Loss: 0.06045913514657993\n",
      "Epoch: 8 - Batch: 701, Training Loss: 0.06054401436626022\n",
      "Epoch: 8 - Batch: 702, Training Loss: 0.060627267536881746\n",
      "Epoch: 8 - Batch: 703, Training Loss: 0.060718447235636845\n",
      "Epoch: 8 - Batch: 704, Training Loss: 0.060802531819389036\n",
      "Epoch: 8 - Batch: 705, Training Loss: 0.06089854216941356\n",
      "Epoch: 8 - Batch: 706, Training Loss: 0.06098109100069573\n",
      "Epoch: 8 - Batch: 707, Training Loss: 0.06105671100207229\n",
      "Epoch: 8 - Batch: 708, Training Loss: 0.061134286076571814\n",
      "Epoch: 8 - Batch: 709, Training Loss: 0.0612234537293563\n",
      "Epoch: 8 - Batch: 710, Training Loss: 0.061305365789292465\n",
      "Epoch: 8 - Batch: 711, Training Loss: 0.06140484314899935\n",
      "Epoch: 8 - Batch: 712, Training Loss: 0.06147734295101111\n",
      "Epoch: 8 - Batch: 713, Training Loss: 0.06156282861459117\n",
      "Epoch: 8 - Batch: 714, Training Loss: 0.061645803352197014\n",
      "Epoch: 8 - Batch: 715, Training Loss: 0.06173077156492331\n",
      "Epoch: 8 - Batch: 716, Training Loss: 0.0618204466664969\n",
      "Epoch: 8 - Batch: 717, Training Loss: 0.06190922160696232\n",
      "Epoch: 8 - Batch: 718, Training Loss: 0.06200118849106491\n",
      "Epoch: 8 - Batch: 719, Training Loss: 0.06209303635121578\n",
      "Epoch: 8 - Batch: 720, Training Loss: 0.0621792292021598\n",
      "Epoch: 8 - Batch: 721, Training Loss: 0.062269608791343016\n",
      "Epoch: 8 - Batch: 722, Training Loss: 0.062351468252404216\n",
      "Epoch: 8 - Batch: 723, Training Loss: 0.062435897626291655\n",
      "Epoch: 8 - Batch: 724, Training Loss: 0.06251616944646954\n",
      "Epoch: 8 - Batch: 725, Training Loss: 0.06260378357288651\n",
      "Epoch: 8 - Batch: 726, Training Loss: 0.06268825965808399\n",
      "Epoch: 8 - Batch: 727, Training Loss: 0.06276919244237207\n",
      "Epoch: 8 - Batch: 728, Training Loss: 0.0628532460376398\n",
      "Epoch: 8 - Batch: 729, Training Loss: 0.06294530712895924\n",
      "Epoch: 8 - Batch: 730, Training Loss: 0.06302723941543012\n",
      "Epoch: 8 - Batch: 731, Training Loss: 0.06311451249553592\n",
      "Epoch: 8 - Batch: 732, Training Loss: 0.06320444618044406\n",
      "Epoch: 8 - Batch: 733, Training Loss: 0.06329797066834633\n",
      "Epoch: 8 - Batch: 734, Training Loss: 0.06338695638116518\n",
      "Epoch: 8 - Batch: 735, Training Loss: 0.06346947813459099\n",
      "Epoch: 8 - Batch: 736, Training Loss: 0.06355519708289238\n",
      "Epoch: 8 - Batch: 737, Training Loss: 0.0636379042421012\n",
      "Epoch: 8 - Batch: 738, Training Loss: 0.0637303128850895\n",
      "Epoch: 8 - Batch: 739, Training Loss: 0.06381359593834647\n",
      "Epoch: 8 - Batch: 740, Training Loss: 0.06389597860191197\n",
      "Epoch: 8 - Batch: 741, Training Loss: 0.06398010965603501\n",
      "Epoch: 8 - Batch: 742, Training Loss: 0.06406151447105368\n",
      "Epoch: 8 - Batch: 743, Training Loss: 0.06415311754318216\n",
      "Epoch: 8 - Batch: 744, Training Loss: 0.06424085396785245\n",
      "Epoch: 8 - Batch: 745, Training Loss: 0.06432745672428786\n",
      "Epoch: 8 - Batch: 746, Training Loss: 0.06441848922156378\n",
      "Epoch: 8 - Batch: 747, Training Loss: 0.06450172148607856\n",
      "Epoch: 8 - Batch: 748, Training Loss: 0.06458990956049654\n",
      "Epoch: 8 - Batch: 749, Training Loss: 0.06467581283962154\n",
      "Epoch: 8 - Batch: 750, Training Loss: 0.06475703896426444\n",
      "Epoch: 8 - Batch: 751, Training Loss: 0.06483489402861738\n",
      "Epoch: 8 - Batch: 752, Training Loss: 0.06492302370926437\n",
      "Epoch: 8 - Batch: 753, Training Loss: 0.06501327867729352\n",
      "Epoch: 8 - Batch: 754, Training Loss: 0.06509631902448969\n",
      "Epoch: 8 - Batch: 755, Training Loss: 0.06518082777856792\n",
      "Epoch: 8 - Batch: 756, Training Loss: 0.0652670834405011\n",
      "Epoch: 8 - Batch: 757, Training Loss: 0.06536183861481214\n",
      "Epoch: 8 - Batch: 758, Training Loss: 0.06544647740808687\n",
      "Epoch: 8 - Batch: 759, Training Loss: 0.06553275380314484\n",
      "Epoch: 8 - Batch: 760, Training Loss: 0.06562398859665762\n",
      "Epoch: 8 - Batch: 761, Training Loss: 0.06571631541917375\n",
      "Epoch: 8 - Batch: 762, Training Loss: 0.06580223742359709\n",
      "Epoch: 8 - Batch: 763, Training Loss: 0.06589271597700135\n",
      "Epoch: 8 - Batch: 764, Training Loss: 0.06597580947290803\n",
      "Epoch: 8 - Batch: 765, Training Loss: 0.06605852956251916\n",
      "Epoch: 8 - Batch: 766, Training Loss: 0.06615327787315273\n",
      "Epoch: 8 - Batch: 767, Training Loss: 0.06624045366325584\n",
      "Epoch: 8 - Batch: 768, Training Loss: 0.0663242674808004\n",
      "Epoch: 8 - Batch: 769, Training Loss: 0.06641106069656351\n",
      "Epoch: 8 - Batch: 770, Training Loss: 0.06649242033239819\n",
      "Epoch: 8 - Batch: 771, Training Loss: 0.06657624680851625\n",
      "Epoch: 8 - Batch: 772, Training Loss: 0.0666496455422286\n",
      "Epoch: 8 - Batch: 773, Training Loss: 0.06672967238286834\n",
      "Epoch: 8 - Batch: 774, Training Loss: 0.06681130447148485\n",
      "Epoch: 8 - Batch: 775, Training Loss: 0.06689316729333863\n",
      "Epoch: 8 - Batch: 776, Training Loss: 0.06697540717882106\n",
      "Epoch: 8 - Batch: 777, Training Loss: 0.06705776084937266\n",
      "Epoch: 8 - Batch: 778, Training Loss: 0.06714782289308102\n",
      "Epoch: 8 - Batch: 779, Training Loss: 0.06724450858943103\n",
      "Epoch: 8 - Batch: 780, Training Loss: 0.06733073946551897\n",
      "Epoch: 8 - Batch: 781, Training Loss: 0.06742063883821763\n",
      "Epoch: 8 - Batch: 782, Training Loss: 0.06750210230648024\n",
      "Epoch: 8 - Batch: 783, Training Loss: 0.06758801281995837\n",
      "Epoch: 8 - Batch: 784, Training Loss: 0.06766423562643539\n",
      "Epoch: 8 - Batch: 785, Training Loss: 0.06775621104843384\n",
      "Epoch: 8 - Batch: 786, Training Loss: 0.06783759039803523\n",
      "Epoch: 8 - Batch: 787, Training Loss: 0.06792554285865321\n",
      "Epoch: 8 - Batch: 788, Training Loss: 0.06801023671579598\n",
      "Epoch: 8 - Batch: 789, Training Loss: 0.06808667380368927\n",
      "Epoch: 8 - Batch: 790, Training Loss: 0.06817709010211785\n",
      "Epoch: 8 - Batch: 791, Training Loss: 0.06826989993971971\n",
      "Epoch: 8 - Batch: 792, Training Loss: 0.06835305659257951\n",
      "Epoch: 8 - Batch: 793, Training Loss: 0.06844922407819067\n",
      "Epoch: 8 - Batch: 794, Training Loss: 0.0685318697670206\n",
      "Epoch: 8 - Batch: 795, Training Loss: 0.06862410215991449\n",
      "Epoch: 8 - Batch: 796, Training Loss: 0.06870666756039828\n",
      "Epoch: 8 - Batch: 797, Training Loss: 0.06879639319741904\n",
      "Epoch: 8 - Batch: 798, Training Loss: 0.06888134856623401\n",
      "Epoch: 8 - Batch: 799, Training Loss: 0.0689695163090925\n",
      "Epoch: 8 - Batch: 800, Training Loss: 0.06905535399716094\n",
      "Epoch: 8 - Batch: 801, Training Loss: 0.06914050565726722\n",
      "Epoch: 8 - Batch: 802, Training Loss: 0.06922500565722214\n",
      "Epoch: 8 - Batch: 803, Training Loss: 0.06931635815878807\n",
      "Epoch: 8 - Batch: 804, Training Loss: 0.06940686569902829\n",
      "Epoch: 8 - Batch: 805, Training Loss: 0.06949379504542445\n",
      "Epoch: 8 - Batch: 806, Training Loss: 0.06957695681360823\n",
      "Epoch: 8 - Batch: 807, Training Loss: 0.06965920791202912\n",
      "Epoch: 8 - Batch: 808, Training Loss: 0.06974821213391882\n",
      "Epoch: 8 - Batch: 809, Training Loss: 0.0698342733208714\n",
      "Epoch: 8 - Batch: 810, Training Loss: 0.0699168414828888\n",
      "Epoch: 8 - Batch: 811, Training Loss: 0.07000076064645354\n",
      "Epoch: 8 - Batch: 812, Training Loss: 0.07008124511707481\n",
      "Epoch: 8 - Batch: 813, Training Loss: 0.07016738498611237\n",
      "Epoch: 8 - Batch: 814, Training Loss: 0.07025640632703925\n",
      "Epoch: 8 - Batch: 815, Training Loss: 0.07033579381395928\n",
      "Epoch: 8 - Batch: 816, Training Loss: 0.07042340152768749\n",
      "Epoch: 8 - Batch: 817, Training Loss: 0.07051512088694581\n",
      "Epoch: 8 - Batch: 818, Training Loss: 0.07059558350623742\n",
      "Epoch: 8 - Batch: 819, Training Loss: 0.07068500052488859\n",
      "Epoch: 8 - Batch: 820, Training Loss: 0.07077648573451573\n",
      "Epoch: 8 - Batch: 821, Training Loss: 0.07085624531183274\n",
      "Epoch: 8 - Batch: 822, Training Loss: 0.07094778750666339\n",
      "Epoch: 8 - Batch: 823, Training Loss: 0.07103256862703247\n",
      "Epoch: 8 - Batch: 824, Training Loss: 0.07112159555280584\n",
      "Epoch: 8 - Batch: 825, Training Loss: 0.07120764081923918\n",
      "Epoch: 8 - Batch: 826, Training Loss: 0.07128692375091375\n",
      "Epoch: 8 - Batch: 827, Training Loss: 0.0713719901865098\n",
      "Epoch: 8 - Batch: 828, Training Loss: 0.07145310174148672\n",
      "Epoch: 8 - Batch: 829, Training Loss: 0.07154250621276709\n",
      "Epoch: 8 - Batch: 830, Training Loss: 0.07163561364113791\n",
      "Epoch: 8 - Batch: 831, Training Loss: 0.07172459245938963\n",
      "Epoch: 8 - Batch: 832, Training Loss: 0.07181188446866536\n",
      "Epoch: 8 - Batch: 833, Training Loss: 0.0719024687016979\n",
      "Epoch: 8 - Batch: 834, Training Loss: 0.07199515426410964\n",
      "Epoch: 8 - Batch: 835, Training Loss: 0.07208047057216242\n",
      "Epoch: 8 - Batch: 836, Training Loss: 0.07215920016540224\n",
      "Epoch: 8 - Batch: 837, Training Loss: 0.07224455022347309\n",
      "Epoch: 8 - Batch: 838, Training Loss: 0.07232392743649965\n",
      "Epoch: 8 - Batch: 839, Training Loss: 0.07241814187534808\n",
      "Epoch: 8 - Batch: 840, Training Loss: 0.07251494321014552\n",
      "Epoch: 8 - Batch: 841, Training Loss: 0.07259850490646773\n",
      "Epoch: 8 - Batch: 842, Training Loss: 0.07268644832877773\n",
      "Epoch: 8 - Batch: 843, Training Loss: 0.07276802603309823\n",
      "Epoch: 8 - Batch: 844, Training Loss: 0.07285867407176624\n",
      "Epoch: 8 - Batch: 845, Training Loss: 0.07294958092853007\n",
      "Epoch: 8 - Batch: 846, Training Loss: 0.073035380727369\n",
      "Epoch: 8 - Batch: 847, Training Loss: 0.07312967726469632\n",
      "Epoch: 8 - Batch: 848, Training Loss: 0.07322247476497692\n",
      "Epoch: 8 - Batch: 849, Training Loss: 0.0733128484418835\n",
      "Epoch: 8 - Batch: 850, Training Loss: 0.07339696580571915\n",
      "Epoch: 8 - Batch: 851, Training Loss: 0.07348760055566506\n",
      "Epoch: 8 - Batch: 852, Training Loss: 0.07357527513310289\n",
      "Epoch: 8 - Batch: 853, Training Loss: 0.07367202497487439\n",
      "Epoch: 8 - Batch: 854, Training Loss: 0.07376253983595873\n",
      "Epoch: 8 - Batch: 855, Training Loss: 0.07384341556708611\n",
      "Epoch: 8 - Batch: 856, Training Loss: 0.0739303064991289\n",
      "Epoch: 8 - Batch: 857, Training Loss: 0.07401703761436453\n",
      "Epoch: 8 - Batch: 858, Training Loss: 0.07410097761607881\n",
      "Epoch: 8 - Batch: 859, Training Loss: 0.07418538324819078\n",
      "Epoch: 8 - Batch: 860, Training Loss: 0.07426978213400588\n",
      "Epoch: 8 - Batch: 861, Training Loss: 0.07435694529024721\n",
      "Epoch: 8 - Batch: 862, Training Loss: 0.07444003049537515\n",
      "Epoch: 8 - Batch: 863, Training Loss: 0.07452461876555858\n",
      "Epoch: 8 - Batch: 864, Training Loss: 0.07461076819802794\n",
      "Epoch: 8 - Batch: 865, Training Loss: 0.07469689311030295\n",
      "Epoch: 8 - Batch: 866, Training Loss: 0.07477951547127853\n",
      "Epoch: 8 - Batch: 867, Training Loss: 0.07486761902547001\n",
      "Epoch: 8 - Batch: 868, Training Loss: 0.07494524716218906\n",
      "Epoch: 8 - Batch: 869, Training Loss: 0.0750413498722301\n",
      "Epoch: 8 - Batch: 870, Training Loss: 0.07513115469174796\n",
      "Epoch: 8 - Batch: 871, Training Loss: 0.07521819255961905\n",
      "Epoch: 8 - Batch: 872, Training Loss: 0.07530461631060438\n",
      "Epoch: 8 - Batch: 873, Training Loss: 0.07538406985192551\n",
      "Epoch: 8 - Batch: 874, Training Loss: 0.0754697361704623\n",
      "Epoch: 8 - Batch: 875, Training Loss: 0.07555614131352992\n",
      "Epoch: 8 - Batch: 876, Training Loss: 0.07564292909271682\n",
      "Epoch: 8 - Batch: 877, Training Loss: 0.07572159978559559\n",
      "Epoch: 8 - Batch: 878, Training Loss: 0.0758085880570645\n",
      "Epoch: 8 - Batch: 879, Training Loss: 0.0758863829854709\n",
      "Epoch: 8 - Batch: 880, Training Loss: 0.07597490469886493\n",
      "Epoch: 8 - Batch: 881, Training Loss: 0.07606138849930581\n",
      "Epoch: 8 - Batch: 882, Training Loss: 0.07614341161984511\n",
      "Epoch: 8 - Batch: 883, Training Loss: 0.0762296713901001\n",
      "Epoch: 8 - Batch: 884, Training Loss: 0.07631665197004923\n",
      "Epoch: 8 - Batch: 885, Training Loss: 0.07640186284559085\n",
      "Epoch: 8 - Batch: 886, Training Loss: 0.07649728923319386\n",
      "Epoch: 8 - Batch: 887, Training Loss: 0.07658461190352392\n",
      "Epoch: 8 - Batch: 888, Training Loss: 0.07668695799623358\n",
      "Epoch: 8 - Batch: 889, Training Loss: 0.07676414472620878\n",
      "Epoch: 8 - Batch: 890, Training Loss: 0.07684841306005939\n",
      "Epoch: 8 - Batch: 891, Training Loss: 0.07693577870405333\n",
      "Epoch: 8 - Batch: 892, Training Loss: 0.07702247351160887\n",
      "Epoch: 8 - Batch: 893, Training Loss: 0.0771141716867537\n",
      "Epoch: 8 - Batch: 894, Training Loss: 0.07719949842314815\n",
      "Epoch: 8 - Batch: 895, Training Loss: 0.07728239927891871\n",
      "Epoch: 8 - Batch: 896, Training Loss: 0.07735959402166591\n",
      "Epoch: 8 - Batch: 897, Training Loss: 0.0774464748960427\n",
      "Epoch: 8 - Batch: 898, Training Loss: 0.0775300771257474\n",
      "Epoch: 8 - Batch: 899, Training Loss: 0.0776199206005578\n",
      "Epoch: 8 - Batch: 900, Training Loss: 0.07771107595827845\n",
      "Epoch: 8 - Batch: 901, Training Loss: 0.07779380145357616\n",
      "Epoch: 8 - Batch: 902, Training Loss: 0.07789206363347237\n",
      "Epoch: 8 - Batch: 903, Training Loss: 0.07797689478269856\n",
      "Epoch: 8 - Batch: 904, Training Loss: 0.07806386293883545\n",
      "Epoch: 8 - Batch: 905, Training Loss: 0.07814715737801287\n",
      "Epoch: 8 - Batch: 906, Training Loss: 0.0782334301342715\n",
      "Epoch: 8 - Batch: 907, Training Loss: 0.07832039335424429\n",
      "Epoch: 8 - Batch: 908, Training Loss: 0.07840278304211339\n",
      "Epoch: 8 - Batch: 909, Training Loss: 0.07850155232483474\n",
      "Epoch: 8 - Batch: 910, Training Loss: 0.07858546461236615\n",
      "Epoch: 8 - Batch: 911, Training Loss: 0.07868491327559968\n",
      "Epoch: 8 - Batch: 912, Training Loss: 0.07877358873289814\n",
      "Epoch: 8 - Batch: 913, Training Loss: 0.07886500655986974\n",
      "Epoch: 8 - Batch: 914, Training Loss: 0.07895910783761967\n",
      "Epoch: 8 - Batch: 915, Training Loss: 0.07905159860997651\n",
      "Epoch: 8 - Batch: 916, Training Loss: 0.07913716429625182\n",
      "Epoch: 8 - Batch: 917, Training Loss: 0.07923178199314161\n",
      "Epoch: 8 - Batch: 918, Training Loss: 0.07932347341581165\n",
      "Epoch: 8 - Batch: 919, Training Loss: 0.07940142952634723\n",
      "Epoch: 8 - Batch: 920, Training Loss: 0.0794964986258676\n",
      "Epoch: 8 - Batch: 921, Training Loss: 0.07958355546615413\n",
      "Epoch: 8 - Batch: 922, Training Loss: 0.07967644725396462\n",
      "Epoch: 8 - Batch: 923, Training Loss: 0.07976562765181361\n",
      "Epoch: 8 - Batch: 924, Training Loss: 0.07984580686485787\n",
      "Epoch: 8 - Batch: 925, Training Loss: 0.07993491586439842\n",
      "Epoch: 8 - Batch: 926, Training Loss: 0.0800173946935838\n",
      "Epoch: 8 - Batch: 927, Training Loss: 0.08010417585003238\n",
      "Epoch: 8 - Batch: 928, Training Loss: 0.08018595257628815\n",
      "Epoch: 8 - Batch: 929, Training Loss: 0.08026754162328358\n",
      "Epoch: 8 - Batch: 930, Training Loss: 0.0803563281226514\n",
      "Epoch: 8 - Batch: 931, Training Loss: 0.08044808416026544\n",
      "Epoch: 8 - Batch: 932, Training Loss: 0.08052993121604816\n",
      "Epoch: 8 - Batch: 933, Training Loss: 0.08062409896212035\n",
      "Epoch: 8 - Batch: 934, Training Loss: 0.08070444573291499\n",
      "Epoch: 8 - Batch: 935, Training Loss: 0.08078492585427528\n",
      "Epoch: 8 - Batch: 936, Training Loss: 0.08087061855256261\n",
      "Epoch: 8 - Batch: 937, Training Loss: 0.08095520060775094\n",
      "Epoch: 8 - Batch: 938, Training Loss: 0.08102847569024385\n",
      "Epoch: 8 - Batch: 939, Training Loss: 0.08111971656901525\n",
      "Epoch: 8 - Batch: 940, Training Loss: 0.0812002954136574\n",
      "Epoch: 8 - Batch: 941, Training Loss: 0.0812956701105408\n",
      "Epoch: 8 - Batch: 942, Training Loss: 0.08137974435862023\n",
      "Epoch: 8 - Batch: 943, Training Loss: 0.08146022001333893\n",
      "Epoch: 8 - Batch: 944, Training Loss: 0.08154829490194669\n",
      "Epoch: 8 - Batch: 945, Training Loss: 0.0816289802293477\n",
      "Epoch: 8 - Batch: 946, Training Loss: 0.08171826995180219\n",
      "Epoch: 8 - Batch: 947, Training Loss: 0.08181217180605156\n",
      "Epoch: 8 - Batch: 948, Training Loss: 0.08190918150998862\n",
      "Epoch: 8 - Batch: 949, Training Loss: 0.08198707902905952\n",
      "Epoch: 8 - Batch: 950, Training Loss: 0.08207028267123609\n",
      "Epoch: 8 - Batch: 951, Training Loss: 0.08215922636796981\n",
      "Epoch: 8 - Batch: 952, Training Loss: 0.08224199458363637\n",
      "Epoch: 8 - Batch: 953, Training Loss: 0.08232731875001296\n",
      "Epoch: 8 - Batch: 954, Training Loss: 0.08241556248755795\n",
      "Epoch: 8 - Batch: 955, Training Loss: 0.08250752923574614\n",
      "Epoch: 8 - Batch: 956, Training Loss: 0.08260199024186007\n",
      "Epoch: 8 - Batch: 957, Training Loss: 0.08268722894278727\n",
      "Epoch: 8 - Batch: 958, Training Loss: 0.08277036055032887\n",
      "Epoch: 8 - Batch: 959, Training Loss: 0.08286820170446414\n",
      "Epoch: 8 - Batch: 960, Training Loss: 0.08294608541586704\n",
      "Epoch: 8 - Batch: 961, Training Loss: 0.08302987295868583\n",
      "Epoch: 8 - Batch: 962, Training Loss: 0.08311447775482539\n",
      "Epoch: 8 - Batch: 963, Training Loss: 0.08319726102610132\n",
      "Epoch: 8 - Batch: 964, Training Loss: 0.08328262946350658\n",
      "Epoch: 8 - Batch: 965, Training Loss: 0.08336470652970904\n",
      "Epoch: 8 - Batch: 966, Training Loss: 0.08346313731142538\n",
      "Epoch: 8 - Batch: 967, Training Loss: 0.083552403835762\n",
      "Epoch: 8 - Batch: 968, Training Loss: 0.0836417419252111\n",
      "Epoch: 8 - Batch: 969, Training Loss: 0.08373299302227462\n",
      "Epoch: 8 - Batch: 970, Training Loss: 0.08382278412679336\n",
      "Epoch: 8 - Batch: 971, Training Loss: 0.08391030454640562\n",
      "Epoch: 8 - Batch: 972, Training Loss: 0.08398850465418885\n",
      "Epoch: 8 - Batch: 973, Training Loss: 0.08407999144536543\n",
      "Epoch: 8 - Batch: 974, Training Loss: 0.08416440243991849\n",
      "Epoch: 8 - Batch: 975, Training Loss: 0.08425847062868859\n",
      "Epoch: 8 - Batch: 976, Training Loss: 0.08435127984231977\n",
      "Epoch: 8 - Batch: 977, Training Loss: 0.084434964287884\n",
      "Epoch: 8 - Batch: 978, Training Loss: 0.08451940551103644\n",
      "Epoch: 8 - Batch: 979, Training Loss: 0.08460370218921855\n",
      "Epoch: 8 - Batch: 980, Training Loss: 0.08469281071305867\n",
      "Epoch: 8 - Batch: 981, Training Loss: 0.08477751848289425\n",
      "Epoch: 8 - Batch: 982, Training Loss: 0.08486391133806401\n",
      "Epoch: 8 - Batch: 983, Training Loss: 0.08495149702748059\n",
      "Epoch: 8 - Batch: 984, Training Loss: 0.08504148161233362\n",
      "Epoch: 8 - Batch: 985, Training Loss: 0.08513397301483906\n",
      "Epoch: 8 - Batch: 986, Training Loss: 0.08522484599802624\n",
      "Epoch: 8 - Batch: 987, Training Loss: 0.08531188778888131\n",
      "Epoch: 8 - Batch: 988, Training Loss: 0.0853931538167581\n",
      "Epoch: 8 - Batch: 989, Training Loss: 0.08547884620614905\n",
      "Epoch: 8 - Batch: 990, Training Loss: 0.08556945185187839\n",
      "Epoch: 8 - Batch: 991, Training Loss: 0.08566955080376336\n",
      "Epoch: 8 - Batch: 992, Training Loss: 0.08575665800033715\n",
      "Epoch: 8 - Batch: 993, Training Loss: 0.08584311392596902\n",
      "Epoch: 8 - Batch: 994, Training Loss: 0.08592261624202799\n",
      "Epoch: 8 - Batch: 995, Training Loss: 0.08601095031322926\n",
      "Epoch: 8 - Batch: 996, Training Loss: 0.08609654252134745\n",
      "Epoch: 8 - Batch: 997, Training Loss: 0.08618505764289282\n",
      "Epoch: 8 - Batch: 998, Training Loss: 0.08627613826622417\n",
      "Epoch: 8 - Batch: 999, Training Loss: 0.08636319272183067\n",
      "Epoch: 8 - Batch: 1000, Training Loss: 0.08644675227441202\n",
      "Epoch: 8 - Batch: 1001, Training Loss: 0.08653160453361658\n",
      "Epoch: 8 - Batch: 1002, Training Loss: 0.0866193115204622\n",
      "Epoch: 8 - Batch: 1003, Training Loss: 0.08670452621396303\n",
      "Epoch: 8 - Batch: 1004, Training Loss: 0.0867981140812238\n",
      "Epoch: 8 - Batch: 1005, Training Loss: 0.08688679776134381\n",
      "Epoch: 8 - Batch: 1006, Training Loss: 0.08697787986119984\n",
      "Epoch: 8 - Batch: 1007, Training Loss: 0.0870678617833661\n",
      "Epoch: 8 - Batch: 1008, Training Loss: 0.0871524328541993\n",
      "Epoch: 8 - Batch: 1009, Training Loss: 0.08723925909716296\n",
      "Epoch: 8 - Batch: 1010, Training Loss: 0.08733018903466402\n",
      "Epoch: 8 - Batch: 1011, Training Loss: 0.08741634275461509\n",
      "Epoch: 8 - Batch: 1012, Training Loss: 0.08750948217847257\n",
      "Epoch: 8 - Batch: 1013, Training Loss: 0.08759788483084731\n",
      "Epoch: 8 - Batch: 1014, Training Loss: 0.08768631920070197\n",
      "Epoch: 8 - Batch: 1015, Training Loss: 0.08776518053824629\n",
      "Epoch: 8 - Batch: 1016, Training Loss: 0.08785350647311702\n",
      "Epoch: 8 - Batch: 1017, Training Loss: 0.08793594269313622\n",
      "Epoch: 8 - Batch: 1018, Training Loss: 0.08802127231462283\n",
      "Epoch: 8 - Batch: 1019, Training Loss: 0.08810932209613312\n",
      "Epoch: 8 - Batch: 1020, Training Loss: 0.08820228879749281\n",
      "Epoch: 8 - Batch: 1021, Training Loss: 0.08828158942275181\n",
      "Epoch: 8 - Batch: 1022, Training Loss: 0.08836175413002224\n",
      "Epoch: 8 - Batch: 1023, Training Loss: 0.08844431817136199\n",
      "Epoch: 8 - Batch: 1024, Training Loss: 0.08852293220028948\n",
      "Epoch: 8 - Batch: 1025, Training Loss: 0.08861430490985636\n",
      "Epoch: 8 - Batch: 1026, Training Loss: 0.08869612119956001\n",
      "Epoch: 8 - Batch: 1027, Training Loss: 0.08878275763805628\n",
      "Epoch: 8 - Batch: 1028, Training Loss: 0.08886431875464137\n",
      "Epoch: 8 - Batch: 1029, Training Loss: 0.08895478775079176\n",
      "Epoch: 8 - Batch: 1030, Training Loss: 0.0890376151942495\n",
      "Epoch: 8 - Batch: 1031, Training Loss: 0.08912996578943078\n",
      "Epoch: 8 - Batch: 1032, Training Loss: 0.08921464171822786\n",
      "Epoch: 8 - Batch: 1033, Training Loss: 0.08929760645376905\n",
      "Epoch: 8 - Batch: 1034, Training Loss: 0.08938163462686499\n",
      "Epoch: 8 - Batch: 1035, Training Loss: 0.089468010412421\n",
      "Epoch: 8 - Batch: 1036, Training Loss: 0.08954873291241193\n",
      "Epoch: 8 - Batch: 1037, Training Loss: 0.08963261115956267\n",
      "Epoch: 8 - Batch: 1038, Training Loss: 0.08971918090115337\n",
      "Epoch: 8 - Batch: 1039, Training Loss: 0.08980612960299647\n",
      "Epoch: 8 - Batch: 1040, Training Loss: 0.08989565133751921\n",
      "Epoch: 8 - Batch: 1041, Training Loss: 0.08997747299584188\n",
      "Epoch: 8 - Batch: 1042, Training Loss: 0.090065559702825\n",
      "Epoch: 8 - Batch: 1043, Training Loss: 0.09015683402271808\n",
      "Epoch: 8 - Batch: 1044, Training Loss: 0.0902428162186893\n",
      "Epoch: 8 - Batch: 1045, Training Loss: 0.09033378429871491\n",
      "Epoch: 8 - Batch: 1046, Training Loss: 0.09041425703127388\n",
      "Epoch: 8 - Batch: 1047, Training Loss: 0.09049895791272025\n",
      "Epoch: 8 - Batch: 1048, Training Loss: 0.09058900374554678\n",
      "Epoch: 8 - Batch: 1049, Training Loss: 0.0906752183427957\n",
      "Epoch: 8 - Batch: 1050, Training Loss: 0.09076439633125294\n",
      "Epoch: 8 - Batch: 1051, Training Loss: 0.0908490604663864\n",
      "Epoch: 8 - Batch: 1052, Training Loss: 0.09093541490720279\n",
      "Epoch: 8 - Batch: 1053, Training Loss: 0.09101552571205555\n",
      "Epoch: 8 - Batch: 1054, Training Loss: 0.09110038400091737\n",
      "Epoch: 8 - Batch: 1055, Training Loss: 0.0911854956095195\n",
      "Epoch: 8 - Batch: 1056, Training Loss: 0.09127495320479866\n",
      "Epoch: 8 - Batch: 1057, Training Loss: 0.09136321386368713\n",
      "Epoch: 8 - Batch: 1058, Training Loss: 0.091446754789817\n",
      "Epoch: 8 - Batch: 1059, Training Loss: 0.09153302560002848\n",
      "Epoch: 8 - Batch: 1060, Training Loss: 0.09162786169283425\n",
      "Epoch: 8 - Batch: 1061, Training Loss: 0.09170648419787833\n",
      "Epoch: 8 - Batch: 1062, Training Loss: 0.0917945786643384\n",
      "Epoch: 8 - Batch: 1063, Training Loss: 0.0918751564834446\n",
      "Epoch: 8 - Batch: 1064, Training Loss: 0.09195791981237049\n",
      "Epoch: 8 - Batch: 1065, Training Loss: 0.09204712228395452\n",
      "Epoch: 8 - Batch: 1066, Training Loss: 0.09213369132743941\n",
      "Epoch: 8 - Batch: 1067, Training Loss: 0.09222275250982091\n",
      "Epoch: 8 - Batch: 1068, Training Loss: 0.09230772476908974\n",
      "Epoch: 8 - Batch: 1069, Training Loss: 0.09239703118430441\n",
      "Epoch: 8 - Batch: 1070, Training Loss: 0.09248805863421354\n",
      "Epoch: 8 - Batch: 1071, Training Loss: 0.09258488498343954\n",
      "Epoch: 8 - Batch: 1072, Training Loss: 0.09266932051672074\n",
      "Epoch: 8 - Batch: 1073, Training Loss: 0.0927609901065949\n",
      "Epoch: 8 - Batch: 1074, Training Loss: 0.09285068807885619\n",
      "Epoch: 8 - Batch: 1075, Training Loss: 0.09293563090647829\n",
      "Epoch: 8 - Batch: 1076, Training Loss: 0.09302336159803183\n",
      "Epoch: 8 - Batch: 1077, Training Loss: 0.09311330225782015\n",
      "Epoch: 8 - Batch: 1078, Training Loss: 0.09319700299457927\n",
      "Epoch: 8 - Batch: 1079, Training Loss: 0.09328611422435165\n",
      "Epoch: 8 - Batch: 1080, Training Loss: 0.09337564704825431\n",
      "Epoch: 8 - Batch: 1081, Training Loss: 0.09347299831744846\n",
      "Epoch: 8 - Batch: 1082, Training Loss: 0.0935608470630191\n",
      "Epoch: 8 - Batch: 1083, Training Loss: 0.09364014065779659\n",
      "Epoch: 8 - Batch: 1084, Training Loss: 0.0937247347352319\n",
      "Epoch: 8 - Batch: 1085, Training Loss: 0.09381587876263345\n",
      "Epoch: 8 - Batch: 1086, Training Loss: 0.09390309166354723\n",
      "Epoch: 8 - Batch: 1087, Training Loss: 0.09399292750243919\n",
      "Epoch: 8 - Batch: 1088, Training Loss: 0.09408060392089942\n",
      "Epoch: 8 - Batch: 1089, Training Loss: 0.09416846394736574\n",
      "Epoch: 8 - Batch: 1090, Training Loss: 0.09425436586116874\n",
      "Epoch: 8 - Batch: 1091, Training Loss: 0.09433531493665172\n",
      "Epoch: 8 - Batch: 1092, Training Loss: 0.09442538579008472\n",
      "Epoch: 8 - Batch: 1093, Training Loss: 0.09450836296179402\n",
      "Epoch: 8 - Batch: 1094, Training Loss: 0.09458717482649469\n",
      "Epoch: 8 - Batch: 1095, Training Loss: 0.09466302893425695\n",
      "Epoch: 8 - Batch: 1096, Training Loss: 0.09474983875011132\n",
      "Epoch: 8 - Batch: 1097, Training Loss: 0.09484027583726604\n",
      "Epoch: 8 - Batch: 1098, Training Loss: 0.09492398374659901\n",
      "Epoch: 8 - Batch: 1099, Training Loss: 0.095009791212246\n",
      "Epoch: 8 - Batch: 1100, Training Loss: 0.09511009093441972\n",
      "Epoch: 8 - Batch: 1101, Training Loss: 0.09519436610105816\n",
      "Epoch: 8 - Batch: 1102, Training Loss: 0.09528039251863463\n",
      "Epoch: 8 - Batch: 1103, Training Loss: 0.09537257154659054\n",
      "Epoch: 8 - Batch: 1104, Training Loss: 0.09545593507353148\n",
      "Epoch: 8 - Batch: 1105, Training Loss: 0.09554081593654049\n",
      "Epoch: 8 - Batch: 1106, Training Loss: 0.09562851607206449\n",
      "Epoch: 8 - Batch: 1107, Training Loss: 0.09569743278731359\n",
      "Epoch: 8 - Batch: 1108, Training Loss: 0.0957833803498231\n",
      "Epoch: 8 - Batch: 1109, Training Loss: 0.09586947715252786\n",
      "Epoch: 8 - Batch: 1110, Training Loss: 0.09595609298763584\n",
      "Epoch: 8 - Batch: 1111, Training Loss: 0.09603886428608824\n",
      "Epoch: 8 - Batch: 1112, Training Loss: 0.09612899864243829\n",
      "Epoch: 8 - Batch: 1113, Training Loss: 0.0962105649731942\n",
      "Epoch: 8 - Batch: 1114, Training Loss: 0.09629846913477477\n",
      "Epoch: 8 - Batch: 1115, Training Loss: 0.09638704241532987\n",
      "Epoch: 8 - Batch: 1116, Training Loss: 0.09648017535854138\n",
      "Epoch: 8 - Batch: 1117, Training Loss: 0.09656344462266411\n",
      "Epoch: 8 - Batch: 1118, Training Loss: 0.09664640717739688\n",
      "Epoch: 8 - Batch: 1119, Training Loss: 0.09672876606829724\n",
      "Epoch: 8 - Batch: 1120, Training Loss: 0.09682291814096729\n",
      "Epoch: 8 - Batch: 1121, Training Loss: 0.09690069218797866\n",
      "Epoch: 8 - Batch: 1122, Training Loss: 0.09698472892368215\n",
      "Epoch: 8 - Batch: 1123, Training Loss: 0.09707673967107018\n",
      "Epoch: 8 - Batch: 1124, Training Loss: 0.09716699813580632\n",
      "Epoch: 8 - Batch: 1125, Training Loss: 0.09724738684855093\n",
      "Epoch: 8 - Batch: 1126, Training Loss: 0.0973382812629687\n",
      "Epoch: 8 - Batch: 1127, Training Loss: 0.0974218464127524\n",
      "Epoch: 8 - Batch: 1128, Training Loss: 0.09751253274839315\n",
      "Epoch: 8 - Batch: 1129, Training Loss: 0.09760095820967633\n",
      "Epoch: 8 - Batch: 1130, Training Loss: 0.09769243643206743\n",
      "Epoch: 8 - Batch: 1131, Training Loss: 0.09777278008918659\n",
      "Epoch: 8 - Batch: 1132, Training Loss: 0.09786629615039573\n",
      "Epoch: 8 - Batch: 1133, Training Loss: 0.09794995500080621\n",
      "Epoch: 8 - Batch: 1134, Training Loss: 0.09803759400573733\n",
      "Epoch: 8 - Batch: 1135, Training Loss: 0.09812203149124363\n",
      "Epoch: 8 - Batch: 1136, Training Loss: 0.098206390714764\n",
      "Epoch: 8 - Batch: 1137, Training Loss: 0.0982909606673923\n",
      "Epoch: 8 - Batch: 1138, Training Loss: 0.09838126382325617\n",
      "Epoch: 8 - Batch: 1139, Training Loss: 0.09846238511464686\n",
      "Epoch: 8 - Batch: 1140, Training Loss: 0.09855356493696052\n",
      "Epoch: 8 - Batch: 1141, Training Loss: 0.09864313311096448\n",
      "Epoch: 8 - Batch: 1142, Training Loss: 0.0987247350389388\n",
      "Epoch: 8 - Batch: 1143, Training Loss: 0.09880858472553058\n",
      "Epoch: 8 - Batch: 1144, Training Loss: 0.0989001648899038\n",
      "Epoch: 8 - Batch: 1145, Training Loss: 0.0989867487048134\n",
      "Epoch: 8 - Batch: 1146, Training Loss: 0.09907390581286368\n",
      "Epoch: 8 - Batch: 1147, Training Loss: 0.09915860159751985\n",
      "Epoch: 8 - Batch: 1148, Training Loss: 0.09924268374099067\n",
      "Epoch: 8 - Batch: 1149, Training Loss: 0.09932160499207614\n",
      "Epoch: 8 - Batch: 1150, Training Loss: 0.09941542041711941\n",
      "Epoch: 8 - Batch: 1151, Training Loss: 0.09949080130181107\n",
      "Epoch: 8 - Batch: 1152, Training Loss: 0.09957466831748957\n",
      "Epoch: 8 - Batch: 1153, Training Loss: 0.09966683959214644\n",
      "Epoch: 8 - Batch: 1154, Training Loss: 0.09975504460026376\n",
      "Epoch: 8 - Batch: 1155, Training Loss: 0.09983633619858258\n",
      "Epoch: 8 - Batch: 1156, Training Loss: 0.09992044166990774\n",
      "Epoch: 8 - Batch: 1157, Training Loss: 0.10000374978701669\n",
      "Epoch: 8 - Batch: 1158, Training Loss: 0.10008696747780044\n",
      "Epoch: 8 - Batch: 1159, Training Loss: 0.10016248221346988\n",
      "Epoch: 8 - Batch: 1160, Training Loss: 0.10025103089969549\n",
      "Epoch: 8 - Batch: 1161, Training Loss: 0.10034306011908684\n",
      "Epoch: 8 - Batch: 1162, Training Loss: 0.10042239566817014\n",
      "Epoch: 8 - Batch: 1163, Training Loss: 0.10050564583014097\n",
      "Epoch: 8 - Batch: 1164, Training Loss: 0.10058529825130505\n",
      "Epoch: 8 - Batch: 1165, Training Loss: 0.1006740040731173\n",
      "Epoch: 8 - Batch: 1166, Training Loss: 0.10075650419761885\n",
      "Epoch: 8 - Batch: 1167, Training Loss: 0.10084609080655856\n",
      "Epoch: 8 - Batch: 1168, Training Loss: 0.1009297177541513\n",
      "Epoch: 8 - Batch: 1169, Training Loss: 0.10101287922579455\n",
      "Epoch: 8 - Batch: 1170, Training Loss: 0.10110424694977392\n",
      "Epoch: 8 - Batch: 1171, Training Loss: 0.10119736985385319\n",
      "Epoch: 8 - Batch: 1172, Training Loss: 0.10129275008615965\n",
      "Epoch: 8 - Batch: 1173, Training Loss: 0.10137695578200306\n",
      "Epoch: 8 - Batch: 1174, Training Loss: 0.1014681050791768\n",
      "Epoch: 8 - Batch: 1175, Training Loss: 0.10155052965998057\n",
      "Epoch: 8 - Batch: 1176, Training Loss: 0.10163870992549814\n",
      "Epoch: 8 - Batch: 1177, Training Loss: 0.10172455885739469\n",
      "Epoch: 8 - Batch: 1178, Training Loss: 0.10181370561642829\n",
      "Epoch: 8 - Batch: 1179, Training Loss: 0.10189900492282451\n",
      "Epoch: 8 - Batch: 1180, Training Loss: 0.10198810421684093\n",
      "Epoch: 8 - Batch: 1181, Training Loss: 0.10207066540397815\n",
      "Epoch: 8 - Batch: 1182, Training Loss: 0.10215923194441431\n",
      "Epoch: 8 - Batch: 1183, Training Loss: 0.10224060212597127\n",
      "Epoch: 8 - Batch: 1184, Training Loss: 0.10233109091620145\n",
      "Epoch: 8 - Batch: 1185, Training Loss: 0.10242022819829422\n",
      "Epoch: 8 - Batch: 1186, Training Loss: 0.10251191420909975\n",
      "Epoch: 8 - Batch: 1187, Training Loss: 0.10259385869079957\n",
      "Epoch: 8 - Batch: 1188, Training Loss: 0.10267526100993551\n",
      "Epoch: 8 - Batch: 1189, Training Loss: 0.10275908773317068\n",
      "Epoch: 8 - Batch: 1190, Training Loss: 0.10284868511023806\n",
      "Epoch: 8 - Batch: 1191, Training Loss: 0.10292881008107864\n",
      "Epoch: 8 - Batch: 1192, Training Loss: 0.10301909391238519\n",
      "Epoch: 8 - Batch: 1193, Training Loss: 0.1031026405283863\n",
      "Epoch: 8 - Batch: 1194, Training Loss: 0.10319161340347174\n",
      "Epoch: 8 - Batch: 1195, Training Loss: 0.1032733153212031\n",
      "Epoch: 8 - Batch: 1196, Training Loss: 0.10335632062719434\n",
      "Epoch: 8 - Batch: 1197, Training Loss: 0.10343835084617237\n",
      "Epoch: 8 - Batch: 1198, Training Loss: 0.10352535424120786\n",
      "Epoch: 8 - Batch: 1199, Training Loss: 0.1036073839906634\n",
      "Epoch: 8 - Batch: 1200, Training Loss: 0.10369354931894029\n",
      "Epoch: 8 - Batch: 1201, Training Loss: 0.10378352275263412\n",
      "Epoch: 8 - Batch: 1202, Training Loss: 0.10387157609880861\n",
      "Epoch: 8 - Batch: 1203, Training Loss: 0.10395401464790649\n",
      "Epoch: 8 - Batch: 1204, Training Loss: 0.10404327163970095\n",
      "Epoch: 8 - Batch: 1205, Training Loss: 0.10412671022228341\n",
      "Epoch: 8 - Batch: 1206, Training Loss: 0.10421258901210369\n",
      "Epoch: 8 - Batch: 1207, Training Loss: 0.10430348667611135\n",
      "Epoch: 8 - Batch: 1208, Training Loss: 0.10438643997630867\n",
      "Epoch: 8 - Batch: 1209, Training Loss: 0.1044816334683998\n",
      "Epoch: 8 - Batch: 1210, Training Loss: 0.10457794735235955\n",
      "Epoch: 8 - Batch: 1211, Training Loss: 0.10466988980275876\n",
      "Epoch: 8 - Batch: 1212, Training Loss: 0.10475019075186494\n",
      "Epoch: 8 - Batch: 1213, Training Loss: 0.10484028850399439\n",
      "Epoch: 8 - Batch: 1214, Training Loss: 0.10492487278941456\n",
      "Epoch: 8 - Batch: 1215, Training Loss: 0.10501165156118313\n",
      "Epoch: 8 - Batch: 1216, Training Loss: 0.10509495383397263\n",
      "Epoch: 8 - Batch: 1217, Training Loss: 0.10518757695607087\n",
      "Epoch: 8 - Batch: 1218, Training Loss: 0.10527222733295973\n",
      "Epoch: 8 - Batch: 1219, Training Loss: 0.10535745423404534\n",
      "Epoch: 8 - Batch: 1220, Training Loss: 0.1054359977110405\n",
      "Epoch: 8 - Batch: 1221, Training Loss: 0.10552149734909262\n",
      "Epoch: 8 - Batch: 1222, Training Loss: 0.10561645251527355\n",
      "Epoch: 8 - Batch: 1223, Training Loss: 0.10570601786596463\n",
      "Epoch: 8 - Batch: 1224, Training Loss: 0.10579209839600828\n",
      "Epoch: 8 - Batch: 1225, Training Loss: 0.10588155212390482\n",
      "Epoch: 8 - Batch: 1226, Training Loss: 0.10596457851516271\n",
      "Epoch: 8 - Batch: 1227, Training Loss: 0.10604175281514773\n",
      "Epoch: 8 - Batch: 1228, Training Loss: 0.10612317862894206\n",
      "Epoch: 8 - Batch: 1229, Training Loss: 0.10621521379149969\n",
      "Epoch: 8 - Batch: 1230, Training Loss: 0.10629681649789288\n",
      "Epoch: 8 - Batch: 1231, Training Loss: 0.1063867517025712\n",
      "Epoch: 8 - Batch: 1232, Training Loss: 0.10647791195321044\n",
      "Epoch: 8 - Batch: 1233, Training Loss: 0.10656740207527803\n",
      "Epoch: 8 - Batch: 1234, Training Loss: 0.10665395228843981\n",
      "Epoch: 8 - Batch: 1235, Training Loss: 0.10673507280141165\n",
      "Epoch: 8 - Batch: 1236, Training Loss: 0.10682926928102476\n",
      "Epoch: 8 - Batch: 1237, Training Loss: 0.10691732082942232\n",
      "Epoch: 8 - Batch: 1238, Training Loss: 0.1070016097604833\n",
      "Epoch: 8 - Batch: 1239, Training Loss: 0.10708829973689953\n",
      "Epoch: 8 - Batch: 1240, Training Loss: 0.10717190582163101\n",
      "Epoch: 8 - Batch: 1241, Training Loss: 0.10726399879105648\n",
      "Epoch: 8 - Batch: 1242, Training Loss: 0.10735695759702481\n",
      "Epoch: 8 - Batch: 1243, Training Loss: 0.10743506551777347\n",
      "Epoch: 8 - Batch: 1244, Training Loss: 0.10751915642154553\n",
      "Epoch: 8 - Batch: 1245, Training Loss: 0.10759489229613078\n",
      "Epoch: 8 - Batch: 1246, Training Loss: 0.10769447574320914\n",
      "Epoch: 8 - Batch: 1247, Training Loss: 0.10777522752039863\n",
      "Epoch: 8 - Batch: 1248, Training Loss: 0.10785840065596906\n",
      "Epoch: 8 - Batch: 1249, Training Loss: 0.10795318536077368\n",
      "Epoch: 8 - Batch: 1250, Training Loss: 0.10803333815699984\n",
      "Epoch: 8 - Batch: 1251, Training Loss: 0.10812624845388122\n",
      "Epoch: 8 - Batch: 1252, Training Loss: 0.10821260891817695\n",
      "Epoch: 8 - Batch: 1253, Training Loss: 0.10830509099818976\n",
      "Epoch: 8 - Batch: 1254, Training Loss: 0.10838986388338145\n",
      "Epoch: 8 - Batch: 1255, Training Loss: 0.10848271596565176\n",
      "Epoch: 8 - Batch: 1256, Training Loss: 0.10857364413626554\n",
      "Epoch: 8 - Batch: 1257, Training Loss: 0.10865888737796946\n",
      "Epoch: 8 - Batch: 1258, Training Loss: 0.10873719952864631\n",
      "Epoch: 8 - Batch: 1259, Training Loss: 0.10883009801950819\n",
      "Epoch: 8 - Batch: 1260, Training Loss: 0.1089205602014045\n",
      "Epoch: 8 - Batch: 1261, Training Loss: 0.10900702804425857\n",
      "Epoch: 8 - Batch: 1262, Training Loss: 0.10908923611341424\n",
      "Epoch: 8 - Batch: 1263, Training Loss: 0.1091809130179546\n",
      "Epoch: 8 - Batch: 1264, Training Loss: 0.10926873825033308\n",
      "Epoch: 8 - Batch: 1265, Training Loss: 0.10935645420752947\n",
      "Epoch: 8 - Batch: 1266, Training Loss: 0.10944313082726638\n",
      "Epoch: 8 - Batch: 1267, Training Loss: 0.10954166108597176\n",
      "Epoch: 8 - Batch: 1268, Training Loss: 0.10961629152545091\n",
      "Epoch: 8 - Batch: 1269, Training Loss: 0.10970007053655179\n",
      "Epoch: 8 - Batch: 1270, Training Loss: 0.1097858926871325\n",
      "Epoch: 8 - Batch: 1271, Training Loss: 0.10986536772146352\n",
      "Epoch: 8 - Batch: 1272, Training Loss: 0.1099557304263708\n",
      "Epoch: 8 - Batch: 1273, Training Loss: 0.11004089347857543\n",
      "Epoch: 8 - Batch: 1274, Training Loss: 0.1101273713976293\n",
      "Epoch: 8 - Batch: 1275, Training Loss: 0.11020848413876831\n",
      "Epoch: 8 - Batch: 1276, Training Loss: 0.11028409717144262\n",
      "Epoch: 8 - Batch: 1277, Training Loss: 0.11036822056493553\n",
      "Epoch: 8 - Batch: 1278, Training Loss: 0.11045916084428727\n",
      "Epoch: 8 - Batch: 1279, Training Loss: 0.11054478129171218\n",
      "Epoch: 8 - Batch: 1280, Training Loss: 0.11063267380172143\n",
      "Epoch: 8 - Batch: 1281, Training Loss: 0.1107164238815877\n",
      "Epoch: 8 - Batch: 1282, Training Loss: 0.1108084714741355\n",
      "Epoch: 8 - Batch: 1283, Training Loss: 0.11089762578877446\n",
      "Epoch: 8 - Batch: 1284, Training Loss: 0.11098484484908197\n",
      "Epoch: 8 - Batch: 1285, Training Loss: 0.1110705586635256\n",
      "Epoch: 8 - Batch: 1286, Training Loss: 0.11116036256871017\n",
      "Epoch: 8 - Batch: 1287, Training Loss: 0.11125269273966304\n",
      "Epoch: 8 - Batch: 1288, Training Loss: 0.11133332139075693\n",
      "Epoch: 8 - Batch: 1289, Training Loss: 0.11142214112737484\n",
      "Epoch: 8 - Batch: 1290, Training Loss: 0.11149960403542218\n",
      "Epoch: 8 - Batch: 1291, Training Loss: 0.1115880279584014\n",
      "Epoch: 8 - Batch: 1292, Training Loss: 0.11167263457085165\n",
      "Epoch: 8 - Batch: 1293, Training Loss: 0.11175839341620901\n",
      "Epoch: 8 - Batch: 1294, Training Loss: 0.11183210630989193\n",
      "Epoch: 8 - Batch: 1295, Training Loss: 0.11191519926831893\n",
      "Epoch: 8 - Batch: 1296, Training Loss: 0.11200130176707286\n",
      "Epoch: 8 - Batch: 1297, Training Loss: 0.11209695422975578\n",
      "Epoch: 8 - Batch: 1298, Training Loss: 0.11218764931951984\n",
      "Epoch: 8 - Batch: 1299, Training Loss: 0.11227372218275544\n",
      "Epoch: 8 - Batch: 1300, Training Loss: 0.11235665916704618\n",
      "Epoch: 8 - Batch: 1301, Training Loss: 0.11244843808152585\n",
      "Epoch: 8 - Batch: 1302, Training Loss: 0.11252708407505037\n",
      "Epoch: 8 - Batch: 1303, Training Loss: 0.11261651870079499\n",
      "Epoch: 8 - Batch: 1304, Training Loss: 0.1127012889418337\n",
      "Epoch: 8 - Batch: 1305, Training Loss: 0.11278718872425172\n",
      "Epoch: 8 - Batch: 1306, Training Loss: 0.11286902853332546\n",
      "Epoch: 8 - Batch: 1307, Training Loss: 0.11295577290021562\n",
      "Epoch: 8 - Batch: 1308, Training Loss: 0.11304111442039064\n",
      "Epoch: 8 - Batch: 1309, Training Loss: 0.11312790009290424\n",
      "Epoch: 8 - Batch: 1310, Training Loss: 0.11321167700968769\n",
      "Epoch: 8 - Batch: 1311, Training Loss: 0.11330395824527661\n",
      "Epoch: 8 - Batch: 1312, Training Loss: 0.11338857704059994\n",
      "Epoch: 8 - Batch: 1313, Training Loss: 0.11347347515856053\n",
      "Epoch: 8 - Batch: 1314, Training Loss: 0.11355644234056102\n",
      "Epoch: 8 - Batch: 1315, Training Loss: 0.11364326470378619\n",
      "Epoch: 8 - Batch: 1316, Training Loss: 0.1137386404570952\n",
      "Epoch: 8 - Batch: 1317, Training Loss: 0.11382593934240429\n",
      "Epoch: 8 - Batch: 1318, Training Loss: 0.11391364781813045\n",
      "Epoch: 8 - Batch: 1319, Training Loss: 0.11399513448451092\n",
      "Epoch: 8 - Batch: 1320, Training Loss: 0.11408233905757838\n",
      "Epoch: 8 - Batch: 1321, Training Loss: 0.11417541132736364\n",
      "Epoch: 8 - Batch: 1322, Training Loss: 0.11425758010490024\n",
      "Epoch: 8 - Batch: 1323, Training Loss: 0.11434023897124958\n",
      "Epoch: 8 - Batch: 1324, Training Loss: 0.11441999362475837\n",
      "Epoch: 8 - Batch: 1325, Training Loss: 0.11450785900748785\n",
      "Epoch: 8 - Batch: 1326, Training Loss: 0.11459219409384537\n",
      "Epoch: 8 - Batch: 1327, Training Loss: 0.11467380127923603\n",
      "Epoch: 8 - Batch: 1328, Training Loss: 0.11475951863363212\n",
      "Epoch: 8 - Batch: 1329, Training Loss: 0.11485133995958427\n",
      "Epoch: 8 - Batch: 1330, Training Loss: 0.11493760907086567\n",
      "Epoch: 8 - Batch: 1331, Training Loss: 0.11503173417688208\n",
      "Epoch: 8 - Batch: 1332, Training Loss: 0.11511849622500081\n",
      "Epoch: 8 - Batch: 1333, Training Loss: 0.1151972287095107\n",
      "Epoch: 8 - Batch: 1334, Training Loss: 0.11528264542741957\n",
      "Epoch: 8 - Batch: 1335, Training Loss: 0.11536556355049757\n",
      "Epoch: 8 - Batch: 1336, Training Loss: 0.11544825697617349\n",
      "Epoch: 8 - Batch: 1337, Training Loss: 0.11552806705804804\n",
      "Epoch: 8 - Batch: 1338, Training Loss: 0.11562964815034796\n",
      "Epoch: 8 - Batch: 1339, Training Loss: 0.11571456152778953\n",
      "Epoch: 8 - Batch: 1340, Training Loss: 0.1158056893072417\n",
      "Epoch: 8 - Batch: 1341, Training Loss: 0.11590598091161863\n",
      "Epoch: 8 - Batch: 1342, Training Loss: 0.11598583266361435\n",
      "Epoch: 8 - Batch: 1343, Training Loss: 0.11607554275079153\n",
      "Epoch: 8 - Batch: 1344, Training Loss: 0.11615525448005987\n",
      "Epoch: 8 - Batch: 1345, Training Loss: 0.11623670570367012\n",
      "Epoch: 8 - Batch: 1346, Training Loss: 0.11632174486989405\n",
      "Epoch: 8 - Batch: 1347, Training Loss: 0.11640727407550733\n",
      "Epoch: 8 - Batch: 1348, Training Loss: 0.11648950900629187\n",
      "Epoch: 8 - Batch: 1349, Training Loss: 0.11657443539444882\n",
      "Epoch: 8 - Batch: 1350, Training Loss: 0.11666603975776416\n",
      "Epoch: 8 - Batch: 1351, Training Loss: 0.11674774072409468\n",
      "Epoch: 8 - Batch: 1352, Training Loss: 0.11683856514259357\n",
      "Epoch: 8 - Batch: 1353, Training Loss: 0.1169239146322951\n",
      "Epoch: 8 - Batch: 1354, Training Loss: 0.11701119403242076\n",
      "Epoch: 8 - Batch: 1355, Training Loss: 0.11709610700829705\n",
      "Epoch: 8 - Batch: 1356, Training Loss: 0.11717313101783913\n",
      "Epoch: 8 - Batch: 1357, Training Loss: 0.11725425466302022\n",
      "Epoch: 8 - Batch: 1358, Training Loss: 0.11733427599912655\n",
      "Epoch: 8 - Batch: 1359, Training Loss: 0.11742318666248179\n",
      "Epoch: 8 - Batch: 1360, Training Loss: 0.11751099743233194\n",
      "Epoch: 8 - Batch: 1361, Training Loss: 0.11759910713356131\n",
      "Epoch: 8 - Batch: 1362, Training Loss: 0.11768900995972145\n",
      "Epoch: 8 - Batch: 1363, Training Loss: 0.11778524546431467\n",
      "Epoch: 8 - Batch: 1364, Training Loss: 0.11787642035664216\n",
      "Epoch: 8 - Batch: 1365, Training Loss: 0.11797004146733094\n",
      "Epoch: 8 - Batch: 1366, Training Loss: 0.11805733161705051\n",
      "Epoch: 8 - Batch: 1367, Training Loss: 0.11813800812854894\n",
      "Epoch: 8 - Batch: 1368, Training Loss: 0.1182305696505318\n",
      "Epoch: 8 - Batch: 1369, Training Loss: 0.11832135848416815\n",
      "Epoch: 8 - Batch: 1370, Training Loss: 0.11842187177729646\n",
      "Epoch: 8 - Batch: 1371, Training Loss: 0.1185050660104894\n",
      "Epoch: 8 - Batch: 1372, Training Loss: 0.11859051508220472\n",
      "Epoch: 8 - Batch: 1373, Training Loss: 0.11867361169796481\n",
      "Epoch: 8 - Batch: 1374, Training Loss: 0.11874944032795394\n",
      "Epoch: 8 - Batch: 1375, Training Loss: 0.11883863332877508\n",
      "Epoch: 8 - Batch: 1376, Training Loss: 0.11892726636842313\n",
      "Epoch: 8 - Batch: 1377, Training Loss: 0.11900378744524115\n",
      "Epoch: 8 - Batch: 1378, Training Loss: 0.11909856592887275\n",
      "Epoch: 8 - Batch: 1379, Training Loss: 0.11917758760834808\n",
      "Epoch: 8 - Batch: 1380, Training Loss: 0.11926492566789561\n",
      "Epoch: 8 - Batch: 1381, Training Loss: 0.11935734639491015\n",
      "Epoch: 8 - Batch: 1382, Training Loss: 0.11944251370370684\n",
      "Epoch: 8 - Batch: 1383, Training Loss: 0.1195254994627354\n",
      "Epoch: 8 - Batch: 1384, Training Loss: 0.11960542334821292\n",
      "Epoch: 8 - Batch: 1385, Training Loss: 0.11968144054708393\n",
      "Epoch: 8 - Batch: 1386, Training Loss: 0.11977306291288009\n",
      "Epoch: 8 - Batch: 1387, Training Loss: 0.1198571616626201\n",
      "Epoch: 8 - Batch: 1388, Training Loss: 0.11994059608127941\n",
      "Epoch: 8 - Batch: 1389, Training Loss: 0.1200246277449044\n",
      "Epoch: 8 - Batch: 1390, Training Loss: 0.1201010836198753\n",
      "Epoch: 8 - Batch: 1391, Training Loss: 0.12019048867188085\n",
      "Epoch: 8 - Batch: 1392, Training Loss: 0.12027162590232457\n",
      "Epoch: 8 - Batch: 1393, Training Loss: 0.12035651808329681\n",
      "Epoch: 8 - Batch: 1394, Training Loss: 0.12044788341641821\n",
      "Epoch: 8 - Batch: 1395, Training Loss: 0.12052697472113677\n",
      "Epoch: 8 - Batch: 1396, Training Loss: 0.12061936119154318\n",
      "Epoch: 8 - Batch: 1397, Training Loss: 0.12070521777789193\n",
      "Epoch: 8 - Batch: 1398, Training Loss: 0.12079287091174332\n",
      "Epoch: 8 - Batch: 1399, Training Loss: 0.12087546471215994\n",
      "Epoch: 8 - Batch: 1400, Training Loss: 0.12096287133434716\n",
      "Epoch: 8 - Batch: 1401, Training Loss: 0.12105274885571615\n",
      "Epoch: 8 - Batch: 1402, Training Loss: 0.12113330450841839\n",
      "Epoch: 8 - Batch: 1403, Training Loss: 0.12122594182121615\n",
      "Epoch: 8 - Batch: 1404, Training Loss: 0.12130698452319079\n",
      "Epoch: 8 - Batch: 1405, Training Loss: 0.12138433450490088\n",
      "Epoch: 8 - Batch: 1406, Training Loss: 0.12147228294368802\n",
      "Epoch: 8 - Batch: 1407, Training Loss: 0.12155665168739473\n",
      "Epoch: 8 - Batch: 1408, Training Loss: 0.12163768235476653\n",
      "Epoch: 8 - Batch: 1409, Training Loss: 0.12172867006824582\n",
      "Epoch: 8 - Batch: 1410, Training Loss: 0.12181424317421209\n",
      "Epoch: 8 - Batch: 1411, Training Loss: 0.12189751629336161\n",
      "Epoch: 8 - Batch: 1412, Training Loss: 0.12198690603028482\n",
      "Epoch: 8 - Batch: 1413, Training Loss: 0.12207234727456598\n",
      "Epoch: 8 - Batch: 1414, Training Loss: 0.12215474613047951\n",
      "Epoch: 8 - Batch: 1415, Training Loss: 0.1222370863763946\n",
      "Epoch: 8 - Batch: 1416, Training Loss: 0.12233394921863851\n",
      "Epoch: 8 - Batch: 1417, Training Loss: 0.12241669654055416\n",
      "Epoch: 8 - Batch: 1418, Training Loss: 0.12250491320642072\n",
      "Epoch: 8 - Batch: 1419, Training Loss: 0.12260672016992893\n",
      "Epoch: 8 - Batch: 1420, Training Loss: 0.12268882189841808\n",
      "Epoch: 8 - Batch: 1421, Training Loss: 0.12277836720791227\n",
      "Epoch: 8 - Batch: 1422, Training Loss: 0.12285336505491935\n",
      "Epoch: 8 - Batch: 1423, Training Loss: 0.12293735928648147\n",
      "Epoch: 8 - Batch: 1424, Training Loss: 0.12301902534752145\n",
      "Epoch: 8 - Batch: 1425, Training Loss: 0.12310196868889961\n",
      "Epoch: 8 - Batch: 1426, Training Loss: 0.12319268731038961\n",
      "Epoch: 8 - Batch: 1427, Training Loss: 0.12327503644016449\n",
      "Epoch: 8 - Batch: 1428, Training Loss: 0.12336196956127438\n",
      "Epoch: 8 - Batch: 1429, Training Loss: 0.1234535126519045\n",
      "Epoch: 8 - Batch: 1430, Training Loss: 0.123535415136053\n",
      "Epoch: 8 - Batch: 1431, Training Loss: 0.12362342359958399\n",
      "Epoch: 8 - Batch: 1432, Training Loss: 0.12370499680019532\n",
      "Epoch: 8 - Batch: 1433, Training Loss: 0.12379506423105648\n",
      "Epoch: 8 - Batch: 1434, Training Loss: 0.12388238068021352\n",
      "Epoch: 8 - Batch: 1435, Training Loss: 0.1239667303217683\n",
      "Epoch: 8 - Batch: 1436, Training Loss: 0.12405815359767199\n",
      "Epoch: 8 - Batch: 1437, Training Loss: 0.1241470056077833\n",
      "Epoch: 8 - Batch: 1438, Training Loss: 0.12424362240170761\n",
      "Epoch: 8 - Batch: 1439, Training Loss: 0.12433496059519339\n",
      "Epoch: 8 - Batch: 1440, Training Loss: 0.12442026399137764\n",
      "Epoch: 8 - Batch: 1441, Training Loss: 0.12450096630807934\n",
      "Epoch: 8 - Batch: 1442, Training Loss: 0.12458933849709346\n",
      "Epoch: 8 - Batch: 1443, Training Loss: 0.12468049211881647\n",
      "Epoch: 8 - Batch: 1444, Training Loss: 0.1247712760101108\n",
      "Epoch: 8 - Batch: 1445, Training Loss: 0.12486076713300265\n",
      "Epoch: 8 - Batch: 1446, Training Loss: 0.12494201640633999\n",
      "Epoch: 8 - Batch: 1447, Training Loss: 0.12502670784791312\n",
      "Epoch: 8 - Batch: 1448, Training Loss: 0.1251218335310717\n",
      "Epoch: 8 - Batch: 1449, Training Loss: 0.1252042479368288\n",
      "Epoch: 8 - Batch: 1450, Training Loss: 0.1252841713157163\n",
      "Epoch: 8 - Batch: 1451, Training Loss: 0.12536879523254152\n",
      "Epoch: 8 - Batch: 1452, Training Loss: 0.12545250758998233\n",
      "Epoch: 8 - Batch: 1453, Training Loss: 0.1255345965795849\n",
      "Epoch: 8 - Batch: 1454, Training Loss: 0.12562241612209213\n",
      "Epoch: 8 - Batch: 1455, Training Loss: 0.12571469630743337\n",
      "Epoch: 8 - Batch: 1456, Training Loss: 0.12579969311160827\n",
      "Epoch: 8 - Batch: 1457, Training Loss: 0.1258848920266822\n",
      "Epoch: 8 - Batch: 1458, Training Loss: 0.12598276195142596\n",
      "Epoch: 8 - Batch: 1459, Training Loss: 0.12607160701136882\n",
      "Epoch: 8 - Batch: 1460, Training Loss: 0.12615127041071011\n",
      "Epoch: 8 - Batch: 1461, Training Loss: 0.12623407542829093\n",
      "Epoch: 8 - Batch: 1462, Training Loss: 0.12632332598886284\n",
      "Epoch: 8 - Batch: 1463, Training Loss: 0.12640455054431216\n",
      "Epoch: 8 - Batch: 1464, Training Loss: 0.12648999707072134\n",
      "Epoch: 8 - Batch: 1465, Training Loss: 0.12658638670225048\n",
      "Epoch: 8 - Batch: 1466, Training Loss: 0.12667182008820782\n",
      "Epoch: 8 - Batch: 1467, Training Loss: 0.12675207817485282\n",
      "Epoch: 8 - Batch: 1468, Training Loss: 0.12684128739891162\n",
      "Epoch: 8 - Batch: 1469, Training Loss: 0.12692584402575027\n",
      "Epoch: 8 - Batch: 1470, Training Loss: 0.12700974901097134\n",
      "Epoch: 8 - Batch: 1471, Training Loss: 0.1270929966214581\n",
      "Epoch: 8 - Batch: 1472, Training Loss: 0.12717012437693712\n",
      "Epoch: 8 - Batch: 1473, Training Loss: 0.1272520967025563\n",
      "Epoch: 8 - Batch: 1474, Training Loss: 0.127341562520657\n",
      "Epoch: 8 - Batch: 1475, Training Loss: 0.12741403646186414\n",
      "Epoch: 8 - Batch: 1476, Training Loss: 0.12749898111815278\n",
      "Epoch: 8 - Batch: 1477, Training Loss: 0.127596893627351\n",
      "Epoch: 8 - Batch: 1478, Training Loss: 0.12768533297661525\n",
      "Epoch: 8 - Batch: 1479, Training Loss: 0.12777418754438855\n",
      "Epoch: 8 - Batch: 1480, Training Loss: 0.12785026245128062\n",
      "Epoch: 8 - Batch: 1481, Training Loss: 0.12793807375243252\n",
      "Epoch: 8 - Batch: 1482, Training Loss: 0.12802240587956276\n",
      "Epoch: 8 - Batch: 1483, Training Loss: 0.128099940402146\n",
      "Epoch: 8 - Batch: 1484, Training Loss: 0.1281900595546164\n",
      "Epoch: 8 - Batch: 1485, Training Loss: 0.1282690439873667\n",
      "Epoch: 8 - Batch: 1486, Training Loss: 0.1283599732885412\n",
      "Epoch: 8 - Batch: 1487, Training Loss: 0.12844275695520452\n",
      "Epoch: 8 - Batch: 1488, Training Loss: 0.1285305436249594\n",
      "Epoch: 8 - Batch: 1489, Training Loss: 0.12861171700245705\n",
      "Epoch: 8 - Batch: 1490, Training Loss: 0.12869628985253337\n",
      "Epoch: 8 - Batch: 1491, Training Loss: 0.12877548265442326\n",
      "Epoch: 8 - Batch: 1492, Training Loss: 0.12886658612373061\n",
      "Epoch: 8 - Batch: 1493, Training Loss: 0.12895223485370774\n",
      "Epoch: 8 - Batch: 1494, Training Loss: 0.12903888061469665\n",
      "Epoch: 8 - Batch: 1495, Training Loss: 0.12912517158966358\n",
      "Epoch: 8 - Batch: 1496, Training Loss: 0.12920863690413845\n",
      "Epoch: 8 - Batch: 1497, Training Loss: 0.12930075383774478\n",
      "Epoch: 8 - Batch: 1498, Training Loss: 0.12938040567200576\n",
      "Epoch: 8 - Batch: 1499, Training Loss: 0.12945929857875974\n",
      "Epoch: 8 - Batch: 1500, Training Loss: 0.12954167134528533\n",
      "Epoch: 8 - Batch: 1501, Training Loss: 0.12961852457837678\n",
      "Epoch: 8 - Batch: 1502, Training Loss: 0.12969899598885926\n",
      "Epoch: 8 - Batch: 1503, Training Loss: 0.1297805922640299\n",
      "Epoch: 8 - Batch: 1504, Training Loss: 0.12986373239093357\n",
      "Epoch: 8 - Batch: 1505, Training Loss: 0.12994904291644618\n",
      "Epoch: 8 - Batch: 1506, Training Loss: 0.13003016449819949\n",
      "Epoch: 8 - Batch: 1507, Training Loss: 0.13011123687893794\n",
      "Epoch: 8 - Batch: 1508, Training Loss: 0.13019579401236664\n",
      "Epoch: 8 - Batch: 1509, Training Loss: 0.13027836189019937\n",
      "Epoch: 8 - Batch: 1510, Training Loss: 0.13035892989257872\n",
      "Epoch: 8 - Batch: 1511, Training Loss: 0.13044742705488876\n",
      "Epoch: 8 - Batch: 1512, Training Loss: 0.13053234232277616\n",
      "Epoch: 8 - Batch: 1513, Training Loss: 0.13061880361702707\n",
      "Epoch: 8 - Batch: 1514, Training Loss: 0.1307119746533397\n",
      "Epoch: 8 - Batch: 1515, Training Loss: 0.13079657442682419\n",
      "Epoch: 8 - Batch: 1516, Training Loss: 0.13088399551771768\n",
      "Epoch: 8 - Batch: 1517, Training Loss: 0.1309630638343679\n",
      "Epoch: 8 - Batch: 1518, Training Loss: 0.13104564758947437\n",
      "Epoch: 8 - Batch: 1519, Training Loss: 0.13113041763899733\n",
      "Epoch: 8 - Batch: 1520, Training Loss: 0.1312262095498604\n",
      "Epoch: 8 - Batch: 1521, Training Loss: 0.13131837986076056\n",
      "Epoch: 8 - Batch: 1522, Training Loss: 0.1313968491779137\n",
      "Epoch: 8 - Batch: 1523, Training Loss: 0.13147780336872064\n",
      "Epoch: 8 - Batch: 1524, Training Loss: 0.13156330165998456\n",
      "Epoch: 8 - Batch: 1525, Training Loss: 0.1316545945445499\n",
      "Epoch: 8 - Batch: 1526, Training Loss: 0.1317409541068682\n",
      "Epoch: 8 - Batch: 1527, Training Loss: 0.13182332395123408\n",
      "Epoch: 8 - Batch: 1528, Training Loss: 0.13190200884716824\n",
      "Epoch: 8 - Batch: 1529, Training Loss: 0.13199146581229879\n",
      "Epoch: 8 - Batch: 1530, Training Loss: 0.13206931372261166\n",
      "Epoch: 8 - Batch: 1531, Training Loss: 0.13215910654459426\n",
      "Epoch: 8 - Batch: 1532, Training Loss: 0.13224698326999867\n",
      "Epoch: 8 - Batch: 1533, Training Loss: 0.13233096696819438\n",
      "Epoch: 8 - Batch: 1534, Training Loss: 0.13241666239094774\n",
      "Epoch: 8 - Batch: 1535, Training Loss: 0.13249671158332926\n",
      "Epoch: 8 - Batch: 1536, Training Loss: 0.13257799606467557\n",
      "Epoch: 8 - Batch: 1537, Training Loss: 0.13266541061314382\n",
      "Epoch: 8 - Batch: 1538, Training Loss: 0.13275337179699545\n",
      "Epoch: 8 - Batch: 1539, Training Loss: 0.13284399711942396\n",
      "Epoch: 8 - Batch: 1540, Training Loss: 0.13292898732557226\n",
      "Epoch: 8 - Batch: 1541, Training Loss: 0.13301261862242597\n",
      "Epoch: 8 - Batch: 1542, Training Loss: 0.1330957272975006\n",
      "Epoch: 8 - Batch: 1543, Training Loss: 0.13318653405948264\n",
      "Epoch: 8 - Batch: 1544, Training Loss: 0.1332761996408699\n",
      "Epoch: 8 - Batch: 1545, Training Loss: 0.13336718810805634\n",
      "Epoch: 8 - Batch: 1546, Training Loss: 0.13344953920241218\n",
      "Epoch: 8 - Batch: 1547, Training Loss: 0.13352957359865728\n",
      "Epoch: 8 - Batch: 1548, Training Loss: 0.13361482348163328\n",
      "Epoch: 8 - Batch: 1549, Training Loss: 0.1337164118638877\n",
      "Epoch: 8 - Batch: 1550, Training Loss: 0.1337985115660166\n",
      "Epoch: 8 - Batch: 1551, Training Loss: 0.13387986004451416\n",
      "Epoch: 8 - Batch: 1552, Training Loss: 0.13396933364680355\n",
      "Epoch: 8 - Batch: 1553, Training Loss: 0.1340606328452108\n",
      "Epoch: 8 - Batch: 1554, Training Loss: 0.13414517670227322\n",
      "Epoch: 8 - Batch: 1555, Training Loss: 0.1342341791881653\n",
      "Epoch: 8 - Batch: 1556, Training Loss: 0.13431713075236498\n",
      "Epoch: 8 - Batch: 1557, Training Loss: 0.1343991369515608\n",
      "Epoch: 8 - Batch: 1558, Training Loss: 0.134485399260944\n",
      "Epoch: 8 - Batch: 1559, Training Loss: 0.13456895520561568\n",
      "Epoch: 8 - Batch: 1560, Training Loss: 0.13465926448998364\n",
      "Epoch: 8 - Batch: 1561, Training Loss: 0.13474739544946163\n",
      "Epoch: 8 - Batch: 1562, Training Loss: 0.1348348291742169\n",
      "Epoch: 8 - Batch: 1563, Training Loss: 0.13492341654662468\n",
      "Epoch: 8 - Batch: 1564, Training Loss: 0.1350129136744621\n",
      "Epoch: 8 - Batch: 1565, Training Loss: 0.1350959805311453\n",
      "Epoch: 8 - Batch: 1566, Training Loss: 0.13518032821429704\n",
      "Epoch: 8 - Batch: 1567, Training Loss: 0.13526109505327386\n",
      "Epoch: 8 - Batch: 1568, Training Loss: 0.13534730368264478\n",
      "Epoch: 8 - Batch: 1569, Training Loss: 0.13543269817237039\n",
      "Epoch: 8 - Batch: 1570, Training Loss: 0.13551675380017628\n",
      "Epoch: 8 - Batch: 1571, Training Loss: 0.13560659427869182\n",
      "Epoch: 8 - Batch: 1572, Training Loss: 0.13569355833268482\n",
      "Epoch: 8 - Batch: 1573, Training Loss: 0.1357745722269834\n",
      "Epoch: 8 - Batch: 1574, Training Loss: 0.13586397505493505\n",
      "Epoch: 8 - Batch: 1575, Training Loss: 0.13593762222782493\n",
      "Epoch: 8 - Batch: 1576, Training Loss: 0.13602759210303253\n",
      "Epoch: 8 - Batch: 1577, Training Loss: 0.13611341088046483\n",
      "Epoch: 8 - Batch: 1578, Training Loss: 0.1362016106496996\n",
      "Epoch: 8 - Batch: 1579, Training Loss: 0.1362874378661809\n",
      "Epoch: 8 - Batch: 1580, Training Loss: 0.13637287306844892\n",
      "Epoch: 8 - Batch: 1581, Training Loss: 0.13645795192785723\n",
      "Epoch: 8 - Batch: 1582, Training Loss: 0.13654182012351987\n",
      "Epoch: 8 - Batch: 1583, Training Loss: 0.13662254231461443\n",
      "Epoch: 8 - Batch: 1584, Training Loss: 0.1367043089893821\n",
      "Epoch: 8 - Batch: 1585, Training Loss: 0.13678666338892917\n",
      "Epoch: 8 - Batch: 1586, Training Loss: 0.13686681019874355\n",
      "Epoch: 8 - Batch: 1587, Training Loss: 0.13694903900720193\n",
      "Epoch: 8 - Batch: 1588, Training Loss: 0.1370391406699595\n",
      "Epoch: 8 - Batch: 1589, Training Loss: 0.1371267908260498\n",
      "Epoch: 8 - Batch: 1590, Training Loss: 0.13721062920085628\n",
      "Epoch: 8 - Batch: 1591, Training Loss: 0.13729935248149172\n",
      "Epoch: 8 - Batch: 1592, Training Loss: 0.1373890737012943\n",
      "Epoch: 8 - Batch: 1593, Training Loss: 0.13746999057124107\n",
      "Epoch: 8 - Batch: 1594, Training Loss: 0.1375543481205431\n",
      "Epoch: 8 - Batch: 1595, Training Loss: 0.1376394468667001\n",
      "Epoch: 8 - Batch: 1596, Training Loss: 0.1377270958304504\n",
      "Epoch: 8 - Batch: 1597, Training Loss: 0.13781675443077956\n",
      "Epoch: 8 - Batch: 1598, Training Loss: 0.13790095627777416\n",
      "Epoch: 8 - Batch: 1599, Training Loss: 0.1379800745553243\n",
      "Epoch: 8 - Batch: 1600, Training Loss: 0.1380735505773851\n",
      "Epoch: 8 - Batch: 1601, Training Loss: 0.13816150356312693\n",
      "Epoch: 8 - Batch: 1602, Training Loss: 0.13824449936746563\n",
      "Epoch: 8 - Batch: 1603, Training Loss: 0.13832450931765153\n",
      "Epoch: 8 - Batch: 1604, Training Loss: 0.13841908031558714\n",
      "Epoch: 8 - Batch: 1605, Training Loss: 0.13850705554186804\n",
      "Epoch: 8 - Batch: 1606, Training Loss: 0.1386034140242866\n",
      "Epoch: 8 - Batch: 1607, Training Loss: 0.13869116226115433\n",
      "Epoch: 8 - Batch: 1608, Training Loss: 0.13877478766426518\n",
      "Epoch: 8 - Batch: 1609, Training Loss: 0.13885826951705205\n",
      "Epoch: 8 - Batch: 1610, Training Loss: 0.13894305899615708\n",
      "Epoch: 8 - Batch: 1611, Training Loss: 0.13903618708969545\n",
      "Epoch: 8 - Batch: 1612, Training Loss: 0.13911994916091908\n",
      "Epoch: 8 - Batch: 1613, Training Loss: 0.13920617366756372\n",
      "Epoch: 8 - Batch: 1614, Training Loss: 0.13929447284583032\n",
      "Epoch: 8 - Batch: 1615, Training Loss: 0.1393809715288986\n",
      "Epoch: 8 - Batch: 1616, Training Loss: 0.13946019456556583\n",
      "Epoch: 8 - Batch: 1617, Training Loss: 0.1395448883794631\n",
      "Epoch: 8 - Batch: 1618, Training Loss: 0.13963752979440475\n",
      "Epoch: 8 - Batch: 1619, Training Loss: 0.13972439545254606\n",
      "Epoch: 8 - Batch: 1620, Training Loss: 0.13981267825312677\n",
      "Epoch: 8 - Batch: 1621, Training Loss: 0.13990482663856216\n",
      "Epoch: 8 - Batch: 1622, Training Loss: 0.13999470247335694\n",
      "Epoch: 8 - Batch: 1623, Training Loss: 0.140070288675292\n",
      "Epoch: 8 - Batch: 1624, Training Loss: 0.1401641676588537\n",
      "Epoch: 8 - Batch: 1625, Training Loss: 0.14025328231119794\n",
      "Epoch: 8 - Batch: 1626, Training Loss: 0.14034083257020014\n",
      "Epoch: 8 - Batch: 1627, Training Loss: 0.14043012353441806\n",
      "Epoch: 8 - Batch: 1628, Training Loss: 0.14050945238045\n",
      "Epoch: 8 - Batch: 1629, Training Loss: 0.14059591950048658\n",
      "Epoch: 8 - Batch: 1630, Training Loss: 0.14067371184034133\n",
      "Epoch: 8 - Batch: 1631, Training Loss: 0.14074781090391808\n",
      "Epoch: 8 - Batch: 1632, Training Loss: 0.14082672088646375\n",
      "Epoch: 8 - Batch: 1633, Training Loss: 0.1409181122410159\n",
      "Epoch: 8 - Batch: 1634, Training Loss: 0.1410058079098983\n",
      "Epoch: 8 - Batch: 1635, Training Loss: 0.1410897709429264\n",
      "Epoch: 8 - Batch: 1636, Training Loss: 0.14117534514238586\n",
      "Epoch: 8 - Batch: 1637, Training Loss: 0.14125716161742732\n",
      "Epoch: 8 - Batch: 1638, Training Loss: 0.14133781527059391\n",
      "Epoch: 8 - Batch: 1639, Training Loss: 0.1414267983021922\n",
      "Epoch: 8 - Batch: 1640, Training Loss: 0.14151463151645305\n",
      "Epoch: 8 - Batch: 1641, Training Loss: 0.1415985260424428\n",
      "Epoch: 8 - Batch: 1642, Training Loss: 0.14167585359209806\n",
      "Epoch: 8 - Batch: 1643, Training Loss: 0.1417539557364845\n",
      "Epoch: 8 - Batch: 1644, Training Loss: 0.14184010570025563\n",
      "Epoch: 8 - Batch: 1645, Training Loss: 0.1419279118304822\n",
      "Epoch: 8 - Batch: 1646, Training Loss: 0.1420207654201668\n",
      "Epoch: 8 - Batch: 1647, Training Loss: 0.14210547677937826\n",
      "Epoch: 8 - Batch: 1648, Training Loss: 0.1421942772408623\n",
      "Epoch: 8 - Batch: 1649, Training Loss: 0.14227283894274365\n",
      "Epoch: 8 - Batch: 1650, Training Loss: 0.14235895024504433\n",
      "Epoch: 8 - Batch: 1651, Training Loss: 0.14245282461369413\n",
      "Epoch: 8 - Batch: 1652, Training Loss: 0.14254592275046196\n",
      "Epoch: 8 - Batch: 1653, Training Loss: 0.14262993998244825\n",
      "Epoch: 8 - Batch: 1654, Training Loss: 0.142715562660105\n",
      "Epoch: 8 - Batch: 1655, Training Loss: 0.14279535494954826\n",
      "Epoch: 8 - Batch: 1656, Training Loss: 0.142877967666779\n",
      "Epoch: 8 - Batch: 1657, Training Loss: 0.1429667238819461\n",
      "Epoch: 8 - Batch: 1658, Training Loss: 0.14305435890829188\n",
      "Epoch: 8 - Batch: 1659, Training Loss: 0.14314125960146018\n",
      "Epoch: 8 - Batch: 1660, Training Loss: 0.14323186431432244\n",
      "Epoch: 8 - Batch: 1661, Training Loss: 0.1433203984021053\n",
      "Epoch: 8 - Batch: 1662, Training Loss: 0.14340504094538206\n",
      "Epoch: 8 - Batch: 1663, Training Loss: 0.1435006687952017\n",
      "Epoch: 8 - Batch: 1664, Training Loss: 0.14358696177799507\n",
      "Epoch: 8 - Batch: 1665, Training Loss: 0.14367921396496877\n",
      "Epoch: 8 - Batch: 1666, Training Loss: 0.14377359131576609\n",
      "Epoch: 8 - Batch: 1667, Training Loss: 0.14385196085938964\n",
      "Epoch: 8 - Batch: 1668, Training Loss: 0.14394061739987402\n",
      "Epoch: 8 - Batch: 1669, Training Loss: 0.14401912285172525\n",
      "Epoch: 8 - Batch: 1670, Training Loss: 0.1441054721216163\n",
      "Epoch: 8 - Batch: 1671, Training Loss: 0.14419683434106223\n",
      "Epoch: 8 - Batch: 1672, Training Loss: 0.14428279684797843\n",
      "Epoch: 8 - Batch: 1673, Training Loss: 0.14436341138868586\n",
      "Epoch: 8 - Batch: 1674, Training Loss: 0.14444791562052114\n",
      "Epoch: 8 - Batch: 1675, Training Loss: 0.14452560978692958\n",
      "Epoch: 8 - Batch: 1676, Training Loss: 0.14461617651789344\n",
      "Epoch: 8 - Batch: 1677, Training Loss: 0.14471185530472552\n",
      "Epoch: 8 - Batch: 1678, Training Loss: 0.1447903200378564\n",
      "Epoch: 8 - Batch: 1679, Training Loss: 0.14487189952142004\n",
      "Epoch: 8 - Batch: 1680, Training Loss: 0.14495485519394155\n",
      "Epoch: 8 - Batch: 1681, Training Loss: 0.14504389496980416\n",
      "Epoch: 8 - Batch: 1682, Training Loss: 0.14513327486528885\n",
      "Epoch: 8 - Batch: 1683, Training Loss: 0.14521329437890654\n",
      "Epoch: 8 - Batch: 1684, Training Loss: 0.1453008177392123\n",
      "Epoch: 8 - Batch: 1685, Training Loss: 0.14538259424306266\n",
      "Epoch: 8 - Batch: 1686, Training Loss: 0.1454669214278509\n",
      "Epoch: 8 - Batch: 1687, Training Loss: 0.14554995248344407\n",
      "Epoch: 8 - Batch: 1688, Training Loss: 0.14563567772087568\n",
      "Epoch: 8 - Batch: 1689, Training Loss: 0.14571926074340372\n",
      "Epoch: 8 - Batch: 1690, Training Loss: 0.1458062554090276\n",
      "Epoch: 8 - Batch: 1691, Training Loss: 0.14589280166213786\n",
      "Epoch: 8 - Batch: 1692, Training Loss: 0.1459765876234072\n",
      "Epoch: 8 - Batch: 1693, Training Loss: 0.14605921181304934\n",
      "Epoch: 8 - Batch: 1694, Training Loss: 0.14613916627309018\n",
      "Epoch: 8 - Batch: 1695, Training Loss: 0.14622950579223545\n",
      "Epoch: 8 - Batch: 1696, Training Loss: 0.14631027103730695\n",
      "Epoch: 8 - Batch: 1697, Training Loss: 0.14639564790758328\n",
      "Epoch: 8 - Batch: 1698, Training Loss: 0.1464867131293413\n",
      "Epoch: 8 - Batch: 1699, Training Loss: 0.14656800550607899\n",
      "Epoch: 8 - Batch: 1700, Training Loss: 0.14665376470357822\n",
      "Epoch: 8 - Batch: 1701, Training Loss: 0.1467363886769929\n",
      "Epoch: 8 - Batch: 1702, Training Loss: 0.14683354871461837\n",
      "Epoch: 8 - Batch: 1703, Training Loss: 0.1469213452258118\n",
      "Epoch: 8 - Batch: 1704, Training Loss: 0.147000111268824\n",
      "Epoch: 8 - Batch: 1705, Training Loss: 0.14708628441217922\n",
      "Epoch: 8 - Batch: 1706, Training Loss: 0.14717705599704192\n",
      "Epoch: 8 - Batch: 1707, Training Loss: 0.14726949839622622\n",
      "Epoch: 8 - Batch: 1708, Training Loss: 0.14735744962125868\n",
      "Epoch: 8 - Batch: 1709, Training Loss: 0.1474473596447044\n",
      "Epoch: 8 - Batch: 1710, Training Loss: 0.1475444682201936\n",
      "Epoch: 8 - Batch: 1711, Training Loss: 0.14763487855692212\n",
      "Epoch: 8 - Batch: 1712, Training Loss: 0.14771748429927264\n",
      "Epoch: 8 - Batch: 1713, Training Loss: 0.14780817484208206\n",
      "Epoch: 8 - Batch: 1714, Training Loss: 0.14789883219088687\n",
      "Epoch: 8 - Batch: 1715, Training Loss: 0.1479859969039659\n",
      "Epoch: 8 - Batch: 1716, Training Loss: 0.14807272629555976\n",
      "Epoch: 8 - Batch: 1717, Training Loss: 0.1481614052310314\n",
      "Epoch: 8 - Batch: 1718, Training Loss: 0.14825134665070483\n",
      "Epoch: 8 - Batch: 1719, Training Loss: 0.14833326996682492\n",
      "Epoch: 8 - Batch: 1720, Training Loss: 0.1484145576730494\n",
      "Epoch: 8 - Batch: 1721, Training Loss: 0.14850835555277853\n",
      "Epoch: 8 - Batch: 1722, Training Loss: 0.14859174838780764\n",
      "Epoch: 8 - Batch: 1723, Training Loss: 0.1486760083196768\n",
      "Epoch: 8 - Batch: 1724, Training Loss: 0.1487574107314817\n",
      "Epoch: 8 - Batch: 1725, Training Loss: 0.14884219958661604\n",
      "Epoch: 8 - Batch: 1726, Training Loss: 0.14892769109451554\n",
      "Epoch: 8 - Batch: 1727, Training Loss: 0.14902011620785863\n",
      "Epoch: 8 - Batch: 1728, Training Loss: 0.1491038314318578\n",
      "Epoch: 8 - Batch: 1729, Training Loss: 0.14919052814839293\n",
      "Epoch: 8 - Batch: 1730, Training Loss: 0.14927364494126433\n",
      "Epoch: 8 - Batch: 1731, Training Loss: 0.14936250289454192\n",
      "Epoch: 8 - Batch: 1732, Training Loss: 0.14944436292397245\n",
      "Epoch: 8 - Batch: 1733, Training Loss: 0.1495347746569126\n",
      "Epoch: 8 - Batch: 1734, Training Loss: 0.14962026514545404\n",
      "Epoch: 8 - Batch: 1735, Training Loss: 0.1497077082050578\n",
      "Epoch: 8 - Batch: 1736, Training Loss: 0.14978989219265199\n",
      "Epoch: 8 - Batch: 1737, Training Loss: 0.14986852181466856\n",
      "Epoch: 8 - Batch: 1738, Training Loss: 0.14994977754394015\n",
      "Epoch: 8 - Batch: 1739, Training Loss: 0.15003803072753633\n",
      "Epoch: 8 - Batch: 1740, Training Loss: 0.15012451832740265\n",
      "Epoch: 8 - Batch: 1741, Training Loss: 0.15021586835161962\n",
      "Epoch: 8 - Batch: 1742, Training Loss: 0.15030075866685774\n",
      "Epoch: 8 - Batch: 1743, Training Loss: 0.15038631166984787\n",
      "Epoch: 8 - Batch: 1744, Training Loss: 0.15047251282392649\n",
      "Epoch: 8 - Batch: 1745, Training Loss: 0.15055903852307184\n",
      "Epoch: 8 - Batch: 1746, Training Loss: 0.15064692732632456\n",
      "Epoch: 8 - Batch: 1747, Training Loss: 0.1507345543213646\n",
      "Epoch: 8 - Batch: 1748, Training Loss: 0.15081420840439116\n",
      "Epoch: 8 - Batch: 1749, Training Loss: 0.15090359631264783\n",
      "Epoch: 8 - Batch: 1750, Training Loss: 0.15098954989863667\n",
      "Epoch: 8 - Batch: 1751, Training Loss: 0.15107342128210993\n",
      "Epoch: 8 - Batch: 1752, Training Loss: 0.1511497998773854\n",
      "Epoch: 8 - Batch: 1753, Training Loss: 0.1512400707226883\n",
      "Epoch: 8 - Batch: 1754, Training Loss: 0.15132728125745581\n",
      "Epoch: 8 - Batch: 1755, Training Loss: 0.15141717018327902\n",
      "Epoch: 8 - Batch: 1756, Training Loss: 0.15150088392457559\n",
      "Epoch: 8 - Batch: 1757, Training Loss: 0.15159080957817794\n",
      "Epoch: 8 - Batch: 1758, Training Loss: 0.1516790129985679\n",
      "Epoch: 8 - Batch: 1759, Training Loss: 0.15176707294276895\n",
      "Epoch: 8 - Batch: 1760, Training Loss: 0.1518484135814765\n",
      "Epoch: 8 - Batch: 1761, Training Loss: 0.15193300444021154\n",
      "Epoch: 8 - Batch: 1762, Training Loss: 0.15201564768876008\n",
      "Epoch: 8 - Batch: 1763, Training Loss: 0.15210005170225505\n",
      "Epoch: 8 - Batch: 1764, Training Loss: 0.1521829175291765\n",
      "Epoch: 8 - Batch: 1765, Training Loss: 0.15227477340209938\n",
      "Epoch: 8 - Batch: 1766, Training Loss: 0.15235940510311333\n",
      "Epoch: 8 - Batch: 1767, Training Loss: 0.15244250641409832\n",
      "Epoch: 8 - Batch: 1768, Training Loss: 0.15252855761752002\n",
      "Epoch: 8 - Batch: 1769, Training Loss: 0.15261113961809508\n",
      "Epoch: 8 - Batch: 1770, Training Loss: 0.15270202757040066\n",
      "Epoch: 8 - Batch: 1771, Training Loss: 0.1527889239637314\n",
      "Epoch: 8 - Batch: 1772, Training Loss: 0.15287511790199065\n",
      "Epoch: 8 - Batch: 1773, Training Loss: 0.152962905467545\n",
      "Epoch: 8 - Batch: 1774, Training Loss: 0.15304670052272368\n",
      "Epoch: 8 - Batch: 1775, Training Loss: 0.15313957244849719\n",
      "Epoch: 8 - Batch: 1776, Training Loss: 0.15322394600480943\n",
      "Epoch: 8 - Batch: 1777, Training Loss: 0.15331539404802458\n",
      "Epoch: 8 - Batch: 1778, Training Loss: 0.15340589488521342\n",
      "Epoch: 8 - Batch: 1779, Training Loss: 0.15349491084516542\n",
      "Epoch: 8 - Batch: 1780, Training Loss: 0.15358557650946067\n",
      "Epoch: 8 - Batch: 1781, Training Loss: 0.15366688242821552\n",
      "Epoch: 8 - Batch: 1782, Training Loss: 0.15375177910054105\n",
      "Epoch: 8 - Batch: 1783, Training Loss: 0.15383621979856965\n",
      "Epoch: 8 - Batch: 1784, Training Loss: 0.15392594455214678\n",
      "Epoch: 8 - Batch: 1785, Training Loss: 0.15400969098703582\n",
      "Epoch: 8 - Batch: 1786, Training Loss: 0.1540904818396169\n",
      "Epoch: 8 - Batch: 1787, Training Loss: 0.15417697104574435\n",
      "Epoch: 8 - Batch: 1788, Training Loss: 0.15425797236178249\n",
      "Epoch: 8 - Batch: 1789, Training Loss: 0.15435058686567182\n",
      "Epoch: 8 - Batch: 1790, Training Loss: 0.1544379884281364\n",
      "Epoch: 8 - Batch: 1791, Training Loss: 0.15452501400810964\n",
      "Epoch: 8 - Batch: 1792, Training Loss: 0.15460667193281907\n",
      "Epoch: 8 - Batch: 1793, Training Loss: 0.15469327308917125\n",
      "Epoch: 8 - Batch: 1794, Training Loss: 0.15477687087076814\n",
      "Epoch: 8 - Batch: 1795, Training Loss: 0.1548608569474659\n",
      "Epoch: 8 - Batch: 1796, Training Loss: 0.15494456329378323\n",
      "Epoch: 8 - Batch: 1797, Training Loss: 0.15503306096268332\n",
      "Epoch: 8 - Batch: 1798, Training Loss: 0.15514247551881655\n",
      "Epoch: 8 - Batch: 1799, Training Loss: 0.15522781840431354\n",
      "Epoch: 8 - Batch: 1800, Training Loss: 0.15531924146398976\n",
      "Epoch: 8 - Batch: 1801, Training Loss: 0.15539980931809885\n",
      "Epoch: 8 - Batch: 1802, Training Loss: 0.15548577725541335\n",
      "Epoch: 8 - Batch: 1803, Training Loss: 0.15558227826261994\n",
      "Epoch: 8 - Batch: 1804, Training Loss: 0.1556785555238253\n",
      "Epoch: 8 - Batch: 1805, Training Loss: 0.15576880057628079\n",
      "Epoch: 8 - Batch: 1806, Training Loss: 0.15586201614309503\n",
      "Epoch: 8 - Batch: 1807, Training Loss: 0.1559471325828365\n",
      "Epoch: 8 - Batch: 1808, Training Loss: 0.15603845039533937\n",
      "Epoch: 8 - Batch: 1809, Training Loss: 0.15612388695675145\n",
      "Epoch: 8 - Batch: 1810, Training Loss: 0.15621211121529094\n",
      "Epoch: 8 - Batch: 1811, Training Loss: 0.15629285990969458\n",
      "Epoch: 8 - Batch: 1812, Training Loss: 0.15638318452150074\n",
      "Epoch: 8 - Batch: 1813, Training Loss: 0.15646802443720612\n",
      "Epoch: 8 - Batch: 1814, Training Loss: 0.15656181466866093\n",
      "Epoch: 8 - Batch: 1815, Training Loss: 0.15665143916065222\n",
      "Epoch: 8 - Batch: 1816, Training Loss: 0.15673287291308344\n",
      "Epoch: 8 - Batch: 1817, Training Loss: 0.15681262728240755\n",
      "Epoch: 8 - Batch: 1818, Training Loss: 0.15690314338525532\n",
      "Epoch: 8 - Batch: 1819, Training Loss: 0.15698409436971789\n",
      "Epoch: 8 - Batch: 1820, Training Loss: 0.15707553645123296\n",
      "Epoch: 8 - Batch: 1821, Training Loss: 0.15716160452657474\n",
      "Epoch: 8 - Batch: 1822, Training Loss: 0.15725210843419357\n",
      "Epoch: 8 - Batch: 1823, Training Loss: 0.1573324514796979\n",
      "Epoch: 8 - Batch: 1824, Training Loss: 0.15742002879184475\n",
      "Epoch: 8 - Batch: 1825, Training Loss: 0.15750678082554298\n",
      "Epoch: 8 - Batch: 1826, Training Loss: 0.15760442180864848\n",
      "Epoch: 8 - Batch: 1827, Training Loss: 0.1576967897205606\n",
      "Epoch: 8 - Batch: 1828, Training Loss: 0.15778339039503442\n",
      "Epoch: 8 - Batch: 1829, Training Loss: 0.1578585063207703\n",
      "Epoch: 8 - Batch: 1830, Training Loss: 0.15794307421614875\n",
      "Epoch: 8 - Batch: 1831, Training Loss: 0.15803291729557178\n",
      "Epoch: 8 - Batch: 1832, Training Loss: 0.1581258430681616\n",
      "Epoch: 8 - Batch: 1833, Training Loss: 0.15820664530666315\n",
      "Epoch: 8 - Batch: 1834, Training Loss: 0.1582942455389211\n",
      "Epoch: 8 - Batch: 1835, Training Loss: 0.15837951611177642\n",
      "Epoch: 8 - Batch: 1836, Training Loss: 0.1584629784484902\n",
      "Epoch: 8 - Batch: 1837, Training Loss: 0.15854576697718248\n",
      "Epoch: 8 - Batch: 1838, Training Loss: 0.15864181556857837\n",
      "Epoch: 8 - Batch: 1839, Training Loss: 0.15871973168321116\n",
      "Epoch: 8 - Batch: 1840, Training Loss: 0.15880974582944737\n",
      "Epoch: 8 - Batch: 1841, Training Loss: 0.1589031945327127\n",
      "Epoch: 8 - Batch: 1842, Training Loss: 0.15898433031752138\n",
      "Epoch: 8 - Batch: 1843, Training Loss: 0.15906580924913657\n",
      "Epoch: 8 - Batch: 1844, Training Loss: 0.15915993497912365\n",
      "Epoch: 8 - Batch: 1845, Training Loss: 0.15925710309130042\n",
      "Epoch: 8 - Batch: 1846, Training Loss: 0.1593520344539266\n",
      "Epoch: 8 - Batch: 1847, Training Loss: 0.15944241054640282\n",
      "Epoch: 8 - Batch: 1848, Training Loss: 0.15952383528523778\n",
      "Epoch: 8 - Batch: 1849, Training Loss: 0.15961229124077123\n",
      "Epoch: 8 - Batch: 1850, Training Loss: 0.15970553243362884\n",
      "Epoch: 8 - Batch: 1851, Training Loss: 0.15979239360659475\n",
      "Epoch: 8 - Batch: 1852, Training Loss: 0.1598819120421042\n",
      "Epoch: 8 - Batch: 1853, Training Loss: 0.15996770543443228\n",
      "Epoch: 8 - Batch: 1854, Training Loss: 0.1600531595782261\n",
      "Epoch: 8 - Batch: 1855, Training Loss: 0.1601366670508784\n",
      "Epoch: 8 - Batch: 1856, Training Loss: 0.1602257443885404\n",
      "Epoch: 8 - Batch: 1857, Training Loss: 0.16031524884957776\n",
      "Epoch: 8 - Batch: 1858, Training Loss: 0.16040371260165576\n",
      "Epoch: 8 - Batch: 1859, Training Loss: 0.16048504603689978\n",
      "Epoch: 8 - Batch: 1860, Training Loss: 0.16056589636438917\n",
      "Epoch: 8 - Batch: 1861, Training Loss: 0.16064880809331217\n",
      "Epoch: 8 - Batch: 1862, Training Loss: 0.16073498286764024\n",
      "Epoch: 8 - Batch: 1863, Training Loss: 0.16082521838755354\n",
      "Epoch: 8 - Batch: 1864, Training Loss: 0.16091002622745917\n",
      "Epoch: 8 - Batch: 1865, Training Loss: 0.160991600750147\n",
      "Epoch: 8 - Batch: 1866, Training Loss: 0.16107572109792165\n",
      "Epoch: 8 - Batch: 1867, Training Loss: 0.16115856116906327\n",
      "Epoch: 8 - Batch: 1868, Training Loss: 0.161247526346451\n",
      "Epoch: 8 - Batch: 1869, Training Loss: 0.16133674986623414\n",
      "Epoch: 8 - Batch: 1870, Training Loss: 0.16142255135164726\n",
      "Epoch: 8 - Batch: 1871, Training Loss: 0.16151468489152282\n",
      "Epoch: 8 - Batch: 1872, Training Loss: 0.1616076473918918\n",
      "Epoch: 8 - Batch: 1873, Training Loss: 0.1616997433761459\n",
      "Epoch: 8 - Batch: 1874, Training Loss: 0.16178848420209552\n",
      "Epoch: 8 - Batch: 1875, Training Loss: 0.1618794764254619\n",
      "Epoch: 8 - Batch: 1876, Training Loss: 0.16195855394104622\n",
      "Epoch: 8 - Batch: 1877, Training Loss: 0.16204794457376892\n",
      "Epoch: 8 - Batch: 1878, Training Loss: 0.1621366963840737\n",
      "Epoch: 8 - Batch: 1879, Training Loss: 0.1622224461478776\n",
      "Epoch: 8 - Batch: 1880, Training Loss: 0.16230559334850825\n",
      "Epoch: 8 - Batch: 1881, Training Loss: 0.16239219542537162\n",
      "Epoch: 8 - Batch: 1882, Training Loss: 0.16248499633586821\n",
      "Epoch: 8 - Batch: 1883, Training Loss: 0.16256952366697452\n",
      "Epoch: 8 - Batch: 1884, Training Loss: 0.16265428445578411\n",
      "Epoch: 8 - Batch: 1885, Training Loss: 0.16273514722561955\n",
      "Epoch: 8 - Batch: 1886, Training Loss: 0.1628155390891072\n",
      "Epoch: 8 - Batch: 1887, Training Loss: 0.1629008140791807\n",
      "Epoch: 8 - Batch: 1888, Training Loss: 0.16300076030651925\n",
      "Epoch: 8 - Batch: 1889, Training Loss: 0.1630834674224826\n",
      "Epoch: 8 - Batch: 1890, Training Loss: 0.16316667073105104\n",
      "Epoch: 8 - Batch: 1891, Training Loss: 0.1632516172777855\n",
      "Epoch: 8 - Batch: 1892, Training Loss: 0.16333842477073915\n",
      "Epoch: 8 - Batch: 1893, Training Loss: 0.16343046417753296\n",
      "Epoch: 8 - Batch: 1894, Training Loss: 0.16352273709763143\n",
      "Epoch: 8 - Batch: 1895, Training Loss: 0.16361307509700654\n",
      "Epoch: 8 - Batch: 1896, Training Loss: 0.16368935770582205\n",
      "Epoch: 8 - Batch: 1897, Training Loss: 0.1637721937736666\n",
      "Epoch: 8 - Batch: 1898, Training Loss: 0.16386339295760513\n",
      "Epoch: 8 - Batch: 1899, Training Loss: 0.16394199396963935\n",
      "Epoch: 8 - Batch: 1900, Training Loss: 0.16402405836491243\n",
      "Epoch: 8 - Batch: 1901, Training Loss: 0.16411655545111123\n",
      "Epoch: 8 - Batch: 1902, Training Loss: 0.16420242430682402\n",
      "Epoch: 8 - Batch: 1903, Training Loss: 0.16428492613643358\n",
      "Epoch: 8 - Batch: 1904, Training Loss: 0.16436145463294255\n",
      "Epoch: 8 - Batch: 1905, Training Loss: 0.1644463830412818\n",
      "Epoch: 8 - Batch: 1906, Training Loss: 0.16453453147193883\n",
      "Epoch: 8 - Batch: 1907, Training Loss: 0.16461030167726734\n",
      "Epoch: 8 - Batch: 1908, Training Loss: 0.164698438036254\n",
      "Epoch: 8 - Batch: 1909, Training Loss: 0.16478556226537397\n",
      "Epoch: 8 - Batch: 1910, Training Loss: 0.16486640518552825\n",
      "Epoch: 8 - Batch: 1911, Training Loss: 0.16495573283427983\n",
      "Epoch: 8 - Batch: 1912, Training Loss: 0.1650389451873342\n",
      "Epoch: 8 - Batch: 1913, Training Loss: 0.16512197316014154\n",
      "Epoch: 8 - Batch: 1914, Training Loss: 0.1652024111912816\n",
      "Epoch: 8 - Batch: 1915, Training Loss: 0.16529253297554913\n",
      "Epoch: 8 - Batch: 1916, Training Loss: 0.1653886163919225\n",
      "Epoch: 8 - Batch: 1917, Training Loss: 0.16546846677859625\n",
      "Epoch: 8 - Batch: 1918, Training Loss: 0.16555754299187542\n",
      "Epoch: 8 - Batch: 1919, Training Loss: 0.16564145844250572\n",
      "Epoch: 8 - Batch: 1920, Training Loss: 0.1657311744975609\n",
      "Epoch: 8 - Batch: 1921, Training Loss: 0.16581592734155567\n",
      "Epoch: 8 - Batch: 1922, Training Loss: 0.1659003170284367\n",
      "Epoch: 8 - Batch: 1923, Training Loss: 0.16598021309470656\n",
      "Epoch: 8 - Batch: 1924, Training Loss: 0.16606977124811206\n",
      "Epoch: 8 - Batch: 1925, Training Loss: 0.16614926373508834\n",
      "Epoch: 8 - Batch: 1926, Training Loss: 0.1662415818961501\n",
      "Epoch: 8 - Batch: 1927, Training Loss: 0.16632439342526653\n",
      "Epoch: 8 - Batch: 1928, Training Loss: 0.16640530188087602\n",
      "Epoch: 8 - Batch: 1929, Training Loss: 0.16649172907914492\n",
      "Epoch: 8 - Batch: 1930, Training Loss: 0.1665826124041828\n",
      "Epoch: 8 - Batch: 1931, Training Loss: 0.166670966683385\n",
      "Epoch: 8 - Batch: 1932, Training Loss: 0.1667512745270583\n",
      "Epoch: 8 - Batch: 1933, Training Loss: 0.16683542491809447\n",
      "Epoch: 8 - Batch: 1934, Training Loss: 0.16693946168716275\n",
      "Epoch: 8 - Batch: 1935, Training Loss: 0.16702684485175914\n",
      "Epoch: 8 - Batch: 1936, Training Loss: 0.16710677796088247\n",
      "Epoch: 8 - Batch: 1937, Training Loss: 0.16718913714832334\n",
      "Epoch: 8 - Batch: 1938, Training Loss: 0.1672740959458881\n",
      "Epoch: 8 - Batch: 1939, Training Loss: 0.16736575164778117\n",
      "Epoch: 8 - Batch: 1940, Training Loss: 0.16745596965944787\n",
      "Epoch: 8 - Batch: 1941, Training Loss: 0.167547717591621\n",
      "Epoch: 8 - Batch: 1942, Training Loss: 0.167631665204542\n",
      "Epoch: 8 - Batch: 1943, Training Loss: 0.1677134562821234\n",
      "Epoch: 8 - Batch: 1944, Training Loss: 0.16779668711953694\n",
      "Epoch: 8 - Batch: 1945, Training Loss: 0.16789087627830593\n",
      "Epoch: 8 - Batch: 1946, Training Loss: 0.16797359342104562\n",
      "Epoch: 8 - Batch: 1947, Training Loss: 0.16805235596735085\n",
      "Epoch: 8 - Batch: 1948, Training Loss: 0.1681387581511912\n",
      "Epoch: 8 - Batch: 1949, Training Loss: 0.16822292635319244\n",
      "Epoch: 8 - Batch: 1950, Training Loss: 0.16830152816835722\n",
      "Epoch: 8 - Batch: 1951, Training Loss: 0.16839147926266515\n",
      "Epoch: 8 - Batch: 1952, Training Loss: 0.16847614368198324\n",
      "Epoch: 8 - Batch: 1953, Training Loss: 0.16856732793387094\n",
      "Epoch: 8 - Batch: 1954, Training Loss: 0.16864594199368807\n",
      "Epoch: 8 - Batch: 1955, Training Loss: 0.16872724678188217\n",
      "Epoch: 8 - Batch: 1956, Training Loss: 0.16882463426608746\n",
      "Epoch: 8 - Batch: 1957, Training Loss: 0.1689066456794541\n",
      "Epoch: 8 - Batch: 1958, Training Loss: 0.16899289828331315\n",
      "Epoch: 8 - Batch: 1959, Training Loss: 0.16908177275019104\n",
      "Epoch: 8 - Batch: 1960, Training Loss: 0.1691626996530921\n",
      "Epoch: 8 - Batch: 1961, Training Loss: 0.16925439851398688\n",
      "Epoch: 8 - Batch: 1962, Training Loss: 0.1693418907560717\n",
      "Epoch: 8 - Batch: 1963, Training Loss: 0.16942168483711395\n",
      "Epoch: 8 - Batch: 1964, Training Loss: 0.16950944961847159\n",
      "Epoch: 8 - Batch: 1965, Training Loss: 0.16959144140456248\n",
      "Epoch: 8 - Batch: 1966, Training Loss: 0.16968149258129633\n",
      "Epoch: 8 - Batch: 1967, Training Loss: 0.16976864095375707\n",
      "Epoch: 8 - Batch: 1968, Training Loss: 0.1698653994631609\n",
      "Epoch: 8 - Batch: 1969, Training Loss: 0.16995687284823477\n",
      "Epoch: 8 - Batch: 1970, Training Loss: 0.17004037940037586\n",
      "Epoch: 8 - Batch: 1971, Training Loss: 0.17012649039378017\n",
      "Epoch: 8 - Batch: 1972, Training Loss: 0.1702155055753133\n",
      "Epoch: 8 - Batch: 1973, Training Loss: 0.17029915803774673\n",
      "Epoch: 8 - Batch: 1974, Training Loss: 0.17038265949927556\n",
      "Epoch: 8 - Batch: 1975, Training Loss: 0.1704667823614667\n",
      "Epoch: 8 - Batch: 1976, Training Loss: 0.17055224806416291\n",
      "Epoch: 8 - Batch: 1977, Training Loss: 0.1706303630586386\n",
      "Epoch: 8 - Batch: 1978, Training Loss: 0.17071365644139042\n",
      "Epoch: 8 - Batch: 1979, Training Loss: 0.17080257887987552\n",
      "Epoch: 8 - Batch: 1980, Training Loss: 0.17088617366517755\n",
      "Epoch: 8 - Batch: 1981, Training Loss: 0.17096387402804733\n",
      "Epoch: 8 - Batch: 1982, Training Loss: 0.17104775653179013\n",
      "Epoch: 8 - Batch: 1983, Training Loss: 0.17113222670471095\n",
      "Epoch: 8 - Batch: 1984, Training Loss: 0.17121438926107452\n",
      "Epoch: 8 - Batch: 1985, Training Loss: 0.1712974842149721\n",
      "Epoch: 8 - Batch: 1986, Training Loss: 0.17139105951015035\n",
      "Epoch: 8 - Batch: 1987, Training Loss: 0.17148692021841433\n",
      "Epoch: 8 - Batch: 1988, Training Loss: 0.17157540267725688\n",
      "Epoch: 8 - Batch: 1989, Training Loss: 0.17166640623712026\n",
      "Epoch: 8 - Batch: 1990, Training Loss: 0.17175877105389067\n",
      "Epoch: 8 - Batch: 1991, Training Loss: 0.17183645479195747\n",
      "Epoch: 8 - Batch: 1992, Training Loss: 0.17192090672145832\n",
      "Epoch: 8 - Batch: 1993, Training Loss: 0.1720053111735861\n",
      "Epoch: 8 - Batch: 1994, Training Loss: 0.17209881148108006\n",
      "Epoch: 8 - Batch: 1995, Training Loss: 0.17218705941343782\n",
      "Epoch: 8 - Batch: 1996, Training Loss: 0.17227583161708135\n",
      "Epoch: 8 - Batch: 1997, Training Loss: 0.17235644817179313\n",
      "Epoch: 8 - Batch: 1998, Training Loss: 0.17243994853982877\n",
      "Epoch: 8 - Batch: 1999, Training Loss: 0.17252156194021454\n",
      "Epoch: 8 - Batch: 2000, Training Loss: 0.17262369263948096\n",
      "Epoch: 8 - Batch: 2001, Training Loss: 0.17271349276995185\n",
      "Epoch: 8 - Batch: 2002, Training Loss: 0.17279096320271492\n",
      "Epoch: 8 - Batch: 2003, Training Loss: 0.17287818157727247\n",
      "Epoch: 8 - Batch: 2004, Training Loss: 0.1729623240049601\n",
      "Epoch: 8 - Batch: 2005, Training Loss: 0.17304828196492164\n",
      "Epoch: 8 - Batch: 2006, Training Loss: 0.17313568265629842\n",
      "Epoch: 8 - Batch: 2007, Training Loss: 0.17322170586135258\n",
      "Epoch: 8 - Batch: 2008, Training Loss: 0.17331468030365546\n",
      "Epoch: 8 - Batch: 2009, Training Loss: 0.17340022194543683\n",
      "Epoch: 8 - Batch: 2010, Training Loss: 0.17348828035133396\n",
      "Epoch: 8 - Batch: 2011, Training Loss: 0.17357497651185563\n",
      "Epoch: 8 - Batch: 2012, Training Loss: 0.1736628603851222\n",
      "Epoch: 8 - Batch: 2013, Training Loss: 0.17375538474069305\n",
      "Epoch: 8 - Batch: 2014, Training Loss: 0.17383853802040442\n",
      "Epoch: 8 - Batch: 2015, Training Loss: 0.17392858704721947\n",
      "Epoch: 8 - Batch: 2016, Training Loss: 0.17401512622042478\n",
      "Epoch: 8 - Batch: 2017, Training Loss: 0.17410358935471001\n",
      "Epoch: 8 - Batch: 2018, Training Loss: 0.17419217465751208\n",
      "Epoch: 8 - Batch: 2019, Training Loss: 0.17427884595805337\n",
      "Epoch: 8 - Batch: 2020, Training Loss: 0.17436744466078025\n",
      "Epoch: 8 - Batch: 2021, Training Loss: 0.17445298677208412\n",
      "Epoch: 8 - Batch: 2022, Training Loss: 0.1745374006702631\n",
      "Epoch: 8 - Batch: 2023, Training Loss: 0.1746222622952058\n",
      "Epoch: 8 - Batch: 2024, Training Loss: 0.17470440532314047\n",
      "Epoch: 8 - Batch: 2025, Training Loss: 0.17479681346571663\n",
      "Epoch: 8 - Batch: 2026, Training Loss: 0.17487610770299858\n",
      "Epoch: 8 - Batch: 2027, Training Loss: 0.1749646402524775\n",
      "Epoch: 8 - Batch: 2028, Training Loss: 0.17505742359912613\n",
      "Epoch: 8 - Batch: 2029, Training Loss: 0.17513988623596344\n",
      "Epoch: 8 - Batch: 2030, Training Loss: 0.17523078727929747\n",
      "Epoch: 8 - Batch: 2031, Training Loss: 0.1753212729373184\n",
      "Epoch: 8 - Batch: 2032, Training Loss: 0.1754065038787686\n",
      "Epoch: 8 - Batch: 2033, Training Loss: 0.1754917775282318\n",
      "Epoch: 8 - Batch: 2034, Training Loss: 0.17557688731434531\n",
      "Epoch: 8 - Batch: 2035, Training Loss: 0.17566331286310755\n",
      "Epoch: 8 - Batch: 2036, Training Loss: 0.1757496574624854\n",
      "Epoch: 8 - Batch: 2037, Training Loss: 0.17582925307760586\n",
      "Epoch: 8 - Batch: 2038, Training Loss: 0.1759127530131866\n",
      "Epoch: 8 - Batch: 2039, Training Loss: 0.1760051476137456\n",
      "Epoch: 8 - Batch: 2040, Training Loss: 0.17608867647438303\n",
      "Epoch: 8 - Batch: 2041, Training Loss: 0.17617610428068373\n",
      "Epoch: 8 - Batch: 2042, Training Loss: 0.17624975426453066\n",
      "Epoch: 8 - Batch: 2043, Training Loss: 0.17633553843321295\n",
      "Epoch: 8 - Batch: 2044, Training Loss: 0.1764213212056836\n",
      "Epoch: 8 - Batch: 2045, Training Loss: 0.1765074503374831\n",
      "Epoch: 8 - Batch: 2046, Training Loss: 0.17658840481865268\n",
      "Epoch: 8 - Batch: 2047, Training Loss: 0.17666974995489143\n",
      "Epoch: 8 - Batch: 2048, Training Loss: 0.1767554729559133\n",
      "Epoch: 8 - Batch: 2049, Training Loss: 0.17684346318838015\n",
      "Epoch: 8 - Batch: 2050, Training Loss: 0.17692601369388067\n",
      "Epoch: 8 - Batch: 2051, Training Loss: 0.1770118035030108\n",
      "Epoch: 8 - Batch: 2052, Training Loss: 0.17710193163818783\n",
      "Epoch: 8 - Batch: 2053, Training Loss: 0.1771895025623576\n",
      "Epoch: 8 - Batch: 2054, Training Loss: 0.1772779452415248\n",
      "Epoch: 8 - Batch: 2055, Training Loss: 0.17736183205128309\n",
      "Epoch: 8 - Batch: 2056, Training Loss: 0.17744450434226894\n",
      "Epoch: 8 - Batch: 2057, Training Loss: 0.17753199147520768\n",
      "Epoch: 8 - Batch: 2058, Training Loss: 0.17761880757401435\n",
      "Epoch: 8 - Batch: 2059, Training Loss: 0.17770720551263042\n",
      "Epoch: 8 - Batch: 2060, Training Loss: 0.1777940102378725\n",
      "Epoch: 8 - Batch: 2061, Training Loss: 0.17787618811549635\n",
      "Epoch: 8 - Batch: 2062, Training Loss: 0.17796159297178435\n",
      "Epoch: 8 - Batch: 2063, Training Loss: 0.17805265575943896\n",
      "Epoch: 8 - Batch: 2064, Training Loss: 0.1781411777693735\n",
      "Epoch: 8 - Batch: 2065, Training Loss: 0.1782193464747511\n",
      "Epoch: 8 - Batch: 2066, Training Loss: 0.17830409640276412\n",
      "Epoch: 8 - Batch: 2067, Training Loss: 0.17839738464053986\n",
      "Epoch: 8 - Batch: 2068, Training Loss: 0.17847939869805948\n",
      "Epoch: 8 - Batch: 2069, Training Loss: 0.17856304115842825\n",
      "Epoch: 8 - Batch: 2070, Training Loss: 0.17865062438902965\n",
      "Epoch: 8 - Batch: 2071, Training Loss: 0.17873956531805185\n",
      "Epoch: 8 - Batch: 2072, Training Loss: 0.17882892809449935\n",
      "Epoch: 8 - Batch: 2073, Training Loss: 0.17891162254571125\n",
      "Epoch: 8 - Batch: 2074, Training Loss: 0.17899949849887473\n",
      "Epoch: 8 - Batch: 2075, Training Loss: 0.17908455346823726\n",
      "Epoch: 8 - Batch: 2076, Training Loss: 0.17916184139884328\n",
      "Epoch: 8 - Batch: 2077, Training Loss: 0.1792491680354028\n",
      "Epoch: 8 - Batch: 2078, Training Loss: 0.17933538098314508\n",
      "Epoch: 8 - Batch: 2079, Training Loss: 0.179420270038086\n",
      "Epoch: 8 - Batch: 2080, Training Loss: 0.17950297913098612\n",
      "Epoch: 8 - Batch: 2081, Training Loss: 0.1795916959309756\n",
      "Epoch: 8 - Batch: 2082, Training Loss: 0.17968783264432975\n",
      "Epoch: 8 - Batch: 2083, Training Loss: 0.17977430720876897\n",
      "Epoch: 8 - Batch: 2084, Training Loss: 0.17987218667223284\n",
      "Epoch: 8 - Batch: 2085, Training Loss: 0.17995564594469457\n",
      "Epoch: 8 - Batch: 2086, Training Loss: 0.1800370343882053\n",
      "Epoch: 8 - Batch: 2087, Training Loss: 0.1801246390253256\n",
      "Epoch: 8 - Batch: 2088, Training Loss: 0.18020656596788917\n",
      "Epoch: 8 - Batch: 2089, Training Loss: 0.18029028359510216\n",
      "Epoch: 8 - Batch: 2090, Training Loss: 0.1803771863949437\n",
      "Epoch: 8 - Batch: 2091, Training Loss: 0.18046355908535805\n",
      "Epoch: 8 - Batch: 2092, Training Loss: 0.18054571110835518\n",
      "Epoch: 8 - Batch: 2093, Training Loss: 0.18062617733555647\n",
      "Epoch: 8 - Batch: 2094, Training Loss: 0.18071412769567907\n",
      "Epoch: 8 - Batch: 2095, Training Loss: 0.18079915550687223\n",
      "Epoch: 8 - Batch: 2096, Training Loss: 0.18088547965113203\n",
      "Epoch: 8 - Batch: 2097, Training Loss: 0.18097546122115643\n",
      "Epoch: 8 - Batch: 2098, Training Loss: 0.18105965870653418\n",
      "Epoch: 8 - Batch: 2099, Training Loss: 0.1811379232253898\n",
      "Epoch: 8 - Batch: 2100, Training Loss: 0.1812229389093112\n",
      "Epoch: 8 - Batch: 2101, Training Loss: 0.18130617783363187\n",
      "Epoch: 8 - Batch: 2102, Training Loss: 0.1813876072676917\n",
      "Epoch: 8 - Batch: 2103, Training Loss: 0.18147304706633782\n",
      "Epoch: 8 - Batch: 2104, Training Loss: 0.18155907715978117\n",
      "Epoch: 8 - Batch: 2105, Training Loss: 0.1816499726862259\n",
      "Epoch: 8 - Batch: 2106, Training Loss: 0.18173450647326647\n",
      "Epoch: 8 - Batch: 2107, Training Loss: 0.18181331026341588\n",
      "Epoch: 8 - Batch: 2108, Training Loss: 0.1818974605926728\n",
      "Epoch: 8 - Batch: 2109, Training Loss: 0.18198671250621082\n",
      "Epoch: 8 - Batch: 2110, Training Loss: 0.18207099303529037\n",
      "Epoch: 8 - Batch: 2111, Training Loss: 0.18215994459034793\n",
      "Epoch: 8 - Batch: 2112, Training Loss: 0.18225028370174998\n",
      "Epoch: 8 - Batch: 2113, Training Loss: 0.1823311457981913\n",
      "Epoch: 8 - Batch: 2114, Training Loss: 0.18242123276983724\n",
      "Epoch: 8 - Batch: 2115, Training Loss: 0.18250468360324998\n",
      "Epoch: 8 - Batch: 2116, Training Loss: 0.18258963583367777\n",
      "Epoch: 8 - Batch: 2117, Training Loss: 0.1826710906281301\n",
      "Epoch: 8 - Batch: 2118, Training Loss: 0.18276671605002426\n",
      "Epoch: 8 - Batch: 2119, Training Loss: 0.18285880205692542\n",
      "Epoch: 8 - Batch: 2120, Training Loss: 0.18294120149356413\n",
      "Epoch: 8 - Batch: 2121, Training Loss: 0.1830262702953064\n",
      "Epoch: 8 - Batch: 2122, Training Loss: 0.1831108677088226\n",
      "Epoch: 8 - Batch: 2123, Training Loss: 0.18319754906085198\n",
      "Epoch: 8 - Batch: 2124, Training Loss: 0.18328648986631563\n",
      "Epoch: 8 - Batch: 2125, Training Loss: 0.18337575493266137\n",
      "Epoch: 8 - Batch: 2126, Training Loss: 0.18346835989783059\n",
      "Epoch: 8 - Batch: 2127, Training Loss: 0.18355743225608298\n",
      "Epoch: 8 - Batch: 2128, Training Loss: 0.18364462068622583\n",
      "Epoch: 8 - Batch: 2129, Training Loss: 0.18372505572724895\n",
      "Epoch: 8 - Batch: 2130, Training Loss: 0.18381197459588003\n",
      "Epoch: 8 - Batch: 2131, Training Loss: 0.1838990866606607\n",
      "Epoch: 8 - Batch: 2132, Training Loss: 0.18399040381830328\n",
      "Epoch: 8 - Batch: 2133, Training Loss: 0.18407518042260734\n",
      "Epoch: 8 - Batch: 2134, Training Loss: 0.1841596194464176\n",
      "Epoch: 8 - Batch: 2135, Training Loss: 0.1842380707858016\n",
      "Epoch: 8 - Batch: 2136, Training Loss: 0.18432683839924496\n",
      "Epoch: 8 - Batch: 2137, Training Loss: 0.18441864238993247\n",
      "Epoch: 8 - Batch: 2138, Training Loss: 0.18451001079348386\n",
      "Epoch: 8 - Batch: 2139, Training Loss: 0.1845907126900865\n",
      "Epoch: 8 - Batch: 2140, Training Loss: 0.18468041661169202\n",
      "Epoch: 8 - Batch: 2141, Training Loss: 0.18476222681613705\n",
      "Epoch: 8 - Batch: 2142, Training Loss: 0.18485442020159654\n",
      "Epoch: 8 - Batch: 2143, Training Loss: 0.1849408127664037\n",
      "Epoch: 8 - Batch: 2144, Training Loss: 0.18502383073714637\n",
      "Epoch: 8 - Batch: 2145, Training Loss: 0.1851073617168129\n",
      "Epoch: 8 - Batch: 2146, Training Loss: 0.18519321620266632\n",
      "Epoch: 8 - Batch: 2147, Training Loss: 0.18527974043146492\n",
      "Epoch: 8 - Batch: 2148, Training Loss: 0.1853640967390035\n",
      "Epoch: 8 - Batch: 2149, Training Loss: 0.18545227140731874\n",
      "Epoch: 8 - Batch: 2150, Training Loss: 0.1855345728373152\n",
      "Epoch: 8 - Batch: 2151, Training Loss: 0.18562740204889777\n",
      "Epoch: 8 - Batch: 2152, Training Loss: 0.18571875320985345\n",
      "Epoch: 8 - Batch: 2153, Training Loss: 0.18580364008398595\n",
      "Epoch: 8 - Batch: 2154, Training Loss: 0.18588252455786883\n",
      "Epoch: 8 - Batch: 2155, Training Loss: 0.18597269815023662\n",
      "Epoch: 8 - Batch: 2156, Training Loss: 0.18606153102730638\n",
      "Epoch: 8 - Batch: 2157, Training Loss: 0.18614218993788928\n",
      "Epoch: 8 - Batch: 2158, Training Loss: 0.18623099845539082\n",
      "Epoch: 8 - Batch: 2159, Training Loss: 0.1863224555801594\n",
      "Epoch: 8 - Batch: 2160, Training Loss: 0.18642135448815614\n",
      "Epoch: 8 - Batch: 2161, Training Loss: 0.18651313700436756\n",
      "Epoch: 8 - Batch: 2162, Training Loss: 0.1866158005189935\n",
      "Epoch: 8 - Batch: 2163, Training Loss: 0.18670000783345395\n",
      "Epoch: 8 - Batch: 2164, Training Loss: 0.1867855489562894\n",
      "Epoch: 8 - Batch: 2165, Training Loss: 0.18687186941850442\n",
      "Epoch: 8 - Batch: 2166, Training Loss: 0.18695666062125124\n",
      "Epoch: 8 - Batch: 2167, Training Loss: 0.1870461659780881\n",
      "Epoch: 8 - Batch: 2168, Training Loss: 0.18713709309845422\n",
      "Epoch: 8 - Batch: 2169, Training Loss: 0.18721759415063297\n",
      "Epoch: 8 - Batch: 2170, Training Loss: 0.1873039223846808\n",
      "Epoch: 8 - Batch: 2171, Training Loss: 0.1873912714347614\n",
      "Epoch: 8 - Batch: 2172, Training Loss: 0.1874786423423496\n",
      "Epoch: 8 - Batch: 2173, Training Loss: 0.18756334484241297\n",
      "Epoch: 8 - Batch: 2174, Training Loss: 0.18765604853704201\n",
      "Epoch: 8 - Batch: 2175, Training Loss: 0.18774241826205113\n",
      "Epoch: 8 - Batch: 2176, Training Loss: 0.1878340822670788\n",
      "Epoch: 8 - Batch: 2177, Training Loss: 0.18791502239074478\n",
      "Epoch: 8 - Batch: 2178, Training Loss: 0.18799817832696497\n",
      "Epoch: 8 - Batch: 2179, Training Loss: 0.18809085357841568\n",
      "Epoch: 8 - Batch: 2180, Training Loss: 0.18818014318966747\n",
      "Epoch: 8 - Batch: 2181, Training Loss: 0.188264192880485\n",
      "Epoch: 8 - Batch: 2182, Training Loss: 0.18835331741751327\n",
      "Epoch: 8 - Batch: 2183, Training Loss: 0.18843449698529433\n",
      "Epoch: 8 - Batch: 2184, Training Loss: 0.18852866437304672\n",
      "Epoch: 8 - Batch: 2185, Training Loss: 0.18862610732886329\n",
      "Epoch: 8 - Batch: 2186, Training Loss: 0.18871209855078663\n",
      "Epoch: 8 - Batch: 2187, Training Loss: 0.1887896205136432\n",
      "Epoch: 8 - Batch: 2188, Training Loss: 0.18888065280704752\n",
      "Epoch: 8 - Batch: 2189, Training Loss: 0.1889709726494936\n",
      "Epoch: 8 - Batch: 2190, Training Loss: 0.18905125112329946\n",
      "Epoch: 8 - Batch: 2191, Training Loss: 0.18913562260382805\n",
      "Epoch: 8 - Batch: 2192, Training Loss: 0.1892231085691385\n",
      "Epoch: 8 - Batch: 2193, Training Loss: 0.18930969867317832\n",
      "Epoch: 8 - Batch: 2194, Training Loss: 0.1893877514744576\n",
      "Epoch: 8 - Batch: 2195, Training Loss: 0.18947831910122093\n",
      "Epoch: 8 - Batch: 2196, Training Loss: 0.18956061684917258\n",
      "Epoch: 8 - Batch: 2197, Training Loss: 0.18965545929288785\n",
      "Epoch: 8 - Batch: 2198, Training Loss: 0.18974320415660714\n",
      "Epoch: 8 - Batch: 2199, Training Loss: 0.189828928862737\n",
      "Epoch: 8 - Batch: 2200, Training Loss: 0.18992114670043364\n",
      "Epoch: 8 - Batch: 2201, Training Loss: 0.19000897158067026\n",
      "Epoch: 8 - Batch: 2202, Training Loss: 0.19009395252610517\n",
      "Epoch: 8 - Batch: 2203, Training Loss: 0.19018505943034023\n",
      "Epoch: 8 - Batch: 2204, Training Loss: 0.19026148628758555\n",
      "Epoch: 8 - Batch: 2205, Training Loss: 0.19035303566116796\n",
      "Epoch: 8 - Batch: 2206, Training Loss: 0.19043708319588878\n",
      "Epoch: 8 - Batch: 2207, Training Loss: 0.19052606417641513\n",
      "Epoch: 8 - Batch: 2208, Training Loss: 0.19060845115217404\n",
      "Epoch: 8 - Batch: 2209, Training Loss: 0.1906886921321377\n",
      "Epoch: 8 - Batch: 2210, Training Loss: 0.1907765619197295\n",
      "Epoch: 8 - Batch: 2211, Training Loss: 0.1908589780738401\n",
      "Epoch: 8 - Batch: 2212, Training Loss: 0.19094500098853168\n",
      "Epoch: 8 - Batch: 2213, Training Loss: 0.19102745075674596\n",
      "Epoch: 8 - Batch: 2214, Training Loss: 0.19111812578677934\n",
      "Epoch: 8 - Batch: 2215, Training Loss: 0.19119878666812112\n",
      "Epoch: 8 - Batch: 2216, Training Loss: 0.19128223621652493\n",
      "Epoch: 8 - Batch: 2217, Training Loss: 0.19137178640040395\n",
      "Epoch: 8 - Batch: 2218, Training Loss: 0.19146599026264044\n",
      "Epoch: 8 - Batch: 2219, Training Loss: 0.1915533596566364\n",
      "Epoch: 8 - Batch: 2220, Training Loss: 0.1916319186772972\n",
      "Epoch: 8 - Batch: 2221, Training Loss: 0.1917204279915907\n",
      "Epoch: 8 - Batch: 2222, Training Loss: 0.19181019035021266\n",
      "Epoch: 8 - Batch: 2223, Training Loss: 0.19189165507877248\n",
      "Epoch: 8 - Batch: 2224, Training Loss: 0.19197961592901602\n",
      "Epoch: 8 - Batch: 2225, Training Loss: 0.1920633588054188\n",
      "Epoch: 8 - Batch: 2226, Training Loss: 0.19214426521613429\n",
      "Epoch: 8 - Batch: 2227, Training Loss: 0.19223890070545535\n",
      "Epoch: 8 - Batch: 2228, Training Loss: 0.19232689549723272\n",
      "Epoch: 8 - Batch: 2229, Training Loss: 0.19241573508353177\n",
      "Epoch: 8 - Batch: 2230, Training Loss: 0.19250327969590822\n",
      "Epoch: 8 - Batch: 2231, Training Loss: 0.19260130434066897\n",
      "Epoch: 8 - Batch: 2232, Training Loss: 0.1926811438603682\n",
      "Epoch: 8 - Batch: 2233, Training Loss: 0.19277769179511228\n",
      "Epoch: 8 - Batch: 2234, Training Loss: 0.19286707231456762\n",
      "Epoch: 8 - Batch: 2235, Training Loss: 0.19296258973393274\n",
      "Epoch: 8 - Batch: 2236, Training Loss: 0.19304823507840557\n",
      "Epoch: 8 - Batch: 2237, Training Loss: 0.19313580285801027\n",
      "Epoch: 8 - Batch: 2238, Training Loss: 0.1932259169878256\n",
      "Epoch: 8 - Batch: 2239, Training Loss: 0.193314884407801\n",
      "Epoch: 8 - Batch: 2240, Training Loss: 0.19340151428533825\n",
      "Epoch: 8 - Batch: 2241, Training Loss: 0.19348530983475112\n",
      "Epoch: 8 - Batch: 2242, Training Loss: 0.19357470117175757\n",
      "Epoch: 8 - Batch: 2243, Training Loss: 0.19366224224418155\n",
      "Epoch: 8 - Batch: 2244, Training Loss: 0.19375029815419595\n",
      "Epoch: 8 - Batch: 2245, Training Loss: 0.19382998965075163\n",
      "Epoch: 8 - Batch: 2246, Training Loss: 0.19391245357259787\n",
      "Epoch: 8 - Batch: 2247, Training Loss: 0.19399277232031323\n",
      "Epoch: 8 - Batch: 2248, Training Loss: 0.19407870275142972\n",
      "Epoch: 8 - Batch: 2249, Training Loss: 0.19416202805553304\n",
      "Epoch: 8 - Batch: 2250, Training Loss: 0.19424331430994457\n",
      "Epoch: 8 - Batch: 2251, Training Loss: 0.19431944833960305\n",
      "Epoch: 8 - Batch: 2252, Training Loss: 0.1944096797635504\n",
      "Epoch: 8 - Batch: 2253, Training Loss: 0.19448743439951346\n",
      "Epoch: 8 - Batch: 2254, Training Loss: 0.1945778878767099\n",
      "Epoch: 8 - Batch: 2255, Training Loss: 0.19466622484535917\n",
      "Epoch: 8 - Batch: 2256, Training Loss: 0.19475435811538205\n",
      "Epoch: 8 - Batch: 2257, Training Loss: 0.19484853616005943\n",
      "Epoch: 8 - Batch: 2258, Training Loss: 0.1949409276036975\n",
      "Epoch: 8 - Batch: 2259, Training Loss: 0.1950384340624311\n",
      "Epoch: 8 - Batch: 2260, Training Loss: 0.1951271262001339\n",
      "Epoch: 8 - Batch: 2261, Training Loss: 0.19520803388350244\n",
      "Epoch: 8 - Batch: 2262, Training Loss: 0.1953085612870172\n",
      "Epoch: 8 - Batch: 2263, Training Loss: 0.19539558050990302\n",
      "Epoch: 8 - Batch: 2264, Training Loss: 0.19549083449626048\n",
      "Epoch: 8 - Batch: 2265, Training Loss: 0.19558304034877771\n",
      "Epoch: 8 - Batch: 2266, Training Loss: 0.19566928125781996\n",
      "Epoch: 8 - Batch: 2267, Training Loss: 0.19575455540769532\n",
      "Epoch: 8 - Batch: 2268, Training Loss: 0.195839560582962\n",
      "Epoch: 8 - Batch: 2269, Training Loss: 0.1959260549847265\n",
      "Epoch: 8 - Batch: 2270, Training Loss: 0.19601570942113253\n",
      "Epoch: 8 - Batch: 2271, Training Loss: 0.19610076655276973\n",
      "Epoch: 8 - Batch: 2272, Training Loss: 0.19618624971428913\n",
      "Epoch: 8 - Batch: 2273, Training Loss: 0.19627566384167022\n",
      "Epoch: 8 - Batch: 2274, Training Loss: 0.19636251039765962\n",
      "Epoch: 8 - Batch: 2275, Training Loss: 0.19644622305164092\n",
      "Epoch: 8 - Batch: 2276, Training Loss: 0.1965348723639501\n",
      "Epoch: 8 - Batch: 2277, Training Loss: 0.19661689423654802\n",
      "Epoch: 8 - Batch: 2278, Training Loss: 0.19670462064061986\n",
      "Epoch: 8 - Batch: 2279, Training Loss: 0.19678940212548668\n",
      "Epoch: 8 - Batch: 2280, Training Loss: 0.19687773771645814\n",
      "Epoch: 8 - Batch: 2281, Training Loss: 0.1969665804226106\n",
      "Epoch: 8 - Batch: 2282, Training Loss: 0.19705203606146288\n",
      "Epoch: 8 - Batch: 2283, Training Loss: 0.1971413978803316\n",
      "Epoch: 8 - Batch: 2284, Training Loss: 0.1972274632744528\n",
      "Epoch: 8 - Batch: 2285, Training Loss: 0.19731702831624753\n",
      "Epoch: 8 - Batch: 2286, Training Loss: 0.1974045665591807\n",
      "Epoch: 8 - Batch: 2287, Training Loss: 0.19748700573224925\n",
      "Epoch: 8 - Batch: 2288, Training Loss: 0.19757131077534523\n",
      "Epoch: 8 - Batch: 2289, Training Loss: 0.19766064788250384\n",
      "Epoch: 8 - Batch: 2290, Training Loss: 0.19775283127825455\n",
      "Epoch: 8 - Batch: 2291, Training Loss: 0.19783768816472683\n",
      "Epoch: 8 - Batch: 2292, Training Loss: 0.1979249819100002\n",
      "Epoch: 8 - Batch: 2293, Training Loss: 0.1980147244560086\n",
      "Epoch: 8 - Batch: 2294, Training Loss: 0.19809323090250613\n",
      "Epoch: 8 - Batch: 2295, Training Loss: 0.1981842286612956\n",
      "Epoch: 8 - Batch: 2296, Training Loss: 0.19827035906574816\n",
      "Epoch: 8 - Batch: 2297, Training Loss: 0.1983599754214089\n",
      "Epoch: 8 - Batch: 2298, Training Loss: 0.1984555337510101\n",
      "Epoch: 8 - Batch: 2299, Training Loss: 0.19853656503617467\n",
      "Epoch: 8 - Batch: 2300, Training Loss: 0.19862181841585766\n",
      "Epoch: 8 - Batch: 2301, Training Loss: 0.19870499970629243\n",
      "Epoch: 8 - Batch: 2302, Training Loss: 0.19878991752566388\n",
      "Epoch: 8 - Batch: 2303, Training Loss: 0.1988754499932229\n",
      "Epoch: 8 - Batch: 2304, Training Loss: 0.19895837016119491\n",
      "Epoch: 8 - Batch: 2305, Training Loss: 0.1990522899993913\n",
      "Epoch: 8 - Batch: 2306, Training Loss: 0.19914145510686967\n",
      "Epoch: 8 - Batch: 2307, Training Loss: 0.19922705871944207\n",
      "Epoch: 8 - Batch: 2308, Training Loss: 0.19932006169141425\n",
      "Epoch: 8 - Batch: 2309, Training Loss: 0.19941519279879322\n",
      "Epoch: 8 - Batch: 2310, Training Loss: 0.19950280922339925\n",
      "Epoch: 8 - Batch: 2311, Training Loss: 0.19959622271247765\n",
      "Epoch: 8 - Batch: 2312, Training Loss: 0.19968567154745556\n",
      "Epoch: 8 - Batch: 2313, Training Loss: 0.19976683974167206\n",
      "Epoch: 8 - Batch: 2314, Training Loss: 0.19985559523402163\n",
      "Epoch: 8 - Batch: 2315, Training Loss: 0.1999484574110255\n",
      "Epoch: 8 - Batch: 2316, Training Loss: 0.20003324370231992\n",
      "Epoch: 8 - Batch: 2317, Training Loss: 0.20013402742186984\n",
      "Epoch: 8 - Batch: 2318, Training Loss: 0.20021677706668625\n",
      "Epoch: 8 - Batch: 2319, Training Loss: 0.20029997007047162\n",
      "Epoch: 8 - Batch: 2320, Training Loss: 0.20038191743108566\n",
      "Epoch: 8 - Batch: 2321, Training Loss: 0.20046885398712325\n",
      "Epoch: 8 - Batch: 2322, Training Loss: 0.20055568300397636\n",
      "Epoch: 8 - Batch: 2323, Training Loss: 0.20064633584907202\n",
      "Epoch: 8 - Batch: 2324, Training Loss: 0.20072589087520865\n",
      "Epoch: 8 - Batch: 2325, Training Loss: 0.20080813574627857\n",
      "Epoch: 8 - Batch: 2326, Training Loss: 0.20088875710124005\n",
      "Epoch: 8 - Batch: 2327, Training Loss: 0.2009653228321182\n",
      "Epoch: 8 - Batch: 2328, Training Loss: 0.20105568212063157\n",
      "Epoch: 8 - Batch: 2329, Training Loss: 0.2011448802614884\n",
      "Epoch: 8 - Batch: 2330, Training Loss: 0.201228145108393\n",
      "Epoch: 8 - Batch: 2331, Training Loss: 0.20130600193345527\n",
      "Epoch: 8 - Batch: 2332, Training Loss: 0.2013945983379931\n",
      "Epoch: 8 - Batch: 2333, Training Loss: 0.2014728741624561\n",
      "Epoch: 8 - Batch: 2334, Training Loss: 0.20156136279304823\n",
      "Epoch: 8 - Batch: 2335, Training Loss: 0.20165223401552607\n",
      "Epoch: 8 - Batch: 2336, Training Loss: 0.20174267204172575\n",
      "Epoch: 8 - Batch: 2337, Training Loss: 0.20182814277325498\n",
      "Epoch: 8 - Batch: 2338, Training Loss: 0.20190946230198414\n",
      "Epoch: 8 - Batch: 2339, Training Loss: 0.20198913810660393\n",
      "Epoch: 8 - Batch: 2340, Training Loss: 0.20207053392038218\n",
      "Epoch: 8 - Batch: 2341, Training Loss: 0.2021624725960677\n",
      "Epoch: 8 - Batch: 2342, Training Loss: 0.20225852103301542\n",
      "Epoch: 8 - Batch: 2343, Training Loss: 0.20233364291427344\n",
      "Epoch: 8 - Batch: 2344, Training Loss: 0.2024206053187598\n",
      "Epoch: 8 - Batch: 2345, Training Loss: 0.2025103053112034\n",
      "Epoch: 8 - Batch: 2346, Training Loss: 0.2025990296420865\n",
      "Epoch: 8 - Batch: 2347, Training Loss: 0.2026767277005893\n",
      "Epoch: 8 - Batch: 2348, Training Loss: 0.20275995667844665\n",
      "Epoch: 8 - Batch: 2349, Training Loss: 0.20284332092202717\n",
      "Epoch: 8 - Batch: 2350, Training Loss: 0.2029191714527694\n",
      "Epoch: 8 - Batch: 2351, Training Loss: 0.20300322298460932\n",
      "Epoch: 8 - Batch: 2352, Training Loss: 0.20309189005846012\n",
      "Epoch: 8 - Batch: 2353, Training Loss: 0.2031797279741238\n",
      "Epoch: 8 - Batch: 2354, Training Loss: 0.2032610239834829\n",
      "Epoch: 8 - Batch: 2355, Training Loss: 0.20334336493985966\n",
      "Epoch: 8 - Batch: 2356, Training Loss: 0.20342622077707231\n",
      "Epoch: 8 - Batch: 2357, Training Loss: 0.20351495184807436\n",
      "Epoch: 8 - Batch: 2358, Training Loss: 0.20359865704529717\n",
      "Epoch: 8 - Batch: 2359, Training Loss: 0.20368771187059123\n",
      "Epoch: 8 - Batch: 2360, Training Loss: 0.20377048119210683\n",
      "Epoch: 8 - Batch: 2361, Training Loss: 0.20386026567388726\n",
      "Epoch: 8 - Batch: 2362, Training Loss: 0.20394018112031578\n",
      "Epoch: 8 - Batch: 2363, Training Loss: 0.20402634145064932\n",
      "Epoch: 8 - Batch: 2364, Training Loss: 0.20411505568062094\n",
      "Epoch: 8 - Batch: 2365, Training Loss: 0.20420530962311412\n",
      "Epoch: 8 - Batch: 2366, Training Loss: 0.2042907077639949\n",
      "Epoch: 8 - Batch: 2367, Training Loss: 0.20437618787361814\n",
      "Epoch: 8 - Batch: 2368, Training Loss: 0.2044549915216752\n",
      "Epoch: 8 - Batch: 2369, Training Loss: 0.20453746641552073\n",
      "Epoch: 8 - Batch: 2370, Training Loss: 0.2046332507028212\n",
      "Epoch: 8 - Batch: 2371, Training Loss: 0.2047201400409587\n",
      "Epoch: 8 - Batch: 2372, Training Loss: 0.20480648312056637\n",
      "Epoch: 8 - Batch: 2373, Training Loss: 0.2048977659103981\n",
      "Epoch: 8 - Batch: 2374, Training Loss: 0.20498700427574107\n",
      "Epoch: 8 - Batch: 2375, Training Loss: 0.20506896629657714\n",
      "Epoch: 8 - Batch: 2376, Training Loss: 0.20515508893537482\n",
      "Epoch: 8 - Batch: 2377, Training Loss: 0.20523491273224848\n",
      "Epoch: 8 - Batch: 2378, Training Loss: 0.2053173838513803\n",
      "Epoch: 8 - Batch: 2379, Training Loss: 0.20541704093317684\n",
      "Epoch: 8 - Batch: 2380, Training Loss: 0.20549889181927464\n",
      "Epoch: 8 - Batch: 2381, Training Loss: 0.20558533333797954\n",
      "Epoch: 8 - Batch: 2382, Training Loss: 0.20566782730926525\n",
      "Epoch: 8 - Batch: 2383, Training Loss: 0.20574713973436584\n",
      "Epoch: 8 - Batch: 2384, Training Loss: 0.20583409128423355\n",
      "Epoch: 8 - Batch: 2385, Training Loss: 0.20591669049651468\n",
      "Epoch: 8 - Batch: 2386, Training Loss: 0.2060056285342668\n",
      "Epoch: 8 - Batch: 2387, Training Loss: 0.20609307603580046\n",
      "Epoch: 8 - Batch: 2388, Training Loss: 0.2061771413011733\n",
      "Epoch: 8 - Batch: 2389, Training Loss: 0.20625973306287382\n",
      "Epoch: 8 - Batch: 2390, Training Loss: 0.2063570325363236\n",
      "Epoch: 8 - Batch: 2391, Training Loss: 0.20644715496927352\n",
      "Epoch: 8 - Batch: 2392, Training Loss: 0.20653604198352218\n",
      "Epoch: 8 - Batch: 2393, Training Loss: 0.2066211412918706\n",
      "Epoch: 8 - Batch: 2394, Training Loss: 0.20670318287205736\n",
      "Epoch: 8 - Batch: 2395, Training Loss: 0.20678857410601517\n",
      "Epoch: 8 - Batch: 2396, Training Loss: 0.2068736395037194\n",
      "Epoch: 8 - Batch: 2397, Training Loss: 0.20695833619035298\n",
      "Epoch: 8 - Batch: 2398, Training Loss: 0.20704049139498282\n",
      "Epoch: 8 - Batch: 2399, Training Loss: 0.2071164518495895\n",
      "Epoch: 8 - Batch: 2400, Training Loss: 0.20720896604246958\n",
      "Epoch: 8 - Batch: 2401, Training Loss: 0.207300310574509\n",
      "Epoch: 8 - Batch: 2402, Training Loss: 0.20737816767140013\n",
      "Epoch: 8 - Batch: 2403, Training Loss: 0.20746883056798385\n",
      "Epoch: 8 - Batch: 2404, Training Loss: 0.20756294499854147\n",
      "Epoch: 8 - Batch: 2405, Training Loss: 0.20764792627140657\n",
      "Epoch: 8 - Batch: 2406, Training Loss: 0.20772264916357117\n",
      "Epoch: 8 - Batch: 2407, Training Loss: 0.20780512214225916\n",
      "Epoch: 8 - Batch: 2408, Training Loss: 0.20788699466345914\n",
      "Epoch: 8 - Batch: 2409, Training Loss: 0.20796974419089495\n",
      "Epoch: 8 - Batch: 2410, Training Loss: 0.208058454121029\n",
      "Epoch: 8 - Batch: 2411, Training Loss: 0.20813959333458745\n",
      "Epoch: 8 - Batch: 2412, Training Loss: 0.20821506919255897\n",
      "Epoch 8 - Batch 2412, Training Loss: 0.20821506919255897, Validation Loss: 0.20789601153402187\n",
      "Validation loss decreased (0.208014 --> 0.207896). Saving model...\n",
      "Epoch: 9 - Batch: 1, Training Loss: 8.975437056168197e-05\n",
      "Epoch: 9 - Batch: 2, Training Loss: 0.00018131951021515514\n",
      "Epoch: 9 - Batch: 3, Training Loss: 0.0002681578619167777\n",
      "Epoch: 9 - Batch: 4, Training Loss: 0.0003571510067824305\n",
      "Epoch: 9 - Batch: 5, Training Loss: 0.0004441804047148817\n",
      "Epoch: 9 - Batch: 6, Training Loss: 0.0005272279491966241\n",
      "Epoch: 9 - Batch: 7, Training Loss: 0.0006086992140037701\n",
      "Epoch: 9 - Batch: 8, Training Loss: 0.0006894687156673293\n",
      "Epoch: 9 - Batch: 9, Training Loss: 0.000771469490642769\n",
      "Epoch: 9 - Batch: 10, Training Loss: 0.00084989100299269\n",
      "Epoch: 9 - Batch: 11, Training Loss: 0.00093570533849509\n",
      "Epoch: 9 - Batch: 12, Training Loss: 0.0010191536080679095\n",
      "Epoch: 9 - Batch: 13, Training Loss: 0.0011011116997420689\n",
      "Epoch: 9 - Batch: 14, Training Loss: 0.0011844446768313893\n",
      "Epoch: 9 - Batch: 15, Training Loss: 0.001277696952890994\n",
      "Epoch: 9 - Batch: 16, Training Loss: 0.0013704357443263084\n",
      "Epoch: 9 - Batch: 17, Training Loss: 0.00145626740891542\n",
      "Epoch: 9 - Batch: 18, Training Loss: 0.0015353450665920726\n",
      "Epoch: 9 - Batch: 19, Training Loss: 0.0016136674664506865\n",
      "Epoch: 9 - Batch: 20, Training Loss: 0.0016992838302655007\n",
      "Epoch: 9 - Batch: 21, Training Loss: 0.0017828591674516846\n",
      "Epoch: 9 - Batch: 22, Training Loss: 0.0018702583699479428\n",
      "Epoch: 9 - Batch: 23, Training Loss: 0.0019420240673655104\n",
      "Epoch: 9 - Batch: 24, Training Loss: 0.0020213158211205926\n",
      "Epoch: 9 - Batch: 25, Training Loss: 0.0021059244969986366\n",
      "Epoch: 9 - Batch: 26, Training Loss: 0.002185901537364593\n",
      "Epoch: 9 - Batch: 27, Training Loss: 0.0022675906976755973\n",
      "Epoch: 9 - Batch: 28, Training Loss: 0.002356685203798177\n",
      "Epoch: 9 - Batch: 29, Training Loss: 0.002443870940364613\n",
      "Epoch: 9 - Batch: 30, Training Loss: 0.002530887840349678\n",
      "Epoch: 9 - Batch: 31, Training Loss: 0.0026155431781143294\n",
      "Epoch: 9 - Batch: 32, Training Loss: 0.0027007868028497617\n",
      "Epoch: 9 - Batch: 33, Training Loss: 0.0027978377135634224\n",
      "Epoch: 9 - Batch: 34, Training Loss: 0.002879493543955422\n",
      "Epoch: 9 - Batch: 35, Training Loss: 0.002968782123493318\n",
      "Epoch: 9 - Batch: 36, Training Loss: 0.003060080240763242\n",
      "Epoch: 9 - Batch: 37, Training Loss: 0.0031472837141596063\n",
      "Epoch: 9 - Batch: 38, Training Loss: 0.00323385089808831\n",
      "Epoch: 9 - Batch: 39, Training Loss: 0.003323798631603643\n",
      "Epoch: 9 - Batch: 40, Training Loss: 0.00340828618921253\n",
      "Epoch: 9 - Batch: 41, Training Loss: 0.0034979200592978083\n",
      "Epoch: 9 - Batch: 42, Training Loss: 0.003584380970517201\n",
      "Epoch: 9 - Batch: 43, Training Loss: 0.00366827778851808\n",
      "Epoch: 9 - Batch: 44, Training Loss: 0.0037534859087040174\n",
      "Epoch: 9 - Batch: 45, Training Loss: 0.0038342441294719133\n",
      "Epoch: 9 - Batch: 46, Training Loss: 0.003919371139647356\n",
      "Epoch: 9 - Batch: 47, Training Loss: 0.004005866159204622\n",
      "Epoch: 9 - Batch: 48, Training Loss: 0.004090301414479071\n",
      "Epoch: 9 - Batch: 49, Training Loss: 0.004180411473040162\n",
      "Epoch: 9 - Batch: 50, Training Loss: 0.004262722960702618\n",
      "Epoch: 9 - Batch: 51, Training Loss: 0.004351625765734051\n",
      "Epoch: 9 - Batch: 52, Training Loss: 0.004435458673074669\n",
      "Epoch: 9 - Batch: 53, Training Loss: 0.004517745189554063\n",
      "Epoch: 9 - Batch: 54, Training Loss: 0.0046097934802076706\n",
      "Epoch: 9 - Batch: 55, Training Loss: 0.00469946893392313\n",
      "Epoch: 9 - Batch: 56, Training Loss: 0.004789624913019525\n",
      "Epoch: 9 - Batch: 57, Training Loss: 0.004878748931101898\n",
      "Epoch: 9 - Batch: 58, Training Loss: 0.0049691225091616316\n",
      "Epoch: 9 - Batch: 59, Training Loss: 0.005051675794729546\n",
      "Epoch: 9 - Batch: 60, Training Loss: 0.005138934109588563\n",
      "Epoch: 9 - Batch: 61, Training Loss: 0.005225539015942743\n",
      "Epoch: 9 - Batch: 62, Training Loss: 0.005315962876154614\n",
      "Epoch: 9 - Batch: 63, Training Loss: 0.0053995711540503685\n",
      "Epoch: 9 - Batch: 64, Training Loss: 0.005486452096384358\n",
      "Epoch: 9 - Batch: 65, Training Loss: 0.0055707604817391235\n",
      "Epoch: 9 - Batch: 66, Training Loss: 0.005658224997285191\n",
      "Epoch: 9 - Batch: 67, Training Loss: 0.005741660447658393\n",
      "Epoch: 9 - Batch: 68, Training Loss: 0.005831391292672054\n",
      "Epoch: 9 - Batch: 69, Training Loss: 0.005911359010545373\n",
      "Epoch: 9 - Batch: 70, Training Loss: 0.0059888992291777885\n",
      "Epoch: 9 - Batch: 71, Training Loss: 0.006079601923970638\n",
      "Epoch: 9 - Batch: 72, Training Loss: 0.0061702300573561715\n",
      "Epoch: 9 - Batch: 73, Training Loss: 0.006258592942411429\n",
      "Epoch: 9 - Batch: 74, Training Loss: 0.006336995995113901\n",
      "Epoch: 9 - Batch: 75, Training Loss: 0.0064176368229029385\n",
      "Epoch: 9 - Batch: 76, Training Loss: 0.006511240901045538\n",
      "Epoch: 9 - Batch: 77, Training Loss: 0.006601110588802429\n",
      "Epoch: 9 - Batch: 78, Training Loss: 0.0066854232925285355\n",
      "Epoch: 9 - Batch: 79, Training Loss: 0.006766165469218645\n",
      "Epoch: 9 - Batch: 80, Training Loss: 0.006860834356415924\n",
      "Epoch: 9 - Batch: 81, Training Loss: 0.006937566372391399\n",
      "Epoch: 9 - Batch: 82, Training Loss: 0.007016273953675433\n",
      "Epoch: 9 - Batch: 83, Training Loss: 0.007096622126859614\n",
      "Epoch: 9 - Batch: 84, Training Loss: 0.007179250078859614\n",
      "Epoch: 9 - Batch: 85, Training Loss: 0.007259025298689136\n",
      "Epoch: 9 - Batch: 86, Training Loss: 0.007347283980640803\n",
      "Epoch: 9 - Batch: 87, Training Loss: 0.007436175646868906\n",
      "Epoch: 9 - Batch: 88, Training Loss: 0.0075169471872485495\n",
      "Epoch: 9 - Batch: 89, Training Loss: 0.007606417373144014\n",
      "Epoch: 9 - Batch: 90, Training Loss: 0.007693153189782479\n",
      "Epoch: 9 - Batch: 91, Training Loss: 0.007776639811929383\n",
      "Epoch: 9 - Batch: 92, Training Loss: 0.007861448837997111\n",
      "Epoch: 9 - Batch: 93, Training Loss: 0.00794956290702124\n",
      "Epoch: 9 - Batch: 94, Training Loss: 0.008024171229519851\n",
      "Epoch: 9 - Batch: 95, Training Loss: 0.008115212289403328\n",
      "Epoch: 9 - Batch: 96, Training Loss: 0.008199732837145205\n",
      "Epoch: 9 - Batch: 97, Training Loss: 0.008289985754102419\n",
      "Epoch: 9 - Batch: 98, Training Loss: 0.008375416495906772\n",
      "Epoch: 9 - Batch: 99, Training Loss: 0.008470725058075999\n",
      "Epoch: 9 - Batch: 100, Training Loss: 0.008555140611939564\n",
      "Epoch: 9 - Batch: 101, Training Loss: 0.008627107376186409\n",
      "Epoch: 9 - Batch: 102, Training Loss: 0.00871520200945054\n",
      "Epoch: 9 - Batch: 103, Training Loss: 0.008817980334681659\n",
      "Epoch: 9 - Batch: 104, Training Loss: 0.008920778373679513\n",
      "Epoch: 9 - Batch: 105, Training Loss: 0.009009265398010488\n",
      "Epoch: 9 - Batch: 106, Training Loss: 0.009089435010555372\n",
      "Epoch: 9 - Batch: 107, Training Loss: 0.009171348664396834\n",
      "Epoch: 9 - Batch: 108, Training Loss: 0.00925740085628693\n",
      "Epoch: 9 - Batch: 109, Training Loss: 0.009336267092927772\n",
      "Epoch: 9 - Batch: 110, Training Loss: 0.009418588644434168\n",
      "Epoch: 9 - Batch: 111, Training Loss: 0.009507041560427268\n",
      "Epoch: 9 - Batch: 112, Training Loss: 0.009588539779532213\n",
      "Epoch: 9 - Batch: 113, Training Loss: 0.009672421344230029\n",
      "Epoch: 9 - Batch: 114, Training Loss: 0.009752024088678866\n",
      "Epoch: 9 - Batch: 115, Training Loss: 0.009842234828924856\n",
      "Epoch: 9 - Batch: 116, Training Loss: 0.009934927452707765\n",
      "Epoch: 9 - Batch: 117, Training Loss: 0.010019558659487499\n",
      "Epoch: 9 - Batch: 118, Training Loss: 0.010104809333238238\n",
      "Epoch: 9 - Batch: 119, Training Loss: 0.010183554748150444\n",
      "Epoch: 9 - Batch: 120, Training Loss: 0.010276085794416826\n",
      "Epoch: 9 - Batch: 121, Training Loss: 0.010363108897041128\n",
      "Epoch: 9 - Batch: 122, Training Loss: 0.010458911668700761\n",
      "Epoch: 9 - Batch: 123, Training Loss: 0.010550132549520749\n",
      "Epoch: 9 - Batch: 124, Training Loss: 0.010636645299730017\n",
      "Epoch: 9 - Batch: 125, Training Loss: 0.010722322565554386\n",
      "Epoch: 9 - Batch: 126, Training Loss: 0.010813825647925856\n",
      "Epoch: 9 - Batch: 127, Training Loss: 0.010898564010858536\n",
      "Epoch: 9 - Batch: 128, Training Loss: 0.010989210770695562\n",
      "Epoch: 9 - Batch: 129, Training Loss: 0.011071756939290965\n",
      "Epoch: 9 - Batch: 130, Training Loss: 0.011168263630190892\n",
      "Epoch: 9 - Batch: 131, Training Loss: 0.011241264653640797\n",
      "Epoch: 9 - Batch: 132, Training Loss: 0.011334216966607281\n",
      "Epoch: 9 - Batch: 133, Training Loss: 0.011422759023916661\n",
      "Epoch: 9 - Batch: 134, Training Loss: 0.011509493642037188\n",
      "Epoch: 9 - Batch: 135, Training Loss: 0.011589853862180045\n",
      "Epoch: 9 - Batch: 136, Training Loss: 0.011682627738065783\n",
      "Epoch: 9 - Batch: 137, Training Loss: 0.011772515039094052\n",
      "Epoch: 9 - Batch: 138, Training Loss: 0.01185464863951131\n",
      "Epoch: 9 - Batch: 139, Training Loss: 0.01193348511089733\n",
      "Epoch: 9 - Batch: 140, Training Loss: 0.01202929528371414\n",
      "Epoch: 9 - Batch: 141, Training Loss: 0.012116808104475537\n",
      "Epoch: 9 - Batch: 142, Training Loss: 0.012201113017835032\n",
      "Epoch: 9 - Batch: 143, Training Loss: 0.01228815950391502\n",
      "Epoch: 9 - Batch: 144, Training Loss: 0.012368736377798305\n",
      "Epoch: 9 - Batch: 145, Training Loss: 0.01245257003884608\n",
      "Epoch: 9 - Batch: 146, Training Loss: 0.012528048157939073\n",
      "Epoch: 9 - Batch: 147, Training Loss: 0.012611464228153624\n",
      "Epoch: 9 - Batch: 148, Training Loss: 0.012694547406921339\n",
      "Epoch: 9 - Batch: 149, Training Loss: 0.012794051795061153\n",
      "Epoch: 9 - Batch: 150, Training Loss: 0.012877790031840355\n",
      "Epoch: 9 - Batch: 151, Training Loss: 0.012961038513414895\n",
      "Epoch: 9 - Batch: 152, Training Loss: 0.013054305567077143\n",
      "Epoch: 9 - Batch: 153, Training Loss: 0.013136162291316448\n",
      "Epoch: 9 - Batch: 154, Training Loss: 0.01321852264638564\n",
      "Epoch: 9 - Batch: 155, Training Loss: 0.013300570713396295\n",
      "Epoch: 9 - Batch: 156, Training Loss: 0.013394668933072097\n",
      "Epoch: 9 - Batch: 157, Training Loss: 0.013487795723067785\n",
      "Epoch: 9 - Batch: 158, Training Loss: 0.013567716587538742\n",
      "Epoch: 9 - Batch: 159, Training Loss: 0.013648608189761935\n",
      "Epoch: 9 - Batch: 160, Training Loss: 0.01373499297038042\n",
      "Epoch: 9 - Batch: 161, Training Loss: 0.01381916993886084\n",
      "Epoch: 9 - Batch: 162, Training Loss: 0.013914105669281772\n",
      "Epoch: 9 - Batch: 163, Training Loss: 0.01399896667025378\n",
      "Epoch: 9 - Batch: 164, Training Loss: 0.014086393858761733\n",
      "Epoch: 9 - Batch: 165, Training Loss: 0.014162948802978442\n",
      "Epoch: 9 - Batch: 166, Training Loss: 0.014246506940814392\n",
      "Epoch: 9 - Batch: 167, Training Loss: 0.014325260331431037\n",
      "Epoch: 9 - Batch: 168, Training Loss: 0.014407000515245481\n",
      "Epoch: 9 - Batch: 169, Training Loss: 0.014497121928837366\n",
      "Epoch: 9 - Batch: 170, Training Loss: 0.014581733971685912\n",
      "Epoch: 9 - Batch: 171, Training Loss: 0.014675034330555456\n",
      "Epoch: 9 - Batch: 172, Training Loss: 0.01476540511001402\n",
      "Epoch: 9 - Batch: 173, Training Loss: 0.014849003793588325\n",
      "Epoch: 9 - Batch: 174, Training Loss: 0.014946111596836576\n",
      "Epoch: 9 - Batch: 175, Training Loss: 0.015031749879938255\n",
      "Epoch: 9 - Batch: 176, Training Loss: 0.01512425209381687\n",
      "Epoch: 9 - Batch: 177, Training Loss: 0.015207553452273112\n",
      "Epoch: 9 - Batch: 178, Training Loss: 0.015296072595649295\n",
      "Epoch: 9 - Batch: 179, Training Loss: 0.015378219478610736\n",
      "Epoch: 9 - Batch: 180, Training Loss: 0.015463511390027716\n",
      "Epoch: 9 - Batch: 181, Training Loss: 0.01554876118033482\n",
      "Epoch: 9 - Batch: 182, Training Loss: 0.015634554245232746\n",
      "Epoch: 9 - Batch: 183, Training Loss: 0.01571934337219591\n",
      "Epoch: 9 - Batch: 184, Training Loss: 0.01580581957378593\n",
      "Epoch: 9 - Batch: 185, Training Loss: 0.01589596935024309\n",
      "Epoch: 9 - Batch: 186, Training Loss: 0.01597781467620611\n",
      "Epoch: 9 - Batch: 187, Training Loss: 0.01607031792179862\n",
      "Epoch: 9 - Batch: 188, Training Loss: 0.016167026457946692\n",
      "Epoch: 9 - Batch: 189, Training Loss: 0.01625478526524841\n",
      "Epoch: 9 - Batch: 190, Training Loss: 0.016340771100018945\n",
      "Epoch: 9 - Batch: 191, Training Loss: 0.016424257431803254\n",
      "Epoch: 9 - Batch: 192, Training Loss: 0.016508474970733742\n",
      "Epoch: 9 - Batch: 193, Training Loss: 0.01659203486310111\n",
      "Epoch: 9 - Batch: 194, Training Loss: 0.01668274450806243\n",
      "Epoch: 9 - Batch: 195, Training Loss: 0.0167798040616967\n",
      "Epoch: 9 - Batch: 196, Training Loss: 0.01686404754174487\n",
      "Epoch: 9 - Batch: 197, Training Loss: 0.0169436903685875\n",
      "Epoch: 9 - Batch: 198, Training Loss: 0.017023074099327597\n",
      "Epoch: 9 - Batch: 199, Training Loss: 0.017100472528345352\n",
      "Epoch: 9 - Batch: 200, Training Loss: 0.017187228305867656\n",
      "Epoch: 9 - Batch: 201, Training Loss: 0.017273856113799176\n",
      "Epoch: 9 - Batch: 202, Training Loss: 0.017353688318832203\n",
      "Epoch: 9 - Batch: 203, Training Loss: 0.017445817348820652\n",
      "Epoch: 9 - Batch: 204, Training Loss: 0.017535149402434554\n",
      "Epoch: 9 - Batch: 205, Training Loss: 0.01763007138078881\n",
      "Epoch: 9 - Batch: 206, Training Loss: 0.017717715921142997\n",
      "Epoch: 9 - Batch: 207, Training Loss: 0.017814679531562783\n",
      "Epoch: 9 - Batch: 208, Training Loss: 0.017906242502763695\n",
      "Epoch: 9 - Batch: 209, Training Loss: 0.017993932969831115\n",
      "Epoch: 9 - Batch: 210, Training Loss: 0.018077310082034684\n",
      "Epoch: 9 - Batch: 211, Training Loss: 0.01815878916809808\n",
      "Epoch: 9 - Batch: 212, Training Loss: 0.018246585493953665\n",
      "Epoch: 9 - Batch: 213, Training Loss: 0.018333292984794424\n",
      "Epoch: 9 - Batch: 214, Training Loss: 0.01841824854512515\n",
      "Epoch: 9 - Batch: 215, Training Loss: 0.018500744542771112\n",
      "Epoch: 9 - Batch: 216, Training Loss: 0.01858039774235406\n",
      "Epoch: 9 - Batch: 217, Training Loss: 0.0186692287969352\n",
      "Epoch: 9 - Batch: 218, Training Loss: 0.01875713680736461\n",
      "Epoch: 9 - Batch: 219, Training Loss: 0.018846315302411914\n",
      "Epoch: 9 - Batch: 220, Training Loss: 0.018932937408116326\n",
      "Epoch: 9 - Batch: 221, Training Loss: 0.019023582604937687\n",
      "Epoch: 9 - Batch: 222, Training Loss: 0.01910786480205767\n",
      "Epoch: 9 - Batch: 223, Training Loss: 0.019197142038920624\n",
      "Epoch: 9 - Batch: 224, Training Loss: 0.019283264183484104\n",
      "Epoch: 9 - Batch: 225, Training Loss: 0.019371899916708568\n",
      "Epoch: 9 - Batch: 226, Training Loss: 0.019459819875249817\n",
      "Epoch: 9 - Batch: 227, Training Loss: 0.019550971760975186\n",
      "Epoch: 9 - Batch: 228, Training Loss: 0.019641303718337173\n",
      "Epoch: 9 - Batch: 229, Training Loss: 0.019731523373332584\n",
      "Epoch: 9 - Batch: 230, Training Loss: 0.019814529204447668\n",
      "Epoch: 9 - Batch: 231, Training Loss: 0.019900504820926073\n",
      "Epoch: 9 - Batch: 232, Training Loss: 0.019980992714119194\n",
      "Epoch: 9 - Batch: 233, Training Loss: 0.020071092857106607\n",
      "Epoch: 9 - Batch: 234, Training Loss: 0.020150861139123514\n",
      "Epoch: 9 - Batch: 235, Training Loss: 0.020243164009814633\n",
      "Epoch: 9 - Batch: 236, Training Loss: 0.02033072501841074\n",
      "Epoch: 9 - Batch: 237, Training Loss: 0.02041701952097428\n",
      "Epoch: 9 - Batch: 238, Training Loss: 0.02050167942422737\n",
      "Epoch: 9 - Batch: 239, Training Loss: 0.020586126547300597\n",
      "Epoch: 9 - Batch: 240, Training Loss: 0.02066662474527683\n",
      "Epoch: 9 - Batch: 241, Training Loss: 0.020750370401746994\n",
      "Epoch: 9 - Batch: 242, Training Loss: 0.020834158018700914\n",
      "Epoch: 9 - Batch: 243, Training Loss: 0.020923087246728377\n",
      "Epoch: 9 - Batch: 244, Training Loss: 0.021009415005075794\n",
      "Epoch: 9 - Batch: 245, Training Loss: 0.021092677695883646\n",
      "Epoch: 9 - Batch: 246, Training Loss: 0.021181619397146786\n",
      "Epoch: 9 - Batch: 247, Training Loss: 0.021261373117788515\n",
      "Epoch: 9 - Batch: 248, Training Loss: 0.02135053074379663\n",
      "Epoch: 9 - Batch: 249, Training Loss: 0.02144164607472483\n",
      "Epoch: 9 - Batch: 250, Training Loss: 0.0215244053818199\n",
      "Epoch: 9 - Batch: 251, Training Loss: 0.021608753546850004\n",
      "Epoch: 9 - Batch: 252, Training Loss: 0.02169430999094574\n",
      "Epoch: 9 - Batch: 253, Training Loss: 0.021770512916851992\n",
      "Epoch: 9 - Batch: 254, Training Loss: 0.021854075064162908\n",
      "Epoch: 9 - Batch: 255, Training Loss: 0.021946759743131018\n",
      "Epoch: 9 - Batch: 256, Training Loss: 0.022037801698813985\n",
      "Epoch: 9 - Batch: 257, Training Loss: 0.022124139817545862\n",
      "Epoch: 9 - Batch: 258, Training Loss: 0.022210042299718208\n",
      "Epoch: 9 - Batch: 259, Training Loss: 0.022302329144865324\n",
      "Epoch: 9 - Batch: 260, Training Loss: 0.022393784799287175\n",
      "Epoch: 9 - Batch: 261, Training Loss: 0.022479479814297328\n",
      "Epoch: 9 - Batch: 262, Training Loss: 0.022563530388558486\n",
      "Epoch: 9 - Batch: 263, Training Loss: 0.022651361718551435\n",
      "Epoch: 9 - Batch: 264, Training Loss: 0.022739433752956676\n",
      "Epoch: 9 - Batch: 265, Training Loss: 0.022830630972314237\n",
      "Epoch: 9 - Batch: 266, Training Loss: 0.02292154813386117\n",
      "Epoch: 9 - Batch: 267, Training Loss: 0.023002827957050123\n",
      "Epoch: 9 - Batch: 268, Training Loss: 0.023093753526756418\n",
      "Epoch: 9 - Batch: 269, Training Loss: 0.023180223000583363\n",
      "Epoch: 9 - Batch: 270, Training Loss: 0.02326656888497014\n",
      "Epoch: 9 - Batch: 271, Training Loss: 0.023348140596700942\n",
      "Epoch: 9 - Batch: 272, Training Loss: 0.023439722435292517\n",
      "Epoch: 9 - Batch: 273, Training Loss: 0.02352650324577716\n",
      "Epoch: 9 - Batch: 274, Training Loss: 0.02361860167649057\n",
      "Epoch: 9 - Batch: 275, Training Loss: 0.023699264991935805\n",
      "Epoch: 9 - Batch: 276, Training Loss: 0.023788829928705745\n",
      "Epoch: 9 - Batch: 277, Training Loss: 0.02387910054542532\n",
      "Epoch: 9 - Batch: 278, Training Loss: 0.023964577918226645\n",
      "Epoch: 9 - Batch: 279, Training Loss: 0.024052433996997268\n",
      "Epoch: 9 - Batch: 280, Training Loss: 0.024138367863041448\n",
      "Epoch: 9 - Batch: 281, Training Loss: 0.024217229768955094\n",
      "Epoch: 9 - Batch: 282, Training Loss: 0.024302707265314968\n",
      "Epoch: 9 - Batch: 283, Training Loss: 0.024389103153847146\n",
      "Epoch: 9 - Batch: 284, Training Loss: 0.024473064055490257\n",
      "Epoch: 9 - Batch: 285, Training Loss: 0.024552161025368356\n",
      "Epoch: 9 - Batch: 286, Training Loss: 0.024639574505055127\n",
      "Epoch: 9 - Batch: 287, Training Loss: 0.02471878718133787\n",
      "Epoch: 9 - Batch: 288, Training Loss: 0.02480509175392328\n",
      "Epoch: 9 - Batch: 289, Training Loss: 0.024894277427722367\n",
      "Epoch: 9 - Batch: 290, Training Loss: 0.02497807717812595\n",
      "Epoch: 9 - Batch: 291, Training Loss: 0.02507165263393032\n",
      "Epoch: 9 - Batch: 292, Training Loss: 0.025172260387570505\n",
      "Epoch: 9 - Batch: 293, Training Loss: 0.025260947354347946\n",
      "Epoch: 9 - Batch: 294, Training Loss: 0.02534614754825287\n",
      "Epoch: 9 - Batch: 295, Training Loss: 0.025439152206799286\n",
      "Epoch: 9 - Batch: 296, Training Loss: 0.025529530683955547\n",
      "Epoch: 9 - Batch: 297, Training Loss: 0.025610353402286817\n",
      "Epoch: 9 - Batch: 298, Training Loss: 0.025703913122909778\n",
      "Epoch: 9 - Batch: 299, Training Loss: 0.025788060801835797\n",
      "Epoch: 9 - Batch: 300, Training Loss: 0.025874507256704777\n",
      "Epoch: 9 - Batch: 301, Training Loss: 0.02596303796516129\n",
      "Epoch: 9 - Batch: 302, Training Loss: 0.026048533414578558\n",
      "Epoch: 9 - Batch: 303, Training Loss: 0.02613306309329732\n",
      "Epoch: 9 - Batch: 304, Training Loss: 0.026213397316721145\n",
      "Epoch: 9 - Batch: 305, Training Loss: 0.026297973793240923\n",
      "Epoch: 9 - Batch: 306, Training Loss: 0.026375217248946675\n",
      "Epoch: 9 - Batch: 307, Training Loss: 0.02646887883741662\n",
      "Epoch: 9 - Batch: 308, Training Loss: 0.02655832684455226\n",
      "Epoch: 9 - Batch: 309, Training Loss: 0.026641626127224854\n",
      "Epoch: 9 - Batch: 310, Training Loss: 0.026724473704548418\n",
      "Epoch: 9 - Batch: 311, Training Loss: 0.02681209641210673\n",
      "Epoch: 9 - Batch: 312, Training Loss: 0.026899211547317396\n",
      "Epoch: 9 - Batch: 313, Training Loss: 0.026990480819843697\n",
      "Epoch: 9 - Batch: 314, Training Loss: 0.027074501072836554\n",
      "Epoch: 9 - Batch: 315, Training Loss: 0.027151035518689734\n",
      "Epoch: 9 - Batch: 316, Training Loss: 0.027235489381881892\n",
      "Epoch: 9 - Batch: 317, Training Loss: 0.027330110902908825\n",
      "Epoch: 9 - Batch: 318, Training Loss: 0.02741936764140825\n",
      "Epoch: 9 - Batch: 319, Training Loss: 0.02749932121800546\n",
      "Epoch: 9 - Batch: 320, Training Loss: 0.027584288979743053\n",
      "Epoch: 9 - Batch: 321, Training Loss: 0.02767157479873542\n",
      "Epoch: 9 - Batch: 322, Training Loss: 0.02775966573142096\n",
      "Epoch: 9 - Batch: 323, Training Loss: 0.027846219849032943\n",
      "Epoch: 9 - Batch: 324, Training Loss: 0.027924236249963246\n",
      "Epoch: 9 - Batch: 325, Training Loss: 0.02800635502335444\n",
      "Epoch: 9 - Batch: 326, Training Loss: 0.02808368021304137\n",
      "Epoch: 9 - Batch: 327, Training Loss: 0.028172475163814045\n",
      "Epoch: 9 - Batch: 328, Training Loss: 0.028256261032414475\n",
      "Epoch: 9 - Batch: 329, Training Loss: 0.028339445374745437\n",
      "Epoch: 9 - Batch: 330, Training Loss: 0.028429465328233556\n",
      "Epoch: 9 - Batch: 331, Training Loss: 0.02850871269357936\n",
      "Epoch: 9 - Batch: 332, Training Loss: 0.028597903538303788\n",
      "Epoch: 9 - Batch: 333, Training Loss: 0.028685509238027616\n",
      "Epoch: 9 - Batch: 334, Training Loss: 0.02878294594795945\n",
      "Epoch: 9 - Batch: 335, Training Loss: 0.028875207840457287\n",
      "Epoch: 9 - Batch: 336, Training Loss: 0.028956865901081125\n",
      "Epoch: 9 - Batch: 337, Training Loss: 0.029047726812597926\n",
      "Epoch: 9 - Batch: 338, Training Loss: 0.029137723784195647\n",
      "Epoch: 9 - Batch: 339, Training Loss: 0.02922503130006948\n",
      "Epoch: 9 - Batch: 340, Training Loss: 0.02931471267841744\n",
      "Epoch: 9 - Batch: 341, Training Loss: 0.029399698146095322\n",
      "Epoch: 9 - Batch: 342, Training Loss: 0.029495199113639434\n",
      "Epoch: 9 - Batch: 343, Training Loss: 0.02957989475002534\n",
      "Epoch: 9 - Batch: 344, Training Loss: 0.029658727860618784\n",
      "Epoch: 9 - Batch: 345, Training Loss: 0.02973838815246253\n",
      "Epoch: 9 - Batch: 346, Training Loss: 0.029827993165448333\n",
      "Epoch: 9 - Batch: 347, Training Loss: 0.029912596114387558\n",
      "Epoch: 9 - Batch: 348, Training Loss: 0.02999334425895566\n",
      "Epoch: 9 - Batch: 349, Training Loss: 0.030081590523273\n",
      "Epoch: 9 - Batch: 350, Training Loss: 0.030167625151315137\n",
      "Epoch: 9 - Batch: 351, Training Loss: 0.030249095915710154\n",
      "Epoch: 9 - Batch: 352, Training Loss: 0.030320316948503206\n",
      "Epoch: 9 - Batch: 353, Training Loss: 0.030408134625276324\n",
      "Epoch: 9 - Batch: 354, Training Loss: 0.03049011528491974\n",
      "Epoch: 9 - Batch: 355, Training Loss: 0.03057514122420085\n",
      "Epoch: 9 - Batch: 356, Training Loss: 0.03066275973076844\n",
      "Epoch: 9 - Batch: 357, Training Loss: 0.030752609686224813\n",
      "Epoch: 9 - Batch: 358, Training Loss: 0.03084197219084349\n",
      "Epoch: 9 - Batch: 359, Training Loss: 0.03093288603484334\n",
      "Epoch: 9 - Batch: 360, Training Loss: 0.03102445046403515\n",
      "Epoch: 9 - Batch: 361, Training Loss: 0.031106520425620957\n",
      "Epoch: 9 - Batch: 362, Training Loss: 0.031193124763605803\n",
      "Epoch: 9 - Batch: 363, Training Loss: 0.03127251673570122\n",
      "Epoch: 9 - Batch: 364, Training Loss: 0.03136403251371376\n",
      "Epoch: 9 - Batch: 365, Training Loss: 0.03144818788415955\n",
      "Epoch: 9 - Batch: 366, Training Loss: 0.03153753050820744\n",
      "Epoch: 9 - Batch: 367, Training Loss: 0.03162114181946561\n",
      "Epoch: 9 - Batch: 368, Training Loss: 0.03170735411234756\n",
      "Epoch: 9 - Batch: 369, Training Loss: 0.03179132146374701\n",
      "Epoch: 9 - Batch: 370, Training Loss: 0.03187869557767958\n",
      "Epoch: 9 - Batch: 371, Training Loss: 0.031969757030901824\n",
      "Epoch: 9 - Batch: 372, Training Loss: 0.03205635547514381\n",
      "Epoch: 9 - Batch: 373, Training Loss: 0.03213795601926237\n",
      "Epoch: 9 - Batch: 374, Training Loss: 0.032219664145810886\n",
      "Epoch: 9 - Batch: 375, Training Loss: 0.0323081210589231\n",
      "Epoch: 9 - Batch: 376, Training Loss: 0.032390501930889604\n",
      "Epoch: 9 - Batch: 377, Training Loss: 0.0324767966496806\n",
      "Epoch: 9 - Batch: 378, Training Loss: 0.03256773776305256\n",
      "Epoch: 9 - Batch: 379, Training Loss: 0.03265758577246176\n",
      "Epoch: 9 - Batch: 380, Training Loss: 0.03274962249185711\n",
      "Epoch: 9 - Batch: 381, Training Loss: 0.03284034575914862\n",
      "Epoch: 9 - Batch: 382, Training Loss: 0.032925495040752796\n",
      "Epoch: 9 - Batch: 383, Training Loss: 0.03301075378905481\n",
      "Epoch: 9 - Batch: 384, Training Loss: 0.03309816637912002\n",
      "Epoch: 9 - Batch: 385, Training Loss: 0.03318626970472818\n",
      "Epoch: 9 - Batch: 386, Training Loss: 0.03326694449876276\n",
      "Epoch: 9 - Batch: 387, Training Loss: 0.03335066012432721\n",
      "Epoch: 9 - Batch: 388, Training Loss: 0.0334313808388971\n",
      "Epoch: 9 - Batch: 389, Training Loss: 0.033517052316003375\n",
      "Epoch: 9 - Batch: 390, Training Loss: 0.033609355038424236\n",
      "Epoch: 9 - Batch: 391, Training Loss: 0.03370411900392615\n",
      "Epoch: 9 - Batch: 392, Training Loss: 0.033786860123202575\n",
      "Epoch: 9 - Batch: 393, Training Loss: 0.03387879839114487\n",
      "Epoch: 9 - Batch: 394, Training Loss: 0.03397035008762804\n",
      "Epoch: 9 - Batch: 395, Training Loss: 0.0340612382808728\n",
      "Epoch: 9 - Batch: 396, Training Loss: 0.034153036717603454\n",
      "Epoch: 9 - Batch: 397, Training Loss: 0.03423967840733813\n",
      "Epoch: 9 - Batch: 398, Training Loss: 0.0343274659543587\n",
      "Epoch: 9 - Batch: 399, Training Loss: 0.03440832591941503\n",
      "Epoch: 9 - Batch: 400, Training Loss: 0.03449566305227343\n",
      "Epoch: 9 - Batch: 401, Training Loss: 0.034578305114659896\n",
      "Epoch: 9 - Batch: 402, Training Loss: 0.03467203332590029\n",
      "Epoch: 9 - Batch: 403, Training Loss: 0.034756659645939346\n",
      "Epoch: 9 - Batch: 404, Training Loss: 0.034844550951252724\n",
      "Epoch: 9 - Batch: 405, Training Loss: 0.03493064260774386\n",
      "Epoch: 9 - Batch: 406, Training Loss: 0.035011411831400684\n",
      "Epoch: 9 - Batch: 407, Training Loss: 0.03510828154967792\n",
      "Epoch: 9 - Batch: 408, Training Loss: 0.03519710764956119\n",
      "Epoch: 9 - Batch: 409, Training Loss: 0.035284042914411916\n",
      "Epoch: 9 - Batch: 410, Training Loss: 0.03537599644801312\n",
      "Epoch: 9 - Batch: 411, Training Loss: 0.035463628416284795\n",
      "Epoch: 9 - Batch: 412, Training Loss: 0.03554819405054176\n",
      "Epoch: 9 - Batch: 413, Training Loss: 0.035632177075343344\n",
      "Epoch: 9 - Batch: 414, Training Loss: 0.03572424818108331\n",
      "Epoch: 9 - Batch: 415, Training Loss: 0.03581116066791525\n",
      "Epoch: 9 - Batch: 416, Training Loss: 0.03589199201681128\n",
      "Epoch: 9 - Batch: 417, Training Loss: 0.03598161946802986\n",
      "Epoch: 9 - Batch: 418, Training Loss: 0.036064832580969305\n",
      "Epoch: 9 - Batch: 419, Training Loss: 0.036160294491655594\n",
      "Epoch: 9 - Batch: 420, Training Loss: 0.036248995710911835\n",
      "Epoch: 9 - Batch: 421, Training Loss: 0.036336835529377214\n",
      "Epoch: 9 - Batch: 422, Training Loss: 0.036416637246337896\n",
      "Epoch: 9 - Batch: 423, Training Loss: 0.03650045609889339\n",
      "Epoch: 9 - Batch: 424, Training Loss: 0.036584893683246515\n",
      "Epoch: 9 - Batch: 425, Training Loss: 0.0366692004561622\n",
      "Epoch: 9 - Batch: 426, Training Loss: 0.0367547136671211\n",
      "Epoch: 9 - Batch: 427, Training Loss: 0.03683861189986738\n",
      "Epoch: 9 - Batch: 428, Training Loss: 0.03692311053450032\n",
      "Epoch: 9 - Batch: 429, Training Loss: 0.03701050530124462\n",
      "Epoch: 9 - Batch: 430, Training Loss: 0.03709452980465162\n",
      "Epoch: 9 - Batch: 431, Training Loss: 0.03718240031506094\n",
      "Epoch: 9 - Batch: 432, Training Loss: 0.03726914306333409\n",
      "Epoch: 9 - Batch: 433, Training Loss: 0.03734854368452804\n",
      "Epoch: 9 - Batch: 434, Training Loss: 0.03743982399701084\n",
      "Epoch: 9 - Batch: 435, Training Loss: 0.03753213850691742\n",
      "Epoch: 9 - Batch: 436, Training Loss: 0.03761809299488368\n",
      "Epoch: 9 - Batch: 437, Training Loss: 0.03770206478821302\n",
      "Epoch: 9 - Batch: 438, Training Loss: 0.03778975899910452\n",
      "Epoch: 9 - Batch: 439, Training Loss: 0.03787429246489286\n",
      "Epoch: 9 - Batch: 440, Training Loss: 0.0379612949950185\n",
      "Epoch: 9 - Batch: 441, Training Loss: 0.03804623700715416\n",
      "Epoch: 9 - Batch: 442, Training Loss: 0.038128165969900034\n",
      "Epoch: 9 - Batch: 443, Training Loss: 0.03820843208167288\n",
      "Epoch: 9 - Batch: 444, Training Loss: 0.03828852684244785\n",
      "Epoch: 9 - Batch: 445, Training Loss: 0.038376674605888714\n",
      "Epoch: 9 - Batch: 446, Training Loss: 0.038466478202176926\n",
      "Epoch: 9 - Batch: 447, Training Loss: 0.03854982823652415\n",
      "Epoch: 9 - Batch: 448, Training Loss: 0.038627774011386964\n",
      "Epoch: 9 - Batch: 449, Training Loss: 0.03871027243078051\n",
      "Epoch: 9 - Batch: 450, Training Loss: 0.03879980491489716\n",
      "Epoch: 9 - Batch: 451, Training Loss: 0.038874225422221034\n",
      "Epoch: 9 - Batch: 452, Training Loss: 0.038962738918971465\n",
      "Epoch: 9 - Batch: 453, Training Loss: 0.03904976948453222\n",
      "Epoch: 9 - Batch: 454, Training Loss: 0.03913340236911331\n",
      "Epoch: 9 - Batch: 455, Training Loss: 0.03922443760321113\n",
      "Epoch: 9 - Batch: 456, Training Loss: 0.03930707440818127\n",
      "Epoch: 9 - Batch: 457, Training Loss: 0.039392268924570796\n",
      "Epoch: 9 - Batch: 458, Training Loss: 0.03947463223268934\n",
      "Epoch: 9 - Batch: 459, Training Loss: 0.03956913187556203\n",
      "Epoch: 9 - Batch: 460, Training Loss: 0.0396518199449748\n",
      "Epoch: 9 - Batch: 461, Training Loss: 0.03973594617413644\n",
      "Epoch: 9 - Batch: 462, Training Loss: 0.03981617119651331\n",
      "Epoch: 9 - Batch: 463, Training Loss: 0.03989874724156623\n",
      "Epoch: 9 - Batch: 464, Training Loss: 0.039990709325419135\n",
      "Epoch: 9 - Batch: 465, Training Loss: 0.04007482821520288\n",
      "Epoch: 9 - Batch: 466, Training Loss: 0.040158302005794316\n",
      "Epoch: 9 - Batch: 467, Training Loss: 0.040246475543548814\n",
      "Epoch: 9 - Batch: 468, Training Loss: 0.04033065750379467\n",
      "Epoch: 9 - Batch: 469, Training Loss: 0.04041200317751314\n",
      "Epoch: 9 - Batch: 470, Training Loss: 0.04049761340664591\n",
      "Epoch: 9 - Batch: 471, Training Loss: 0.04058111258851948\n",
      "Epoch: 9 - Batch: 472, Training Loss: 0.04066335197358978\n",
      "Epoch: 9 - Batch: 473, Training Loss: 0.04074309110814461\n",
      "Epoch: 9 - Batch: 474, Training Loss: 0.04082639688381903\n",
      "Epoch: 9 - Batch: 475, Training Loss: 0.04091469965763946\n",
      "Epoch: 9 - Batch: 476, Training Loss: 0.04100163355099028\n",
      "Epoch: 9 - Batch: 477, Training Loss: 0.04108201618465421\n",
      "Epoch: 9 - Batch: 478, Training Loss: 0.041172427052684486\n",
      "Epoch: 9 - Batch: 479, Training Loss: 0.04125336131967518\n",
      "Epoch: 9 - Batch: 480, Training Loss: 0.04134147839117208\n",
      "Epoch: 9 - Batch: 481, Training Loss: 0.04143044027572445\n",
      "Epoch: 9 - Batch: 482, Training Loss: 0.041518062297532805\n",
      "Epoch: 9 - Batch: 483, Training Loss: 0.041603525807064765\n",
      "Epoch: 9 - Batch: 484, Training Loss: 0.041693476555408726\n",
      "Epoch: 9 - Batch: 485, Training Loss: 0.04178201838413479\n",
      "Epoch: 9 - Batch: 486, Training Loss: 0.04186452623722368\n",
      "Epoch: 9 - Batch: 487, Training Loss: 0.041957115325119164\n",
      "Epoch: 9 - Batch: 488, Training Loss: 0.04204557964556648\n",
      "Epoch: 9 - Batch: 489, Training Loss: 0.04212862251953502\n",
      "Epoch: 9 - Batch: 490, Training Loss: 0.04221842651738852\n",
      "Epoch: 9 - Batch: 491, Training Loss: 0.04230007658995206\n",
      "Epoch: 9 - Batch: 492, Training Loss: 0.04238536881644334\n",
      "Epoch: 9 - Batch: 493, Training Loss: 0.04247711845372447\n",
      "Epoch: 9 - Batch: 494, Training Loss: 0.042561573150936845\n",
      "Epoch: 9 - Batch: 495, Training Loss: 0.04265381665792236\n",
      "Epoch: 9 - Batch: 496, Training Loss: 0.04274092945169849\n",
      "Epoch: 9 - Batch: 497, Training Loss: 0.04282803701277001\n",
      "Epoch: 9 - Batch: 498, Training Loss: 0.042913769583401595\n",
      "Epoch: 9 - Batch: 499, Training Loss: 0.04300447047423961\n",
      "Epoch: 9 - Batch: 500, Training Loss: 0.04308792335254636\n",
      "Epoch: 9 - Batch: 501, Training Loss: 0.04317637287685725\n",
      "Epoch: 9 - Batch: 502, Training Loss: 0.043255988409616065\n",
      "Epoch: 9 - Batch: 503, Training Loss: 0.04334549058482026\n",
      "Epoch: 9 - Batch: 504, Training Loss: 0.043432423341432415\n",
      "Epoch: 9 - Batch: 505, Training Loss: 0.043518273119705034\n",
      "Epoch: 9 - Batch: 506, Training Loss: 0.043602544480740135\n",
      "Epoch: 9 - Batch: 507, Training Loss: 0.043684884405402996\n",
      "Epoch: 9 - Batch: 508, Training Loss: 0.04377423814354251\n",
      "Epoch: 9 - Batch: 509, Training Loss: 0.04385709428965156\n",
      "Epoch: 9 - Batch: 510, Training Loss: 0.04394834529404616\n",
      "Epoch: 9 - Batch: 511, Training Loss: 0.044033332923998685\n",
      "Epoch: 9 - Batch: 512, Training Loss: 0.04411883460694482\n",
      "Epoch: 9 - Batch: 513, Training Loss: 0.044204778456519885\n",
      "Epoch: 9 - Batch: 514, Training Loss: 0.044286950656628334\n",
      "Epoch: 9 - Batch: 515, Training Loss: 0.044368422088239524\n",
      "Epoch: 9 - Batch: 516, Training Loss: 0.044454879953740645\n",
      "Epoch: 9 - Batch: 517, Training Loss: 0.04454493623344262\n",
      "Epoch: 9 - Batch: 518, Training Loss: 0.0446236897908633\n",
      "Epoch: 9 - Batch: 519, Training Loss: 0.0447103363117373\n",
      "Epoch: 9 - Batch: 520, Training Loss: 0.04480448584527914\n",
      "Epoch: 9 - Batch: 521, Training Loss: 0.044879302487493944\n",
      "Epoch: 9 - Batch: 522, Training Loss: 0.04496025307656916\n",
      "Epoch: 9 - Batch: 523, Training Loss: 0.04505133678886428\n",
      "Epoch: 9 - Batch: 524, Training Loss: 0.04514522843717738\n",
      "Epoch: 9 - Batch: 525, Training Loss: 0.04523938879444825\n",
      "Epoch: 9 - Batch: 526, Training Loss: 0.04532951755359596\n",
      "Epoch: 9 - Batch: 527, Training Loss: 0.045414754901557025\n",
      "Epoch: 9 - Batch: 528, Training Loss: 0.04550197210230835\n",
      "Epoch: 9 - Batch: 529, Training Loss: 0.04558986851058987\n",
      "Epoch: 9 - Batch: 530, Training Loss: 0.04567123319379133\n",
      "Epoch: 9 - Batch: 531, Training Loss: 0.04575471958117699\n",
      "Epoch: 9 - Batch: 532, Training Loss: 0.04583769582619714\n",
      "Epoch: 9 - Batch: 533, Training Loss: 0.04593341626461663\n",
      "Epoch: 9 - Batch: 534, Training Loss: 0.0460247114967944\n",
      "Epoch: 9 - Batch: 535, Training Loss: 0.046115468161962124\n",
      "Epoch: 9 - Batch: 536, Training Loss: 0.04620164628472692\n",
      "Epoch: 9 - Batch: 537, Training Loss: 0.04628473225591788\n",
      "Epoch: 9 - Batch: 538, Training Loss: 0.046369521450838246\n",
      "Epoch: 9 - Batch: 539, Training Loss: 0.046458876986754674\n",
      "Epoch: 9 - Batch: 540, Training Loss: 0.04654568344181648\n",
      "Epoch: 9 - Batch: 541, Training Loss: 0.046638827789482196\n",
      "Epoch: 9 - Batch: 542, Training Loss: 0.046725636141167745\n",
      "Epoch: 9 - Batch: 543, Training Loss: 0.04680528260680969\n",
      "Epoch: 9 - Batch: 544, Training Loss: 0.046885272318105\n",
      "Epoch: 9 - Batch: 545, Training Loss: 0.04696655215982774\n",
      "Epoch: 9 - Batch: 546, Training Loss: 0.047055672668847276\n",
      "Epoch: 9 - Batch: 547, Training Loss: 0.04714455996110269\n",
      "Epoch: 9 - Batch: 548, Training Loss: 0.04724456397677535\n",
      "Epoch: 9 - Batch: 549, Training Loss: 0.0473291246686903\n",
      "Epoch: 9 - Batch: 550, Training Loss: 0.047421951872446445\n",
      "Epoch: 9 - Batch: 551, Training Loss: 0.04750809489840496\n",
      "Epoch: 9 - Batch: 552, Training Loss: 0.047588343100365915\n",
      "Epoch: 9 - Batch: 553, Training Loss: 0.04768165737192825\n",
      "Epoch: 9 - Batch: 554, Training Loss: 0.047769393846021954\n",
      "Epoch: 9 - Batch: 555, Training Loss: 0.047862718237029575\n",
      "Epoch: 9 - Batch: 556, Training Loss: 0.04794815718312168\n",
      "Epoch: 9 - Batch: 557, Training Loss: 0.04803662416007784\n",
      "Epoch: 9 - Batch: 558, Training Loss: 0.04813211764615171\n",
      "Epoch: 9 - Batch: 559, Training Loss: 0.0482153113542208\n",
      "Epoch: 9 - Batch: 560, Training Loss: 0.04829945995983595\n",
      "Epoch: 9 - Batch: 561, Training Loss: 0.048375876660568404\n",
      "Epoch: 9 - Batch: 562, Training Loss: 0.04846321853807514\n",
      "Epoch: 9 - Batch: 563, Training Loss: 0.048551362168482486\n",
      "Epoch: 9 - Batch: 564, Training Loss: 0.048641375388029595\n",
      "Epoch: 9 - Batch: 565, Training Loss: 0.04872452026575952\n",
      "Epoch: 9 - Batch: 566, Training Loss: 0.048803347247848856\n",
      "Epoch: 9 - Batch: 567, Training Loss: 0.04889492849335939\n",
      "Epoch: 9 - Batch: 568, Training Loss: 0.04898457323659712\n",
      "Epoch: 9 - Batch: 569, Training Loss: 0.049065144908665426\n",
      "Epoch: 9 - Batch: 570, Training Loss: 0.049155999253045264\n",
      "Epoch: 9 - Batch: 571, Training Loss: 0.0492420791035169\n",
      "Epoch: 9 - Batch: 572, Training Loss: 0.04932908842318489\n",
      "Epoch: 9 - Batch: 573, Training Loss: 0.049407257116206645\n",
      "Epoch: 9 - Batch: 574, Training Loss: 0.04948968927114955\n",
      "Epoch: 9 - Batch: 575, Training Loss: 0.04957306750773593\n",
      "Epoch: 9 - Batch: 576, Training Loss: 0.04966020678628737\n",
      "Epoch: 9 - Batch: 577, Training Loss: 0.04974594140181296\n",
      "Epoch: 9 - Batch: 578, Training Loss: 0.049823034499118575\n",
      "Epoch: 9 - Batch: 579, Training Loss: 0.04990785554125535\n",
      "Epoch: 9 - Batch: 580, Training Loss: 0.04999871880656254\n",
      "Epoch: 9 - Batch: 581, Training Loss: 0.050080045396662866\n",
      "Epoch: 9 - Batch: 582, Training Loss: 0.05016078508982611\n",
      "Epoch: 9 - Batch: 583, Training Loss: 0.05024568139765393\n",
      "Epoch: 9 - Batch: 584, Training Loss: 0.050338417433733566\n",
      "Epoch: 9 - Batch: 585, Training Loss: 0.050430352198790954\n",
      "Epoch: 9 - Batch: 586, Training Loss: 0.05052080088809355\n",
      "Epoch: 9 - Batch: 587, Training Loss: 0.05060151604252866\n",
      "Epoch: 9 - Batch: 588, Training Loss: 0.05068147993626484\n",
      "Epoch: 9 - Batch: 589, Training Loss: 0.05076997961299139\n",
      "Epoch: 9 - Batch: 590, Training Loss: 0.05085327476880839\n",
      "Epoch: 9 - Batch: 591, Training Loss: 0.05094855139727023\n",
      "Epoch: 9 - Batch: 592, Training Loss: 0.05103842744829248\n",
      "Epoch: 9 - Batch: 593, Training Loss: 0.051119134386083974\n",
      "Epoch: 9 - Batch: 594, Training Loss: 0.0511975613535439\n",
      "Epoch: 9 - Batch: 595, Training Loss: 0.05127687019544652\n",
      "Epoch: 9 - Batch: 596, Training Loss: 0.05135609747017201\n",
      "Epoch: 9 - Batch: 597, Training Loss: 0.05143880893539631\n",
      "Epoch: 9 - Batch: 598, Training Loss: 0.051523742996539244\n",
      "Epoch: 9 - Batch: 599, Training Loss: 0.05160984869545965\n",
      "Epoch: 9 - Batch: 600, Training Loss: 0.051697160091071975\n",
      "Epoch: 9 - Batch: 601, Training Loss: 0.051787459799652276\n",
      "Epoch: 9 - Batch: 602, Training Loss: 0.05188251266975703\n",
      "Epoch: 9 - Batch: 603, Training Loss: 0.05196775804284596\n",
      "Epoch: 9 - Batch: 604, Training Loss: 0.05204769017585672\n",
      "Epoch: 9 - Batch: 605, Training Loss: 0.05213814966417664\n",
      "Epoch: 9 - Batch: 606, Training Loss: 0.05222290515232442\n",
      "Epoch: 9 - Batch: 607, Training Loss: 0.05230994613387099\n",
      "Epoch: 9 - Batch: 608, Training Loss: 0.052392683398120636\n",
      "Epoch: 9 - Batch: 609, Training Loss: 0.052476470792669164\n",
      "Epoch: 9 - Batch: 610, Training Loss: 0.052557780881525074\n",
      "Epoch: 9 - Batch: 611, Training Loss: 0.05264055574464165\n",
      "Epoch: 9 - Batch: 612, Training Loss: 0.052724834074379\n",
      "Epoch: 9 - Batch: 613, Training Loss: 0.052816002640508695\n",
      "Epoch: 9 - Batch: 614, Training Loss: 0.0529111107536414\n",
      "Epoch: 9 - Batch: 615, Training Loss: 0.05299519147001096\n",
      "Epoch: 9 - Batch: 616, Training Loss: 0.05307352353832615\n",
      "Epoch: 9 - Batch: 617, Training Loss: 0.05316126804990357\n",
      "Epoch: 9 - Batch: 618, Training Loss: 0.053246824920276305\n",
      "Epoch: 9 - Batch: 619, Training Loss: 0.053341324791732314\n",
      "Epoch: 9 - Batch: 620, Training Loss: 0.05344241959464491\n",
      "Epoch: 9 - Batch: 621, Training Loss: 0.05352589502238713\n",
      "Epoch: 9 - Batch: 622, Training Loss: 0.05361164107943451\n",
      "Epoch: 9 - Batch: 623, Training Loss: 0.0536999940056706\n",
      "Epoch: 9 - Batch: 624, Training Loss: 0.053785798654182634\n",
      "Epoch: 9 - Batch: 625, Training Loss: 0.053871773041253464\n",
      "Epoch: 9 - Batch: 626, Training Loss: 0.053957840369066\n",
      "Epoch: 9 - Batch: 627, Training Loss: 0.05404769877592722\n",
      "Epoch: 9 - Batch: 628, Training Loss: 0.054138966133295996\n",
      "Epoch: 9 - Batch: 629, Training Loss: 0.05422561106026469\n",
      "Epoch: 9 - Batch: 630, Training Loss: 0.054303836577863834\n",
      "Epoch: 9 - Batch: 631, Training Loss: 0.05439553829913906\n",
      "Epoch: 9 - Batch: 632, Training Loss: 0.05448328944581065\n",
      "Epoch: 9 - Batch: 633, Training Loss: 0.05457123457322864\n",
      "Epoch: 9 - Batch: 634, Training Loss: 0.05465683324345902\n",
      "Epoch: 9 - Batch: 635, Training Loss: 0.05473888970009525\n",
      "Epoch: 9 - Batch: 636, Training Loss: 0.05482019754018554\n",
      "Epoch: 9 - Batch: 637, Training Loss: 0.054908395270704236\n",
      "Epoch: 9 - Batch: 638, Training Loss: 0.0549967934379036\n",
      "Epoch: 9 - Batch: 639, Training Loss: 0.055086506434884636\n",
      "Epoch: 9 - Batch: 640, Training Loss: 0.055170798689670626\n",
      "Epoch: 9 - Batch: 641, Training Loss: 0.05525548184664293\n",
      "Epoch: 9 - Batch: 642, Training Loss: 0.05534074034782784\n",
      "Epoch: 9 - Batch: 643, Training Loss: 0.05543519331028015\n",
      "Epoch: 9 - Batch: 644, Training Loss: 0.05552558588546702\n",
      "Epoch: 9 - Batch: 645, Training Loss: 0.05561039087734808\n",
      "Epoch: 9 - Batch: 646, Training Loss: 0.055702499829072064\n",
      "Epoch: 9 - Batch: 647, Training Loss: 0.055785379772557944\n",
      "Epoch: 9 - Batch: 648, Training Loss: 0.05586747880747069\n",
      "Epoch: 9 - Batch: 649, Training Loss: 0.05594643126326809\n",
      "Epoch: 9 - Batch: 650, Training Loss: 0.05602646090250901\n",
      "Epoch: 9 - Batch: 651, Training Loss: 0.05611939552806899\n",
      "Epoch: 9 - Batch: 652, Training Loss: 0.056196780186486286\n",
      "Epoch: 9 - Batch: 653, Training Loss: 0.05627853170679774\n",
      "Epoch: 9 - Batch: 654, Training Loss: 0.05636491196517328\n",
      "Epoch: 9 - Batch: 655, Training Loss: 0.05645557376826383\n",
      "Epoch: 9 - Batch: 656, Training Loss: 0.05654713384819466\n",
      "Epoch: 9 - Batch: 657, Training Loss: 0.056632079344681445\n",
      "Epoch: 9 - Batch: 658, Training Loss: 0.05672373694319828\n",
      "Epoch: 9 - Batch: 659, Training Loss: 0.0568089418508619\n",
      "Epoch: 9 - Batch: 660, Training Loss: 0.056884616335381326\n",
      "Epoch: 9 - Batch: 661, Training Loss: 0.05696572511646878\n",
      "Epoch: 9 - Batch: 662, Training Loss: 0.05705756403098059\n",
      "Epoch: 9 - Batch: 663, Training Loss: 0.057145117378946564\n",
      "Epoch: 9 - Batch: 664, Training Loss: 0.05722948142125041\n",
      "Epoch: 9 - Batch: 665, Training Loss: 0.057317718591658434\n",
      "Epoch: 9 - Batch: 666, Training Loss: 0.05741489263464562\n",
      "Epoch: 9 - Batch: 667, Training Loss: 0.05750914378273942\n",
      "Epoch: 9 - Batch: 668, Training Loss: 0.05760164625609099\n",
      "Epoch: 9 - Batch: 669, Training Loss: 0.05768641758444495\n",
      "Epoch: 9 - Batch: 670, Training Loss: 0.0577741657595334\n",
      "Epoch: 9 - Batch: 671, Training Loss: 0.057861322546331445\n",
      "Epoch: 9 - Batch: 672, Training Loss: 0.05795455123506375\n",
      "Epoch: 9 - Batch: 673, Training Loss: 0.05804558884148574\n",
      "Epoch: 9 - Batch: 674, Training Loss: 0.0581305487634333\n",
      "Epoch: 9 - Batch: 675, Training Loss: 0.05822096262775844\n",
      "Epoch: 9 - Batch: 676, Training Loss: 0.05831037573578148\n",
      "Epoch: 9 - Batch: 677, Training Loss: 0.05839448037926435\n",
      "Epoch: 9 - Batch: 678, Training Loss: 0.058470343142303066\n",
      "Epoch: 9 - Batch: 679, Training Loss: 0.05855136666799066\n",
      "Epoch: 9 - Batch: 680, Training Loss: 0.05863840021131248\n",
      "Epoch: 9 - Batch: 681, Training Loss: 0.058732427397177586\n",
      "Epoch: 9 - Batch: 682, Training Loss: 0.05881443479489133\n",
      "Epoch: 9 - Batch: 683, Training Loss: 0.05889681101487841\n",
      "Epoch: 9 - Batch: 684, Training Loss: 0.05897658389945133\n",
      "Epoch: 9 - Batch: 685, Training Loss: 0.059065318553651346\n",
      "Epoch: 9 - Batch: 686, Training Loss: 0.059155085490117616\n",
      "Epoch: 9 - Batch: 687, Training Loss: 0.059236570667617554\n",
      "Epoch: 9 - Batch: 688, Training Loss: 0.05932668582914678\n",
      "Epoch: 9 - Batch: 689, Training Loss: 0.05941692803975559\n",
      "Epoch: 9 - Batch: 690, Training Loss: 0.059503101770013916\n",
      "Epoch: 9 - Batch: 691, Training Loss: 0.05959306746276457\n",
      "Epoch: 9 - Batch: 692, Training Loss: 0.059683116539003045\n",
      "Epoch: 9 - Batch: 693, Training Loss: 0.05977443137374486\n",
      "Epoch: 9 - Batch: 694, Training Loss: 0.05985467507199664\n",
      "Epoch: 9 - Batch: 695, Training Loss: 0.05995509530700261\n",
      "Epoch: 9 - Batch: 696, Training Loss: 0.06004415325215009\n",
      "Epoch: 9 - Batch: 697, Training Loss: 0.06013282353234528\n",
      "Epoch: 9 - Batch: 698, Training Loss: 0.06022371726101311\n",
      "Epoch: 9 - Batch: 699, Training Loss: 0.0603107272108297\n",
      "Epoch: 9 - Batch: 700, Training Loss: 0.06039970969877038\n",
      "Epoch: 9 - Batch: 701, Training Loss: 0.06047963422057443\n",
      "Epoch: 9 - Batch: 702, Training Loss: 0.06056847089706962\n",
      "Epoch: 9 - Batch: 703, Training Loss: 0.0606466715546884\n",
      "Epoch: 9 - Batch: 704, Training Loss: 0.06073544068790194\n",
      "Epoch: 9 - Batch: 705, Training Loss: 0.060817515072883854\n",
      "Epoch: 9 - Batch: 706, Training Loss: 0.060900271457819205\n",
      "Epoch: 9 - Batch: 707, Training Loss: 0.0609941967696594\n",
      "Epoch: 9 - Batch: 708, Training Loss: 0.061079899235250146\n",
      "Epoch: 9 - Batch: 709, Training Loss: 0.061162456789765986\n",
      "Epoch: 9 - Batch: 710, Training Loss: 0.0612484104807796\n",
      "Epoch: 9 - Batch: 711, Training Loss: 0.061328046845510034\n",
      "Epoch: 9 - Batch: 712, Training Loss: 0.0614210239319659\n",
      "Epoch: 9 - Batch: 713, Training Loss: 0.061501428701144153\n",
      "Epoch: 9 - Batch: 714, Training Loss: 0.06158299040463236\n",
      "Epoch: 9 - Batch: 715, Training Loss: 0.06166789699870951\n",
      "Epoch: 9 - Batch: 716, Training Loss: 0.06175871194026759\n",
      "Epoch: 9 - Batch: 717, Training Loss: 0.061835201262538116\n",
      "Epoch: 9 - Batch: 718, Training Loss: 0.06191235183310351\n",
      "Epoch: 9 - Batch: 719, Training Loss: 0.06199674696273867\n",
      "Epoch: 9 - Batch: 720, Training Loss: 0.0620802615770752\n",
      "Epoch: 9 - Batch: 721, Training Loss: 0.062163829253631245\n",
      "Epoch: 9 - Batch: 722, Training Loss: 0.06224192659159007\n",
      "Epoch: 9 - Batch: 723, Training Loss: 0.06232609017250154\n",
      "Epoch: 9 - Batch: 724, Training Loss: 0.06241098829046213\n",
      "Epoch: 9 - Batch: 725, Training Loss: 0.06250756968486171\n",
      "Epoch: 9 - Batch: 726, Training Loss: 0.06258530258069782\n",
      "Epoch: 9 - Batch: 727, Training Loss: 0.06267289721575345\n",
      "Epoch: 9 - Batch: 728, Training Loss: 0.06276254310190776\n",
      "Epoch: 9 - Batch: 729, Training Loss: 0.06285098681278885\n",
      "Epoch: 9 - Batch: 730, Training Loss: 0.062932652348705\n",
      "Epoch: 9 - Batch: 731, Training Loss: 0.0630272808199004\n",
      "Epoch: 9 - Batch: 732, Training Loss: 0.06311110594305233\n",
      "Epoch: 9 - Batch: 733, Training Loss: 0.06319411176798949\n",
      "Epoch: 9 - Batch: 734, Training Loss: 0.06327112262472387\n",
      "Epoch: 9 - Batch: 735, Training Loss: 0.06336007228315767\n",
      "Epoch: 9 - Batch: 736, Training Loss: 0.0634459321190963\n",
      "Epoch: 9 - Batch: 737, Training Loss: 0.06353667812733903\n",
      "Epoch: 9 - Batch: 738, Training Loss: 0.06362228607063863\n",
      "Epoch: 9 - Batch: 739, Training Loss: 0.06370987035766565\n",
      "Epoch: 9 - Batch: 740, Training Loss: 0.06379538861599135\n",
      "Epoch: 9 - Batch: 741, Training Loss: 0.06387989365713513\n",
      "Epoch: 9 - Batch: 742, Training Loss: 0.06397391945914448\n",
      "Epoch: 9 - Batch: 743, Training Loss: 0.06405914611311299\n",
      "Epoch: 9 - Batch: 744, Training Loss: 0.06413582125508765\n",
      "Epoch: 9 - Batch: 745, Training Loss: 0.06421729842599351\n",
      "Epoch: 9 - Batch: 746, Training Loss: 0.06430135611104926\n",
      "Epoch: 9 - Batch: 747, Training Loss: 0.0643771683571746\n",
      "Epoch: 9 - Batch: 748, Training Loss: 0.06446334736338302\n",
      "Epoch: 9 - Batch: 749, Training Loss: 0.0645475554068389\n",
      "Epoch: 9 - Batch: 750, Training Loss: 0.06463479499022166\n",
      "Epoch: 9 - Batch: 751, Training Loss: 0.06471381122076492\n",
      "Epoch: 9 - Batch: 752, Training Loss: 0.0648022552220086\n",
      "Epoch: 9 - Batch: 753, Training Loss: 0.06488961048090636\n",
      "Epoch: 9 - Batch: 754, Training Loss: 0.06497362505020589\n",
      "Epoch: 9 - Batch: 755, Training Loss: 0.06505731094758309\n",
      "Epoch: 9 - Batch: 756, Training Loss: 0.06514038577379279\n",
      "Epoch: 9 - Batch: 757, Training Loss: 0.0652224514046514\n",
      "Epoch: 9 - Batch: 758, Training Loss: 0.06531144028056914\n",
      "Epoch: 9 - Batch: 759, Training Loss: 0.06538786676096085\n",
      "Epoch: 9 - Batch: 760, Training Loss: 0.06546756153799606\n",
      "Epoch: 9 - Batch: 761, Training Loss: 0.06555893873891624\n",
      "Epoch: 9 - Batch: 762, Training Loss: 0.06564509538102703\n",
      "Epoch: 9 - Batch: 763, Training Loss: 0.06573345689458238\n",
      "Epoch: 9 - Batch: 764, Training Loss: 0.06581642717172455\n",
      "Epoch: 9 - Batch: 765, Training Loss: 0.06589645860874238\n",
      "Epoch: 9 - Batch: 766, Training Loss: 0.06598083981950682\n",
      "Epoch: 9 - Batch: 767, Training Loss: 0.06606818781933975\n",
      "Epoch: 9 - Batch: 768, Training Loss: 0.06615205243798235\n",
      "Epoch: 9 - Batch: 769, Training Loss: 0.06623495519037666\n",
      "Epoch: 9 - Batch: 770, Training Loss: 0.06632203522878104\n",
      "Epoch: 9 - Batch: 771, Training Loss: 0.06640703729655019\n",
      "Epoch: 9 - Batch: 772, Training Loss: 0.066490847512363\n",
      "Epoch: 9 - Batch: 773, Training Loss: 0.06657996065729294\n",
      "Epoch: 9 - Batch: 774, Training Loss: 0.06666599131292766\n",
      "Epoch: 9 - Batch: 775, Training Loss: 0.06675067149831683\n",
      "Epoch: 9 - Batch: 776, Training Loss: 0.06683493327738634\n",
      "Epoch: 9 - Batch: 777, Training Loss: 0.06692970267328655\n",
      "Epoch: 9 - Batch: 778, Training Loss: 0.06701089536298567\n",
      "Epoch: 9 - Batch: 779, Training Loss: 0.0670860363649294\n",
      "Epoch: 9 - Batch: 780, Training Loss: 0.06717062360339893\n",
      "Epoch: 9 - Batch: 781, Training Loss: 0.06726058877720366\n",
      "Epoch: 9 - Batch: 782, Training Loss: 0.06735051320139844\n",
      "Epoch: 9 - Batch: 783, Training Loss: 0.06742967787518431\n",
      "Epoch: 9 - Batch: 784, Training Loss: 0.06752426045055611\n",
      "Epoch: 9 - Batch: 785, Training Loss: 0.06760813463880845\n",
      "Epoch: 9 - Batch: 786, Training Loss: 0.06769554290432439\n",
      "Epoch: 9 - Batch: 787, Training Loss: 0.06778417123021376\n",
      "Epoch: 9 - Batch: 788, Training Loss: 0.06786439722252524\n",
      "Epoch: 9 - Batch: 789, Training Loss: 0.06793816318460563\n",
      "Epoch: 9 - Batch: 790, Training Loss: 0.06802378811720591\n",
      "Epoch: 9 - Batch: 791, Training Loss: 0.06811234528572603\n",
      "Epoch: 9 - Batch: 792, Training Loss: 0.0682040982343763\n",
      "Epoch: 9 - Batch: 793, Training Loss: 0.06828896588279833\n",
      "Epoch: 9 - Batch: 794, Training Loss: 0.06837265848322689\n",
      "Epoch: 9 - Batch: 795, Training Loss: 0.06845475609721631\n",
      "Epoch: 9 - Batch: 796, Training Loss: 0.0685398225884137\n",
      "Epoch: 9 - Batch: 797, Training Loss: 0.06863025040249919\n",
      "Epoch: 9 - Batch: 798, Training Loss: 0.06871437361065429\n",
      "Epoch: 9 - Batch: 799, Training Loss: 0.06879949636423766\n",
      "Epoch: 9 - Batch: 800, Training Loss: 0.06889570034286671\n",
      "Epoch: 9 - Batch: 801, Training Loss: 0.06898323846224133\n",
      "Epoch: 9 - Batch: 802, Training Loss: 0.06906758220387534\n",
      "Epoch: 9 - Batch: 803, Training Loss: 0.06915286200037643\n",
      "Epoch: 9 - Batch: 804, Training Loss: 0.06923909945595719\n",
      "Epoch: 9 - Batch: 805, Training Loss: 0.06932846393133475\n",
      "Epoch: 9 - Batch: 806, Training Loss: 0.06942035861399834\n",
      "Epoch: 9 - Batch: 807, Training Loss: 0.06950777138322345\n",
      "Epoch: 9 - Batch: 808, Training Loss: 0.06958234307061183\n",
      "Epoch: 9 - Batch: 809, Training Loss: 0.06967729991101111\n",
      "Epoch: 9 - Batch: 810, Training Loss: 0.06976155428274551\n",
      "Epoch: 9 - Batch: 811, Training Loss: 0.06985633392164957\n",
      "Epoch: 9 - Batch: 812, Training Loss: 0.06994146765958809\n",
      "Epoch: 9 - Batch: 813, Training Loss: 0.07003360200877214\n",
      "Epoch: 9 - Batch: 814, Training Loss: 0.07012402556008762\n",
      "Epoch: 9 - Batch: 815, Training Loss: 0.07021613100274879\n",
      "Epoch: 9 - Batch: 816, Training Loss: 0.07030200853844978\n",
      "Epoch: 9 - Batch: 817, Training Loss: 0.07038600758261744\n",
      "Epoch: 9 - Batch: 818, Training Loss: 0.07046951984934151\n",
      "Epoch: 9 - Batch: 819, Training Loss: 0.07056350846022713\n",
      "Epoch: 9 - Batch: 820, Training Loss: 0.07065249946752987\n",
      "Epoch: 9 - Batch: 821, Training Loss: 0.07073885180167298\n",
      "Epoch: 9 - Batch: 822, Training Loss: 0.07082613674339963\n",
      "Epoch: 9 - Batch: 823, Training Loss: 0.07092251470482369\n",
      "Epoch: 9 - Batch: 824, Training Loss: 0.07101281732938579\n",
      "Epoch: 9 - Batch: 825, Training Loss: 0.0711060335634162\n",
      "Epoch: 9 - Batch: 826, Training Loss: 0.07118941686590315\n",
      "Epoch: 9 - Batch: 827, Training Loss: 0.07128198283599384\n",
      "Epoch: 9 - Batch: 828, Training Loss: 0.07137065778670224\n",
      "Epoch: 9 - Batch: 829, Training Loss: 0.0714511993537297\n",
      "Epoch: 9 - Batch: 830, Training Loss: 0.07153983464832132\n",
      "Epoch: 9 - Batch: 831, Training Loss: 0.07162439120102482\n",
      "Epoch: 9 - Batch: 832, Training Loss: 0.0717086500332229\n",
      "Epoch: 9 - Batch: 833, Training Loss: 0.07180174568646384\n",
      "Epoch: 9 - Batch: 834, Training Loss: 0.07187965750743698\n",
      "Epoch: 9 - Batch: 835, Training Loss: 0.07196110864455625\n",
      "Epoch: 9 - Batch: 836, Training Loss: 0.07205230991045634\n",
      "Epoch: 9 - Batch: 837, Training Loss: 0.07214119101242837\n",
      "Epoch: 9 - Batch: 838, Training Loss: 0.07222836221850927\n",
      "Epoch: 9 - Batch: 839, Training Loss: 0.07231989082189935\n",
      "Epoch: 9 - Batch: 840, Training Loss: 0.07239778594393438\n",
      "Epoch: 9 - Batch: 841, Training Loss: 0.07248166537106927\n",
      "Epoch: 9 - Batch: 842, Training Loss: 0.07256661770652183\n",
      "Epoch: 9 - Batch: 843, Training Loss: 0.07265810101952522\n",
      "Epoch: 9 - Batch: 844, Training Loss: 0.07274436998253636\n",
      "Epoch: 9 - Batch: 845, Training Loss: 0.07283320297080881\n",
      "Epoch: 9 - Batch: 846, Training Loss: 0.07292011771876222\n",
      "Epoch: 9 - Batch: 847, Training Loss: 0.07300691915734688\n",
      "Epoch: 9 - Batch: 848, Training Loss: 0.07309037648993938\n",
      "Epoch: 9 - Batch: 849, Training Loss: 0.07317627241733063\n",
      "Epoch: 9 - Batch: 850, Training Loss: 0.07325921404124493\n",
      "Epoch: 9 - Batch: 851, Training Loss: 0.07334705351374636\n",
      "Epoch: 9 - Batch: 852, Training Loss: 0.07343222764915297\n",
      "Epoch: 9 - Batch: 853, Training Loss: 0.0735166135922099\n",
      "Epoch: 9 - Batch: 854, Training Loss: 0.07360299619201997\n",
      "Epoch: 9 - Batch: 855, Training Loss: 0.07369022670620512\n",
      "Epoch: 9 - Batch: 856, Training Loss: 0.07377776498002793\n",
      "Epoch: 9 - Batch: 857, Training Loss: 0.07386711834131386\n",
      "Epoch: 9 - Batch: 858, Training Loss: 0.07395682443754985\n",
      "Epoch: 9 - Batch: 859, Training Loss: 0.07403834088771893\n",
      "Epoch: 9 - Batch: 860, Training Loss: 0.07412952281052794\n",
      "Epoch: 9 - Batch: 861, Training Loss: 0.07422163369594324\n",
      "Epoch: 9 - Batch: 862, Training Loss: 0.0743138472708699\n",
      "Epoch: 9 - Batch: 863, Training Loss: 0.07439236746943412\n",
      "Epoch: 9 - Batch: 864, Training Loss: 0.0744817221651524\n",
      "Epoch: 9 - Batch: 865, Training Loss: 0.07457347952484293\n",
      "Epoch: 9 - Batch: 866, Training Loss: 0.07467158139369777\n",
      "Epoch: 9 - Batch: 867, Training Loss: 0.07474812782473034\n",
      "Epoch: 9 - Batch: 868, Training Loss: 0.07482969566290058\n",
      "Epoch: 9 - Batch: 869, Training Loss: 0.07490459641759867\n",
      "Epoch: 9 - Batch: 870, Training Loss: 0.07499166893746524\n",
      "Epoch: 9 - Batch: 871, Training Loss: 0.07507821840927573\n",
      "Epoch: 9 - Batch: 872, Training Loss: 0.07516516578904234\n",
      "Epoch: 9 - Batch: 873, Training Loss: 0.07524619660468441\n",
      "Epoch: 9 - Batch: 874, Training Loss: 0.07533414336307527\n",
      "Epoch: 9 - Batch: 875, Training Loss: 0.07541549657386531\n",
      "Epoch: 9 - Batch: 876, Training Loss: 0.07550352847027542\n",
      "Epoch: 9 - Batch: 877, Training Loss: 0.07560031737508267\n",
      "Epoch: 9 - Batch: 878, Training Loss: 0.07568760258863814\n",
      "Epoch: 9 - Batch: 879, Training Loss: 0.07577211532130171\n",
      "Epoch: 9 - Batch: 880, Training Loss: 0.07585766921391336\n",
      "Epoch: 9 - Batch: 881, Training Loss: 0.075944353784643\n",
      "Epoch: 9 - Batch: 882, Training Loss: 0.0760365886239962\n",
      "Epoch: 9 - Batch: 883, Training Loss: 0.07612793268651313\n",
      "Epoch: 9 - Batch: 884, Training Loss: 0.07620933459172795\n",
      "Epoch: 9 - Batch: 885, Training Loss: 0.0762981854465668\n",
      "Epoch: 9 - Batch: 886, Training Loss: 0.07638777604026976\n",
      "Epoch: 9 - Batch: 887, Training Loss: 0.07647050026291441\n",
      "Epoch: 9 - Batch: 888, Training Loss: 0.07655735923653811\n",
      "Epoch: 9 - Batch: 889, Training Loss: 0.07663489659479009\n",
      "Epoch: 9 - Batch: 890, Training Loss: 0.0767232714341351\n",
      "Epoch: 9 - Batch: 891, Training Loss: 0.07680095865037509\n",
      "Epoch: 9 - Batch: 892, Training Loss: 0.07688724658580166\n",
      "Epoch: 9 - Batch: 893, Training Loss: 0.07697481033121967\n",
      "Epoch: 9 - Batch: 894, Training Loss: 0.07706303494807895\n",
      "Epoch: 9 - Batch: 895, Training Loss: 0.07714832671122566\n",
      "Epoch: 9 - Batch: 896, Training Loss: 0.07723511442245536\n",
      "Epoch: 9 - Batch: 897, Training Loss: 0.07732043023627394\n",
      "Epoch: 9 - Batch: 898, Training Loss: 0.0774036946569015\n",
      "Epoch: 9 - Batch: 899, Training Loss: 0.07749189660076676\n",
      "Epoch: 9 - Batch: 900, Training Loss: 0.07757721828979441\n",
      "Epoch: 9 - Batch: 901, Training Loss: 0.07766656987183723\n",
      "Epoch: 9 - Batch: 902, Training Loss: 0.07775942420287314\n",
      "Epoch: 9 - Batch: 903, Training Loss: 0.07784339021366232\n",
      "Epoch: 9 - Batch: 904, Training Loss: 0.07793019662547863\n",
      "Epoch: 9 - Batch: 905, Training Loss: 0.0780126829732018\n",
      "Epoch: 9 - Batch: 906, Training Loss: 0.07810098168812384\n",
      "Epoch: 9 - Batch: 907, Training Loss: 0.07818530323864216\n",
      "Epoch: 9 - Batch: 908, Training Loss: 0.07827059589141044\n",
      "Epoch: 9 - Batch: 909, Training Loss: 0.07835349151447638\n",
      "Epoch: 9 - Batch: 910, Training Loss: 0.07843260514622502\n",
      "Epoch: 9 - Batch: 911, Training Loss: 0.0785167151336152\n",
      "Epoch: 9 - Batch: 912, Training Loss: 0.07860331924301672\n",
      "Epoch: 9 - Batch: 913, Training Loss: 0.07868166958564154\n",
      "Epoch: 9 - Batch: 914, Training Loss: 0.07876766510740243\n",
      "Epoch: 9 - Batch: 915, Training Loss: 0.07884632004656601\n",
      "Epoch: 9 - Batch: 916, Training Loss: 0.07893194124152016\n",
      "Epoch: 9 - Batch: 917, Training Loss: 0.07901430785482993\n",
      "Epoch: 9 - Batch: 918, Training Loss: 0.07910040244583665\n",
      "Epoch: 9 - Batch: 919, Training Loss: 0.07917935938680944\n",
      "Epoch: 9 - Batch: 920, Training Loss: 0.07925598631623768\n",
      "Epoch: 9 - Batch: 921, Training Loss: 0.07935159604619589\n",
      "Epoch: 9 - Batch: 922, Training Loss: 0.0794440929841244\n",
      "Epoch: 9 - Batch: 923, Training Loss: 0.07953166847378262\n",
      "Epoch: 9 - Batch: 924, Training Loss: 0.07962358319764311\n",
      "Epoch: 9 - Batch: 925, Training Loss: 0.07970462962490804\n",
      "Epoch: 9 - Batch: 926, Training Loss: 0.07978829885423677\n",
      "Epoch: 9 - Batch: 927, Training Loss: 0.07987430966019038\n",
      "Epoch: 9 - Batch: 928, Training Loss: 0.07995246426753737\n",
      "Epoch: 9 - Batch: 929, Training Loss: 0.08003767866449767\n",
      "Epoch: 9 - Batch: 930, Training Loss: 0.08012638127996553\n",
      "Epoch: 9 - Batch: 931, Training Loss: 0.08022104761567875\n",
      "Epoch: 9 - Batch: 932, Training Loss: 0.08030996924485535\n",
      "Epoch: 9 - Batch: 933, Training Loss: 0.0803914438086758\n",
      "Epoch: 9 - Batch: 934, Training Loss: 0.08048237220787291\n",
      "Epoch: 9 - Batch: 935, Training Loss: 0.08056224259867598\n",
      "Epoch: 9 - Batch: 936, Training Loss: 0.08064660249559046\n",
      "Epoch: 9 - Batch: 937, Training Loss: 0.08073111618583277\n",
      "Epoch: 9 - Batch: 938, Training Loss: 0.0808148638315956\n",
      "Epoch: 9 - Batch: 939, Training Loss: 0.08090049022218679\n",
      "Epoch: 9 - Batch: 940, Training Loss: 0.08098902780667663\n",
      "Epoch: 9 - Batch: 941, Training Loss: 0.08107386584429203\n",
      "Epoch: 9 - Batch: 942, Training Loss: 0.08115638268626547\n",
      "Epoch: 9 - Batch: 943, Training Loss: 0.08124975588907847\n",
      "Epoch: 9 - Batch: 944, Training Loss: 0.08133982881211721\n",
      "Epoch: 9 - Batch: 945, Training Loss: 0.08141737798874453\n",
      "Epoch: 9 - Batch: 946, Training Loss: 0.08149553229337309\n",
      "Epoch: 9 - Batch: 947, Training Loss: 0.08158004657051852\n",
      "Epoch: 9 - Batch: 948, Training Loss: 0.08166600209019868\n",
      "Epoch: 9 - Batch: 949, Training Loss: 0.08175606871175134\n",
      "Epoch: 9 - Batch: 950, Training Loss: 0.08183721541913587\n",
      "Epoch: 9 - Batch: 951, Training Loss: 0.08191856501583832\n",
      "Epoch: 9 - Batch: 952, Training Loss: 0.08201065946178847\n",
      "Epoch: 9 - Batch: 953, Training Loss: 0.0821020640494614\n",
      "Epoch: 9 - Batch: 954, Training Loss: 0.08219480841956527\n",
      "Epoch: 9 - Batch: 955, Training Loss: 0.08228497553128705\n",
      "Epoch: 9 - Batch: 956, Training Loss: 0.08237126328137581\n",
      "Epoch: 9 - Batch: 957, Training Loss: 0.08245996765755302\n",
      "Epoch: 9 - Batch: 958, Training Loss: 0.08255002690883814\n",
      "Epoch: 9 - Batch: 959, Training Loss: 0.08263169353824745\n",
      "Epoch: 9 - Batch: 960, Training Loss: 0.0827172599967637\n",
      "Epoch: 9 - Batch: 961, Training Loss: 0.08280039943791741\n",
      "Epoch: 9 - Batch: 962, Training Loss: 0.082886828279515\n",
      "Epoch: 9 - Batch: 963, Training Loss: 0.0829606314759646\n",
      "Epoch: 9 - Batch: 964, Training Loss: 0.08304682802130926\n",
      "Epoch: 9 - Batch: 965, Training Loss: 0.08313656962209476\n",
      "Epoch: 9 - Batch: 966, Training Loss: 0.08321414536998838\n",
      "Epoch: 9 - Batch: 967, Training Loss: 0.08330707848813404\n",
      "Epoch: 9 - Batch: 968, Training Loss: 0.08339572866535305\n",
      "Epoch: 9 - Batch: 969, Training Loss: 0.08347845439541202\n",
      "Epoch: 9 - Batch: 970, Training Loss: 0.08356050914076232\n",
      "Epoch: 9 - Batch: 971, Training Loss: 0.08364785038812046\n",
      "Epoch: 9 - Batch: 972, Training Loss: 0.08373125248386294\n",
      "Epoch: 9 - Batch: 973, Training Loss: 0.08380930993094374\n",
      "Epoch: 9 - Batch: 974, Training Loss: 0.0838939053489893\n",
      "Epoch: 9 - Batch: 975, Training Loss: 0.0839871130945473\n",
      "Epoch: 9 - Batch: 976, Training Loss: 0.08407522662609174\n",
      "Epoch: 9 - Batch: 977, Training Loss: 0.08417388484820995\n",
      "Epoch: 9 - Batch: 978, Training Loss: 0.0842601271040404\n",
      "Epoch: 9 - Batch: 979, Training Loss: 0.08434736846048836\n",
      "Epoch: 9 - Batch: 980, Training Loss: 0.08443456460068476\n",
      "Epoch: 9 - Batch: 981, Training Loss: 0.084518652674966\n",
      "Epoch: 9 - Batch: 982, Training Loss: 0.08460744344328174\n",
      "Epoch: 9 - Batch: 983, Training Loss: 0.08469479588504454\n",
      "Epoch: 9 - Batch: 984, Training Loss: 0.08477919564812535\n",
      "Epoch: 9 - Batch: 985, Training Loss: 0.08487083185247916\n",
      "Epoch: 9 - Batch: 986, Training Loss: 0.08496405048972339\n",
      "Epoch: 9 - Batch: 987, Training Loss: 0.08504155038502284\n",
      "Epoch: 9 - Batch: 988, Training Loss: 0.08513016628709993\n",
      "Epoch: 9 - Batch: 989, Training Loss: 0.08521701497447431\n",
      "Epoch: 9 - Batch: 990, Training Loss: 0.08530522226224689\n",
      "Epoch: 9 - Batch: 991, Training Loss: 0.08538552096258743\n",
      "Epoch: 9 - Batch: 992, Training Loss: 0.08547124357439985\n",
      "Epoch: 9 - Batch: 993, Training Loss: 0.08555609480806844\n",
      "Epoch: 9 - Batch: 994, Training Loss: 0.08564162446467043\n",
      "Epoch: 9 - Batch: 995, Training Loss: 0.08572307704742473\n",
      "Epoch: 9 - Batch: 996, Training Loss: 0.08580353637388097\n",
      "Epoch: 9 - Batch: 997, Training Loss: 0.08588102705171255\n",
      "Epoch: 9 - Batch: 998, Training Loss: 0.08597180107067869\n",
      "Epoch: 9 - Batch: 999, Training Loss: 0.08605569609708058\n",
      "Epoch: 9 - Batch: 1000, Training Loss: 0.08615715247555751\n",
      "Epoch: 9 - Batch: 1001, Training Loss: 0.0862508178881646\n",
      "Epoch: 9 - Batch: 1002, Training Loss: 0.08633460357760513\n",
      "Epoch: 9 - Batch: 1003, Training Loss: 0.08642436873482827\n",
      "Epoch: 9 - Batch: 1004, Training Loss: 0.08650948036196418\n",
      "Epoch: 9 - Batch: 1005, Training Loss: 0.08659775764565562\n",
      "Epoch: 9 - Batch: 1006, Training Loss: 0.08667420252391553\n",
      "Epoch: 9 - Batch: 1007, Training Loss: 0.08675531392444426\n",
      "Epoch: 9 - Batch: 1008, Training Loss: 0.08683809120313049\n",
      "Epoch: 9 - Batch: 1009, Training Loss: 0.08691703274870789\n",
      "Epoch: 9 - Batch: 1010, Training Loss: 0.08700465031623049\n",
      "Epoch: 9 - Batch: 1011, Training Loss: 0.08709286313324821\n",
      "Epoch: 9 - Batch: 1012, Training Loss: 0.08718110720440128\n",
      "Epoch: 9 - Batch: 1013, Training Loss: 0.08727080790112861\n",
      "Epoch: 9 - Batch: 1014, Training Loss: 0.08736475030729426\n",
      "Epoch: 9 - Batch: 1015, Training Loss: 0.08744793385267258\n",
      "Epoch: 9 - Batch: 1016, Training Loss: 0.08753220092622598\n",
      "Epoch: 9 - Batch: 1017, Training Loss: 0.08761671805757393\n",
      "Epoch: 9 - Batch: 1018, Training Loss: 0.08769904095586853\n",
      "Epoch: 9 - Batch: 1019, Training Loss: 0.08778065929241836\n",
      "Epoch: 9 - Batch: 1020, Training Loss: 0.08787314528923723\n",
      "Epoch: 9 - Batch: 1021, Training Loss: 0.08795959492573889\n",
      "Epoch: 9 - Batch: 1022, Training Loss: 0.08804762454249372\n",
      "Epoch: 9 - Batch: 1023, Training Loss: 0.08813622503681957\n",
      "Epoch: 9 - Batch: 1024, Training Loss: 0.08823071144039359\n",
      "Epoch: 9 - Batch: 1025, Training Loss: 0.08832522477355367\n",
      "Epoch: 9 - Batch: 1026, Training Loss: 0.08841723699128845\n",
      "Epoch: 9 - Batch: 1027, Training Loss: 0.08850511068950838\n",
      "Epoch: 9 - Batch: 1028, Training Loss: 0.08860195574549894\n",
      "Epoch: 9 - Batch: 1029, Training Loss: 0.08868620573090479\n",
      "Epoch: 9 - Batch: 1030, Training Loss: 0.08877363723778409\n",
      "Epoch: 9 - Batch: 1031, Training Loss: 0.08885937344103706\n",
      "Epoch: 9 - Batch: 1032, Training Loss: 0.08893777097067232\n",
      "Epoch: 9 - Batch: 1033, Training Loss: 0.08901945528131022\n",
      "Epoch: 9 - Batch: 1034, Training Loss: 0.08910147660407261\n",
      "Epoch: 9 - Batch: 1035, Training Loss: 0.08918335456802674\n",
      "Epoch: 9 - Batch: 1036, Training Loss: 0.08927129839190204\n",
      "Epoch: 9 - Batch: 1037, Training Loss: 0.08936131521318089\n",
      "Epoch: 9 - Batch: 1038, Training Loss: 0.08944771524092451\n",
      "Epoch: 9 - Batch: 1039, Training Loss: 0.08953103293999906\n",
      "Epoch: 9 - Batch: 1040, Training Loss: 0.08962764872692118\n",
      "Epoch: 9 - Batch: 1041, Training Loss: 0.08972016252441391\n",
      "Epoch: 9 - Batch: 1042, Training Loss: 0.08981174621638374\n",
      "Epoch: 9 - Batch: 1043, Training Loss: 0.08989610813348052\n",
      "Epoch: 9 - Batch: 1044, Training Loss: 0.08997902354444832\n",
      "Epoch: 9 - Batch: 1045, Training Loss: 0.09006071893211028\n",
      "Epoch: 9 - Batch: 1046, Training Loss: 0.09013804383143462\n",
      "Epoch: 9 - Batch: 1047, Training Loss: 0.09021938281445756\n",
      "Epoch: 9 - Batch: 1048, Training Loss: 0.09030920736627594\n",
      "Epoch: 9 - Batch: 1049, Training Loss: 0.09038862704637632\n",
      "Epoch: 9 - Batch: 1050, Training Loss: 0.09047821728793741\n",
      "Epoch: 9 - Batch: 1051, Training Loss: 0.09055310432145845\n",
      "Epoch: 9 - Batch: 1052, Training Loss: 0.09063457391822516\n",
      "Epoch: 9 - Batch: 1053, Training Loss: 0.09072189742257544\n",
      "Epoch: 9 - Batch: 1054, Training Loss: 0.09080619099943792\n",
      "Epoch: 9 - Batch: 1055, Training Loss: 0.09088010633145004\n",
      "Epoch: 9 - Batch: 1056, Training Loss: 0.09096225249159395\n",
      "Epoch: 9 - Batch: 1057, Training Loss: 0.0910466296619938\n",
      "Epoch: 9 - Batch: 1058, Training Loss: 0.09113609879146366\n",
      "Epoch: 9 - Batch: 1059, Training Loss: 0.09122017298394175\n",
      "Epoch: 9 - Batch: 1060, Training Loss: 0.09131028194900967\n",
      "Epoch: 9 - Batch: 1061, Training Loss: 0.09139516182355027\n",
      "Epoch: 9 - Batch: 1062, Training Loss: 0.09148097085221292\n",
      "Epoch: 9 - Batch: 1063, Training Loss: 0.09156330603840537\n",
      "Epoch: 9 - Batch: 1064, Training Loss: 0.09165261089060436\n",
      "Epoch: 9 - Batch: 1065, Training Loss: 0.09174768492628883\n",
      "Epoch: 9 - Batch: 1066, Training Loss: 0.09184093657837776\n",
      "Epoch: 9 - Batch: 1067, Training Loss: 0.09192287005101073\n",
      "Epoch: 9 - Batch: 1068, Training Loss: 0.0920099712735286\n",
      "Epoch: 9 - Batch: 1069, Training Loss: 0.09209719606077493\n",
      "Epoch: 9 - Batch: 1070, Training Loss: 0.09217510385373931\n",
      "Epoch: 9 - Batch: 1071, Training Loss: 0.09226729613952771\n",
      "Epoch: 9 - Batch: 1072, Training Loss: 0.09235574458270128\n",
      "Epoch: 9 - Batch: 1073, Training Loss: 0.09244882642827422\n",
      "Epoch: 9 - Batch: 1074, Training Loss: 0.09253416490273096\n",
      "Epoch: 9 - Batch: 1075, Training Loss: 0.09261861621443905\n",
      "Epoch: 9 - Batch: 1076, Training Loss: 0.0926990305902946\n",
      "Epoch: 9 - Batch: 1077, Training Loss: 0.09278745477892471\n",
      "Epoch: 9 - Batch: 1078, Training Loss: 0.09287153652083023\n",
      "Epoch: 9 - Batch: 1079, Training Loss: 0.09295380950147042\n",
      "Epoch: 9 - Batch: 1080, Training Loss: 0.09303474273674721\n",
      "Epoch: 9 - Batch: 1081, Training Loss: 0.09311619647477397\n",
      "Epoch: 9 - Batch: 1082, Training Loss: 0.09320343302479431\n",
      "Epoch: 9 - Batch: 1083, Training Loss: 0.09329159622687605\n",
      "Epoch: 9 - Batch: 1084, Training Loss: 0.09337885047665875\n",
      "Epoch: 9 - Batch: 1085, Training Loss: 0.0934593560387246\n",
      "Epoch: 9 - Batch: 1086, Training Loss: 0.09354411827316925\n",
      "Epoch: 9 - Batch: 1087, Training Loss: 0.09362743633056358\n",
      "Epoch: 9 - Batch: 1088, Training Loss: 0.0937113590340808\n",
      "Epoch: 9 - Batch: 1089, Training Loss: 0.09380145152261601\n",
      "Epoch: 9 - Batch: 1090, Training Loss: 0.09388306166723395\n",
      "Epoch: 9 - Batch: 1091, Training Loss: 0.09396944183294058\n",
      "Epoch: 9 - Batch: 1092, Training Loss: 0.09405562654755405\n",
      "Epoch: 9 - Batch: 1093, Training Loss: 0.09413924394407082\n",
      "Epoch: 9 - Batch: 1094, Training Loss: 0.09422889442660322\n",
      "Epoch: 9 - Batch: 1095, Training Loss: 0.09431327206652555\n",
      "Epoch: 9 - Batch: 1096, Training Loss: 0.09440039385536417\n",
      "Epoch: 9 - Batch: 1097, Training Loss: 0.09448636652497114\n",
      "Epoch: 9 - Batch: 1098, Training Loss: 0.09456966329940517\n",
      "Epoch: 9 - Batch: 1099, Training Loss: 0.09465530629626553\n",
      "Epoch: 9 - Batch: 1100, Training Loss: 0.09473570740193277\n",
      "Epoch: 9 - Batch: 1101, Training Loss: 0.09481942961316797\n",
      "Epoch: 9 - Batch: 1102, Training Loss: 0.09490050156762944\n",
      "Epoch: 9 - Batch: 1103, Training Loss: 0.09498373994211455\n",
      "Epoch: 9 - Batch: 1104, Training Loss: 0.09507485049132684\n",
      "Epoch: 9 - Batch: 1105, Training Loss: 0.09515323717252136\n",
      "Epoch: 9 - Batch: 1106, Training Loss: 0.09523473229648462\n",
      "Epoch: 9 - Batch: 1107, Training Loss: 0.0953124163496257\n",
      "Epoch: 9 - Batch: 1108, Training Loss: 0.09539834894301681\n",
      "Epoch: 9 - Batch: 1109, Training Loss: 0.09548402510917009\n",
      "Epoch: 9 - Batch: 1110, Training Loss: 0.09556300421654684\n",
      "Epoch: 9 - Batch: 1111, Training Loss: 0.09564808937744122\n",
      "Epoch: 9 - Batch: 1112, Training Loss: 0.09573435637587141\n",
      "Epoch: 9 - Batch: 1113, Training Loss: 0.09582631153426756\n",
      "Epoch: 9 - Batch: 1114, Training Loss: 0.09590760166841755\n",
      "Epoch: 9 - Batch: 1115, Training Loss: 0.09599011423526514\n",
      "Epoch: 9 - Batch: 1116, Training Loss: 0.09608291838299576\n",
      "Epoch: 9 - Batch: 1117, Training Loss: 0.09616937018177205\n",
      "Epoch: 9 - Batch: 1118, Training Loss: 0.09625059418120788\n",
      "Epoch: 9 - Batch: 1119, Training Loss: 0.09634176990137765\n",
      "Epoch: 9 - Batch: 1120, Training Loss: 0.09643495000688788\n",
      "Epoch: 9 - Batch: 1121, Training Loss: 0.09651159571103789\n",
      "Epoch: 9 - Batch: 1122, Training Loss: 0.09660021947761674\n",
      "Epoch: 9 - Batch: 1123, Training Loss: 0.09668985858040663\n",
      "Epoch: 9 - Batch: 1124, Training Loss: 0.0967820220214514\n",
      "Epoch: 9 - Batch: 1125, Training Loss: 0.09687076677656292\n",
      "Epoch: 9 - Batch: 1126, Training Loss: 0.09696368567929735\n",
      "Epoch: 9 - Batch: 1127, Training Loss: 0.0970450307908244\n",
      "Epoch: 9 - Batch: 1128, Training Loss: 0.09713086650813398\n",
      "Epoch: 9 - Batch: 1129, Training Loss: 0.09721353102110906\n",
      "Epoch: 9 - Batch: 1130, Training Loss: 0.09730723566355595\n",
      "Epoch: 9 - Batch: 1131, Training Loss: 0.09739333778545631\n",
      "Epoch: 9 - Batch: 1132, Training Loss: 0.09747528172967644\n",
      "Epoch: 9 - Batch: 1133, Training Loss: 0.09756013875824104\n",
      "Epoch: 9 - Batch: 1134, Training Loss: 0.0976476513442412\n",
      "Epoch: 9 - Batch: 1135, Training Loss: 0.09773246297739434\n",
      "Epoch: 9 - Batch: 1136, Training Loss: 0.09781624946996545\n",
      "Epoch: 9 - Batch: 1137, Training Loss: 0.09791651133997721\n",
      "Epoch: 9 - Batch: 1138, Training Loss: 0.09800625892099654\n",
      "Epoch: 9 - Batch: 1139, Training Loss: 0.0980951178998101\n",
      "Epoch: 9 - Batch: 1140, Training Loss: 0.09817521247524724\n",
      "Epoch: 9 - Batch: 1141, Training Loss: 0.09826596227055956\n",
      "Epoch: 9 - Batch: 1142, Training Loss: 0.09835753094398758\n",
      "Epoch: 9 - Batch: 1143, Training Loss: 0.09844485216250468\n",
      "Epoch: 9 - Batch: 1144, Training Loss: 0.09852493561767227\n",
      "Epoch: 9 - Batch: 1145, Training Loss: 0.0986041030221021\n",
      "Epoch: 9 - Batch: 1146, Training Loss: 0.09869016485155914\n",
      "Epoch: 9 - Batch: 1147, Training Loss: 0.09877463925636032\n",
      "Epoch: 9 - Batch: 1148, Training Loss: 0.09885901737198308\n",
      "Epoch: 9 - Batch: 1149, Training Loss: 0.09894468228813032\n",
      "Epoch: 9 - Batch: 1150, Training Loss: 0.09903761104465915\n",
      "Epoch: 9 - Batch: 1151, Training Loss: 0.09911881219811898\n",
      "Epoch: 9 - Batch: 1152, Training Loss: 0.09920151401959841\n",
      "Epoch: 9 - Batch: 1153, Training Loss: 0.09930320530759161\n",
      "Epoch: 9 - Batch: 1154, Training Loss: 0.09938962655033835\n",
      "Epoch: 9 - Batch: 1155, Training Loss: 0.09946701154854168\n",
      "Epoch: 9 - Batch: 1156, Training Loss: 0.09954829518351191\n",
      "Epoch: 9 - Batch: 1157, Training Loss: 0.09963675380173212\n",
      "Epoch: 9 - Batch: 1158, Training Loss: 0.09972815997095448\n",
      "Epoch: 9 - Batch: 1159, Training Loss: 0.09981432257476533\n",
      "Epoch: 9 - Batch: 1160, Training Loss: 0.09990060318316986\n",
      "Epoch: 9 - Batch: 1161, Training Loss: 0.09998986934918669\n",
      "Epoch: 9 - Batch: 1162, Training Loss: 0.10007220705597357\n",
      "Epoch: 9 - Batch: 1163, Training Loss: 0.10016183587869205\n",
      "Epoch: 9 - Batch: 1164, Training Loss: 0.10025359971902857\n",
      "Epoch: 9 - Batch: 1165, Training Loss: 0.1003399275267994\n",
      "Epoch: 9 - Batch: 1166, Training Loss: 0.1004282412479074\n",
      "Epoch: 9 - Batch: 1167, Training Loss: 0.1005135144463819\n",
      "Epoch: 9 - Batch: 1168, Training Loss: 0.10060213132457156\n",
      "Epoch: 9 - Batch: 1169, Training Loss: 0.1006851855782826\n",
      "Epoch: 9 - Batch: 1170, Training Loss: 0.10076383097254815\n",
      "Epoch: 9 - Batch: 1171, Training Loss: 0.10085004721930371\n",
      "Epoch: 9 - Batch: 1172, Training Loss: 0.10093790676389168\n",
      "Epoch: 9 - Batch: 1173, Training Loss: 0.10101721915810262\n",
      "Epoch: 9 - Batch: 1174, Training Loss: 0.1011100861168224\n",
      "Epoch: 9 - Batch: 1175, Training Loss: 0.10119610297096705\n",
      "Epoch: 9 - Batch: 1176, Training Loss: 0.10128094777341308\n",
      "Epoch: 9 - Batch: 1177, Training Loss: 0.10136610118187285\n",
      "Epoch: 9 - Batch: 1178, Training Loss: 0.10144551486320559\n",
      "Epoch: 9 - Batch: 1179, Training Loss: 0.10152978736097935\n",
      "Epoch: 9 - Batch: 1180, Training Loss: 0.1016143942205467\n",
      "Epoch: 9 - Batch: 1181, Training Loss: 0.10169620847153427\n",
      "Epoch: 9 - Batch: 1182, Training Loss: 0.10178550791186876\n",
      "Epoch: 9 - Batch: 1183, Training Loss: 0.10186273295735047\n",
      "Epoch: 9 - Batch: 1184, Training Loss: 0.1019481469693271\n",
      "Epoch: 9 - Batch: 1185, Training Loss: 0.10202862996342368\n",
      "Epoch: 9 - Batch: 1186, Training Loss: 0.10211733178811683\n",
      "Epoch: 9 - Batch: 1187, Training Loss: 0.10220783406476279\n",
      "Epoch: 9 - Batch: 1188, Training Loss: 0.1022927565175799\n",
      "Epoch: 9 - Batch: 1189, Training Loss: 0.10238049501185591\n",
      "Epoch: 9 - Batch: 1190, Training Loss: 0.10246783524603392\n",
      "Epoch: 9 - Batch: 1191, Training Loss: 0.10256306004786175\n",
      "Epoch: 9 - Batch: 1192, Training Loss: 0.10264990722164388\n",
      "Epoch: 9 - Batch: 1193, Training Loss: 0.10273448203630116\n",
      "Epoch: 9 - Batch: 1194, Training Loss: 0.10282441650580608\n",
      "Epoch: 9 - Batch: 1195, Training Loss: 0.10290836584236887\n",
      "Epoch: 9 - Batch: 1196, Training Loss: 0.10298985038354226\n",
      "Epoch: 9 - Batch: 1197, Training Loss: 0.10306656366186355\n",
      "Epoch: 9 - Batch: 1198, Training Loss: 0.1031605568320597\n",
      "Epoch: 9 - Batch: 1199, Training Loss: 0.10324846108013125\n",
      "Epoch: 9 - Batch: 1200, Training Loss: 0.10333737121218473\n",
      "Epoch: 9 - Batch: 1201, Training Loss: 0.1034263591552354\n",
      "Epoch: 9 - Batch: 1202, Training Loss: 0.1035087793249989\n",
      "Epoch: 9 - Batch: 1203, Training Loss: 0.10359860007739186\n",
      "Epoch: 9 - Batch: 1204, Training Loss: 0.10368560454120881\n",
      "Epoch: 9 - Batch: 1205, Training Loss: 0.10377116298077514\n",
      "Epoch: 9 - Batch: 1206, Training Loss: 0.10385239161365661\n",
      "Epoch: 9 - Batch: 1207, Training Loss: 0.10393726763934837\n",
      "Epoch: 9 - Batch: 1208, Training Loss: 0.10402801509322614\n",
      "Epoch: 9 - Batch: 1209, Training Loss: 0.10411316649385946\n",
      "Epoch: 9 - Batch: 1210, Training Loss: 0.10419167119818144\n",
      "Epoch: 9 - Batch: 1211, Training Loss: 0.10427524108025762\n",
      "Epoch: 9 - Batch: 1212, Training Loss: 0.10436279176553684\n",
      "Epoch: 9 - Batch: 1213, Training Loss: 0.10445263969463298\n",
      "Epoch: 9 - Batch: 1214, Training Loss: 0.10454360080595633\n",
      "Epoch: 9 - Batch: 1215, Training Loss: 0.10462106228690242\n",
      "Epoch: 9 - Batch: 1216, Training Loss: 0.10470674722571278\n",
      "Epoch: 9 - Batch: 1217, Training Loss: 0.10479027197185045\n",
      "Epoch: 9 - Batch: 1218, Training Loss: 0.10486699612950211\n",
      "Epoch: 9 - Batch: 1219, Training Loss: 0.10495799855880476\n",
      "Epoch: 9 - Batch: 1220, Training Loss: 0.10503935180666236\n",
      "Epoch: 9 - Batch: 1221, Training Loss: 0.1051215430594993\n",
      "Epoch: 9 - Batch: 1222, Training Loss: 0.10521410240685168\n",
      "Epoch: 9 - Batch: 1223, Training Loss: 0.10529558752875028\n",
      "Epoch: 9 - Batch: 1224, Training Loss: 0.10537714666222063\n",
      "Epoch: 9 - Batch: 1225, Training Loss: 0.10547211890419324\n",
      "Epoch: 9 - Batch: 1226, Training Loss: 0.10555556757533135\n",
      "Epoch: 9 - Batch: 1227, Training Loss: 0.10564303700850773\n",
      "Epoch: 9 - Batch: 1228, Training Loss: 0.10572983399280664\n",
      "Epoch: 9 - Batch: 1229, Training Loss: 0.10580694348015397\n",
      "Epoch: 9 - Batch: 1230, Training Loss: 0.10589051159534288\n",
      "Epoch: 9 - Batch: 1231, Training Loss: 0.1059651940529718\n",
      "Epoch: 9 - Batch: 1232, Training Loss: 0.10604961102158078\n",
      "Epoch: 9 - Batch: 1233, Training Loss: 0.10613688358891861\n",
      "Epoch: 9 - Batch: 1234, Training Loss: 0.10622325802150848\n",
      "Epoch: 9 - Batch: 1235, Training Loss: 0.10631949994496841\n",
      "Epoch: 9 - Batch: 1236, Training Loss: 0.10640558053061341\n",
      "Epoch: 9 - Batch: 1237, Training Loss: 0.10649686702720167\n",
      "Epoch: 9 - Batch: 1238, Training Loss: 0.10658814511563054\n",
      "Epoch: 9 - Batch: 1239, Training Loss: 0.10668033213782468\n",
      "Epoch: 9 - Batch: 1240, Training Loss: 0.10676491790594746\n",
      "Epoch: 9 - Batch: 1241, Training Loss: 0.10685072087406321\n",
      "Epoch: 9 - Batch: 1242, Training Loss: 0.10693946935446504\n",
      "Epoch: 9 - Batch: 1243, Training Loss: 0.1070266201980968\n",
      "Epoch: 9 - Batch: 1244, Training Loss: 0.10711111581790111\n",
      "Epoch: 9 - Batch: 1245, Training Loss: 0.10720182876805366\n",
      "Epoch: 9 - Batch: 1246, Training Loss: 0.10728490595423167\n",
      "Epoch: 9 - Batch: 1247, Training Loss: 0.10737995724278698\n",
      "Epoch: 9 - Batch: 1248, Training Loss: 0.1074671431770471\n",
      "Epoch: 9 - Batch: 1249, Training Loss: 0.10755534300806115\n",
      "Epoch: 9 - Batch: 1250, Training Loss: 0.1076393062078933\n",
      "Epoch: 9 - Batch: 1251, Training Loss: 0.10772600619255211\n",
      "Epoch: 9 - Batch: 1252, Training Loss: 0.10781368019544267\n",
      "Epoch: 9 - Batch: 1253, Training Loss: 0.10790293151589966\n",
      "Epoch: 9 - Batch: 1254, Training Loss: 0.10799069587715823\n",
      "Epoch: 9 - Batch: 1255, Training Loss: 0.1080746447194868\n",
      "Epoch: 9 - Batch: 1256, Training Loss: 0.10815938109312682\n",
      "Epoch: 9 - Batch: 1257, Training Loss: 0.10824370983475279\n",
      "Epoch: 9 - Batch: 1258, Training Loss: 0.10833237986165295\n",
      "Epoch: 9 - Batch: 1259, Training Loss: 0.10842537377060547\n",
      "Epoch: 9 - Batch: 1260, Training Loss: 0.10851567937292862\n",
      "Epoch: 9 - Batch: 1261, Training Loss: 0.10861008800875092\n",
      "Epoch: 9 - Batch: 1262, Training Loss: 0.10870206564244742\n",
      "Epoch: 9 - Batch: 1263, Training Loss: 0.10879184190140632\n",
      "Epoch: 9 - Batch: 1264, Training Loss: 0.10888452714874376\n",
      "Epoch: 9 - Batch: 1265, Training Loss: 0.10897232370318267\n",
      "Epoch: 9 - Batch: 1266, Training Loss: 0.10906228472542011\n",
      "Epoch: 9 - Batch: 1267, Training Loss: 0.10914697537621851\n",
      "Epoch: 9 - Batch: 1268, Training Loss: 0.10923484499700627\n",
      "Epoch: 9 - Batch: 1269, Training Loss: 0.10931888223929982\n",
      "Epoch: 9 - Batch: 1270, Training Loss: 0.10940652868245569\n",
      "Epoch: 9 - Batch: 1271, Training Loss: 0.10949943128781729\n",
      "Epoch: 9 - Batch: 1272, Training Loss: 0.10959469304600758\n",
      "Epoch: 9 - Batch: 1273, Training Loss: 0.10968103350617399\n",
      "Epoch: 9 - Batch: 1274, Training Loss: 0.10976899208911814\n",
      "Epoch: 9 - Batch: 1275, Training Loss: 0.10985312066589224\n",
      "Epoch: 9 - Batch: 1276, Training Loss: 0.1099543516921068\n",
      "Epoch: 9 - Batch: 1277, Training Loss: 0.11004012685954867\n",
      "Epoch: 9 - Batch: 1278, Training Loss: 0.11012202731733693\n",
      "Epoch: 9 - Batch: 1279, Training Loss: 0.11020372398382988\n",
      "Epoch: 9 - Batch: 1280, Training Loss: 0.11029824724491952\n",
      "Epoch: 9 - Batch: 1281, Training Loss: 0.11039454085909906\n",
      "Epoch: 9 - Batch: 1282, Training Loss: 0.110486927113278\n",
      "Epoch: 9 - Batch: 1283, Training Loss: 0.11056803413365611\n",
      "Epoch: 9 - Batch: 1284, Training Loss: 0.11064933349762983\n",
      "Epoch: 9 - Batch: 1285, Training Loss: 0.11073726539810498\n",
      "Epoch: 9 - Batch: 1286, Training Loss: 0.1108220622351217\n",
      "Epoch: 9 - Batch: 1287, Training Loss: 0.11091069977847894\n",
      "Epoch: 9 - Batch: 1288, Training Loss: 0.11099849329337749\n",
      "Epoch: 9 - Batch: 1289, Training Loss: 0.11108762186459245\n",
      "Epoch: 9 - Batch: 1290, Training Loss: 0.11116873563126743\n",
      "Epoch: 9 - Batch: 1291, Training Loss: 0.11125573765480301\n",
      "Epoch: 9 - Batch: 1292, Training Loss: 0.11133916431398534\n",
      "Epoch: 9 - Batch: 1293, Training Loss: 0.11142415042416769\n",
      "Epoch: 9 - Batch: 1294, Training Loss: 0.11150547312217368\n",
      "Epoch: 9 - Batch: 1295, Training Loss: 0.11158593726741338\n",
      "Epoch: 9 - Batch: 1296, Training Loss: 0.11167697639360555\n",
      "Epoch: 9 - Batch: 1297, Training Loss: 0.11176153452168057\n",
      "Epoch: 9 - Batch: 1298, Training Loss: 0.11184826466698156\n",
      "Epoch: 9 - Batch: 1299, Training Loss: 0.11193063978729755\n",
      "Epoch: 9 - Batch: 1300, Training Loss: 0.11201826693060782\n",
      "Epoch: 9 - Batch: 1301, Training Loss: 0.11210928444640948\n",
      "Epoch: 9 - Batch: 1302, Training Loss: 0.11219574286792408\n",
      "Epoch: 9 - Batch: 1303, Training Loss: 0.11228214111642458\n",
      "Epoch: 9 - Batch: 1304, Training Loss: 0.11236315339828408\n",
      "Epoch: 9 - Batch: 1305, Training Loss: 0.11244140368319468\n",
      "Epoch: 9 - Batch: 1306, Training Loss: 0.11252911518610532\n",
      "Epoch: 9 - Batch: 1307, Training Loss: 0.11261402834731943\n",
      "Epoch: 9 - Batch: 1308, Training Loss: 0.1127140377501449\n",
      "Epoch: 9 - Batch: 1309, Training Loss: 0.11280035573501096\n",
      "Epoch: 9 - Batch: 1310, Training Loss: 0.1128833920109529\n",
      "Epoch: 9 - Batch: 1311, Training Loss: 0.11298104045081692\n",
      "Epoch: 9 - Batch: 1312, Training Loss: 0.11307349021409084\n",
      "Epoch: 9 - Batch: 1313, Training Loss: 0.11316472104137415\n",
      "Epoch: 9 - Batch: 1314, Training Loss: 0.11326163227532436\n",
      "Epoch: 9 - Batch: 1315, Training Loss: 0.11334707478608065\n",
      "Epoch: 9 - Batch: 1316, Training Loss: 0.1134398464873359\n",
      "Epoch: 9 - Batch: 1317, Training Loss: 0.11352415207408949\n",
      "Epoch: 9 - Batch: 1318, Training Loss: 0.11361503158734608\n",
      "Epoch: 9 - Batch: 1319, Training Loss: 0.11369682716863666\n",
      "Epoch: 9 - Batch: 1320, Training Loss: 0.11378091907323297\n",
      "Epoch: 9 - Batch: 1321, Training Loss: 0.11386212238896744\n",
      "Epoch: 9 - Batch: 1322, Training Loss: 0.11395109500457991\n",
      "Epoch: 9 - Batch: 1323, Training Loss: 0.11403819859942196\n",
      "Epoch: 9 - Batch: 1324, Training Loss: 0.11412500192845243\n",
      "Epoch: 9 - Batch: 1325, Training Loss: 0.11420839353407398\n",
      "Epoch: 9 - Batch: 1326, Training Loss: 0.11430500046184802\n",
      "Epoch: 9 - Batch: 1327, Training Loss: 0.11438916077463583\n",
      "Epoch: 9 - Batch: 1328, Training Loss: 0.1144787021735019\n",
      "Epoch: 9 - Batch: 1329, Training Loss: 0.11456256384527308\n",
      "Epoch: 9 - Batch: 1330, Training Loss: 0.11464723667892848\n",
      "Epoch: 9 - Batch: 1331, Training Loss: 0.11473351249326126\n",
      "Epoch: 9 - Batch: 1332, Training Loss: 0.11482298979612923\n",
      "Epoch: 9 - Batch: 1333, Training Loss: 0.11490989679696152\n",
      "Epoch: 9 - Batch: 1334, Training Loss: 0.11499566937585178\n",
      "Epoch: 9 - Batch: 1335, Training Loss: 0.11508044295914928\n",
      "Epoch: 9 - Batch: 1336, Training Loss: 0.1151616479676359\n",
      "Epoch: 9 - Batch: 1337, Training Loss: 0.11524694007056863\n",
      "Epoch: 9 - Batch: 1338, Training Loss: 0.11533499190315086\n",
      "Epoch: 9 - Batch: 1339, Training Loss: 0.11541869404847745\n",
      "Epoch: 9 - Batch: 1340, Training Loss: 0.11549441070970809\n",
      "Epoch: 9 - Batch: 1341, Training Loss: 0.11558442456558174\n",
      "Epoch: 9 - Batch: 1342, Training Loss: 0.11566097976927141\n",
      "Epoch: 9 - Batch: 1343, Training Loss: 0.11573976174515871\n",
      "Epoch: 9 - Batch: 1344, Training Loss: 0.1158207302378679\n",
      "Epoch: 9 - Batch: 1345, Training Loss: 0.11590531097097974\n",
      "Epoch: 9 - Batch: 1346, Training Loss: 0.11599553078042334\n",
      "Epoch: 9 - Batch: 1347, Training Loss: 0.1160767574116563\n",
      "Epoch: 9 - Batch: 1348, Training Loss: 0.1161714583561195\n",
      "Epoch: 9 - Batch: 1349, Training Loss: 0.11626268395069819\n",
      "Epoch: 9 - Batch: 1350, Training Loss: 0.11635283523456967\n",
      "Epoch: 9 - Batch: 1351, Training Loss: 0.1164363918402796\n",
      "Epoch: 9 - Batch: 1352, Training Loss: 0.1165240433184464\n",
      "Epoch: 9 - Batch: 1353, Training Loss: 0.11661154938055508\n",
      "Epoch: 9 - Batch: 1354, Training Loss: 0.11669667897310423\n",
      "Epoch: 9 - Batch: 1355, Training Loss: 0.11677821601430576\n",
      "Epoch: 9 - Batch: 1356, Training Loss: 0.1168599801005218\n",
      "Epoch: 9 - Batch: 1357, Training Loss: 0.11694291403909425\n",
      "Epoch: 9 - Batch: 1358, Training Loss: 0.11702883047102695\n",
      "Epoch: 9 - Batch: 1359, Training Loss: 0.11712152520766107\n",
      "Epoch: 9 - Batch: 1360, Training Loss: 0.11722077704212361\n",
      "Epoch: 9 - Batch: 1361, Training Loss: 0.11730507296659856\n",
      "Epoch: 9 - Batch: 1362, Training Loss: 0.11738615964306132\n",
      "Epoch: 9 - Batch: 1363, Training Loss: 0.11747424280391404\n",
      "Epoch: 9 - Batch: 1364, Training Loss: 0.11756600626121903\n",
      "Epoch: 9 - Batch: 1365, Training Loss: 0.11765128196175419\n",
      "Epoch: 9 - Batch: 1366, Training Loss: 0.11774037354571705\n",
      "Epoch: 9 - Batch: 1367, Training Loss: 0.11782210461708839\n",
      "Epoch: 9 - Batch: 1368, Training Loss: 0.11790458507724662\n",
      "Epoch: 9 - Batch: 1369, Training Loss: 0.11798959200704473\n",
      "Epoch: 9 - Batch: 1370, Training Loss: 0.11806351808865075\n",
      "Epoch: 9 - Batch: 1371, Training Loss: 0.1181537272967707\n",
      "Epoch: 9 - Batch: 1372, Training Loss: 0.11824418152149636\n",
      "Epoch: 9 - Batch: 1373, Training Loss: 0.1183376672614372\n",
      "Epoch: 9 - Batch: 1374, Training Loss: 0.11842562539339263\n",
      "Epoch: 9 - Batch: 1375, Training Loss: 0.11850573914511682\n",
      "Epoch: 9 - Batch: 1376, Training Loss: 0.11859525244059056\n",
      "Epoch: 9 - Batch: 1377, Training Loss: 0.11867479467841721\n",
      "Epoch: 9 - Batch: 1378, Training Loss: 0.11876350103153123\n",
      "Epoch: 9 - Batch: 1379, Training Loss: 0.11884757492129087\n",
      "Epoch: 9 - Batch: 1380, Training Loss: 0.1189358399479741\n",
      "Epoch: 9 - Batch: 1381, Training Loss: 0.11902772761868996\n",
      "Epoch: 9 - Batch: 1382, Training Loss: 0.11911022438239893\n",
      "Epoch: 9 - Batch: 1383, Training Loss: 0.11919916366809241\n",
      "Epoch: 9 - Batch: 1384, Training Loss: 0.11929280030391307\n",
      "Epoch: 9 - Batch: 1385, Training Loss: 0.11938806452091852\n",
      "Epoch: 9 - Batch: 1386, Training Loss: 0.11947835691483262\n",
      "Epoch: 9 - Batch: 1387, Training Loss: 0.11956834555858403\n",
      "Epoch: 9 - Batch: 1388, Training Loss: 0.11966291493172472\n",
      "Epoch: 9 - Batch: 1389, Training Loss: 0.11974872761154254\n",
      "Epoch: 9 - Batch: 1390, Training Loss: 0.11983776112298665\n",
      "Epoch: 9 - Batch: 1391, Training Loss: 0.11991948810456997\n",
      "Epoch: 9 - Batch: 1392, Training Loss: 0.12001008418068956\n",
      "Epoch: 9 - Batch: 1393, Training Loss: 0.12009896001906735\n",
      "Epoch: 9 - Batch: 1394, Training Loss: 0.12019251636654188\n",
      "Epoch: 9 - Batch: 1395, Training Loss: 0.12028337315738102\n",
      "Epoch: 9 - Batch: 1396, Training Loss: 0.12036501033660982\n",
      "Epoch: 9 - Batch: 1397, Training Loss: 0.12046008029486212\n",
      "Epoch: 9 - Batch: 1398, Training Loss: 0.12055380420626495\n",
      "Epoch: 9 - Batch: 1399, Training Loss: 0.1206437798640127\n",
      "Epoch: 9 - Batch: 1400, Training Loss: 0.120734531877201\n",
      "Epoch: 9 - Batch: 1401, Training Loss: 0.12081915536157133\n",
      "Epoch: 9 - Batch: 1402, Training Loss: 0.12089742275316324\n",
      "Epoch: 9 - Batch: 1403, Training Loss: 0.12098283150772353\n",
      "Epoch: 9 - Batch: 1404, Training Loss: 0.12107033047139348\n",
      "Epoch: 9 - Batch: 1405, Training Loss: 0.12116255767482825\n",
      "Epoch: 9 - Batch: 1406, Training Loss: 0.12125836006345639\n",
      "Epoch: 9 - Batch: 1407, Training Loss: 0.12134371442754273\n",
      "Epoch: 9 - Batch: 1408, Training Loss: 0.12143238012989362\n",
      "Epoch: 9 - Batch: 1409, Training Loss: 0.12151343296984733\n",
      "Epoch: 9 - Batch: 1410, Training Loss: 0.12159897036739249\n",
      "Epoch: 9 - Batch: 1411, Training Loss: 0.12168419762061998\n",
      "Epoch: 9 - Batch: 1412, Training Loss: 0.12177540840670639\n",
      "Epoch: 9 - Batch: 1413, Training Loss: 0.12185859568355294\n",
      "Epoch: 9 - Batch: 1414, Training Loss: 0.12194077166455303\n",
      "Epoch: 9 - Batch: 1415, Training Loss: 0.12203011007525434\n",
      "Epoch: 9 - Batch: 1416, Training Loss: 0.12211400453946483\n",
      "Epoch: 9 - Batch: 1417, Training Loss: 0.12220941923842897\n",
      "Epoch: 9 - Batch: 1418, Training Loss: 0.12229151296032405\n",
      "Epoch: 9 - Batch: 1419, Training Loss: 0.12238169888383515\n",
      "Epoch: 9 - Batch: 1420, Training Loss: 0.12247462236441388\n",
      "Epoch: 9 - Batch: 1421, Training Loss: 0.12256407606142078\n",
      "Epoch: 9 - Batch: 1422, Training Loss: 0.12265869086086256\n",
      "Epoch: 9 - Batch: 1423, Training Loss: 0.1227345468528927\n",
      "Epoch: 9 - Batch: 1424, Training Loss: 0.12281527498715354\n",
      "Epoch: 9 - Batch: 1425, Training Loss: 0.12290332606602862\n",
      "Epoch: 9 - Batch: 1426, Training Loss: 0.12298660367653144\n",
      "Epoch: 9 - Batch: 1427, Training Loss: 0.12306987696866294\n",
      "Epoch: 9 - Batch: 1428, Training Loss: 0.12315367075736645\n",
      "Epoch: 9 - Batch: 1429, Training Loss: 0.12322954097098576\n",
      "Epoch: 9 - Batch: 1430, Training Loss: 0.12331330582227676\n",
      "Epoch: 9 - Batch: 1431, Training Loss: 0.12340383935305807\n",
      "Epoch: 9 - Batch: 1432, Training Loss: 0.12349057161738822\n",
      "Epoch: 9 - Batch: 1433, Training Loss: 0.12357718318354827\n",
      "Epoch: 9 - Batch: 1434, Training Loss: 0.12367173415882077\n",
      "Epoch: 9 - Batch: 1435, Training Loss: 0.12375674761868828\n",
      "Epoch: 9 - Batch: 1436, Training Loss: 0.12384338840644553\n",
      "Epoch: 9 - Batch: 1437, Training Loss: 0.12392714876638321\n",
      "Epoch: 9 - Batch: 1438, Training Loss: 0.12402391084662916\n",
      "Epoch: 9 - Batch: 1439, Training Loss: 0.12411224402820886\n",
      "Epoch: 9 - Batch: 1440, Training Loss: 0.12419851627169952\n",
      "Epoch: 9 - Batch: 1441, Training Loss: 0.12428510381807735\n",
      "Epoch: 9 - Batch: 1442, Training Loss: 0.12436880975665145\n",
      "Epoch: 9 - Batch: 1443, Training Loss: 0.12444200193135695\n",
      "Epoch: 9 - Batch: 1444, Training Loss: 0.12452157770915213\n",
      "Epoch: 9 - Batch: 1445, Training Loss: 0.12461713653916545\n",
      "Epoch: 9 - Batch: 1446, Training Loss: 0.12470540772524244\n",
      "Epoch: 9 - Batch: 1447, Training Loss: 0.12478949536706875\n",
      "Epoch: 9 - Batch: 1448, Training Loss: 0.12487351901174383\n",
      "Epoch: 9 - Batch: 1449, Training Loss: 0.12496204068807029\n",
      "Epoch: 9 - Batch: 1450, Training Loss: 0.12504426669174956\n",
      "Epoch: 9 - Batch: 1451, Training Loss: 0.1251339502200163\n",
      "Epoch: 9 - Batch: 1452, Training Loss: 0.12522737170333292\n",
      "Epoch: 9 - Batch: 1453, Training Loss: 0.1253110132184788\n",
      "Epoch: 9 - Batch: 1454, Training Loss: 0.12538784925222002\n",
      "Epoch: 9 - Batch: 1455, Training Loss: 0.12546796263944648\n",
      "Epoch: 9 - Batch: 1456, Training Loss: 0.12555452916851487\n",
      "Epoch: 9 - Batch: 1457, Training Loss: 0.12563714035361956\n",
      "Epoch: 9 - Batch: 1458, Training Loss: 0.12572179298545194\n",
      "Epoch: 9 - Batch: 1459, Training Loss: 0.12581586719152346\n",
      "Epoch: 9 - Batch: 1460, Training Loss: 0.12590256907280603\n",
      "Epoch: 9 - Batch: 1461, Training Loss: 0.1259914223123249\n",
      "Epoch: 9 - Batch: 1462, Training Loss: 0.12607373169949201\n",
      "Epoch: 9 - Batch: 1463, Training Loss: 0.12615234110115178\n",
      "Epoch: 9 - Batch: 1464, Training Loss: 0.1262404446615212\n",
      "Epoch: 9 - Batch: 1465, Training Loss: 0.1263297466842294\n",
      "Epoch: 9 - Batch: 1466, Training Loss: 0.12641344835385557\n",
      "Epoch: 9 - Batch: 1467, Training Loss: 0.12650212940835637\n",
      "Epoch: 9 - Batch: 1468, Training Loss: 0.12658255927845416\n",
      "Epoch: 9 - Batch: 1469, Training Loss: 0.1266659038391576\n",
      "Epoch: 9 - Batch: 1470, Training Loss: 0.1267536467568297\n",
      "Epoch: 9 - Batch: 1471, Training Loss: 0.126844037250055\n",
      "Epoch: 9 - Batch: 1472, Training Loss: 0.12692612380555415\n",
      "Epoch: 9 - Batch: 1473, Training Loss: 0.12701290494964687\n",
      "Epoch: 9 - Batch: 1474, Training Loss: 0.12710664256369297\n",
      "Epoch: 9 - Batch: 1475, Training Loss: 0.12719058771779881\n",
      "Epoch: 9 - Batch: 1476, Training Loss: 0.12727626223444544\n",
      "Epoch: 9 - Batch: 1477, Training Loss: 0.12736327566243522\n",
      "Epoch: 9 - Batch: 1478, Training Loss: 0.12744527632620797\n",
      "Epoch: 9 - Batch: 1479, Training Loss: 0.12752475722957013\n",
      "Epoch: 9 - Batch: 1480, Training Loss: 0.12760952655009764\n",
      "Epoch: 9 - Batch: 1481, Training Loss: 0.12769800648834576\n",
      "Epoch: 9 - Batch: 1482, Training Loss: 0.12778942112132882\n",
      "Epoch: 9 - Batch: 1483, Training Loss: 0.1278740320706842\n",
      "Epoch: 9 - Batch: 1484, Training Loss: 0.12795651662063046\n",
      "Epoch: 9 - Batch: 1485, Training Loss: 0.12804148734159532\n",
      "Epoch: 9 - Batch: 1486, Training Loss: 0.12812788314981444\n",
      "Epoch: 9 - Batch: 1487, Training Loss: 0.12821660601652873\n",
      "Epoch: 9 - Batch: 1488, Training Loss: 0.1283083704067008\n",
      "Epoch: 9 - Batch: 1489, Training Loss: 0.12839798259843838\n",
      "Epoch: 9 - Batch: 1490, Training Loss: 0.12847764089481153\n",
      "Epoch: 9 - Batch: 1491, Training Loss: 0.12856160646696788\n",
      "Epoch: 9 - Batch: 1492, Training Loss: 0.12864303635819438\n",
      "Epoch: 9 - Batch: 1493, Training Loss: 0.1287272779663305\n",
      "Epoch: 9 - Batch: 1494, Training Loss: 0.12881086779693465\n",
      "Epoch: 9 - Batch: 1495, Training Loss: 0.12889733947010382\n",
      "Epoch: 9 - Batch: 1496, Training Loss: 0.12898718461913256\n",
      "Epoch: 9 - Batch: 1497, Training Loss: 0.12906545321542034\n",
      "Epoch: 9 - Batch: 1498, Training Loss: 0.12916250635018792\n",
      "Epoch: 9 - Batch: 1499, Training Loss: 0.12924261418963545\n",
      "Epoch: 9 - Batch: 1500, Training Loss: 0.129328509752529\n",
      "Epoch: 9 - Batch: 1501, Training Loss: 0.12941784931850275\n",
      "Epoch: 9 - Batch: 1502, Training Loss: 0.1295005279603981\n",
      "Epoch: 9 - Batch: 1503, Training Loss: 0.12958837134970558\n",
      "Epoch: 9 - Batch: 1504, Training Loss: 0.12967990273316307\n",
      "Epoch: 9 - Batch: 1505, Training Loss: 0.12976421552955808\n",
      "Epoch: 9 - Batch: 1506, Training Loss: 0.12985191956335435\n",
      "Epoch: 9 - Batch: 1507, Training Loss: 0.12993524546053872\n",
      "Epoch: 9 - Batch: 1508, Training Loss: 0.13001547974980687\n",
      "Epoch: 9 - Batch: 1509, Training Loss: 0.13010811467792463\n",
      "Epoch: 9 - Batch: 1510, Training Loss: 0.13019264106998593\n",
      "Epoch: 9 - Batch: 1511, Training Loss: 0.13027079770369315\n",
      "Epoch: 9 - Batch: 1512, Training Loss: 0.13034879407578243\n",
      "Epoch: 9 - Batch: 1513, Training Loss: 0.13043900577360718\n",
      "Epoch: 9 - Batch: 1514, Training Loss: 0.13052630643815938\n",
      "Epoch: 9 - Batch: 1515, Training Loss: 0.13060740834939144\n",
      "Epoch: 9 - Batch: 1516, Training Loss: 0.13069546018197367\n",
      "Epoch: 9 - Batch: 1517, Training Loss: 0.1307813456439557\n",
      "Epoch: 9 - Batch: 1518, Training Loss: 0.13087275629217549\n",
      "Epoch: 9 - Batch: 1519, Training Loss: 0.1309632264868438\n",
      "Epoch: 9 - Batch: 1520, Training Loss: 0.13105678543522584\n",
      "Epoch: 9 - Batch: 1521, Training Loss: 0.13115605315314002\n",
      "Epoch: 9 - Batch: 1522, Training Loss: 0.1312521511370665\n",
      "Epoch: 9 - Batch: 1523, Training Loss: 0.13133096527524454\n",
      "Epoch: 9 - Batch: 1524, Training Loss: 0.13141759903869818\n",
      "Epoch: 9 - Batch: 1525, Training Loss: 0.13150313767182886\n",
      "Epoch: 9 - Batch: 1526, Training Loss: 0.1315864691971052\n",
      "Epoch: 9 - Batch: 1527, Training Loss: 0.1316740626089312\n",
      "Epoch: 9 - Batch: 1528, Training Loss: 0.13176878906823508\n",
      "Epoch: 9 - Batch: 1529, Training Loss: 0.1318548510706741\n",
      "Epoch: 9 - Batch: 1530, Training Loss: 0.13193636872553904\n",
      "Epoch: 9 - Batch: 1531, Training Loss: 0.13202477648705985\n",
      "Epoch: 9 - Batch: 1532, Training Loss: 0.13211117049750207\n",
      "Epoch: 9 - Batch: 1533, Training Loss: 0.13219195560461056\n",
      "Epoch: 9 - Batch: 1534, Training Loss: 0.13227948359836195\n",
      "Epoch: 9 - Batch: 1535, Training Loss: 0.13236942079851086\n",
      "Epoch: 9 - Batch: 1536, Training Loss: 0.13245354167140932\n",
      "Epoch: 9 - Batch: 1537, Training Loss: 0.13253740304664\n",
      "Epoch: 9 - Batch: 1538, Training Loss: 0.13262818699353568\n",
      "Epoch: 9 - Batch: 1539, Training Loss: 0.132723283374784\n",
      "Epoch: 9 - Batch: 1540, Training Loss: 0.13280625016139713\n",
      "Epoch: 9 - Batch: 1541, Training Loss: 0.13289231860741454\n",
      "Epoch: 9 - Batch: 1542, Training Loss: 0.1329793474678673\n",
      "Epoch: 9 - Batch: 1543, Training Loss: 0.13306783711781747\n",
      "Epoch: 9 - Batch: 1544, Training Loss: 0.13316147600240374\n",
      "Epoch: 9 - Batch: 1545, Training Loss: 0.13323865164917698\n",
      "Epoch: 9 - Batch: 1546, Training Loss: 0.13332072994478703\n",
      "Epoch: 9 - Batch: 1547, Training Loss: 0.1334044076687661\n",
      "Epoch: 9 - Batch: 1548, Training Loss: 0.13348477545687018\n",
      "Epoch: 9 - Batch: 1549, Training Loss: 0.1335757349248648\n",
      "Epoch: 9 - Batch: 1550, Training Loss: 0.13367726454267256\n",
      "Epoch: 9 - Batch: 1551, Training Loss: 0.13376592568571294\n",
      "Epoch: 9 - Batch: 1552, Training Loss: 0.13385063470348987\n",
      "Epoch: 9 - Batch: 1553, Training Loss: 0.13393386956891215\n",
      "Epoch: 9 - Batch: 1554, Training Loss: 0.13401792589482384\n",
      "Epoch: 9 - Batch: 1555, Training Loss: 0.13410905092509826\n",
      "Epoch: 9 - Batch: 1556, Training Loss: 0.13419804297647073\n",
      "Epoch: 9 - Batch: 1557, Training Loss: 0.13428268380846156\n",
      "Epoch: 9 - Batch: 1558, Training Loss: 0.13436208610387387\n",
      "Epoch: 9 - Batch: 1559, Training Loss: 0.1344487681169415\n",
      "Epoch: 9 - Batch: 1560, Training Loss: 0.13453828002855947\n",
      "Epoch: 9 - Batch: 1561, Training Loss: 0.13462050645233783\n",
      "Epoch: 9 - Batch: 1562, Training Loss: 0.13470993362750186\n",
      "Epoch: 9 - Batch: 1563, Training Loss: 0.1347909050794383\n",
      "Epoch: 9 - Batch: 1564, Training Loss: 0.13488261189132583\n",
      "Epoch: 9 - Batch: 1565, Training Loss: 0.13497323288302715\n",
      "Epoch: 9 - Batch: 1566, Training Loss: 0.1350616609019723\n",
      "Epoch: 9 - Batch: 1567, Training Loss: 0.13515458443197445\n",
      "Epoch: 9 - Batch: 1568, Training Loss: 0.13523463745467104\n",
      "Epoch: 9 - Batch: 1569, Training Loss: 0.13531934964790274\n",
      "Epoch: 9 - Batch: 1570, Training Loss: 0.13541621936617998\n",
      "Epoch: 9 - Batch: 1571, Training Loss: 0.13550288801639038\n",
      "Epoch: 9 - Batch: 1572, Training Loss: 0.13559102745816284\n",
      "Epoch: 9 - Batch: 1573, Training Loss: 0.13567723953481733\n",
      "Epoch: 9 - Batch: 1574, Training Loss: 0.1357679850550038\n",
      "Epoch: 9 - Batch: 1575, Training Loss: 0.1358531790339136\n",
      "Epoch: 9 - Batch: 1576, Training Loss: 0.1359353696936695\n",
      "Epoch: 9 - Batch: 1577, Training Loss: 0.13601949636146402\n",
      "Epoch: 9 - Batch: 1578, Training Loss: 0.1361036592257359\n",
      "Epoch: 9 - Batch: 1579, Training Loss: 0.13620862427783842\n",
      "Epoch: 9 - Batch: 1580, Training Loss: 0.13629507647818\n",
      "Epoch: 9 - Batch: 1581, Training Loss: 0.13638041754118838\n",
      "Epoch: 9 - Batch: 1582, Training Loss: 0.13646121754945806\n",
      "Epoch: 9 - Batch: 1583, Training Loss: 0.13654769908259956\n",
      "Epoch: 9 - Batch: 1584, Training Loss: 0.13664005145702393\n",
      "Epoch: 9 - Batch: 1585, Training Loss: 0.13672645511738893\n",
      "Epoch: 9 - Batch: 1586, Training Loss: 0.1368077940324547\n",
      "Epoch: 9 - Batch: 1587, Training Loss: 0.13690252829448105\n",
      "Epoch: 9 - Batch: 1588, Training Loss: 0.13698811577648468\n",
      "Epoch: 9 - Batch: 1589, Training Loss: 0.13707098406839924\n",
      "Epoch: 9 - Batch: 1590, Training Loss: 0.13715035922417593\n",
      "Epoch: 9 - Batch: 1591, Training Loss: 0.13723066699371408\n",
      "Epoch: 9 - Batch: 1592, Training Loss: 0.13731508507435\n",
      "Epoch: 9 - Batch: 1593, Training Loss: 0.1373964413741038\n",
      "Epoch: 9 - Batch: 1594, Training Loss: 0.13748368166177033\n",
      "Epoch: 9 - Batch: 1595, Training Loss: 0.13757335738113666\n",
      "Epoch: 9 - Batch: 1596, Training Loss: 0.13765754118858287\n",
      "Epoch: 9 - Batch: 1597, Training Loss: 0.13774910444396843\n",
      "Epoch: 9 - Batch: 1598, Training Loss: 0.13783368490525147\n",
      "Epoch: 9 - Batch: 1599, Training Loss: 0.1379242655489081\n",
      "Epoch: 9 - Batch: 1600, Training Loss: 0.13801592579775585\n",
      "Epoch: 9 - Batch: 1601, Training Loss: 0.13810358675286347\n",
      "Epoch: 9 - Batch: 1602, Training Loss: 0.13819574474497617\n",
      "Epoch: 9 - Batch: 1603, Training Loss: 0.1382913911100446\n",
      "Epoch: 9 - Batch: 1604, Training Loss: 0.13836389311139857\n",
      "Epoch: 9 - Batch: 1605, Training Loss: 0.13845487873673834\n",
      "Epoch: 9 - Batch: 1606, Training Loss: 0.13853922842153862\n",
      "Epoch: 9 - Batch: 1607, Training Loss: 0.13862777969013795\n",
      "Epoch: 9 - Batch: 1608, Training Loss: 0.13871161304846727\n",
      "Epoch: 9 - Batch: 1609, Training Loss: 0.13879438236998287\n",
      "Epoch: 9 - Batch: 1610, Training Loss: 0.1388947321951488\n",
      "Epoch: 9 - Batch: 1611, Training Loss: 0.13898384397475677\n",
      "Epoch: 9 - Batch: 1612, Training Loss: 0.1390786956977192\n",
      "Epoch: 9 - Batch: 1613, Training Loss: 0.1391601940465606\n",
      "Epoch: 9 - Batch: 1614, Training Loss: 0.13924476755149726\n",
      "Epoch: 9 - Batch: 1615, Training Loss: 0.13933357768143786\n",
      "Epoch: 9 - Batch: 1616, Training Loss: 0.1394122264426739\n",
      "Epoch: 9 - Batch: 1617, Training Loss: 0.13949869970974835\n",
      "Epoch: 9 - Batch: 1618, Training Loss: 0.13958312936164252\n",
      "Epoch: 9 - Batch: 1619, Training Loss: 0.13967181684736588\n",
      "Epoch: 9 - Batch: 1620, Training Loss: 0.13975419368514572\n",
      "Epoch: 9 - Batch: 1621, Training Loss: 0.139840979641844\n",
      "Epoch: 9 - Batch: 1622, Training Loss: 0.13991760202431758\n",
      "Epoch: 9 - Batch: 1623, Training Loss: 0.14000755862315892\n",
      "Epoch: 9 - Batch: 1624, Training Loss: 0.1400922861685405\n",
      "Epoch: 9 - Batch: 1625, Training Loss: 0.14017335213041227\n",
      "Epoch: 9 - Batch: 1626, Training Loss: 0.14025474152532383\n",
      "Epoch: 9 - Batch: 1627, Training Loss: 0.1403436882801317\n",
      "Epoch: 9 - Batch: 1628, Training Loss: 0.14043448825257138\n",
      "Epoch: 9 - Batch: 1629, Training Loss: 0.14052069665335898\n",
      "Epoch: 9 - Batch: 1630, Training Loss: 0.14061001842690146\n",
      "Epoch: 9 - Batch: 1631, Training Loss: 0.1406869974240636\n",
      "Epoch: 9 - Batch: 1632, Training Loss: 0.14078784296613428\n",
      "Epoch: 9 - Batch: 1633, Training Loss: 0.14087028420880857\n",
      "Epoch: 9 - Batch: 1634, Training Loss: 0.1409542799724967\n",
      "Epoch: 9 - Batch: 1635, Training Loss: 0.14104148591088617\n",
      "Epoch: 9 - Batch: 1636, Training Loss: 0.14113396166470119\n",
      "Epoch: 9 - Batch: 1637, Training Loss: 0.14123022745967306\n",
      "Epoch: 9 - Batch: 1638, Training Loss: 0.1413102716664671\n",
      "Epoch: 9 - Batch: 1639, Training Loss: 0.14140034382962074\n",
      "Epoch: 9 - Batch: 1640, Training Loss: 0.14148743052165308\n",
      "Epoch: 9 - Batch: 1641, Training Loss: 0.14157616567626521\n",
      "Epoch: 9 - Batch: 1642, Training Loss: 0.1416571717431296\n",
      "Epoch: 9 - Batch: 1643, Training Loss: 0.1417370963143571\n",
      "Epoch: 9 - Batch: 1644, Training Loss: 0.14181772318002991\n",
      "Epoch: 9 - Batch: 1645, Training Loss: 0.14189666554727168\n",
      "Epoch: 9 - Batch: 1646, Training Loss: 0.14198979543858697\n",
      "Epoch: 9 - Batch: 1647, Training Loss: 0.14206802268600582\n",
      "Epoch: 9 - Batch: 1648, Training Loss: 0.14214830064704367\n",
      "Epoch: 9 - Batch: 1649, Training Loss: 0.14223469112371134\n",
      "Epoch: 9 - Batch: 1650, Training Loss: 0.1423230755079544\n",
      "Epoch: 9 - Batch: 1651, Training Loss: 0.1424056882931423\n",
      "Epoch: 9 - Batch: 1652, Training Loss: 0.14248004018840307\n",
      "Epoch: 9 - Batch: 1653, Training Loss: 0.14256456674726845\n",
      "Epoch: 9 - Batch: 1654, Training Loss: 0.14266210608493235\n",
      "Epoch: 9 - Batch: 1655, Training Loss: 0.14275303643635454\n",
      "Epoch: 9 - Batch: 1656, Training Loss: 0.14284528689350853\n",
      "Epoch: 9 - Batch: 1657, Training Loss: 0.14293279810183085\n",
      "Epoch: 9 - Batch: 1658, Training Loss: 0.14301704761153627\n",
      "Epoch: 9 - Batch: 1659, Training Loss: 0.1431023290513661\n",
      "Epoch: 9 - Batch: 1660, Training Loss: 0.1431804796863057\n",
      "Epoch: 9 - Batch: 1661, Training Loss: 0.14327595674527027\n",
      "Epoch: 9 - Batch: 1662, Training Loss: 0.14336019061246322\n",
      "Epoch: 9 - Batch: 1663, Training Loss: 0.1434490744389013\n",
      "Epoch: 9 - Batch: 1664, Training Loss: 0.1435348835725887\n",
      "Epoch: 9 - Batch: 1665, Training Loss: 0.14361913269926263\n",
      "Epoch: 9 - Batch: 1666, Training Loss: 0.14369983919842128\n",
      "Epoch: 9 - Batch: 1667, Training Loss: 0.14377937504209293\n",
      "Epoch: 9 - Batch: 1668, Training Loss: 0.14386500916127146\n",
      "Epoch: 9 - Batch: 1669, Training Loss: 0.1439465441884686\n",
      "Epoch: 9 - Batch: 1670, Training Loss: 0.14403432059668586\n",
      "Epoch: 9 - Batch: 1671, Training Loss: 0.14411602546128863\n",
      "Epoch: 9 - Batch: 1672, Training Loss: 0.14420185666253318\n",
      "Epoch: 9 - Batch: 1673, Training Loss: 0.14428477462498504\n",
      "Epoch: 9 - Batch: 1674, Training Loss: 0.14436891154813924\n",
      "Epoch: 9 - Batch: 1675, Training Loss: 0.14445819451811895\n",
      "Epoch: 9 - Batch: 1676, Training Loss: 0.14454454011214313\n",
      "Epoch: 9 - Batch: 1677, Training Loss: 0.14464428720634376\n",
      "Epoch: 9 - Batch: 1678, Training Loss: 0.1447292622271462\n",
      "Epoch: 9 - Batch: 1679, Training Loss: 0.14481551053116767\n",
      "Epoch: 9 - Batch: 1680, Training Loss: 0.14489840453561662\n",
      "Epoch: 9 - Batch: 1681, Training Loss: 0.1449847915341111\n",
      "Epoch: 9 - Batch: 1682, Training Loss: 0.1450720600610843\n",
      "Epoch: 9 - Batch: 1683, Training Loss: 0.1451644146410288\n",
      "Epoch: 9 - Batch: 1684, Training Loss: 0.1452512128979808\n",
      "Epoch: 9 - Batch: 1685, Training Loss: 0.14533677183178131\n",
      "Epoch: 9 - Batch: 1686, Training Loss: 0.14542216415923231\n",
      "Epoch: 9 - Batch: 1687, Training Loss: 0.14550761405878992\n",
      "Epoch: 9 - Batch: 1688, Training Loss: 0.14559362892981983\n",
      "Epoch: 9 - Batch: 1689, Training Loss: 0.14567599478942245\n",
      "Epoch: 9 - Batch: 1690, Training Loss: 0.14575962536963658\n",
      "Epoch: 9 - Batch: 1691, Training Loss: 0.14583817430791965\n",
      "Epoch: 9 - Batch: 1692, Training Loss: 0.14592845567423313\n",
      "Epoch: 9 - Batch: 1693, Training Loss: 0.14601534868205956\n",
      "Epoch: 9 - Batch: 1694, Training Loss: 0.14610103447960185\n",
      "Epoch: 9 - Batch: 1695, Training Loss: 0.14618234212199846\n",
      "Epoch: 9 - Batch: 1696, Training Loss: 0.14627317528226483\n",
      "Epoch: 9 - Batch: 1697, Training Loss: 0.14635683597369772\n",
      "Epoch: 9 - Batch: 1698, Training Loss: 0.146449789837324\n",
      "Epoch: 9 - Batch: 1699, Training Loss: 0.14653342831910743\n",
      "Epoch: 9 - Batch: 1700, Training Loss: 0.146618501358908\n",
      "Epoch: 9 - Batch: 1701, Training Loss: 0.1467129316483663\n",
      "Epoch: 9 - Batch: 1702, Training Loss: 0.14680840926952227\n",
      "Epoch: 9 - Batch: 1703, Training Loss: 0.14689378231566147\n",
      "Epoch: 9 - Batch: 1704, Training Loss: 0.1469830326167604\n",
      "Epoch: 9 - Batch: 1705, Training Loss: 0.1470668825072238\n",
      "Epoch: 9 - Batch: 1706, Training Loss: 0.14715486092749322\n",
      "Epoch: 9 - Batch: 1707, Training Loss: 0.14724131996062265\n",
      "Epoch: 9 - Batch: 1708, Training Loss: 0.14732849078984997\n",
      "Epoch: 9 - Batch: 1709, Training Loss: 0.1474171360556166\n",
      "Epoch: 9 - Batch: 1710, Training Loss: 0.14750705369644695\n",
      "Epoch: 9 - Batch: 1711, Training Loss: 0.1475950852036476\n",
      "Epoch: 9 - Batch: 1712, Training Loss: 0.14767384375901166\n",
      "Epoch: 9 - Batch: 1713, Training Loss: 0.14776088503092083\n",
      "Epoch: 9 - Batch: 1714, Training Loss: 0.1478343242794819\n",
      "Epoch: 9 - Batch: 1715, Training Loss: 0.14791594726183324\n",
      "Epoch: 9 - Batch: 1716, Training Loss: 0.1479973828676427\n",
      "Epoch: 9 - Batch: 1717, Training Loss: 0.14808393591647323\n",
      "Epoch: 9 - Batch: 1718, Training Loss: 0.14816600879404082\n",
      "Epoch: 9 - Batch: 1719, Training Loss: 0.1482613319299411\n",
      "Epoch: 9 - Batch: 1720, Training Loss: 0.1483484407883774\n",
      "Epoch: 9 - Batch: 1721, Training Loss: 0.148431143190582\n",
      "Epoch: 9 - Batch: 1722, Training Loss: 0.14852394788814816\n",
      "Epoch: 9 - Batch: 1723, Training Loss: 0.1486083803077539\n",
      "Epoch: 9 - Batch: 1724, Training Loss: 0.1487042201778683\n",
      "Epoch: 9 - Batch: 1725, Training Loss: 0.14878671276529828\n",
      "Epoch: 9 - Batch: 1726, Training Loss: 0.14887235157970172\n",
      "Epoch: 9 - Batch: 1727, Training Loss: 0.14895711764446143\n",
      "Epoch: 9 - Batch: 1728, Training Loss: 0.14904273461371315\n",
      "Epoch: 9 - Batch: 1729, Training Loss: 0.14912452178066643\n",
      "Epoch: 9 - Batch: 1730, Training Loss: 0.14921789183891432\n",
      "Epoch: 9 - Batch: 1731, Training Loss: 0.149305260232086\n",
      "Epoch: 9 - Batch: 1732, Training Loss: 0.14939076345951402\n",
      "Epoch: 9 - Batch: 1733, Training Loss: 0.14948024790406622\n",
      "Epoch: 9 - Batch: 1734, Training Loss: 0.14956693521779568\n",
      "Epoch: 9 - Batch: 1735, Training Loss: 0.14965263322292277\n",
      "Epoch: 9 - Batch: 1736, Training Loss: 0.14973432738735507\n",
      "Epoch: 9 - Batch: 1737, Training Loss: 0.14982846709799214\n",
      "Epoch: 9 - Batch: 1738, Training Loss: 0.14991209752375806\n",
      "Epoch: 9 - Batch: 1739, Training Loss: 0.15000818101426658\n",
      "Epoch: 9 - Batch: 1740, Training Loss: 0.15009117136349528\n",
      "Epoch: 9 - Batch: 1741, Training Loss: 0.15017967920744202\n",
      "Epoch: 9 - Batch: 1742, Training Loss: 0.1502645713822363\n",
      "Epoch: 9 - Batch: 1743, Training Loss: 0.15034852461707138\n",
      "Epoch: 9 - Batch: 1744, Training Loss: 0.15043063105931923\n",
      "Epoch: 9 - Batch: 1745, Training Loss: 0.15051575732814337\n",
      "Epoch: 9 - Batch: 1746, Training Loss: 0.15059638033878942\n",
      "Epoch: 9 - Batch: 1747, Training Loss: 0.15068464001538742\n",
      "Epoch: 9 - Batch: 1748, Training Loss: 0.15076636991295253\n",
      "Epoch: 9 - Batch: 1749, Training Loss: 0.1508512719971426\n",
      "Epoch: 9 - Batch: 1750, Training Loss: 0.15093799133107041\n",
      "Epoch: 9 - Batch: 1751, Training Loss: 0.15103264495260876\n",
      "Epoch: 9 - Batch: 1752, Training Loss: 0.15111663506349324\n",
      "Epoch: 9 - Batch: 1753, Training Loss: 0.15121105836571547\n",
      "Epoch: 9 - Batch: 1754, Training Loss: 0.1512958275750403\n",
      "Epoch: 9 - Batch: 1755, Training Loss: 0.15139135848699914\n",
      "Epoch: 9 - Batch: 1756, Training Loss: 0.1514818205638707\n",
      "Epoch: 9 - Batch: 1757, Training Loss: 0.1515596432311717\n",
      "Epoch: 9 - Batch: 1758, Training Loss: 0.15164264899432955\n",
      "Epoch: 9 - Batch: 1759, Training Loss: 0.15172566948566665\n",
      "Epoch: 9 - Batch: 1760, Training Loss: 0.15181371081577208\n",
      "Epoch: 9 - Batch: 1761, Training Loss: 0.1518965622172328\n",
      "Epoch: 9 - Batch: 1762, Training Loss: 0.15198047585155242\n",
      "Epoch: 9 - Batch: 1763, Training Loss: 0.15206587625355467\n",
      "Epoch: 9 - Batch: 1764, Training Loss: 0.15215711729913012\n",
      "Epoch: 9 - Batch: 1765, Training Loss: 0.15224172994123764\n",
      "Epoch: 9 - Batch: 1766, Training Loss: 0.1523314770156669\n",
      "Epoch: 9 - Batch: 1767, Training Loss: 0.15242822316303775\n",
      "Epoch: 9 - Batch: 1768, Training Loss: 0.15250966271036498\n",
      "Epoch: 9 - Batch: 1769, Training Loss: 0.1525975425556228\n",
      "Epoch: 9 - Batch: 1770, Training Loss: 0.15268306807919127\n",
      "Epoch: 9 - Batch: 1771, Training Loss: 0.1527665647157587\n",
      "Epoch: 9 - Batch: 1772, Training Loss: 0.15285233031976875\n",
      "Epoch: 9 - Batch: 1773, Training Loss: 0.15294246393476751\n",
      "Epoch: 9 - Batch: 1774, Training Loss: 0.15303353481550716\n",
      "Epoch: 9 - Batch: 1775, Training Loss: 0.15312438970972253\n",
      "Epoch: 9 - Batch: 1776, Training Loss: 0.15320739252600898\n",
      "Epoch: 9 - Batch: 1777, Training Loss: 0.15328862586647124\n",
      "Epoch: 9 - Batch: 1778, Training Loss: 0.15337806125210093\n",
      "Epoch: 9 - Batch: 1779, Training Loss: 0.15346794959102103\n",
      "Epoch: 9 - Batch: 1780, Training Loss: 0.15355006886482436\n",
      "Epoch: 9 - Batch: 1781, Training Loss: 0.15364009961115188\n",
      "Epoch: 9 - Batch: 1782, Training Loss: 0.15372360909780855\n",
      "Epoch: 9 - Batch: 1783, Training Loss: 0.1538007337766797\n",
      "Epoch: 9 - Batch: 1784, Training Loss: 0.15389151414037738\n",
      "Epoch: 9 - Batch: 1785, Training Loss: 0.15397656510644292\n",
      "Epoch: 9 - Batch: 1786, Training Loss: 0.15405329442266405\n",
      "Epoch: 9 - Batch: 1787, Training Loss: 0.1541450651390339\n",
      "Epoch: 9 - Batch: 1788, Training Loss: 0.15422794857922675\n",
      "Epoch: 9 - Batch: 1789, Training Loss: 0.15432083839010047\n",
      "Epoch: 9 - Batch: 1790, Training Loss: 0.15441149400908555\n",
      "Epoch: 9 - Batch: 1791, Training Loss: 0.15449708956564046\n",
      "Epoch: 9 - Batch: 1792, Training Loss: 0.1545811143038087\n",
      "Epoch: 9 - Batch: 1793, Training Loss: 0.15466094711427864\n",
      "Epoch: 9 - Batch: 1794, Training Loss: 0.15474886963048187\n",
      "Epoch: 9 - Batch: 1795, Training Loss: 0.15484260545292897\n",
      "Epoch: 9 - Batch: 1796, Training Loss: 0.15493340645090461\n",
      "Epoch: 9 - Batch: 1797, Training Loss: 0.15501466667770747\n",
      "Epoch: 9 - Batch: 1798, Training Loss: 0.15509989596965104\n",
      "Epoch: 9 - Batch: 1799, Training Loss: 0.15517476869509192\n",
      "Epoch: 9 - Batch: 1800, Training Loss: 0.1552634007462716\n",
      "Epoch: 9 - Batch: 1801, Training Loss: 0.1553475194260058\n",
      "Epoch: 9 - Batch: 1802, Training Loss: 0.1554328208143636\n",
      "Epoch: 9 - Batch: 1803, Training Loss: 0.15552066909041176\n",
      "Epoch: 9 - Batch: 1804, Training Loss: 0.15560820347214022\n",
      "Epoch: 9 - Batch: 1805, Training Loss: 0.1556940978365158\n",
      "Epoch: 9 - Batch: 1806, Training Loss: 0.15578484233734421\n",
      "Epoch: 9 - Batch: 1807, Training Loss: 0.1558684615934172\n",
      "Epoch: 9 - Batch: 1808, Training Loss: 0.15595611354110647\n",
      "Epoch: 9 - Batch: 1809, Training Loss: 0.1560367637469895\n",
      "Epoch: 9 - Batch: 1810, Training Loss: 0.15612465033854417\n",
      "Epoch: 9 - Batch: 1811, Training Loss: 0.15621107273656337\n",
      "Epoch: 9 - Batch: 1812, Training Loss: 0.1562888750537711\n",
      "Epoch: 9 - Batch: 1813, Training Loss: 0.15636889805173992\n",
      "Epoch: 9 - Batch: 1814, Training Loss: 0.15645986994354683\n",
      "Epoch: 9 - Batch: 1815, Training Loss: 0.1565487747564047\n",
      "Epoch: 9 - Batch: 1816, Training Loss: 0.15662976799789155\n",
      "Epoch: 9 - Batch: 1817, Training Loss: 0.15671538689465664\n",
      "Epoch: 9 - Batch: 1818, Training Loss: 0.15680707011921685\n",
      "Epoch: 9 - Batch: 1819, Training Loss: 0.15690173026464668\n",
      "Epoch: 9 - Batch: 1820, Training Loss: 0.15699276739536827\n",
      "Epoch: 9 - Batch: 1821, Training Loss: 0.15709461019085613\n",
      "Epoch: 9 - Batch: 1822, Training Loss: 0.15718224661341354\n",
      "Epoch: 9 - Batch: 1823, Training Loss: 0.15726691340505583\n",
      "Epoch: 9 - Batch: 1824, Training Loss: 0.1573551472455609\n",
      "Epoch: 9 - Batch: 1825, Training Loss: 0.15743845889644442\n",
      "Epoch: 9 - Batch: 1826, Training Loss: 0.15752583968871664\n",
      "Epoch: 9 - Batch: 1827, Training Loss: 0.1576141966553173\n",
      "Epoch: 9 - Batch: 1828, Training Loss: 0.1577053694780391\n",
      "Epoch: 9 - Batch: 1829, Training Loss: 0.15779242767129173\n",
      "Epoch: 9 - Batch: 1830, Training Loss: 0.15788476514455493\n",
      "Epoch: 9 - Batch: 1831, Training Loss: 0.15797189624353034\n",
      "Epoch: 9 - Batch: 1832, Training Loss: 0.1580621757688214\n",
      "Epoch: 9 - Batch: 1833, Training Loss: 0.15815147369556365\n",
      "Epoch: 9 - Batch: 1834, Training Loss: 0.15823816700188278\n",
      "Epoch: 9 - Batch: 1835, Training Loss: 0.15831995283440373\n",
      "Epoch: 9 - Batch: 1836, Training Loss: 0.1584035102556001\n",
      "Epoch: 9 - Batch: 1837, Training Loss: 0.15848988134593117\n",
      "Epoch: 9 - Batch: 1838, Training Loss: 0.15857753595630722\n",
      "Epoch: 9 - Batch: 1839, Training Loss: 0.15866090938028807\n",
      "Epoch: 9 - Batch: 1840, Training Loss: 0.1587473348858048\n",
      "Epoch: 9 - Batch: 1841, Training Loss: 0.1588356075792961\n",
      "Epoch: 9 - Batch: 1842, Training Loss: 0.15892017261429411\n",
      "Epoch: 9 - Batch: 1843, Training Loss: 0.15901285019482347\n",
      "Epoch: 9 - Batch: 1844, Training Loss: 0.1590929681907839\n",
      "Epoch: 9 - Batch: 1845, Training Loss: 0.1591721355952137\n",
      "Epoch: 9 - Batch: 1846, Training Loss: 0.15926065496099528\n",
      "Epoch: 9 - Batch: 1847, Training Loss: 0.1593508369615224\n",
      "Epoch: 9 - Batch: 1848, Training Loss: 0.15944035759019615\n",
      "Epoch: 9 - Batch: 1849, Training Loss: 0.1595176393799422\n",
      "Epoch: 9 - Batch: 1850, Training Loss: 0.15960760884740657\n",
      "Epoch: 9 - Batch: 1851, Training Loss: 0.15969228997801865\n",
      "Epoch: 9 - Batch: 1852, Training Loss: 0.15978600826750744\n",
      "Epoch: 9 - Batch: 1853, Training Loss: 0.15987452115882095\n",
      "Epoch: 9 - Batch: 1854, Training Loss: 0.1599673436920639\n",
      "Epoch: 9 - Batch: 1855, Training Loss: 0.16005034823817005\n",
      "Epoch: 9 - Batch: 1856, Training Loss: 0.16014209306902355\n",
      "Epoch: 9 - Batch: 1857, Training Loss: 0.16022957055821743\n",
      "Epoch: 9 - Batch: 1858, Training Loss: 0.16032062695789495\n",
      "Epoch: 9 - Batch: 1859, Training Loss: 0.16040339044744695\n",
      "Epoch: 9 - Batch: 1860, Training Loss: 0.16049166441976925\n",
      "Epoch: 9 - Batch: 1861, Training Loss: 0.16058433146075426\n",
      "Epoch: 9 - Batch: 1862, Training Loss: 0.16067229561618904\n",
      "Epoch: 9 - Batch: 1863, Training Loss: 0.16075392152070012\n",
      "Epoch: 9 - Batch: 1864, Training Loss: 0.16083344112260028\n",
      "Epoch: 9 - Batch: 1865, Training Loss: 0.1609289260893121\n",
      "Epoch: 9 - Batch: 1866, Training Loss: 0.16100958768111556\n",
      "Epoch: 9 - Batch: 1867, Training Loss: 0.1610898741059141\n",
      "Epoch: 9 - Batch: 1868, Training Loss: 0.16118019496771827\n",
      "Epoch: 9 - Batch: 1869, Training Loss: 0.16126799553781007\n",
      "Epoch: 9 - Batch: 1870, Training Loss: 0.16135182610371615\n",
      "Epoch: 9 - Batch: 1871, Training Loss: 0.1614403461861373\n",
      "Epoch: 9 - Batch: 1872, Training Loss: 0.1615178812338444\n",
      "Epoch: 9 - Batch: 1873, Training Loss: 0.16160754126127483\n",
      "Epoch: 9 - Batch: 1874, Training Loss: 0.16169333041554462\n",
      "Epoch: 9 - Batch: 1875, Training Loss: 0.16178275509482593\n",
      "Epoch: 9 - Batch: 1876, Training Loss: 0.1618659040870556\n",
      "Epoch: 9 - Batch: 1877, Training Loss: 0.16195649984810087\n",
      "Epoch: 9 - Batch: 1878, Training Loss: 0.16204659107633887\n",
      "Epoch: 9 - Batch: 1879, Training Loss: 0.1621338533203598\n",
      "Epoch: 9 - Batch: 1880, Training Loss: 0.1622171699074074\n",
      "Epoch: 9 - Batch: 1881, Training Loss: 0.16230561133863322\n",
      "Epoch: 9 - Batch: 1882, Training Loss: 0.16238818866251714\n",
      "Epoch: 9 - Batch: 1883, Training Loss: 0.1624782463569645\n",
      "Epoch: 9 - Batch: 1884, Training Loss: 0.16256426109825795\n",
      "Epoch: 9 - Batch: 1885, Training Loss: 0.16265324209731807\n",
      "Epoch: 9 - Batch: 1886, Training Loss: 0.16274758052099403\n",
      "Epoch: 9 - Batch: 1887, Training Loss: 0.1628263700771984\n",
      "Epoch: 9 - Batch: 1888, Training Loss: 0.16291446054036146\n",
      "Epoch: 9 - Batch: 1889, Training Loss: 0.16300514250820747\n",
      "Epoch: 9 - Batch: 1890, Training Loss: 0.16309325111594367\n",
      "Epoch: 9 - Batch: 1891, Training Loss: 0.16318060651075583\n",
      "Epoch: 9 - Batch: 1892, Training Loss: 0.16326751540203396\n",
      "Epoch: 9 - Batch: 1893, Training Loss: 0.1633487045270689\n",
      "Epoch: 9 - Batch: 1894, Training Loss: 0.1634298574432706\n",
      "Epoch: 9 - Batch: 1895, Training Loss: 0.1635109272231016\n",
      "Epoch: 9 - Batch: 1896, Training Loss: 0.16359532350183126\n",
      "Epoch: 9 - Batch: 1897, Training Loss: 0.1636753628404184\n",
      "Epoch: 9 - Batch: 1898, Training Loss: 0.16375896853594044\n",
      "Epoch: 9 - Batch: 1899, Training Loss: 0.16384640483071358\n",
      "Epoch: 9 - Batch: 1900, Training Loss: 0.16393459512918546\n",
      "Epoch: 9 - Batch: 1901, Training Loss: 0.1640194505267772\n",
      "Epoch: 9 - Batch: 1902, Training Loss: 0.1641077651436847\n",
      "Epoch: 9 - Batch: 1903, Training Loss: 0.16419465469804964\n",
      "Epoch: 9 - Batch: 1904, Training Loss: 0.16427634774427707\n",
      "Epoch: 9 - Batch: 1905, Training Loss: 0.16435433119214193\n",
      "Epoch: 9 - Batch: 1906, Training Loss: 0.16443162318782426\n",
      "Epoch: 9 - Batch: 1907, Training Loss: 0.16452094922413676\n",
      "Epoch: 9 - Batch: 1908, Training Loss: 0.16461097370604574\n",
      "Epoch: 9 - Batch: 1909, Training Loss: 0.16469697638802466\n",
      "Epoch: 9 - Batch: 1910, Training Loss: 0.164775617111777\n",
      "Epoch: 9 - Batch: 1911, Training Loss: 0.16486542261704482\n",
      "Epoch: 9 - Batch: 1912, Training Loss: 0.16496227097204866\n",
      "Epoch: 9 - Batch: 1913, Training Loss: 0.16505624931521876\n",
      "Epoch: 9 - Batch: 1914, Training Loss: 0.16514394296391885\n",
      "Epoch: 9 - Batch: 1915, Training Loss: 0.16522472521682482\n",
      "Epoch: 9 - Batch: 1916, Training Loss: 0.165303414202547\n",
      "Epoch: 9 - Batch: 1917, Training Loss: 0.1653921246825166\n",
      "Epoch: 9 - Batch: 1918, Training Loss: 0.16548059728756473\n",
      "Epoch: 9 - Batch: 1919, Training Loss: 0.16556498542378592\n",
      "Epoch: 9 - Batch: 1920, Training Loss: 0.1656502971725282\n",
      "Epoch: 9 - Batch: 1921, Training Loss: 0.165740234978114\n",
      "Epoch: 9 - Batch: 1922, Training Loss: 0.1658263534468106\n",
      "Epoch: 9 - Batch: 1923, Training Loss: 0.16590556607984785\n",
      "Epoch: 9 - Batch: 1924, Training Loss: 0.16599682633259996\n",
      "Epoch: 9 - Batch: 1925, Training Loss: 0.16607508682962477\n",
      "Epoch: 9 - Batch: 1926, Training Loss: 0.16616066104761798\n",
      "Epoch: 9 - Batch: 1927, Training Loss: 0.16626098511082615\n",
      "Epoch: 9 - Batch: 1928, Training Loss: 0.16634257918047668\n",
      "Epoch: 9 - Batch: 1929, Training Loss: 0.16642807909653554\n",
      "Epoch: 9 - Batch: 1930, Training Loss: 0.16651073421288287\n",
      "Epoch: 9 - Batch: 1931, Training Loss: 0.1665974212671394\n",
      "Epoch: 9 - Batch: 1932, Training Loss: 0.1666828708763344\n",
      "Epoch: 9 - Batch: 1933, Training Loss: 0.16676234856717426\n",
      "Epoch: 9 - Batch: 1934, Training Loss: 0.16684496518267725\n",
      "Epoch: 9 - Batch: 1935, Training Loss: 0.16693492500639673\n",
      "Epoch: 9 - Batch: 1936, Training Loss: 0.16702633111383983\n",
      "Epoch: 9 - Batch: 1937, Training Loss: 0.16710548498860836\n",
      "Epoch: 9 - Batch: 1938, Training Loss: 0.1671840135582644\n",
      "Epoch: 9 - Batch: 1939, Training Loss: 0.16726722752993578\n",
      "Epoch: 9 - Batch: 1940, Training Loss: 0.16735283369399223\n",
      "Epoch: 9 - Batch: 1941, Training Loss: 0.167435648948399\n",
      "Epoch: 9 - Batch: 1942, Training Loss: 0.16751071661834296\n",
      "Epoch: 9 - Batch: 1943, Training Loss: 0.1675994204632184\n",
      "Epoch: 9 - Batch: 1944, Training Loss: 0.1676782688641825\n",
      "Epoch: 9 - Batch: 1945, Training Loss: 0.16776624478239127\n",
      "Epoch: 9 - Batch: 1946, Training Loss: 0.1678514748033303\n",
      "Epoch: 9 - Batch: 1947, Training Loss: 0.16794018137267178\n",
      "Epoch: 9 - Batch: 1948, Training Loss: 0.16802375132270517\n",
      "Epoch: 9 - Batch: 1949, Training Loss: 0.1681123505134883\n",
      "Epoch: 9 - Batch: 1950, Training Loss: 0.16818754923580892\n",
      "Epoch: 9 - Batch: 1951, Training Loss: 0.1682708254439222\n",
      "Epoch: 9 - Batch: 1952, Training Loss: 0.16836306383558372\n",
      "Epoch: 9 - Batch: 1953, Training Loss: 0.16844606189486597\n",
      "Epoch: 9 - Batch: 1954, Training Loss: 0.16853769040152208\n",
      "Epoch: 9 - Batch: 1955, Training Loss: 0.16862647686382234\n",
      "Epoch: 9 - Batch: 1956, Training Loss: 0.16871094687611704\n",
      "Epoch: 9 - Batch: 1957, Training Loss: 0.16879062559671862\n",
      "Epoch: 9 - Batch: 1958, Training Loss: 0.16887744513663092\n",
      "Epoch: 9 - Batch: 1959, Training Loss: 0.16896897828697566\n",
      "Epoch: 9 - Batch: 1960, Training Loss: 0.169048909944286\n",
      "Epoch: 9 - Batch: 1961, Training Loss: 0.16913288254692385\n",
      "Epoch: 9 - Batch: 1962, Training Loss: 0.1692154134869279\n",
      "Epoch: 9 - Batch: 1963, Training Loss: 0.16929943826834162\n",
      "Epoch: 9 - Batch: 1964, Training Loss: 0.1693846140532549\n",
      "Epoch: 9 - Batch: 1965, Training Loss: 0.16947858629881052\n",
      "Epoch: 9 - Batch: 1966, Training Loss: 0.16956761138356147\n",
      "Epoch: 9 - Batch: 1967, Training Loss: 0.1696587443401169\n",
      "Epoch: 9 - Batch: 1968, Training Loss: 0.16974076259862725\n",
      "Epoch: 9 - Batch: 1969, Training Loss: 0.16982503500991003\n",
      "Epoch: 9 - Batch: 1970, Training Loss: 0.16990542574901485\n",
      "Epoch: 9 - Batch: 1971, Training Loss: 0.17000230522841758\n",
      "Epoch: 9 - Batch: 1972, Training Loss: 0.1700894936523825\n",
      "Epoch: 9 - Batch: 1973, Training Loss: 0.17019441631154633\n",
      "Epoch: 9 - Batch: 1974, Training Loss: 0.17029023094482681\n",
      "Epoch: 9 - Batch: 1975, Training Loss: 0.17037771450074554\n",
      "Epoch: 9 - Batch: 1976, Training Loss: 0.17047150960040724\n",
      "Epoch: 9 - Batch: 1977, Training Loss: 0.170562881809562\n",
      "Epoch: 9 - Batch: 1978, Training Loss: 0.17065117451336057\n",
      "Epoch: 9 - Batch: 1979, Training Loss: 0.17072939070846707\n",
      "Epoch: 9 - Batch: 1980, Training Loss: 0.17081004733939473\n",
      "Epoch: 9 - Batch: 1981, Training Loss: 0.1709046819823397\n",
      "Epoch: 9 - Batch: 1982, Training Loss: 0.17098716899727906\n",
      "Epoch: 9 - Batch: 1983, Training Loss: 0.17107613883282416\n",
      "Epoch: 9 - Batch: 1984, Training Loss: 0.17116437883272298\n",
      "Epoch: 9 - Batch: 1985, Training Loss: 0.1712530659045252\n",
      "Epoch: 9 - Batch: 1986, Training Loss: 0.171337822924799\n",
      "Epoch: 9 - Batch: 1987, Training Loss: 0.17142652253161617\n",
      "Epoch: 9 - Batch: 1988, Training Loss: 0.17151321027780053\n",
      "Epoch: 9 - Batch: 1989, Training Loss: 0.17159940216498787\n",
      "Epoch: 9 - Batch: 1990, Training Loss: 0.17168280453255916\n",
      "Epoch: 9 - Batch: 1991, Training Loss: 0.1717701161320431\n",
      "Epoch: 9 - Batch: 1992, Training Loss: 0.1718600456344943\n",
      "Epoch: 9 - Batch: 1993, Training Loss: 0.1719511725490366\n",
      "Epoch: 9 - Batch: 1994, Training Loss: 0.17204329505513358\n",
      "Epoch: 9 - Batch: 1995, Training Loss: 0.17212502007213595\n",
      "Epoch: 9 - Batch: 1996, Training Loss: 0.1722165758028058\n",
      "Epoch: 9 - Batch: 1997, Training Loss: 0.1723078912615183\n",
      "Epoch: 9 - Batch: 1998, Training Loss: 0.1723938257144656\n",
      "Epoch: 9 - Batch: 1999, Training Loss: 0.17248522686103288\n",
      "Epoch: 9 - Batch: 2000, Training Loss: 0.17257302302008443\n",
      "Epoch: 9 - Batch: 2001, Training Loss: 0.1726626106360263\n",
      "Epoch: 9 - Batch: 2002, Training Loss: 0.17275172440492692\n",
      "Epoch: 9 - Batch: 2003, Training Loss: 0.17284293564200204\n",
      "Epoch: 9 - Batch: 2004, Training Loss: 0.17292722693303134\n",
      "Epoch: 9 - Batch: 2005, Training Loss: 0.17301174999518973\n",
      "Epoch: 9 - Batch: 2006, Training Loss: 0.17310118679556483\n",
      "Epoch: 9 - Batch: 2007, Training Loss: 0.17318975253287042\n",
      "Epoch: 9 - Batch: 2008, Training Loss: 0.17328809776264636\n",
      "Epoch: 9 - Batch: 2009, Training Loss: 0.17336804064525102\n",
      "Epoch: 9 - Batch: 2010, Training Loss: 0.17345159931438875\n",
      "Epoch: 9 - Batch: 2011, Training Loss: 0.17353774837618247\n",
      "Epoch: 9 - Batch: 2012, Training Loss: 0.17361987342634802\n",
      "Epoch: 9 - Batch: 2013, Training Loss: 0.17370972368452284\n",
      "Epoch: 9 - Batch: 2014, Training Loss: 0.1737909336168177\n",
      "Epoch: 9 - Batch: 2015, Training Loss: 0.1738714772966964\n",
      "Epoch: 9 - Batch: 2016, Training Loss: 0.1739551555272655\n",
      "Epoch: 9 - Batch: 2017, Training Loss: 0.1740362198766982\n",
      "Epoch: 9 - Batch: 2018, Training Loss: 0.1741172711659921\n",
      "Epoch: 9 - Batch: 2019, Training Loss: 0.1741965630865512\n",
      "Epoch: 9 - Batch: 2020, Training Loss: 0.17428500207131775\n",
      "Epoch: 9 - Batch: 2021, Training Loss: 0.17437814453471556\n",
      "Epoch: 9 - Batch: 2022, Training Loss: 0.17446874064790274\n",
      "Epoch: 9 - Batch: 2023, Training Loss: 0.17456025598728242\n",
      "Epoch: 9 - Batch: 2024, Training Loss: 0.17464272672956066\n",
      "Epoch: 9 - Batch: 2025, Training Loss: 0.17473290666666\n",
      "Epoch: 9 - Batch: 2026, Training Loss: 0.17482961134160335\n",
      "Epoch: 9 - Batch: 2027, Training Loss: 0.17491823084541222\n",
      "Epoch: 9 - Batch: 2028, Training Loss: 0.17500246399596556\n",
      "Epoch: 9 - Batch: 2029, Training Loss: 0.1750878470627032\n",
      "Epoch: 9 - Batch: 2030, Training Loss: 0.17517617249098386\n",
      "Epoch: 9 - Batch: 2031, Training Loss: 0.17526622290165467\n",
      "Epoch: 9 - Batch: 2032, Training Loss: 0.17535609137853778\n",
      "Epoch: 9 - Batch: 2033, Training Loss: 0.17544969192908375\n",
      "Epoch: 9 - Batch: 2034, Training Loss: 0.1755281051691887\n",
      "Epoch: 9 - Batch: 2035, Training Loss: 0.1756263840665568\n",
      "Epoch: 9 - Batch: 2036, Training Loss: 0.175720911540993\n",
      "Epoch: 9 - Batch: 2037, Training Loss: 0.17580196091512937\n",
      "Epoch: 9 - Batch: 2038, Training Loss: 0.17589558733265792\n",
      "Epoch: 9 - Batch: 2039, Training Loss: 0.1759728293797962\n",
      "Epoch: 9 - Batch: 2040, Training Loss: 0.17605592184398897\n",
      "Epoch: 9 - Batch: 2041, Training Loss: 0.17614571824916955\n",
      "Epoch: 9 - Batch: 2042, Training Loss: 0.17622506020476372\n",
      "Epoch: 9 - Batch: 2043, Training Loss: 0.17631599644992876\n",
      "Epoch: 9 - Batch: 2044, Training Loss: 0.1764081222859287\n",
      "Epoch: 9 - Batch: 2045, Training Loss: 0.17649177810297675\n",
      "Epoch: 9 - Batch: 2046, Training Loss: 0.17658163293899587\n",
      "Epoch: 9 - Batch: 2047, Training Loss: 0.1766608777034342\n",
      "Epoch: 9 - Batch: 2048, Training Loss: 0.1767469932067256\n",
      "Epoch: 9 - Batch: 2049, Training Loss: 0.17683579667686034\n",
      "Epoch: 9 - Batch: 2050, Training Loss: 0.1769257352855767\n",
      "Epoch: 9 - Batch: 2051, Training Loss: 0.17701390000742862\n",
      "Epoch: 9 - Batch: 2052, Training Loss: 0.17709624825993778\n",
      "Epoch: 9 - Batch: 2053, Training Loss: 0.17718318875749312\n",
      "Epoch: 9 - Batch: 2054, Training Loss: 0.17727315129950075\n",
      "Epoch: 9 - Batch: 2055, Training Loss: 0.17734921450539806\n",
      "Epoch: 9 - Batch: 2056, Training Loss: 0.1774343684266258\n",
      "Epoch: 9 - Batch: 2057, Training Loss: 0.17751772617720452\n",
      "Epoch: 9 - Batch: 2058, Training Loss: 0.17760326120860342\n",
      "Epoch: 9 - Batch: 2059, Training Loss: 0.17768682962033286\n",
      "Epoch: 9 - Batch: 2060, Training Loss: 0.17777219157709215\n",
      "Epoch: 9 - Batch: 2061, Training Loss: 0.17785963855967987\n",
      "Epoch: 9 - Batch: 2062, Training Loss: 0.1779443161565174\n",
      "Epoch: 9 - Batch: 2063, Training Loss: 0.17803763298574174\n",
      "Epoch: 9 - Batch: 2064, Training Loss: 0.17812652505353513\n",
      "Epoch: 9 - Batch: 2065, Training Loss: 0.1782113824095299\n",
      "Epoch: 9 - Batch: 2066, Training Loss: 0.17829832350016628\n",
      "Epoch: 9 - Batch: 2067, Training Loss: 0.17838904892973242\n",
      "Epoch: 9 - Batch: 2068, Training Loss: 0.1784746448137175\n",
      "Epoch: 9 - Batch: 2069, Training Loss: 0.17856806814423445\n",
      "Epoch: 9 - Batch: 2070, Training Loss: 0.17864944898889432\n",
      "Epoch: 9 - Batch: 2071, Training Loss: 0.17873346366321863\n",
      "Epoch: 9 - Batch: 2072, Training Loss: 0.17882172218454417\n",
      "Epoch: 9 - Batch: 2073, Training Loss: 0.17890809725963852\n",
      "Epoch: 9 - Batch: 2074, Training Loss: 0.17899462959387805\n",
      "Epoch: 9 - Batch: 2075, Training Loss: 0.17908248911993224\n",
      "Epoch: 9 - Batch: 2076, Training Loss: 0.17917313404492477\n",
      "Epoch: 9 - Batch: 2077, Training Loss: 0.17927026256276402\n",
      "Epoch: 9 - Batch: 2078, Training Loss: 0.17935649754717378\n",
      "Epoch: 9 - Batch: 2079, Training Loss: 0.17945103283668828\n",
      "Epoch: 9 - Batch: 2080, Training Loss: 0.17953632176416628\n",
      "Epoch: 9 - Batch: 2081, Training Loss: 0.17961590444888445\n",
      "Epoch: 9 - Batch: 2082, Training Loss: 0.17970473550346558\n",
      "Epoch: 9 - Batch: 2083, Training Loss: 0.17978918637654082\n",
      "Epoch: 9 - Batch: 2084, Training Loss: 0.1798782164963027\n",
      "Epoch: 9 - Batch: 2085, Training Loss: 0.179958536955304\n",
      "Epoch: 9 - Batch: 2086, Training Loss: 0.180055123088174\n",
      "Epoch: 9 - Batch: 2087, Training Loss: 0.1801322930327201\n",
      "Epoch: 9 - Batch: 2088, Training Loss: 0.18022364418131992\n",
      "Epoch: 9 - Batch: 2089, Training Loss: 0.18030393829146033\n",
      "Epoch: 9 - Batch: 2090, Training Loss: 0.1803886219797344\n",
      "Epoch: 9 - Batch: 2091, Training Loss: 0.1804770760818896\n",
      "Epoch: 9 - Batch: 2092, Training Loss: 0.1805500684685968\n",
      "Epoch: 9 - Batch: 2093, Training Loss: 0.1806435720627482\n",
      "Epoch: 9 - Batch: 2094, Training Loss: 0.180731674140415\n",
      "Epoch: 9 - Batch: 2095, Training Loss: 0.18081389324952715\n",
      "Epoch: 9 - Batch: 2096, Training Loss: 0.1809007530695269\n",
      "Epoch: 9 - Batch: 2097, Training Loss: 0.18099690983025588\n",
      "Epoch: 9 - Batch: 2098, Training Loss: 0.1810882807049783\n",
      "Epoch: 9 - Batch: 2099, Training Loss: 0.181172802630398\n",
      "Epoch: 9 - Batch: 2100, Training Loss: 0.18125944810102432\n",
      "Epoch: 9 - Batch: 2101, Training Loss: 0.18134442223838312\n",
      "Epoch: 9 - Batch: 2102, Training Loss: 0.18142647817607346\n",
      "Epoch: 9 - Batch: 2103, Training Loss: 0.18152694481349307\n",
      "Epoch: 9 - Batch: 2104, Training Loss: 0.1816166142869747\n",
      "Epoch: 9 - Batch: 2105, Training Loss: 0.1817076307339949\n",
      "Epoch: 9 - Batch: 2106, Training Loss: 0.18179158498818798\n",
      "Epoch: 9 - Batch: 2107, Training Loss: 0.18187585151190583\n",
      "Epoch: 9 - Batch: 2108, Training Loss: 0.18196430288959498\n",
      "Epoch: 9 - Batch: 2109, Training Loss: 0.18205079378847458\n",
      "Epoch: 9 - Batch: 2110, Training Loss: 0.18213205505316926\n",
      "Epoch: 9 - Batch: 2111, Training Loss: 0.18221968493947935\n",
      "Epoch: 9 - Batch: 2112, Training Loss: 0.1823053969437902\n",
      "Epoch: 9 - Batch: 2113, Training Loss: 0.18238741464628708\n",
      "Epoch: 9 - Batch: 2114, Training Loss: 0.18247383945987591\n",
      "Epoch: 9 - Batch: 2115, Training Loss: 0.1825612001306382\n",
      "Epoch: 9 - Batch: 2116, Training Loss: 0.1826406179511824\n",
      "Epoch: 9 - Batch: 2117, Training Loss: 0.18272218504800133\n",
      "Epoch: 9 - Batch: 2118, Training Loss: 0.1828109781577516\n",
      "Epoch: 9 - Batch: 2119, Training Loss: 0.18289536992041625\n",
      "Epoch: 9 - Batch: 2120, Training Loss: 0.18298153701558043\n",
      "Epoch: 9 - Batch: 2121, Training Loss: 0.18307251074840972\n",
      "Epoch: 9 - Batch: 2122, Training Loss: 0.1831549015544837\n",
      "Epoch: 9 - Batch: 2123, Training Loss: 0.1832475705229821\n",
      "Epoch: 9 - Batch: 2124, Training Loss: 0.18333689078911028\n",
      "Epoch: 9 - Batch: 2125, Training Loss: 0.18341524057572162\n",
      "Epoch: 9 - Batch: 2126, Training Loss: 0.1835033777069492\n",
      "Epoch: 9 - Batch: 2127, Training Loss: 0.18358666425691314\n",
      "Epoch: 9 - Batch: 2128, Training Loss: 0.18367415697469838\n",
      "Epoch: 9 - Batch: 2129, Training Loss: 0.18376106682355525\n",
      "Epoch: 9 - Batch: 2130, Training Loss: 0.1838441380418553\n",
      "Epoch: 9 - Batch: 2131, Training Loss: 0.1839326697140685\n",
      "Epoch: 9 - Batch: 2132, Training Loss: 0.1840201681031912\n",
      "Epoch: 9 - Batch: 2133, Training Loss: 0.18410640153076321\n",
      "Epoch: 9 - Batch: 2134, Training Loss: 0.184191205095543\n",
      "Epoch: 9 - Batch: 2135, Training Loss: 0.18427571435003337\n",
      "Epoch: 9 - Batch: 2136, Training Loss: 0.18435343083111602\n",
      "Epoch: 9 - Batch: 2137, Training Loss: 0.18444051300708333\n",
      "Epoch: 9 - Batch: 2138, Training Loss: 0.18452094733764482\n",
      "Epoch: 9 - Batch: 2139, Training Loss: 0.18460622869098364\n",
      "Epoch: 9 - Batch: 2140, Training Loss: 0.18468425818294237\n",
      "Epoch: 9 - Batch: 2141, Training Loss: 0.184772214424452\n",
      "Epoch: 9 - Batch: 2142, Training Loss: 0.18485698653740273\n",
      "Epoch: 9 - Batch: 2143, Training Loss: 0.18494720427724062\n",
      "Epoch: 9 - Batch: 2144, Training Loss: 0.1850402755832692\n",
      "Epoch: 9 - Batch: 2145, Training Loss: 0.18512234519271312\n",
      "Epoch: 9 - Batch: 2146, Training Loss: 0.18520773497485798\n",
      "Epoch: 9 - Batch: 2147, Training Loss: 0.18528719183372622\n",
      "Epoch: 9 - Batch: 2148, Training Loss: 0.18537924667916092\n",
      "Epoch: 9 - Batch: 2149, Training Loss: 0.18546636518752002\n",
      "Epoch: 9 - Batch: 2150, Training Loss: 0.18554621270763538\n",
      "Epoch: 9 - Batch: 2151, Training Loss: 0.18563718913404106\n",
      "Epoch: 9 - Batch: 2152, Training Loss: 0.1857330756598642\n",
      "Epoch: 9 - Batch: 2153, Training Loss: 0.1858150884385528\n",
      "Epoch: 9 - Batch: 2154, Training Loss: 0.18589703858541218\n",
      "Epoch: 9 - Batch: 2155, Training Loss: 0.18598260280134074\n",
      "Epoch: 9 - Batch: 2156, Training Loss: 0.1860713082462994\n",
      "Epoch: 9 - Batch: 2157, Training Loss: 0.18615636758345672\n",
      "Epoch: 9 - Batch: 2158, Training Loss: 0.1862497970671895\n",
      "Epoch: 9 - Batch: 2159, Training Loss: 0.1863365339711432\n",
      "Epoch: 9 - Batch: 2160, Training Loss: 0.18642003491990405\n",
      "Epoch: 9 - Batch: 2161, Training Loss: 0.18650773874365079\n",
      "Epoch: 9 - Batch: 2162, Training Loss: 0.18659285510925708\n",
      "Epoch: 9 - Batch: 2163, Training Loss: 0.1866713994758738\n",
      "Epoch: 9 - Batch: 2164, Training Loss: 0.18675324427053505\n",
      "Epoch: 9 - Batch: 2165, Training Loss: 0.18683454008220043\n",
      "Epoch: 9 - Batch: 2166, Training Loss: 0.1869203857397954\n",
      "Epoch: 9 - Batch: 2167, Training Loss: 0.18700751743802976\n",
      "Epoch: 9 - Batch: 2168, Training Loss: 0.1870912000240378\n",
      "Epoch: 9 - Batch: 2169, Training Loss: 0.1871872608415523\n",
      "Epoch: 9 - Batch: 2170, Training Loss: 0.18727690639409852\n",
      "Epoch: 9 - Batch: 2171, Training Loss: 0.1873644611815216\n",
      "Epoch: 9 - Batch: 2172, Training Loss: 0.1874489325344266\n",
      "Epoch: 9 - Batch: 2173, Training Loss: 0.18753312520719878\n",
      "Epoch: 9 - Batch: 2174, Training Loss: 0.1876140320997926\n",
      "Epoch: 9 - Batch: 2175, Training Loss: 0.18769964032109895\n",
      "Epoch: 9 - Batch: 2176, Training Loss: 0.1877864485615818\n",
      "Epoch: 9 - Batch: 2177, Training Loss: 0.18787360723264776\n",
      "Epoch: 9 - Batch: 2178, Training Loss: 0.18794757984022595\n",
      "Epoch: 9 - Batch: 2179, Training Loss: 0.18804011261631204\n",
      "Epoch: 9 - Batch: 2180, Training Loss: 0.18812269436359208\n",
      "Epoch: 9 - Batch: 2181, Training Loss: 0.1882129064568042\n",
      "Epoch: 9 - Batch: 2182, Training Loss: 0.18829523578014343\n",
      "Epoch: 9 - Batch: 2183, Training Loss: 0.18838315990260782\n",
      "Epoch: 9 - Batch: 2184, Training Loss: 0.18846353120595266\n",
      "Epoch: 9 - Batch: 2185, Training Loss: 0.1885454191534377\n",
      "Epoch: 9 - Batch: 2186, Training Loss: 0.18863329589737587\n",
      "Epoch: 9 - Batch: 2187, Training Loss: 0.1887226134328601\n",
      "Epoch: 9 - Batch: 2188, Training Loss: 0.18881237936850212\n",
      "Epoch: 9 - Batch: 2189, Training Loss: 0.18889578802520363\n",
      "Epoch: 9 - Batch: 2190, Training Loss: 0.18897520074277968\n",
      "Epoch: 9 - Batch: 2191, Training Loss: 0.18905371163738505\n",
      "Epoch: 9 - Batch: 2192, Training Loss: 0.18914742793140324\n",
      "Epoch: 9 - Batch: 2193, Training Loss: 0.18922745399980204\n",
      "Epoch: 9 - Batch: 2194, Training Loss: 0.18930902093599486\n",
      "Epoch: 9 - Batch: 2195, Training Loss: 0.18939230848060515\n",
      "Epoch: 9 - Batch: 2196, Training Loss: 0.18947950190251345\n",
      "Epoch: 9 - Batch: 2197, Training Loss: 0.18956351247469386\n",
      "Epoch: 9 - Batch: 2198, Training Loss: 0.189650834626078\n",
      "Epoch: 9 - Batch: 2199, Training Loss: 0.18973602972319273\n",
      "Epoch: 9 - Batch: 2200, Training Loss: 0.18982193628279723\n",
      "Epoch: 9 - Batch: 2201, Training Loss: 0.18990048729686396\n",
      "Epoch: 9 - Batch: 2202, Training Loss: 0.18999437801848795\n",
      "Epoch: 9 - Batch: 2203, Training Loss: 0.19008080781766432\n",
      "Epoch: 9 - Batch: 2204, Training Loss: 0.19016418038909114\n",
      "Epoch: 9 - Batch: 2205, Training Loss: 0.1902493317094114\n",
      "Epoch: 9 - Batch: 2206, Training Loss: 0.19034507936777958\n",
      "Epoch: 9 - Batch: 2207, Training Loss: 0.19042443484067917\n",
      "Epoch: 9 - Batch: 2208, Training Loss: 0.19051288765782542\n",
      "Epoch: 9 - Batch: 2209, Training Loss: 0.1906009716156308\n",
      "Epoch: 9 - Batch: 2210, Training Loss: 0.1906849195003806\n",
      "Epoch: 9 - Batch: 2211, Training Loss: 0.19077376757515208\n",
      "Epoch: 9 - Batch: 2212, Training Loss: 0.19086829778641018\n",
      "Epoch: 9 - Batch: 2213, Training Loss: 0.19095795544769437\n",
      "Epoch: 9 - Batch: 2214, Training Loss: 0.19103780003947207\n",
      "Epoch: 9 - Batch: 2215, Training Loss: 0.19111993928033716\n",
      "Epoch: 9 - Batch: 2216, Training Loss: 0.19120871955853197\n",
      "Epoch: 9 - Batch: 2217, Training Loss: 0.19129114947088718\n",
      "Epoch: 9 - Batch: 2218, Training Loss: 0.19137321741229069\n",
      "Epoch: 9 - Batch: 2219, Training Loss: 0.19146228209755708\n",
      "Epoch: 9 - Batch: 2220, Training Loss: 0.19154949910061475\n",
      "Epoch: 9 - Batch: 2221, Training Loss: 0.19163719778432578\n",
      "Epoch: 9 - Batch: 2222, Training Loss: 0.19172459410172987\n",
      "Epoch: 9 - Batch: 2223, Training Loss: 0.1918130684551315\n",
      "Epoch: 9 - Batch: 2224, Training Loss: 0.19190039992283034\n",
      "Epoch: 9 - Batch: 2225, Training Loss: 0.19199359984095418\n",
      "Epoch: 9 - Batch: 2226, Training Loss: 0.19208066485463882\n",
      "Epoch: 9 - Batch: 2227, Training Loss: 0.19217813060700795\n",
      "Epoch: 9 - Batch: 2228, Training Loss: 0.1922580194203909\n",
      "Epoch: 9 - Batch: 2229, Training Loss: 0.19234370649058624\n",
      "Epoch: 9 - Batch: 2230, Training Loss: 0.19243551083959354\n",
      "Epoch: 9 - Batch: 2231, Training Loss: 0.19251870679025032\n",
      "Epoch: 9 - Batch: 2232, Training Loss: 0.19261091135158073\n",
      "Epoch: 9 - Batch: 2233, Training Loss: 0.19271160698006204\n",
      "Epoch: 9 - Batch: 2234, Training Loss: 0.19280387372223298\n",
      "Epoch: 9 - Batch: 2235, Training Loss: 0.1928867209783043\n",
      "Epoch: 9 - Batch: 2236, Training Loss: 0.19297987663157742\n",
      "Epoch: 9 - Batch: 2237, Training Loss: 0.19306297600269318\n",
      "Epoch: 9 - Batch: 2238, Training Loss: 0.19314582369739736\n",
      "Epoch: 9 - Batch: 2239, Training Loss: 0.19323092547691678\n",
      "Epoch: 9 - Batch: 2240, Training Loss: 0.19331383061399113\n",
      "Epoch: 9 - Batch: 2241, Training Loss: 0.1933971812228856\n",
      "Epoch: 9 - Batch: 2242, Training Loss: 0.1934789735607642\n",
      "Epoch: 9 - Batch: 2243, Training Loss: 0.19356437972677287\n",
      "Epoch: 9 - Batch: 2244, Training Loss: 0.193652898789836\n",
      "Epoch: 9 - Batch: 2245, Training Loss: 0.19373859948236155\n",
      "Epoch: 9 - Batch: 2246, Training Loss: 0.19382160284848354\n",
      "Epoch: 9 - Batch: 2247, Training Loss: 0.1939080696719796\n",
      "Epoch: 9 - Batch: 2248, Training Loss: 0.19399817316958165\n",
      "Epoch: 9 - Batch: 2249, Training Loss: 0.1940858998949057\n",
      "Epoch: 9 - Batch: 2250, Training Loss: 0.1941673571605291\n",
      "Epoch: 9 - Batch: 2251, Training Loss: 0.19425457379908903\n",
      "Epoch: 9 - Batch: 2252, Training Loss: 0.19433597124384014\n",
      "Epoch: 9 - Batch: 2253, Training Loss: 0.19442522606100412\n",
      "Epoch: 9 - Batch: 2254, Training Loss: 0.19450532589385758\n",
      "Epoch: 9 - Batch: 2255, Training Loss: 0.19459090236679435\n",
      "Epoch: 9 - Batch: 2256, Training Loss: 0.19467528258291247\n",
      "Epoch: 9 - Batch: 2257, Training Loss: 0.19475705897556014\n",
      "Epoch: 9 - Batch: 2258, Training Loss: 0.1948387432985539\n",
      "Epoch: 9 - Batch: 2259, Training Loss: 0.19492767280458811\n",
      "Epoch: 9 - Batch: 2260, Training Loss: 0.1950114114614664\n",
      "Epoch: 9 - Batch: 2261, Training Loss: 0.19509946687106866\n",
      "Epoch: 9 - Batch: 2262, Training Loss: 0.19518387844016896\n",
      "Epoch: 9 - Batch: 2263, Training Loss: 0.19526968969288552\n",
      "Epoch: 9 - Batch: 2264, Training Loss: 0.19535344772992244\n",
      "Epoch: 9 - Batch: 2265, Training Loss: 0.19543655478020214\n",
      "Epoch: 9 - Batch: 2266, Training Loss: 0.19552931196950563\n",
      "Epoch: 9 - Batch: 2267, Training Loss: 0.19562025103180564\n",
      "Epoch: 9 - Batch: 2268, Training Loss: 0.19570909830962446\n",
      "Epoch: 9 - Batch: 2269, Training Loss: 0.19579714546551555\n",
      "Epoch: 9 - Batch: 2270, Training Loss: 0.19588205653859014\n",
      "Epoch: 9 - Batch: 2271, Training Loss: 0.19597478231313217\n",
      "Epoch: 9 - Batch: 2272, Training Loss: 0.19606480839512438\n",
      "Epoch: 9 - Batch: 2273, Training Loss: 0.19615141611243558\n",
      "Epoch: 9 - Batch: 2274, Training Loss: 0.19623672067624814\n",
      "Epoch: 9 - Batch: 2275, Training Loss: 0.19632768020602206\n",
      "Epoch: 9 - Batch: 2276, Training Loss: 0.19642404542855957\n",
      "Epoch: 9 - Batch: 2277, Training Loss: 0.19651876188579878\n",
      "Epoch: 9 - Batch: 2278, Training Loss: 0.1966083715940095\n",
      "Epoch: 9 - Batch: 2279, Training Loss: 0.19668090175435712\n",
      "Epoch: 9 - Batch: 2280, Training Loss: 0.19676592359767228\n",
      "Epoch: 9 - Batch: 2281, Training Loss: 0.1968478377148583\n",
      "Epoch: 9 - Batch: 2282, Training Loss: 0.19692815378135314\n",
      "Epoch: 9 - Batch: 2283, Training Loss: 0.19702559331459785\n",
      "Epoch: 9 - Batch: 2284, Training Loss: 0.19710174629196006\n",
      "Epoch: 9 - Batch: 2285, Training Loss: 0.19719731112691893\n",
      "Epoch: 9 - Batch: 2286, Training Loss: 0.19727200655201774\n",
      "Epoch: 9 - Batch: 2287, Training Loss: 0.19736387087016755\n",
      "Epoch: 9 - Batch: 2288, Training Loss: 0.19744156655634615\n",
      "Epoch: 9 - Batch: 2289, Training Loss: 0.1975193056795332\n",
      "Epoch: 9 - Batch: 2290, Training Loss: 0.19761143647023102\n",
      "Epoch: 9 - Batch: 2291, Training Loss: 0.19769527391834837\n",
      "Epoch: 9 - Batch: 2292, Training Loss: 0.19778680470872478\n",
      "Epoch: 9 - Batch: 2293, Training Loss: 0.19787733306858077\n",
      "Epoch: 9 - Batch: 2294, Training Loss: 0.19795868800919053\n",
      "Epoch: 9 - Batch: 2295, Training Loss: 0.19803434404024042\n",
      "Epoch: 9 - Batch: 2296, Training Loss: 0.1981196966868629\n",
      "Epoch: 9 - Batch: 2297, Training Loss: 0.19820531343370923\n",
      "Epoch: 9 - Batch: 2298, Training Loss: 0.19829966522024242\n",
      "Epoch: 9 - Batch: 2299, Training Loss: 0.1983900103674797\n",
      "Epoch: 9 - Batch: 2300, Training Loss: 0.19848498532774042\n",
      "Epoch: 9 - Batch: 2301, Training Loss: 0.19856938321890918\n",
      "Epoch: 9 - Batch: 2302, Training Loss: 0.19865926084530294\n",
      "Epoch: 9 - Batch: 2303, Training Loss: 0.19875661399876499\n",
      "Epoch: 9 - Batch: 2304, Training Loss: 0.1988402694883829\n",
      "Epoch: 9 - Batch: 2305, Training Loss: 0.19893066307057194\n",
      "Epoch: 9 - Batch: 2306, Training Loss: 0.19901792389366954\n",
      "Epoch: 9 - Batch: 2307, Training Loss: 0.19909751736504916\n",
      "Epoch: 9 - Batch: 2308, Training Loss: 0.19917930434666462\n",
      "Epoch: 9 - Batch: 2309, Training Loss: 0.19927668635104823\n",
      "Epoch: 9 - Batch: 2310, Training Loss: 0.19936534436187933\n",
      "Epoch: 9 - Batch: 2311, Training Loss: 0.19945083974951733\n",
      "Epoch: 9 - Batch: 2312, Training Loss: 0.19953611981784725\n",
      "Epoch: 9 - Batch: 2313, Training Loss: 0.1996254703372865\n",
      "Epoch: 9 - Batch: 2314, Training Loss: 0.19971228243279615\n",
      "Epoch: 9 - Batch: 2315, Training Loss: 0.199801195771194\n",
      "Epoch: 9 - Batch: 2316, Training Loss: 0.1998864634466013\n",
      "Epoch: 9 - Batch: 2317, Training Loss: 0.19996530486032935\n",
      "Epoch: 9 - Batch: 2318, Training Loss: 0.2000526802036695\n",
      "Epoch: 9 - Batch: 2319, Training Loss: 0.20013309342425262\n",
      "Epoch: 9 - Batch: 2320, Training Loss: 0.20021196439318595\n",
      "Epoch: 9 - Batch: 2321, Training Loss: 0.2002989275452015\n",
      "Epoch: 9 - Batch: 2322, Training Loss: 0.20038543134977174\n",
      "Epoch: 9 - Batch: 2323, Training Loss: 0.20046612441811593\n",
      "Epoch: 9 - Batch: 2324, Training Loss: 0.20055297111619763\n",
      "Epoch: 9 - Batch: 2325, Training Loss: 0.20063593669193697\n",
      "Epoch: 9 - Batch: 2326, Training Loss: 0.20071969246167448\n",
      "Epoch: 9 - Batch: 2327, Training Loss: 0.2008048165435123\n",
      "Epoch: 9 - Batch: 2328, Training Loss: 0.20088185145709644\n",
      "Epoch: 9 - Batch: 2329, Training Loss: 0.20096482224082868\n",
      "Epoch: 9 - Batch: 2330, Training Loss: 0.2010434760432536\n",
      "Epoch: 9 - Batch: 2331, Training Loss: 0.2011370405517704\n",
      "Epoch: 9 - Batch: 2332, Training Loss: 0.2012300442033146\n",
      "Epoch: 9 - Batch: 2333, Training Loss: 0.2013195157421762\n",
      "Epoch: 9 - Batch: 2334, Training Loss: 0.20140584922128452\n",
      "Epoch: 9 - Batch: 2335, Training Loss: 0.2014974883601539\n",
      "Epoch: 9 - Batch: 2336, Training Loss: 0.20158272045069864\n",
      "Epoch: 9 - Batch: 2337, Training Loss: 0.20167026889956807\n",
      "Epoch: 9 - Batch: 2338, Training Loss: 0.20175199555372125\n",
      "Epoch: 9 - Batch: 2339, Training Loss: 0.20183290622102878\n",
      "Epoch: 9 - Batch: 2340, Training Loss: 0.2019198015085105\n",
      "Epoch: 9 - Batch: 2341, Training Loss: 0.20199779056593356\n",
      "Epoch: 9 - Batch: 2342, Training Loss: 0.20209034993799765\n",
      "Epoch: 9 - Batch: 2343, Training Loss: 0.20217747727461519\n",
      "Epoch: 9 - Batch: 2344, Training Loss: 0.20226631783702678\n",
      "Epoch: 9 - Batch: 2345, Training Loss: 0.20234606546005404\n",
      "Epoch: 9 - Batch: 2346, Training Loss: 0.20243724967487417\n",
      "Epoch: 9 - Batch: 2347, Training Loss: 0.20253433783231287\n",
      "Epoch: 9 - Batch: 2348, Training Loss: 0.20262424708969556\n",
      "Epoch: 9 - Batch: 2349, Training Loss: 0.2027141459002028\n",
      "Epoch: 9 - Batch: 2350, Training Loss: 0.20279938722057128\n",
      "Epoch: 9 - Batch: 2351, Training Loss: 0.20287794627829966\n",
      "Epoch: 9 - Batch: 2352, Training Loss: 0.20296790777623752\n",
      "Epoch: 9 - Batch: 2353, Training Loss: 0.2030481284431755\n",
      "Epoch: 9 - Batch: 2354, Training Loss: 0.20314023609135676\n",
      "Epoch: 9 - Batch: 2355, Training Loss: 0.2032201995032146\n",
      "Epoch: 9 - Batch: 2356, Training Loss: 0.20329721706300036\n",
      "Epoch: 9 - Batch: 2357, Training Loss: 0.2033875249079112\n",
      "Epoch: 9 - Batch: 2358, Training Loss: 0.20347425934687183\n",
      "Epoch: 9 - Batch: 2359, Training Loss: 0.20356250211448218\n",
      "Epoch: 9 - Batch: 2360, Training Loss: 0.20364446336925524\n",
      "Epoch: 9 - Batch: 2361, Training Loss: 0.20373064313534875\n",
      "Epoch: 9 - Batch: 2362, Training Loss: 0.20381613075115393\n",
      "Epoch: 9 - Batch: 2363, Training Loss: 0.20390486096342406\n",
      "Epoch: 9 - Batch: 2364, Training Loss: 0.20398946300420792\n",
      "Epoch: 9 - Batch: 2365, Training Loss: 0.20407261779751154\n",
      "Epoch: 9 - Batch: 2366, Training Loss: 0.20415282528693007\n",
      "Epoch: 9 - Batch: 2367, Training Loss: 0.2042449814441983\n",
      "Epoch: 9 - Batch: 2368, Training Loss: 0.2043364125969596\n",
      "Epoch: 9 - Batch: 2369, Training Loss: 0.20443244203060223\n",
      "Epoch: 9 - Batch: 2370, Training Loss: 0.2045209941393997\n",
      "Epoch: 9 - Batch: 2371, Training Loss: 0.2046088207865236\n",
      "Epoch: 9 - Batch: 2372, Training Loss: 0.20468890833765713\n",
      "Epoch: 9 - Batch: 2373, Training Loss: 0.20478215697491742\n",
      "Epoch: 9 - Batch: 2374, Training Loss: 0.2048790157273733\n",
      "Epoch: 9 - Batch: 2375, Training Loss: 0.204968049615671\n",
      "Epoch: 9 - Batch: 2376, Training Loss: 0.20505132985179303\n",
      "Epoch: 9 - Batch: 2377, Training Loss: 0.20514398351138702\n",
      "Epoch: 9 - Batch: 2378, Training Loss: 0.20523122736989563\n",
      "Epoch: 9 - Batch: 2379, Training Loss: 0.20530704469427738\n",
      "Epoch: 9 - Batch: 2380, Training Loss: 0.2053922965294785\n",
      "Epoch: 9 - Batch: 2381, Training Loss: 0.20547546506249292\n",
      "Epoch: 9 - Batch: 2382, Training Loss: 0.20556743316982515\n",
      "Epoch: 9 - Batch: 2383, Training Loss: 0.20565584654090416\n",
      "Epoch: 9 - Batch: 2384, Training Loss: 0.205738305637789\n",
      "Epoch: 9 - Batch: 2385, Training Loss: 0.2058285811288934\n",
      "Epoch: 9 - Batch: 2386, Training Loss: 0.2059128332333284\n",
      "Epoch: 9 - Batch: 2387, Training Loss: 0.20599372653448167\n",
      "Epoch: 9 - Batch: 2388, Training Loss: 0.20607198781610325\n",
      "Epoch: 9 - Batch: 2389, Training Loss: 0.2061658620797283\n",
      "Epoch: 9 - Batch: 2390, Training Loss: 0.20624805482144576\n",
      "Epoch: 9 - Batch: 2391, Training Loss: 0.20633718727857714\n",
      "Epoch: 9 - Batch: 2392, Training Loss: 0.20642637484899998\n",
      "Epoch: 9 - Batch: 2393, Training Loss: 0.20651570377040462\n",
      "Epoch: 9 - Batch: 2394, Training Loss: 0.20660726066870277\n",
      "Epoch: 9 - Batch: 2395, Training Loss: 0.20669254633423503\n",
      "Epoch: 9 - Batch: 2396, Training Loss: 0.20678012090956\n",
      "Epoch: 9 - Batch: 2397, Training Loss: 0.2068757122354721\n",
      "Epoch: 9 - Batch: 2398, Training Loss: 0.2069672170353074\n",
      "Epoch: 9 - Batch: 2399, Training Loss: 0.20705782039520357\n",
      "Epoch: 9 - Batch: 2400, Training Loss: 0.20715167669959328\n",
      "Epoch: 9 - Batch: 2401, Training Loss: 0.20723582248807348\n",
      "Epoch: 9 - Batch: 2402, Training Loss: 0.20732359394777078\n",
      "Epoch: 9 - Batch: 2403, Training Loss: 0.20740054701617108\n",
      "Epoch: 9 - Batch: 2404, Training Loss: 0.20748634617250555\n",
      "Epoch: 9 - Batch: 2405, Training Loss: 0.20757403896865165\n",
      "Epoch: 9 - Batch: 2406, Training Loss: 0.2076621515611511\n",
      "Epoch: 9 - Batch: 2407, Training Loss: 0.20774777271285974\n",
      "Epoch: 9 - Batch: 2408, Training Loss: 0.20784071507936291\n",
      "Epoch: 9 - Batch: 2409, Training Loss: 0.20792537494554844\n",
      "Epoch: 9 - Batch: 2410, Training Loss: 0.20801815009408725\n",
      "Epoch: 9 - Batch: 2411, Training Loss: 0.208101955627031\n",
      "Epoch: 9 - Batch: 2412, Training Loss: 0.2082034192027935\n",
      "Epoch 9 - Batch 2412, Training Loss: 0.2082034192027935, Validation Loss: 0.2081733136924345\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch: 10 - Batch: 1, Training Loss: 8.226138467021645e-05\n",
      "Epoch: 10 - Batch: 2, Training Loss: 0.00016766452967230952\n",
      "Epoch: 10 - Batch: 3, Training Loss: 0.00025272318107967155\n",
      "Epoch: 10 - Batch: 4, Training Loss: 0.0003310595924779155\n",
      "Epoch: 10 - Batch: 5, Training Loss: 0.00042985497076515334\n",
      "Epoch: 10 - Batch: 6, Training Loss: 0.0005200270866082478\n",
      "Epoch: 10 - Batch: 7, Training Loss: 0.0006009918602050636\n",
      "Epoch: 10 - Batch: 8, Training Loss: 0.0006910358272974764\n",
      "Epoch: 10 - Batch: 9, Training Loss: 0.0007723239474431001\n",
      "Epoch: 10 - Batch: 10, Training Loss: 0.0008596901350946569\n",
      "Epoch: 10 - Batch: 11, Training Loss: 0.000939150113816285\n",
      "Epoch: 10 - Batch: 12, Training Loss: 0.001022642864467295\n",
      "Epoch: 10 - Batch: 13, Training Loss: 0.001114314363321064\n",
      "Epoch: 10 - Batch: 14, Training Loss: 0.001200880151038146\n",
      "Epoch: 10 - Batch: 15, Training Loss: 0.0012922676999869433\n",
      "Epoch: 10 - Batch: 16, Training Loss: 0.0013863475403283563\n",
      "Epoch: 10 - Batch: 17, Training Loss: 0.001478612033733681\n",
      "Epoch: 10 - Batch: 18, Training Loss: 0.001563684776993731\n",
      "Epoch: 10 - Batch: 19, Training Loss: 0.001640217616585752\n",
      "Epoch: 10 - Batch: 20, Training Loss: 0.0017319542616852875\n",
      "Epoch: 10 - Batch: 21, Training Loss: 0.0018117018538229106\n",
      "Epoch: 10 - Batch: 22, Training Loss: 0.0018986179547424537\n",
      "Epoch: 10 - Batch: 23, Training Loss: 0.001987064871967926\n",
      "Epoch: 10 - Batch: 24, Training Loss: 0.0020631906664490105\n",
      "Epoch: 10 - Batch: 25, Training Loss: 0.0021631250630563766\n",
      "Epoch: 10 - Batch: 26, Training Loss: 0.00224774008366599\n",
      "Epoch: 10 - Batch: 27, Training Loss: 0.0023300973435934898\n",
      "Epoch: 10 - Batch: 28, Training Loss: 0.0024171599540643235\n",
      "Epoch: 10 - Batch: 29, Training Loss: 0.0025072274899502495\n",
      "Epoch: 10 - Batch: 30, Training Loss: 0.0025963034561123223\n",
      "Epoch: 10 - Batch: 31, Training Loss: 0.002689131691582365\n",
      "Epoch: 10 - Batch: 32, Training Loss: 0.002775018216167912\n",
      "Epoch: 10 - Batch: 33, Training Loss: 0.002852382431823025\n",
      "Epoch: 10 - Batch: 34, Training Loss: 0.0029424855958169968\n",
      "Epoch: 10 - Batch: 35, Training Loss: 0.0030258128583233552\n",
      "Epoch: 10 - Batch: 36, Training Loss: 0.003105804880163563\n",
      "Epoch: 10 - Batch: 37, Training Loss: 0.003195703635069466\n",
      "Epoch: 10 - Batch: 38, Training Loss: 0.003275345757628357\n",
      "Epoch: 10 - Batch: 39, Training Loss: 0.003357094652320615\n",
      "Epoch: 10 - Batch: 40, Training Loss: 0.0034370679797521278\n",
      "Epoch: 10 - Batch: 41, Training Loss: 0.003519255482587055\n",
      "Epoch: 10 - Batch: 42, Training Loss: 0.0036060066760871343\n",
      "Epoch: 10 - Batch: 43, Training Loss: 0.003690290498002054\n",
      "Epoch: 10 - Batch: 44, Training Loss: 0.0037697500875142477\n",
      "Epoch: 10 - Batch: 45, Training Loss: 0.003855352365654302\n",
      "Epoch: 10 - Batch: 46, Training Loss: 0.003940267559406571\n",
      "Epoch: 10 - Batch: 47, Training Loss: 0.0040263190593687855\n",
      "Epoch: 10 - Batch: 48, Training Loss: 0.004104157832526844\n",
      "Epoch: 10 - Batch: 49, Training Loss: 0.004193397933867441\n",
      "Epoch: 10 - Batch: 50, Training Loss: 0.004279801964908097\n",
      "Epoch: 10 - Batch: 51, Training Loss: 0.004370277497305799\n",
      "Epoch: 10 - Batch: 52, Training Loss: 0.0044538312735249155\n",
      "Epoch: 10 - Batch: 53, Training Loss: 0.004538044685599815\n",
      "Epoch: 10 - Batch: 54, Training Loss: 0.004622867852943652\n",
      "Epoch: 10 - Batch: 55, Training Loss: 0.004702884864500704\n",
      "Epoch: 10 - Batch: 56, Training Loss: 0.00479314906235358\n",
      "Epoch: 10 - Batch: 57, Training Loss: 0.004881554661599756\n",
      "Epoch: 10 - Batch: 58, Training Loss: 0.004970227647727204\n",
      "Epoch: 10 - Batch: 59, Training Loss: 0.005054668209841398\n",
      "Epoch: 10 - Batch: 60, Training Loss: 0.005136359544782892\n",
      "Epoch: 10 - Batch: 61, Training Loss: 0.005223297707081632\n",
      "Epoch: 10 - Batch: 62, Training Loss: 0.0052971409488574385\n",
      "Epoch: 10 - Batch: 63, Training Loss: 0.005385615969475229\n",
      "Epoch: 10 - Batch: 64, Training Loss: 0.005480079391158833\n",
      "Epoch: 10 - Batch: 65, Training Loss: 0.005560612037258955\n",
      "Epoch: 10 - Batch: 66, Training Loss: 0.005645084854332764\n",
      "Epoch: 10 - Batch: 67, Training Loss: 0.005721236058629765\n",
      "Epoch: 10 - Batch: 68, Training Loss: 0.005816338438645722\n",
      "Epoch: 10 - Batch: 69, Training Loss: 0.00589816855455117\n",
      "Epoch: 10 - Batch: 70, Training Loss: 0.005981801142591742\n",
      "Epoch: 10 - Batch: 71, Training Loss: 0.006071665356704845\n",
      "Epoch: 10 - Batch: 72, Training Loss: 0.006157391650562065\n",
      "Epoch: 10 - Batch: 73, Training Loss: 0.006244943701163255\n",
      "Epoch: 10 - Batch: 74, Training Loss: 0.006323993416073706\n",
      "Epoch: 10 - Batch: 75, Training Loss: 0.006407420279127646\n",
      "Epoch: 10 - Batch: 76, Training Loss: 0.0064906854534623635\n",
      "Epoch: 10 - Batch: 77, Training Loss: 0.0065761191174264376\n",
      "Epoch: 10 - Batch: 78, Training Loss: 0.006663870869534921\n",
      "Epoch: 10 - Batch: 79, Training Loss: 0.00675273721861602\n",
      "Epoch: 10 - Batch: 80, Training Loss: 0.006834959651453183\n",
      "Epoch: 10 - Batch: 81, Training Loss: 0.006920808941669528\n",
      "Epoch: 10 - Batch: 82, Training Loss: 0.007007267492920605\n",
      "Epoch: 10 - Batch: 83, Training Loss: 0.007093854081719669\n",
      "Epoch: 10 - Batch: 84, Training Loss: 0.007178714106579128\n",
      "Epoch: 10 - Batch: 85, Training Loss: 0.007270241412604428\n",
      "Epoch: 10 - Batch: 86, Training Loss: 0.007350652952791249\n",
      "Epoch: 10 - Batch: 87, Training Loss: 0.007432342372575209\n",
      "Epoch: 10 - Batch: 88, Training Loss: 0.00750973799063594\n",
      "Epoch: 10 - Batch: 89, Training Loss: 0.007594027595091024\n",
      "Epoch: 10 - Batch: 90, Training Loss: 0.007680153829442525\n",
      "Epoch: 10 - Batch: 91, Training Loss: 0.007774698793591552\n",
      "Epoch: 10 - Batch: 92, Training Loss: 0.007864649078590953\n",
      "Epoch: 10 - Batch: 93, Training Loss: 0.007949204760206675\n",
      "Epoch: 10 - Batch: 94, Training Loss: 0.008035367394907162\n",
      "Epoch: 10 - Batch: 95, Training Loss: 0.00813265658828552\n",
      "Epoch: 10 - Batch: 96, Training Loss: 0.008217460943840037\n",
      "Epoch: 10 - Batch: 97, Training Loss: 0.00831155901524558\n",
      "Epoch: 10 - Batch: 98, Training Loss: 0.008402222023032\n",
      "Epoch: 10 - Batch: 99, Training Loss: 0.00848433946199085\n",
      "Epoch: 10 - Batch: 100, Training Loss: 0.00857208724786986\n",
      "Epoch: 10 - Batch: 101, Training Loss: 0.008652135457960923\n",
      "Epoch: 10 - Batch: 102, Training Loss: 0.008735218686152058\n",
      "Epoch: 10 - Batch: 103, Training Loss: 0.008831156766780375\n",
      "Epoch: 10 - Batch: 104, Training Loss: 0.008920441392444654\n",
      "Epoch: 10 - Batch: 105, Training Loss: 0.009003128016222373\n",
      "Epoch: 10 - Batch: 106, Training Loss: 0.009098644132044777\n",
      "Epoch: 10 - Batch: 107, Training Loss: 0.009185936382259698\n",
      "Epoch: 10 - Batch: 108, Training Loss: 0.00926260731088779\n",
      "Epoch: 10 - Batch: 109, Training Loss: 0.009347960705039514\n",
      "Epoch: 10 - Batch: 110, Training Loss: 0.009430526210548075\n",
      "Epoch: 10 - Batch: 111, Training Loss: 0.009522869441639725\n",
      "Epoch: 10 - Batch: 112, Training Loss: 0.009618211654980186\n",
      "Epoch: 10 - Batch: 113, Training Loss: 0.009703885776239445\n",
      "Epoch: 10 - Batch: 114, Training Loss: 0.009786882982967702\n",
      "Epoch: 10 - Batch: 115, Training Loss: 0.009877335230756557\n",
      "Epoch: 10 - Batch: 116, Training Loss: 0.009961514590094932\n",
      "Epoch: 10 - Batch: 117, Training Loss: 0.010062855657554583\n",
      "Epoch: 10 - Batch: 118, Training Loss: 0.010149814250259652\n",
      "Epoch: 10 - Batch: 119, Training Loss: 0.01024838521532949\n",
      "Epoch: 10 - Batch: 120, Training Loss: 0.010342549007528061\n",
      "Epoch: 10 - Batch: 121, Training Loss: 0.010425647742317289\n",
      "Epoch: 10 - Batch: 122, Training Loss: 0.010507846130660517\n",
      "Epoch: 10 - Batch: 123, Training Loss: 0.010601672509169302\n",
      "Epoch: 10 - Batch: 124, Training Loss: 0.010687848654997289\n",
      "Epoch: 10 - Batch: 125, Training Loss: 0.010775924532073449\n",
      "Epoch: 10 - Batch: 126, Training Loss: 0.0108664771464729\n",
      "Epoch: 10 - Batch: 127, Training Loss: 0.010957382434043126\n",
      "Epoch: 10 - Batch: 128, Training Loss: 0.011040065684672414\n",
      "Epoch: 10 - Batch: 129, Training Loss: 0.011120138068993887\n",
      "Epoch: 10 - Batch: 130, Training Loss: 0.01120017556038069\n",
      "Epoch: 10 - Batch: 131, Training Loss: 0.011290476875222144\n",
      "Epoch: 10 - Batch: 132, Training Loss: 0.011374655789749737\n",
      "Epoch: 10 - Batch: 133, Training Loss: 0.011452075143408024\n",
      "Epoch: 10 - Batch: 134, Training Loss: 0.011532042552384967\n",
      "Epoch: 10 - Batch: 135, Training Loss: 0.011614717240406703\n",
      "Epoch: 10 - Batch: 136, Training Loss: 0.01169691778484664\n",
      "Epoch: 10 - Batch: 137, Training Loss: 0.011791265784310266\n",
      "Epoch: 10 - Batch: 138, Training Loss: 0.011866856990366631\n",
      "Epoch: 10 - Batch: 139, Training Loss: 0.01195277685104911\n",
      "Epoch: 10 - Batch: 140, Training Loss: 0.012047838042822247\n",
      "Epoch: 10 - Batch: 141, Training Loss: 0.012134928849354312\n",
      "Epoch: 10 - Batch: 142, Training Loss: 0.012221204237410086\n",
      "Epoch: 10 - Batch: 143, Training Loss: 0.012299935789052922\n",
      "Epoch: 10 - Batch: 144, Training Loss: 0.01238989994349962\n",
      "Epoch: 10 - Batch: 145, Training Loss: 0.012479710100746867\n",
      "Epoch: 10 - Batch: 146, Training Loss: 0.012569598711495772\n",
      "Epoch: 10 - Batch: 147, Training Loss: 0.012656263753796493\n",
      "Epoch: 10 - Batch: 148, Training Loss: 0.012747635851748547\n",
      "Epoch: 10 - Batch: 149, Training Loss: 0.012834632241014225\n",
      "Epoch: 10 - Batch: 150, Training Loss: 0.012921412526375025\n",
      "Epoch: 10 - Batch: 151, Training Loss: 0.013009571817828648\n",
      "Epoch: 10 - Batch: 152, Training Loss: 0.013089372632811912\n",
      "Epoch: 10 - Batch: 153, Training Loss: 0.013174983770100039\n",
      "Epoch: 10 - Batch: 154, Training Loss: 0.013263761719216161\n",
      "Epoch: 10 - Batch: 155, Training Loss: 0.0133461633366633\n",
      "Epoch: 10 - Batch: 156, Training Loss: 0.013432687324522739\n",
      "Epoch: 10 - Batch: 157, Training Loss: 0.013519757898342154\n",
      "Epoch: 10 - Batch: 158, Training Loss: 0.013601644431081776\n",
      "Epoch: 10 - Batch: 159, Training Loss: 0.013691089707573453\n",
      "Epoch: 10 - Batch: 160, Training Loss: 0.013775235471341938\n",
      "Epoch: 10 - Batch: 161, Training Loss: 0.013859054317719506\n",
      "Epoch: 10 - Batch: 162, Training Loss: 0.013946974233015259\n",
      "Epoch: 10 - Batch: 163, Training Loss: 0.014032745143865075\n",
      "Epoch: 10 - Batch: 164, Training Loss: 0.014119577039632434\n",
      "Epoch: 10 - Batch: 165, Training Loss: 0.014204183991868697\n",
      "Epoch: 10 - Batch: 166, Training Loss: 0.014283927549819646\n",
      "Epoch: 10 - Batch: 167, Training Loss: 0.014369352268144661\n",
      "Epoch: 10 - Batch: 168, Training Loss: 0.014446050392009725\n",
      "Epoch: 10 - Batch: 169, Training Loss: 0.014532336622328307\n",
      "Epoch: 10 - Batch: 170, Training Loss: 0.014614209915769238\n",
      "Epoch: 10 - Batch: 171, Training Loss: 0.014703526691368365\n",
      "Epoch: 10 - Batch: 172, Training Loss: 0.01479617901653002\n",
      "Epoch: 10 - Batch: 173, Training Loss: 0.014884721487760544\n",
      "Epoch: 10 - Batch: 174, Training Loss: 0.014955608282318558\n",
      "Epoch: 10 - Batch: 175, Training Loss: 0.01505092534531606\n",
      "Epoch: 10 - Batch: 176, Training Loss: 0.015140134544663168\n",
      "Epoch: 10 - Batch: 177, Training Loss: 0.015225680628374441\n",
      "Epoch: 10 - Batch: 178, Training Loss: 0.015309565325281512\n",
      "Epoch: 10 - Batch: 179, Training Loss: 0.015390002262928394\n",
      "Epoch: 10 - Batch: 180, Training Loss: 0.015479034446118088\n",
      "Epoch: 10 - Batch: 181, Training Loss: 0.015563487314663914\n",
      "Epoch: 10 - Batch: 182, Training Loss: 0.015656095381152767\n",
      "Epoch: 10 - Batch: 183, Training Loss: 0.015737911992108645\n",
      "Epoch: 10 - Batch: 184, Training Loss: 0.015822797686256974\n",
      "Epoch: 10 - Batch: 185, Training Loss: 0.015910471516165568\n",
      "Epoch: 10 - Batch: 186, Training Loss: 0.01599924683966249\n",
      "Epoch: 10 - Batch: 187, Training Loss: 0.016086142436040576\n",
      "Epoch: 10 - Batch: 188, Training Loss: 0.016171854767781584\n",
      "Epoch: 10 - Batch: 189, Training Loss: 0.016263919238427384\n",
      "Epoch: 10 - Batch: 190, Training Loss: 0.016350363055321313\n",
      "Epoch: 10 - Batch: 191, Training Loss: 0.016447219972932716\n",
      "Epoch: 10 - Batch: 192, Training Loss: 0.01653893478523637\n",
      "Epoch: 10 - Batch: 193, Training Loss: 0.016626683695498193\n",
      "Epoch: 10 - Batch: 194, Training Loss: 0.016714262496525575\n",
      "Epoch: 10 - Batch: 195, Training Loss: 0.016795291267273634\n",
      "Epoch: 10 - Batch: 196, Training Loss: 0.01688037454390012\n",
      "Epoch: 10 - Batch: 197, Training Loss: 0.01696422972266947\n",
      "Epoch: 10 - Batch: 198, Training Loss: 0.017051391482699175\n",
      "Epoch: 10 - Batch: 199, Training Loss: 0.017137160015046893\n",
      "Epoch: 10 - Batch: 200, Training Loss: 0.017223537382152345\n",
      "Epoch: 10 - Batch: 201, Training Loss: 0.01731009798658823\n",
      "Epoch: 10 - Batch: 202, Training Loss: 0.017391952122275903\n",
      "Epoch: 10 - Batch: 203, Training Loss: 0.017469898267814372\n",
      "Epoch: 10 - Batch: 204, Training Loss: 0.01756059710140252\n",
      "Epoch: 10 - Batch: 205, Training Loss: 0.017643261923273996\n",
      "Epoch: 10 - Batch: 206, Training Loss: 0.017733487552325327\n",
      "Epoch: 10 - Batch: 207, Training Loss: 0.017823079288944874\n",
      "Epoch: 10 - Batch: 208, Training Loss: 0.01791426496175589\n",
      "Epoch: 10 - Batch: 209, Training Loss: 0.018012464478933198\n",
      "Epoch: 10 - Batch: 210, Training Loss: 0.018100366051962125\n",
      "Epoch: 10 - Batch: 211, Training Loss: 0.01819071797638588\n",
      "Epoch: 10 - Batch: 212, Training Loss: 0.018275435464101448\n",
      "Epoch: 10 - Batch: 213, Training Loss: 0.018359128954151573\n",
      "Epoch: 10 - Batch: 214, Training Loss: 0.01845245183774488\n",
      "Epoch: 10 - Batch: 215, Training Loss: 0.018547762698103144\n",
      "Epoch: 10 - Batch: 216, Training Loss: 0.018634453144041857\n",
      "Epoch: 10 - Batch: 217, Training Loss: 0.01872162055069732\n",
      "Epoch: 10 - Batch: 218, Training Loss: 0.01880932313885855\n",
      "Epoch: 10 - Batch: 219, Training Loss: 0.018897907409946717\n",
      "Epoch: 10 - Batch: 220, Training Loss: 0.01898606460090498\n",
      "Epoch: 10 - Batch: 221, Training Loss: 0.019074574452678166\n",
      "Epoch: 10 - Batch: 222, Training Loss: 0.01916094252200269\n",
      "Epoch: 10 - Batch: 223, Training Loss: 0.019255226777610098\n",
      "Epoch: 10 - Batch: 224, Training Loss: 0.019337211594998738\n",
      "Epoch: 10 - Batch: 225, Training Loss: 0.01943086129713612\n",
      "Epoch: 10 - Batch: 226, Training Loss: 0.019520913400559087\n",
      "Epoch: 10 - Batch: 227, Training Loss: 0.01959748499429048\n",
      "Epoch: 10 - Batch: 228, Training Loss: 0.019679920689185854\n",
      "Epoch: 10 - Batch: 229, Training Loss: 0.019767707463965487\n",
      "Epoch: 10 - Batch: 230, Training Loss: 0.019849771661544915\n",
      "Epoch: 10 - Batch: 231, Training Loss: 0.019946660797048366\n",
      "Epoch: 10 - Batch: 232, Training Loss: 0.02003417595306637\n",
      "Epoch: 10 - Batch: 233, Training Loss: 0.02011754356979533\n",
      "Epoch: 10 - Batch: 234, Training Loss: 0.020206489972461317\n",
      "Epoch: 10 - Batch: 235, Training Loss: 0.02030417549842428\n",
      "Epoch: 10 - Batch: 236, Training Loss: 0.020386556098561975\n",
      "Epoch: 10 - Batch: 237, Training Loss: 0.02047035739344744\n",
      "Epoch: 10 - Batch: 238, Training Loss: 0.020554223865468308\n",
      "Epoch: 10 - Batch: 239, Training Loss: 0.02064233455516608\n",
      "Epoch: 10 - Batch: 240, Training Loss: 0.020723542720573657\n",
      "Epoch: 10 - Batch: 241, Training Loss: 0.02080955757924771\n",
      "Epoch: 10 - Batch: 242, Training Loss: 0.020901459650712623\n",
      "Epoch: 10 - Batch: 243, Training Loss: 0.020989704012228286\n",
      "Epoch: 10 - Batch: 244, Training Loss: 0.021073666859918565\n",
      "Epoch: 10 - Batch: 245, Training Loss: 0.021152541189644467\n",
      "Epoch: 10 - Batch: 246, Training Loss: 0.0212381914023242\n",
      "Epoch: 10 - Batch: 247, Training Loss: 0.02132584031047315\n",
      "Epoch: 10 - Batch: 248, Training Loss: 0.021410984970987534\n",
      "Epoch: 10 - Batch: 249, Training Loss: 0.02148973728046686\n",
      "Epoch: 10 - Batch: 250, Training Loss: 0.02157812540853399\n",
      "Epoch: 10 - Batch: 251, Training Loss: 0.02166926808296942\n",
      "Epoch: 10 - Batch: 252, Training Loss: 0.021755446761747694\n",
      "Epoch: 10 - Batch: 253, Training Loss: 0.02183970325251124\n",
      "Epoch: 10 - Batch: 254, Training Loss: 0.021926956340843566\n",
      "Epoch: 10 - Batch: 255, Training Loss: 0.022016299817445465\n",
      "Epoch: 10 - Batch: 256, Training Loss: 0.02210227476052977\n",
      "Epoch: 10 - Batch: 257, Training Loss: 0.022184913147049363\n",
      "Epoch: 10 - Batch: 258, Training Loss: 0.02227273476534024\n",
      "Epoch: 10 - Batch: 259, Training Loss: 0.022365630785031104\n",
      "Epoch: 10 - Batch: 260, Training Loss: 0.02245051564515922\n",
      "Epoch: 10 - Batch: 261, Training Loss: 0.02253258922701056\n",
      "Epoch: 10 - Batch: 262, Training Loss: 0.022615646001316027\n",
      "Epoch: 10 - Batch: 263, Training Loss: 0.022704448235865256\n",
      "Epoch: 10 - Batch: 264, Training Loss: 0.022783351311487936\n",
      "Epoch: 10 - Batch: 265, Training Loss: 0.022862979132144606\n",
      "Epoch: 10 - Batch: 266, Training Loss: 0.02295176853513836\n",
      "Epoch: 10 - Batch: 267, Training Loss: 0.023051335196787644\n",
      "Epoch: 10 - Batch: 268, Training Loss: 0.02314116103361495\n",
      "Epoch: 10 - Batch: 269, Training Loss: 0.023227262753950026\n",
      "Epoch: 10 - Batch: 270, Training Loss: 0.02330440180885851\n",
      "Epoch: 10 - Batch: 271, Training Loss: 0.023390123450736305\n",
      "Epoch: 10 - Batch: 272, Training Loss: 0.023484035232915215\n",
      "Epoch: 10 - Batch: 273, Training Loss: 0.023574200991670884\n",
      "Epoch: 10 - Batch: 274, Training Loss: 0.023663679678394625\n",
      "Epoch: 10 - Batch: 275, Training Loss: 0.023754387649137582\n",
      "Epoch: 10 - Batch: 276, Training Loss: 0.023851192134678068\n",
      "Epoch: 10 - Batch: 277, Training Loss: 0.023940034741983682\n",
      "Epoch: 10 - Batch: 278, Training Loss: 0.024022676816726006\n",
      "Epoch: 10 - Batch: 279, Training Loss: 0.024111933394599317\n",
      "Epoch: 10 - Batch: 280, Training Loss: 0.02420010914405187\n",
      "Epoch: 10 - Batch: 281, Training Loss: 0.024278665644118243\n",
      "Epoch: 10 - Batch: 282, Training Loss: 0.02436371290342725\n",
      "Epoch: 10 - Batch: 283, Training Loss: 0.02445221012751657\n",
      "Epoch: 10 - Batch: 284, Training Loss: 0.024543977038283055\n",
      "Epoch: 10 - Batch: 285, Training Loss: 0.024631864803643964\n",
      "Epoch: 10 - Batch: 286, Training Loss: 0.024717062668470204\n",
      "Epoch: 10 - Batch: 287, Training Loss: 0.024798036103521413\n",
      "Epoch: 10 - Batch: 288, Training Loss: 0.02488130779556967\n",
      "Epoch: 10 - Batch: 289, Training Loss: 0.024973201143800916\n",
      "Epoch: 10 - Batch: 290, Training Loss: 0.025061886590808186\n",
      "Epoch: 10 - Batch: 291, Training Loss: 0.025147123432179192\n",
      "Epoch: 10 - Batch: 292, Training Loss: 0.025227747147108983\n",
      "Epoch: 10 - Batch: 293, Training Loss: 0.025317666839011273\n",
      "Epoch: 10 - Batch: 294, Training Loss: 0.025401486883906782\n",
      "Epoch: 10 - Batch: 295, Training Loss: 0.025483199897195965\n",
      "Epoch: 10 - Batch: 296, Training Loss: 0.025571448632684315\n",
      "Epoch: 10 - Batch: 297, Training Loss: 0.02565501937349242\n",
      "Epoch: 10 - Batch: 298, Training Loss: 0.02573213391438448\n",
      "Epoch: 10 - Batch: 299, Training Loss: 0.025811198147424615\n",
      "Epoch: 10 - Batch: 300, Training Loss: 0.025897761809934628\n",
      "Epoch: 10 - Batch: 301, Training Loss: 0.025985647097946596\n",
      "Epoch: 10 - Batch: 302, Training Loss: 0.026067498799530825\n",
      "Epoch: 10 - Batch: 303, Training Loss: 0.026150798156338546\n",
      "Epoch: 10 - Batch: 304, Training Loss: 0.026235784383901513\n",
      "Epoch: 10 - Batch: 305, Training Loss: 0.026313688068544092\n",
      "Epoch: 10 - Batch: 306, Training Loss: 0.026394590636588645\n",
      "Epoch: 10 - Batch: 307, Training Loss: 0.026482244042268836\n",
      "Epoch: 10 - Batch: 308, Training Loss: 0.026568936662838036\n",
      "Epoch: 10 - Batch: 309, Training Loss: 0.02665070422722728\n",
      "Epoch: 10 - Batch: 310, Training Loss: 0.026742295993748392\n",
      "Epoch: 10 - Batch: 311, Training Loss: 0.026829093435213933\n",
      "Epoch: 10 - Batch: 312, Training Loss: 0.026914774574598864\n",
      "Epoch: 10 - Batch: 313, Training Loss: 0.02700352910936966\n",
      "Epoch: 10 - Batch: 314, Training Loss: 0.02709142528906786\n",
      "Epoch: 10 - Batch: 315, Training Loss: 0.027174396591745996\n",
      "Epoch: 10 - Batch: 316, Training Loss: 0.027261826603566828\n",
      "Epoch: 10 - Batch: 317, Training Loss: 0.027349387883991744\n",
      "Epoch: 10 - Batch: 318, Training Loss: 0.027443215627822513\n",
      "Epoch: 10 - Batch: 319, Training Loss: 0.02752141508074542\n",
      "Epoch: 10 - Batch: 320, Training Loss: 0.02761424762223095\n",
      "Epoch: 10 - Batch: 321, Training Loss: 0.02770429859509318\n",
      "Epoch: 10 - Batch: 322, Training Loss: 0.02778657485621288\n",
      "Epoch: 10 - Batch: 323, Training Loss: 0.027873482326567667\n",
      "Epoch: 10 - Batch: 324, Training Loss: 0.027951020222299332\n",
      "Epoch: 10 - Batch: 325, Training Loss: 0.028028714370173995\n",
      "Epoch: 10 - Batch: 326, Training Loss: 0.02811552232029426\n",
      "Epoch: 10 - Batch: 327, Training Loss: 0.02820354469651804\n",
      "Epoch: 10 - Batch: 328, Training Loss: 0.02828697861476522\n",
      "Epoch: 10 - Batch: 329, Training Loss: 0.028366064785626\n",
      "Epoch: 10 - Batch: 330, Training Loss: 0.02846424310015604\n",
      "Epoch: 10 - Batch: 331, Training Loss: 0.028544735799776778\n",
      "Epoch: 10 - Batch: 332, Training Loss: 0.02862730959606408\n",
      "Epoch: 10 - Batch: 333, Training Loss: 0.02871667918676563\n",
      "Epoch: 10 - Batch: 334, Training Loss: 0.02880697259385985\n",
      "Epoch: 10 - Batch: 335, Training Loss: 0.028883271524759866\n",
      "Epoch: 10 - Batch: 336, Training Loss: 0.02897130138863181\n",
      "Epoch: 10 - Batch: 337, Training Loss: 0.02905077160417935\n",
      "Epoch: 10 - Batch: 338, Training Loss: 0.029144814593183067\n",
      "Epoch: 10 - Batch: 339, Training Loss: 0.02923123258633993\n",
      "Epoch: 10 - Batch: 340, Training Loss: 0.029323262213474484\n",
      "Epoch: 10 - Batch: 341, Training Loss: 0.029409532986618393\n",
      "Epoch: 10 - Batch: 342, Training Loss: 0.029494469759871512\n",
      "Epoch: 10 - Batch: 343, Training Loss: 0.029579288367904834\n",
      "Epoch: 10 - Batch: 344, Training Loss: 0.029666665002431838\n",
      "Epoch: 10 - Batch: 345, Training Loss: 0.02974355786692839\n",
      "Epoch: 10 - Batch: 346, Training Loss: 0.02982680096135013\n",
      "Epoch: 10 - Batch: 347, Training Loss: 0.02991973622323664\n",
      "Epoch: 10 - Batch: 348, Training Loss: 0.030008632559977953\n",
      "Epoch: 10 - Batch: 349, Training Loss: 0.030095634484666695\n",
      "Epoch: 10 - Batch: 350, Training Loss: 0.030168827561349615\n",
      "Epoch: 10 - Batch: 351, Training Loss: 0.03025004344298867\n",
      "Epoch: 10 - Batch: 352, Training Loss: 0.030342325802960403\n",
      "Epoch: 10 - Batch: 353, Training Loss: 0.030415748531743267\n",
      "Epoch: 10 - Batch: 354, Training Loss: 0.030504868935738037\n",
      "Epoch: 10 - Batch: 355, Training Loss: 0.03059383463207169\n",
      "Epoch: 10 - Batch: 356, Training Loss: 0.030680913842633786\n",
      "Epoch: 10 - Batch: 357, Training Loss: 0.030770745406399912\n",
      "Epoch: 10 - Batch: 358, Training Loss: 0.030856900151886947\n",
      "Epoch: 10 - Batch: 359, Training Loss: 0.030938415490029066\n",
      "Epoch: 10 - Batch: 360, Training Loss: 0.03101841849004649\n",
      "Epoch: 10 - Batch: 361, Training Loss: 0.031109136561700952\n",
      "Epoch: 10 - Batch: 362, Training Loss: 0.031193886940702673\n",
      "Epoch: 10 - Batch: 363, Training Loss: 0.031278992748230844\n",
      "Epoch: 10 - Batch: 364, Training Loss: 0.031362472129846684\n",
      "Epoch: 10 - Batch: 365, Training Loss: 0.03144590268112336\n",
      "Epoch: 10 - Batch: 366, Training Loss: 0.03154163479582587\n",
      "Epoch: 10 - Batch: 367, Training Loss: 0.03162810908225836\n",
      "Epoch: 10 - Batch: 368, Training Loss: 0.031709141893371026\n",
      "Epoch: 10 - Batch: 369, Training Loss: 0.031797712239410546\n",
      "Epoch: 10 - Batch: 370, Training Loss: 0.03188874781947231\n",
      "Epoch: 10 - Batch: 371, Training Loss: 0.031982147222875956\n",
      "Epoch: 10 - Batch: 372, Training Loss: 0.03206206464006335\n",
      "Epoch: 10 - Batch: 373, Training Loss: 0.03214429777307099\n",
      "Epoch: 10 - Batch: 374, Training Loss: 0.03223083182477437\n",
      "Epoch: 10 - Batch: 375, Training Loss: 0.03231841147835575\n",
      "Epoch: 10 - Batch: 376, Training Loss: 0.032409246627914765\n",
      "Epoch: 10 - Batch: 377, Training Loss: 0.03249298548248672\n",
      "Epoch: 10 - Batch: 378, Training Loss: 0.032587615664968046\n",
      "Epoch: 10 - Batch: 379, Training Loss: 0.03268031396008843\n",
      "Epoch: 10 - Batch: 380, Training Loss: 0.03277051451293193\n",
      "Epoch: 10 - Batch: 381, Training Loss: 0.03286230737099402\n",
      "Epoch: 10 - Batch: 382, Training Loss: 0.0329542861908526\n",
      "Epoch: 10 - Batch: 383, Training Loss: 0.0330372653642104\n",
      "Epoch: 10 - Batch: 384, Training Loss: 0.03313173900418614\n",
      "Epoch: 10 - Batch: 385, Training Loss: 0.03322137699494908\n",
      "Epoch: 10 - Batch: 386, Training Loss: 0.033311218017122245\n",
      "Epoch: 10 - Batch: 387, Training Loss: 0.03340151047281563\n",
      "Epoch: 10 - Batch: 388, Training Loss: 0.033484433612370766\n",
      "Epoch: 10 - Batch: 389, Training Loss: 0.033569379442465644\n",
      "Epoch: 10 - Batch: 390, Training Loss: 0.033657893414916484\n",
      "Epoch: 10 - Batch: 391, Training Loss: 0.033747800435889416\n",
      "Epoch: 10 - Batch: 392, Training Loss: 0.033836209933407864\n",
      "Epoch: 10 - Batch: 393, Training Loss: 0.0339265343166307\n",
      "Epoch: 10 - Batch: 394, Training Loss: 0.034019545678228486\n",
      "Epoch: 10 - Batch: 395, Training Loss: 0.03410175148008475\n",
      "Epoch: 10 - Batch: 396, Training Loss: 0.03419376930737772\n",
      "Epoch: 10 - Batch: 397, Training Loss: 0.0342735664897396\n",
      "Epoch: 10 - Batch: 398, Training Loss: 0.03436386741537162\n",
      "Epoch: 10 - Batch: 399, Training Loss: 0.03444682668344694\n",
      "Epoch: 10 - Batch: 400, Training Loss: 0.03454509846384252\n",
      "Epoch: 10 - Batch: 401, Training Loss: 0.0346278543607216\n",
      "Epoch: 10 - Batch: 402, Training Loss: 0.03471182655561623\n",
      "Epoch: 10 - Batch: 403, Training Loss: 0.03479306722927845\n",
      "Epoch: 10 - Batch: 404, Training Loss: 0.03487236695256004\n",
      "Epoch: 10 - Batch: 405, Training Loss: 0.03495647627891197\n",
      "Epoch: 10 - Batch: 406, Training Loss: 0.03504400975866302\n",
      "Epoch: 10 - Batch: 407, Training Loss: 0.03512919664531205\n",
      "Epoch: 10 - Batch: 408, Training Loss: 0.0352157696303147\n",
      "Epoch: 10 - Batch: 409, Training Loss: 0.03529651805288954\n",
      "Epoch: 10 - Batch: 410, Training Loss: 0.03538448787966178\n",
      "Epoch: 10 - Batch: 411, Training Loss: 0.03547207751059611\n",
      "Epoch: 10 - Batch: 412, Training Loss: 0.035552887718624145\n",
      "Epoch: 10 - Batch: 413, Training Loss: 0.03563775796795366\n",
      "Epoch: 10 - Batch: 414, Training Loss: 0.03572943922175499\n",
      "Epoch: 10 - Batch: 415, Training Loss: 0.035821714415330795\n",
      "Epoch: 10 - Batch: 416, Training Loss: 0.035912337896736896\n",
      "Epoch: 10 - Batch: 417, Training Loss: 0.03600148033739915\n",
      "Epoch: 10 - Batch: 418, Training Loss: 0.03608247803317176\n",
      "Epoch: 10 - Batch: 419, Training Loss: 0.03616659085005275\n",
      "Epoch: 10 - Batch: 420, Training Loss: 0.03625587343700094\n",
      "Epoch: 10 - Batch: 421, Training Loss: 0.036346665248398365\n",
      "Epoch: 10 - Batch: 422, Training Loss: 0.03642743525047405\n",
      "Epoch: 10 - Batch: 423, Training Loss: 0.03651462030746846\n",
      "Epoch: 10 - Batch: 424, Training Loss: 0.03659709292783666\n",
      "Epoch: 10 - Batch: 425, Training Loss: 0.03668131623695146\n",
      "Epoch: 10 - Batch: 426, Training Loss: 0.036767842671270194\n",
      "Epoch: 10 - Batch: 427, Training Loss: 0.03685485385049437\n",
      "Epoch: 10 - Batch: 428, Training Loss: 0.03693705721206926\n",
      "Epoch: 10 - Batch: 429, Training Loss: 0.0370216054245706\n",
      "Epoch: 10 - Batch: 430, Training Loss: 0.03710916124988551\n",
      "Epoch: 10 - Batch: 431, Training Loss: 0.037196632814446885\n",
      "Epoch: 10 - Batch: 432, Training Loss: 0.037279588629060716\n",
      "Epoch: 10 - Batch: 433, Training Loss: 0.03736912129851518\n",
      "Epoch: 10 - Batch: 434, Training Loss: 0.0374558968515835\n",
      "Epoch: 10 - Batch: 435, Training Loss: 0.037539175518511936\n",
      "Epoch: 10 - Batch: 436, Training Loss: 0.037620904928020775\n",
      "Epoch: 10 - Batch: 437, Training Loss: 0.037705421435398054\n",
      "Epoch: 10 - Batch: 438, Training Loss: 0.037795063478881445\n",
      "Epoch: 10 - Batch: 439, Training Loss: 0.03789127258519035\n",
      "Epoch: 10 - Batch: 440, Training Loss: 0.03797706496433832\n",
      "Epoch: 10 - Batch: 441, Training Loss: 0.03806899310047947\n",
      "Epoch: 10 - Batch: 442, Training Loss: 0.03815143305196691\n",
      "Epoch: 10 - Batch: 443, Training Loss: 0.03823825556964028\n",
      "Epoch: 10 - Batch: 444, Training Loss: 0.03832641099371127\n",
      "Epoch: 10 - Batch: 445, Training Loss: 0.03840844144745055\n",
      "Epoch: 10 - Batch: 446, Training Loss: 0.03849406213581463\n",
      "Epoch: 10 - Batch: 447, Training Loss: 0.03857954138052799\n",
      "Epoch: 10 - Batch: 448, Training Loss: 0.03866655467260339\n",
      "Epoch: 10 - Batch: 449, Training Loss: 0.03875324366672913\n",
      "Epoch: 10 - Batch: 450, Training Loss: 0.038841974935424864\n",
      "Epoch: 10 - Batch: 451, Training Loss: 0.038927372538825965\n",
      "Epoch: 10 - Batch: 452, Training Loss: 0.03902182608818138\n",
      "Epoch: 10 - Batch: 453, Training Loss: 0.03910795667179386\n",
      "Epoch: 10 - Batch: 454, Training Loss: 0.03919154295626762\n",
      "Epoch: 10 - Batch: 455, Training Loss: 0.03927409727972736\n",
      "Epoch: 10 - Batch: 456, Training Loss: 0.039360784574923036\n",
      "Epoch: 10 - Batch: 457, Training Loss: 0.03944662502452509\n",
      "Epoch: 10 - Batch: 458, Training Loss: 0.03953911899704838\n",
      "Epoch: 10 - Batch: 459, Training Loss: 0.0396229130761145\n",
      "Epoch: 10 - Batch: 460, Training Loss: 0.03970538055026907\n",
      "Epoch: 10 - Batch: 461, Training Loss: 0.03978807692899435\n",
      "Epoch: 10 - Batch: 462, Training Loss: 0.03989260633835943\n",
      "Epoch: 10 - Batch: 463, Training Loss: 0.03997723550024515\n",
      "Epoch: 10 - Batch: 464, Training Loss: 0.040066406112256925\n",
      "Epoch: 10 - Batch: 465, Training Loss: 0.040158225473628115\n",
      "Epoch: 10 - Batch: 466, Training Loss: 0.04024258294261708\n",
      "Epoch: 10 - Batch: 467, Training Loss: 0.040328401423508846\n",
      "Epoch: 10 - Batch: 468, Training Loss: 0.0404185583354723\n",
      "Epoch: 10 - Batch: 469, Training Loss: 0.040514502470469595\n",
      "Epoch: 10 - Batch: 470, Training Loss: 0.04060179220009008\n",
      "Epoch: 10 - Batch: 471, Training Loss: 0.040688804089775925\n",
      "Epoch: 10 - Batch: 472, Training Loss: 0.040780173710379036\n",
      "Epoch: 10 - Batch: 473, Training Loss: 0.04085814430815466\n",
      "Epoch: 10 - Batch: 474, Training Loss: 0.04095367971764473\n",
      "Epoch: 10 - Batch: 475, Training Loss: 0.04104547908106451\n",
      "Epoch: 10 - Batch: 476, Training Loss: 0.04113358672504401\n",
      "Epoch: 10 - Batch: 477, Training Loss: 0.041223352703931515\n",
      "Epoch: 10 - Batch: 478, Training Loss: 0.04131043460807002\n",
      "Epoch: 10 - Batch: 479, Training Loss: 0.041397825643346084\n",
      "Epoch: 10 - Batch: 480, Training Loss: 0.041479672031912634\n",
      "Epoch: 10 - Batch: 481, Training Loss: 0.04156408267432382\n",
      "Epoch: 10 - Batch: 482, Training Loss: 0.04164759775818284\n",
      "Epoch: 10 - Batch: 483, Training Loss: 0.04173243916276873\n",
      "Epoch: 10 - Batch: 484, Training Loss: 0.04182067085953296\n",
      "Epoch: 10 - Batch: 485, Training Loss: 0.04190161740824358\n",
      "Epoch: 10 - Batch: 486, Training Loss: 0.04197842926157648\n",
      "Epoch: 10 - Batch: 487, Training Loss: 0.04205379652393793\n",
      "Epoch: 10 - Batch: 488, Training Loss: 0.04213271938128456\n",
      "Epoch: 10 - Batch: 489, Training Loss: 0.04222517383978339\n",
      "Epoch: 10 - Batch: 490, Training Loss: 0.042315874452614664\n",
      "Epoch: 10 - Batch: 491, Training Loss: 0.042408922239273145\n",
      "Epoch: 10 - Batch: 492, Training Loss: 0.04248977187483465\n",
      "Epoch: 10 - Batch: 493, Training Loss: 0.04257902545023518\n",
      "Epoch: 10 - Batch: 494, Training Loss: 0.042670615029769945\n",
      "Epoch: 10 - Batch: 495, Training Loss: 0.04275963734680938\n",
      "Epoch: 10 - Batch: 496, Training Loss: 0.042843435336503616\n",
      "Epoch: 10 - Batch: 497, Training Loss: 0.042938435508886576\n",
      "Epoch: 10 - Batch: 498, Training Loss: 0.04302623685121932\n",
      "Epoch: 10 - Batch: 499, Training Loss: 0.043114739691044755\n",
      "Epoch: 10 - Batch: 500, Training Loss: 0.04319898081730254\n",
      "Epoch: 10 - Batch: 501, Training Loss: 0.04328627789247886\n",
      "Epoch: 10 - Batch: 502, Training Loss: 0.043367146061822945\n",
      "Epoch: 10 - Batch: 503, Training Loss: 0.04344574127896113\n",
      "Epoch: 10 - Batch: 504, Training Loss: 0.043530510834249886\n",
      "Epoch: 10 - Batch: 505, Training Loss: 0.04361792701369099\n",
      "Epoch: 10 - Batch: 506, Training Loss: 0.0436933040742455\n",
      "Epoch: 10 - Batch: 507, Training Loss: 0.04378014406309792\n",
      "Epoch: 10 - Batch: 508, Training Loss: 0.04386273094423573\n",
      "Epoch: 10 - Batch: 509, Training Loss: 0.04394528038095479\n",
      "Epoch: 10 - Batch: 510, Training Loss: 0.04403189814975408\n",
      "Epoch: 10 - Batch: 511, Training Loss: 0.04411750711241172\n",
      "Epoch: 10 - Batch: 512, Training Loss: 0.044203132613381345\n",
      "Epoch: 10 - Batch: 513, Training Loss: 0.04429787897796773\n",
      "Epoch: 10 - Batch: 514, Training Loss: 0.044386524033684836\n",
      "Epoch: 10 - Batch: 515, Training Loss: 0.044465367399637974\n",
      "Epoch: 10 - Batch: 516, Training Loss: 0.04454861953854561\n",
      "Epoch: 10 - Batch: 517, Training Loss: 0.04462848176212848\n",
      "Epoch: 10 - Batch: 518, Training Loss: 0.04472045935875741\n",
      "Epoch: 10 - Batch: 519, Training Loss: 0.04481636813747547\n",
      "Epoch: 10 - Batch: 520, Training Loss: 0.04489604552364468\n",
      "Epoch: 10 - Batch: 521, Training Loss: 0.04498708051680332\n",
      "Epoch: 10 - Batch: 522, Training Loss: 0.045078010633464276\n",
      "Epoch: 10 - Batch: 523, Training Loss: 0.04517068421069662\n",
      "Epoch: 10 - Batch: 524, Training Loss: 0.04525782023348025\n",
      "Epoch: 10 - Batch: 525, Training Loss: 0.045344572829369885\n",
      "Epoch: 10 - Batch: 526, Training Loss: 0.04543447779927088\n",
      "Epoch: 10 - Batch: 527, Training Loss: 0.0455325366882839\n",
      "Epoch: 10 - Batch: 528, Training Loss: 0.045624299811980816\n",
      "Epoch: 10 - Batch: 529, Training Loss: 0.04570343259530479\n",
      "Epoch: 10 - Batch: 530, Training Loss: 0.04579078164538539\n",
      "Epoch: 10 - Batch: 531, Training Loss: 0.04588355512588376\n",
      "Epoch: 10 - Batch: 532, Training Loss: 0.04597129831538469\n",
      "Epoch: 10 - Batch: 533, Training Loss: 0.046056143896249596\n",
      "Epoch: 10 - Batch: 534, Training Loss: 0.04613997736578162\n",
      "Epoch: 10 - Batch: 535, Training Loss: 0.04622525894770377\n",
      "Epoch: 10 - Batch: 536, Training Loss: 0.04630401194293305\n",
      "Epoch: 10 - Batch: 537, Training Loss: 0.046388123363602415\n",
      "Epoch: 10 - Batch: 538, Training Loss: 0.04647123977962023\n",
      "Epoch: 10 - Batch: 539, Training Loss: 0.04655650426103899\n",
      "Epoch: 10 - Batch: 540, Training Loss: 0.04663057137237457\n",
      "Epoch: 10 - Batch: 541, Training Loss: 0.04671742482293107\n",
      "Epoch: 10 - Batch: 542, Training Loss: 0.046793699678447515\n",
      "Epoch: 10 - Batch: 543, Training Loss: 0.04687875606255547\n",
      "Epoch: 10 - Batch: 544, Training Loss: 0.04696066775799391\n",
      "Epoch: 10 - Batch: 545, Training Loss: 0.04706119104083102\n",
      "Epoch: 10 - Batch: 546, Training Loss: 0.047152058093207785\n",
      "Epoch: 10 - Batch: 547, Training Loss: 0.04723572055152795\n",
      "Epoch: 10 - Batch: 548, Training Loss: 0.047317241499228266\n",
      "Epoch: 10 - Batch: 549, Training Loss: 0.047404171519018524\n",
      "Epoch: 10 - Batch: 550, Training Loss: 0.04749088476768774\n",
      "Epoch: 10 - Batch: 551, Training Loss: 0.04758413344201559\n",
      "Epoch: 10 - Batch: 552, Training Loss: 0.047673959260309116\n",
      "Epoch: 10 - Batch: 553, Training Loss: 0.04776234179735184\n",
      "Epoch: 10 - Batch: 554, Training Loss: 0.04784587859662611\n",
      "Epoch: 10 - Batch: 555, Training Loss: 0.04793059087634877\n",
      "Epoch: 10 - Batch: 556, Training Loss: 0.04801659751220129\n",
      "Epoch: 10 - Batch: 557, Training Loss: 0.04809447634304143\n",
      "Epoch: 10 - Batch: 558, Training Loss: 0.04818674974501825\n",
      "Epoch: 10 - Batch: 559, Training Loss: 0.048277069982851716\n",
      "Epoch: 10 - Batch: 560, Training Loss: 0.048362453092834844\n",
      "Epoch: 10 - Batch: 561, Training Loss: 0.04845151557258112\n",
      "Epoch: 10 - Batch: 562, Training Loss: 0.04853721929846909\n",
      "Epoch: 10 - Batch: 563, Training Loss: 0.048624330306824164\n",
      "Epoch: 10 - Batch: 564, Training Loss: 0.04870749712237474\n",
      "Epoch: 10 - Batch: 565, Training Loss: 0.048786206044269045\n",
      "Epoch: 10 - Batch: 566, Training Loss: 0.048877221416329865\n",
      "Epoch: 10 - Batch: 567, Training Loss: 0.04895750322621655\n",
      "Epoch: 10 - Batch: 568, Training Loss: 0.04903297071309627\n",
      "Epoch: 10 - Batch: 569, Training Loss: 0.049121660571143796\n",
      "Epoch: 10 - Batch: 570, Training Loss: 0.04920032621047785\n",
      "Epoch: 10 - Batch: 571, Training Loss: 0.049286931716091005\n",
      "Epoch: 10 - Batch: 572, Training Loss: 0.049376778020392205\n",
      "Epoch: 10 - Batch: 573, Training Loss: 0.049466669621258036\n",
      "Epoch: 10 - Batch: 574, Training Loss: 0.04955846763171169\n",
      "Epoch: 10 - Batch: 575, Training Loss: 0.049644216851857964\n",
      "Epoch: 10 - Batch: 576, Training Loss: 0.049727731194365674\n",
      "Epoch: 10 - Batch: 577, Training Loss: 0.04981402110672906\n",
      "Epoch: 10 - Batch: 578, Training Loss: 0.04990629664009088\n",
      "Epoch: 10 - Batch: 579, Training Loss: 0.04998523800650837\n",
      "Epoch: 10 - Batch: 580, Training Loss: 0.05007810731901854\n",
      "Epoch: 10 - Batch: 581, Training Loss: 0.05016517875156988\n",
      "Epoch: 10 - Batch: 582, Training Loss: 0.050259036242121684\n",
      "Epoch: 10 - Batch: 583, Training Loss: 0.050346757432022694\n",
      "Epoch: 10 - Batch: 584, Training Loss: 0.05042853561626936\n",
      "Epoch: 10 - Batch: 585, Training Loss: 0.050514596179251255\n",
      "Epoch: 10 - Batch: 586, Training Loss: 0.05060147703509426\n",
      "Epoch: 10 - Batch: 587, Training Loss: 0.05068856786633803\n",
      "Epoch: 10 - Batch: 588, Training Loss: 0.05077131142589583\n",
      "Epoch: 10 - Batch: 589, Training Loss: 0.050856229554163676\n",
      "Epoch: 10 - Batch: 590, Training Loss: 0.05093892767506452\n",
      "Epoch: 10 - Batch: 591, Training Loss: 0.05103592554209244\n",
      "Epoch: 10 - Batch: 592, Training Loss: 0.05112440633289454\n",
      "Epoch: 10 - Batch: 593, Training Loss: 0.0512047226094389\n",
      "Epoch: 10 - Batch: 594, Training Loss: 0.05128880752062125\n",
      "Epoch: 10 - Batch: 595, Training Loss: 0.05137937498058055\n",
      "Epoch: 10 - Batch: 596, Training Loss: 0.051459063103987804\n",
      "Epoch: 10 - Batch: 597, Training Loss: 0.05153370956901097\n",
      "Epoch: 10 - Batch: 598, Training Loss: 0.05161540175650646\n",
      "Epoch: 10 - Batch: 599, Training Loss: 0.05170578958704499\n",
      "Epoch: 10 - Batch: 600, Training Loss: 0.051797218484862725\n",
      "Epoch: 10 - Batch: 601, Training Loss: 0.05187709695021707\n",
      "Epoch: 10 - Batch: 602, Training Loss: 0.051956770950882\n",
      "Epoch: 10 - Batch: 603, Training Loss: 0.05204491208540662\n",
      "Epoch: 10 - Batch: 604, Training Loss: 0.05213042272634767\n",
      "Epoch: 10 - Batch: 605, Training Loss: 0.0522207082565843\n",
      "Epoch: 10 - Batch: 606, Training Loss: 0.05230425961105582\n",
      "Epoch: 10 - Batch: 607, Training Loss: 0.052388958929486536\n",
      "Epoch: 10 - Batch: 608, Training Loss: 0.05248261481572937\n",
      "Epoch: 10 - Batch: 609, Training Loss: 0.052569263616258625\n",
      "Epoch: 10 - Batch: 610, Training Loss: 0.05264734070914895\n",
      "Epoch: 10 - Batch: 611, Training Loss: 0.05272620967643376\n",
      "Epoch: 10 - Batch: 612, Training Loss: 0.05281787149447509\n",
      "Epoch: 10 - Batch: 613, Training Loss: 0.05290702688407344\n",
      "Epoch: 10 - Batch: 614, Training Loss: 0.05300002080538182\n",
      "Epoch: 10 - Batch: 615, Training Loss: 0.053080523568846495\n",
      "Epoch: 10 - Batch: 616, Training Loss: 0.05316798244394473\n",
      "Epoch: 10 - Batch: 617, Training Loss: 0.0532512248711503\n",
      "Epoch: 10 - Batch: 618, Training Loss: 0.05334341920801063\n",
      "Epoch: 10 - Batch: 619, Training Loss: 0.05343494170760832\n",
      "Epoch: 10 - Batch: 620, Training Loss: 0.053519359102494286\n",
      "Epoch: 10 - Batch: 621, Training Loss: 0.05361067273254023\n",
      "Epoch: 10 - Batch: 622, Training Loss: 0.053701342851121235\n",
      "Epoch: 10 - Batch: 623, Training Loss: 0.05379357575678311\n",
      "Epoch: 10 - Batch: 624, Training Loss: 0.05387994369637116\n",
      "Epoch: 10 - Batch: 625, Training Loss: 0.053962729963941955\n",
      "Epoch: 10 - Batch: 626, Training Loss: 0.054035918357755806\n",
      "Epoch: 10 - Batch: 627, Training Loss: 0.05411787782092988\n",
      "Epoch: 10 - Batch: 628, Training Loss: 0.05420897727458433\n",
      "Epoch: 10 - Batch: 629, Training Loss: 0.05428710441486555\n",
      "Epoch: 10 - Batch: 630, Training Loss: 0.05437488524030097\n",
      "Epoch: 10 - Batch: 631, Training Loss: 0.05446516268363046\n",
      "Epoch: 10 - Batch: 632, Training Loss: 0.0545492555581614\n",
      "Epoch: 10 - Batch: 633, Training Loss: 0.05464297755440669\n",
      "Epoch: 10 - Batch: 634, Training Loss: 0.05472756004822788\n",
      "Epoch: 10 - Batch: 635, Training Loss: 0.05481579831212907\n",
      "Epoch: 10 - Batch: 636, Training Loss: 0.05490139205855121\n",
      "Epoch: 10 - Batch: 637, Training Loss: 0.054995013614050785\n",
      "Epoch: 10 - Batch: 638, Training Loss: 0.05507771427407984\n",
      "Epoch: 10 - Batch: 639, Training Loss: 0.05516495237475999\n",
      "Epoch: 10 - Batch: 640, Training Loss: 0.05524964227802916\n",
      "Epoch: 10 - Batch: 641, Training Loss: 0.055332611906488936\n",
      "Epoch: 10 - Batch: 642, Training Loss: 0.055415438423257565\n",
      "Epoch: 10 - Batch: 643, Training Loss: 0.05550373275185106\n",
      "Epoch: 10 - Batch: 644, Training Loss: 0.055584435050018983\n",
      "Epoch: 10 - Batch: 645, Training Loss: 0.055675047386443834\n",
      "Epoch: 10 - Batch: 646, Training Loss: 0.0557642881550006\n",
      "Epoch: 10 - Batch: 647, Training Loss: 0.055848401775012164\n",
      "Epoch: 10 - Batch: 648, Training Loss: 0.055933527790541275\n",
      "Epoch: 10 - Batch: 649, Training Loss: 0.0560225596215891\n",
      "Epoch: 10 - Batch: 650, Training Loss: 0.056111441971502495\n",
      "Epoch: 10 - Batch: 651, Training Loss: 0.05619341329011949\n",
      "Epoch: 10 - Batch: 652, Training Loss: 0.05629123241376521\n",
      "Epoch: 10 - Batch: 653, Training Loss: 0.05636950391367893\n",
      "Epoch: 10 - Batch: 654, Training Loss: 0.05644896124824758\n",
      "Epoch: 10 - Batch: 655, Training Loss: 0.05653157418788369\n",
      "Epoch: 10 - Batch: 656, Training Loss: 0.056625670801298336\n",
      "Epoch: 10 - Batch: 657, Training Loss: 0.05672496357688658\n",
      "Epoch: 10 - Batch: 658, Training Loss: 0.05680808072189984\n",
      "Epoch: 10 - Batch: 659, Training Loss: 0.05689036766465624\n",
      "Epoch: 10 - Batch: 660, Training Loss: 0.056970158051297834\n",
      "Epoch: 10 - Batch: 661, Training Loss: 0.05705322247387758\n",
      "Epoch: 10 - Batch: 662, Training Loss: 0.05713668512566568\n",
      "Epoch: 10 - Batch: 663, Training Loss: 0.057218027210008245\n",
      "Epoch: 10 - Batch: 664, Training Loss: 0.05730576559925949\n",
      "Epoch: 10 - Batch: 665, Training Loss: 0.057388045208815915\n",
      "Epoch: 10 - Batch: 666, Training Loss: 0.05747944526189002\n",
      "Epoch: 10 - Batch: 667, Training Loss: 0.057566745846129176\n",
      "Epoch: 10 - Batch: 668, Training Loss: 0.05765780714490323\n",
      "Epoch: 10 - Batch: 669, Training Loss: 0.05774599435441134\n",
      "Epoch: 10 - Batch: 670, Training Loss: 0.057827686782846006\n",
      "Epoch: 10 - Batch: 671, Training Loss: 0.05790927841839308\n",
      "Epoch: 10 - Batch: 672, Training Loss: 0.05799558648150754\n",
      "Epoch: 10 - Batch: 673, Training Loss: 0.05808611346986361\n",
      "Epoch: 10 - Batch: 674, Training Loss: 0.05817312021951375\n",
      "Epoch: 10 - Batch: 675, Training Loss: 0.05825682726393687\n",
      "Epoch: 10 - Batch: 676, Training Loss: 0.058336437533101436\n",
      "Epoch: 10 - Batch: 677, Training Loss: 0.058420953447397665\n",
      "Epoch: 10 - Batch: 678, Training Loss: 0.05850612937440326\n",
      "Epoch: 10 - Batch: 679, Training Loss: 0.058585771577275216\n",
      "Epoch: 10 - Batch: 680, Training Loss: 0.058667022951107914\n",
      "Epoch: 10 - Batch: 681, Training Loss: 0.058749118303975854\n",
      "Epoch: 10 - Batch: 682, Training Loss: 0.058839144738109944\n",
      "Epoch: 10 - Batch: 683, Training Loss: 0.0589276623246484\n",
      "Epoch: 10 - Batch: 684, Training Loss: 0.05902066403014545\n",
      "Epoch: 10 - Batch: 685, Training Loss: 0.05910324240427705\n",
      "Epoch: 10 - Batch: 686, Training Loss: 0.05918644046160712\n",
      "Epoch: 10 - Batch: 687, Training Loss: 0.059267719765850164\n",
      "Epoch: 10 - Batch: 688, Training Loss: 0.059362428130004335\n",
      "Epoch: 10 - Batch: 689, Training Loss: 0.05945739628218893\n",
      "Epoch: 10 - Batch: 690, Training Loss: 0.05955063170486224\n",
      "Epoch: 10 - Batch: 691, Training Loss: 0.05963543644962619\n",
      "Epoch: 10 - Batch: 692, Training Loss: 0.05972465244469358\n",
      "Epoch: 10 - Batch: 693, Training Loss: 0.05981667787701534\n",
      "Epoch: 10 - Batch: 694, Training Loss: 0.059898779562258996\n",
      "Epoch: 10 - Batch: 695, Training Loss: 0.059986810822342564\n",
      "Epoch: 10 - Batch: 696, Training Loss: 0.060076426177179044\n",
      "Epoch: 10 - Batch: 697, Training Loss: 0.060161304606084605\n",
      "Epoch: 10 - Batch: 698, Training Loss: 0.06024382427137092\n",
      "Epoch: 10 - Batch: 699, Training Loss: 0.06033818138327764\n",
      "Epoch: 10 - Batch: 700, Training Loss: 0.060421150325987466\n",
      "Epoch: 10 - Batch: 701, Training Loss: 0.06051263439887594\n",
      "Epoch: 10 - Batch: 702, Training Loss: 0.06059462414007282\n",
      "Epoch: 10 - Batch: 703, Training Loss: 0.060675914360713804\n",
      "Epoch: 10 - Batch: 704, Training Loss: 0.060763767239317965\n",
      "Epoch: 10 - Batch: 705, Training Loss: 0.06084997944570893\n",
      "Epoch: 10 - Batch: 706, Training Loss: 0.060934593082462775\n",
      "Epoch: 10 - Batch: 707, Training Loss: 0.0610241687441149\n",
      "Epoch: 10 - Batch: 708, Training Loss: 0.061119123477840896\n",
      "Epoch: 10 - Batch: 709, Training Loss: 0.061198099650702074\n",
      "Epoch: 10 - Batch: 710, Training Loss: 0.06128687517189268\n",
      "Epoch: 10 - Batch: 711, Training Loss: 0.06137901054043477\n",
      "Epoch: 10 - Batch: 712, Training Loss: 0.06147070051746977\n",
      "Epoch: 10 - Batch: 713, Training Loss: 0.06155061224478592\n",
      "Epoch: 10 - Batch: 714, Training Loss: 0.06163510776574339\n",
      "Epoch: 10 - Batch: 715, Training Loss: 0.061727959693565496\n",
      "Epoch: 10 - Batch: 716, Training Loss: 0.06181328512023931\n",
      "Epoch: 10 - Batch: 717, Training Loss: 0.06189900577982662\n",
      "Epoch: 10 - Batch: 718, Training Loss: 0.061986564125735964\n",
      "Epoch: 10 - Batch: 719, Training Loss: 0.06207449294960321\n",
      "Epoch: 10 - Batch: 720, Training Loss: 0.06216096388759898\n",
      "Epoch: 10 - Batch: 721, Training Loss: 0.062251595974858125\n",
      "Epoch: 10 - Batch: 722, Training Loss: 0.062336699762204\n",
      "Epoch: 10 - Batch: 723, Training Loss: 0.062425306341788465\n",
      "Epoch: 10 - Batch: 724, Training Loss: 0.06251274155542427\n",
      "Epoch: 10 - Batch: 725, Training Loss: 0.06260389693785663\n",
      "Epoch: 10 - Batch: 726, Training Loss: 0.06269864701537746\n",
      "Epoch: 10 - Batch: 727, Training Loss: 0.06277854521921025\n",
      "Epoch: 10 - Batch: 728, Training Loss: 0.06286303029329227\n",
      "Epoch: 10 - Batch: 729, Training Loss: 0.0629516609915058\n",
      "Epoch: 10 - Batch: 730, Training Loss: 0.06304526272821387\n",
      "Epoch: 10 - Batch: 731, Training Loss: 0.06313558381242342\n",
      "Epoch: 10 - Batch: 732, Training Loss: 0.06322688508043638\n",
      "Epoch: 10 - Batch: 733, Training Loss: 0.06331218603633924\n",
      "Epoch: 10 - Batch: 734, Training Loss: 0.06339868386067561\n",
      "Epoch: 10 - Batch: 735, Training Loss: 0.06348829711501673\n",
      "Epoch: 10 - Batch: 736, Training Loss: 0.06357899169201281\n",
      "Epoch: 10 - Batch: 737, Training Loss: 0.06366729353296618\n",
      "Epoch: 10 - Batch: 738, Training Loss: 0.06375665574104435\n",
      "Epoch: 10 - Batch: 739, Training Loss: 0.06384620754354035\n",
      "Epoch: 10 - Batch: 740, Training Loss: 0.0639262738858487\n",
      "Epoch: 10 - Batch: 741, Training Loss: 0.06401850525320664\n",
      "Epoch: 10 - Batch: 742, Training Loss: 0.06409585279463535\n",
      "Epoch: 10 - Batch: 743, Training Loss: 0.06417405474344098\n",
      "Epoch: 10 - Batch: 744, Training Loss: 0.06425733650551112\n",
      "Epoch: 10 - Batch: 745, Training Loss: 0.0643485381050193\n",
      "Epoch: 10 - Batch: 746, Training Loss: 0.06443552452928787\n",
      "Epoch: 10 - Batch: 747, Training Loss: 0.06452008669154956\n",
      "Epoch: 10 - Batch: 748, Training Loss: 0.0646136053351324\n",
      "Epoch: 10 - Batch: 749, Training Loss: 0.0646906474830697\n",
      "Epoch: 10 - Batch: 750, Training Loss: 0.06478073038346138\n",
      "Epoch: 10 - Batch: 751, Training Loss: 0.06486108752081841\n",
      "Epoch: 10 - Batch: 752, Training Loss: 0.06494722143433383\n",
      "Epoch: 10 - Batch: 753, Training Loss: 0.06503928130860155\n",
      "Epoch: 10 - Batch: 754, Training Loss: 0.06511913507460164\n",
      "Epoch: 10 - Batch: 755, Training Loss: 0.06522203508647124\n",
      "Epoch: 10 - Batch: 756, Training Loss: 0.06530671951609662\n",
      "Epoch: 10 - Batch: 757, Training Loss: 0.06539205277638253\n",
      "Epoch: 10 - Batch: 758, Training Loss: 0.06548144002977889\n",
      "Epoch: 10 - Batch: 759, Training Loss: 0.06557280245927435\n",
      "Epoch: 10 - Batch: 760, Training Loss: 0.06565984832138366\n",
      "Epoch: 10 - Batch: 761, Training Loss: 0.06574824671716635\n",
      "Epoch: 10 - Batch: 762, Training Loss: 0.0658304579494209\n",
      "Epoch: 10 - Batch: 763, Training Loss: 0.06591098862476215\n",
      "Epoch: 10 - Batch: 764, Training Loss: 0.06599896668053384\n",
      "Epoch: 10 - Batch: 765, Training Loss: 0.06609166780514504\n",
      "Epoch: 10 - Batch: 766, Training Loss: 0.06616646292963826\n",
      "Epoch: 10 - Batch: 767, Training Loss: 0.06624695689573415\n",
      "Epoch: 10 - Batch: 768, Training Loss: 0.06633013436203176\n",
      "Epoch: 10 - Batch: 769, Training Loss: 0.06642319487522096\n",
      "Epoch: 10 - Batch: 770, Training Loss: 0.06650786355113113\n",
      "Epoch: 10 - Batch: 771, Training Loss: 0.06659695192257167\n",
      "Epoch: 10 - Batch: 772, Training Loss: 0.06668567480781976\n",
      "Epoch: 10 - Batch: 773, Training Loss: 0.06677460449919179\n",
      "Epoch: 10 - Batch: 774, Training Loss: 0.06687268155131174\n",
      "Epoch: 10 - Batch: 775, Training Loss: 0.06696403829711389\n",
      "Epoch: 10 - Batch: 776, Training Loss: 0.06705030793969706\n",
      "Epoch: 10 - Batch: 777, Training Loss: 0.06714234962614614\n",
      "Epoch: 10 - Batch: 778, Training Loss: 0.06723502559423644\n",
      "Epoch: 10 - Batch: 779, Training Loss: 0.06732053226894802\n",
      "Epoch: 10 - Batch: 780, Training Loss: 0.0674036079662455\n",
      "Epoch: 10 - Batch: 781, Training Loss: 0.06749481344549217\n",
      "Epoch: 10 - Batch: 782, Training Loss: 0.06758669007625152\n",
      "Epoch: 10 - Batch: 783, Training Loss: 0.06767472436351958\n",
      "Epoch: 10 - Batch: 784, Training Loss: 0.06776235473170802\n",
      "Epoch: 10 - Batch: 785, Training Loss: 0.0678489852455718\n",
      "Epoch: 10 - Batch: 786, Training Loss: 0.06793478535948504\n",
      "Epoch: 10 - Batch: 787, Training Loss: 0.06801126336997027\n",
      "Epoch: 10 - Batch: 788, Training Loss: 0.06809714386489854\n",
      "Epoch: 10 - Batch: 789, Training Loss: 0.06817932976765023\n",
      "Epoch: 10 - Batch: 790, Training Loss: 0.06827016847569552\n",
      "Epoch: 10 - Batch: 791, Training Loss: 0.06835326466517859\n",
      "Epoch: 10 - Batch: 792, Training Loss: 0.06844603476635061\n",
      "Epoch: 10 - Batch: 793, Training Loss: 0.06853011796006911\n",
      "Epoch: 10 - Batch: 794, Training Loss: 0.06861627253257417\n",
      "Epoch: 10 - Batch: 795, Training Loss: 0.06869767720550052\n",
      "Epoch: 10 - Batch: 796, Training Loss: 0.06878951794867887\n",
      "Epoch: 10 - Batch: 797, Training Loss: 0.06886431349327117\n",
      "Epoch: 10 - Batch: 798, Training Loss: 0.06895387058407315\n",
      "Epoch: 10 - Batch: 799, Training Loss: 0.06903400121389534\n",
      "Epoch: 10 - Batch: 800, Training Loss: 0.06912332563159082\n",
      "Epoch: 10 - Batch: 801, Training Loss: 0.06920574552952552\n",
      "Epoch: 10 - Batch: 802, Training Loss: 0.06928740605514243\n",
      "Epoch: 10 - Batch: 803, Training Loss: 0.06937389309899526\n",
      "Epoch: 10 - Batch: 804, Training Loss: 0.06946316387374603\n",
      "Epoch: 10 - Batch: 805, Training Loss: 0.06953611606067883\n",
      "Epoch: 10 - Batch: 806, Training Loss: 0.06963138547949925\n",
      "Epoch: 10 - Batch: 807, Training Loss: 0.06971939467820362\n",
      "Epoch: 10 - Batch: 808, Training Loss: 0.06979722529031941\n",
      "Epoch: 10 - Batch: 809, Training Loss: 0.06988041527309821\n",
      "Epoch: 10 - Batch: 810, Training Loss: 0.06996948611775837\n",
      "Epoch: 10 - Batch: 811, Training Loss: 0.07005277262447683\n",
      "Epoch: 10 - Batch: 812, Training Loss: 0.0701349486919679\n",
      "Epoch: 10 - Batch: 813, Training Loss: 0.07022508739757893\n",
      "Epoch: 10 - Batch: 814, Training Loss: 0.07030874920103879\n",
      "Epoch: 10 - Batch: 815, Training Loss: 0.07039134722715784\n",
      "Epoch: 10 - Batch: 816, Training Loss: 0.07046765259545834\n",
      "Epoch: 10 - Batch: 817, Training Loss: 0.07055155796988884\n",
      "Epoch: 10 - Batch: 818, Training Loss: 0.07064107740622255\n",
      "Epoch: 10 - Batch: 819, Training Loss: 0.07072798079914516\n",
      "Epoch: 10 - Batch: 820, Training Loss: 0.0708117362785201\n",
      "Epoch: 10 - Batch: 821, Training Loss: 0.07090835373966058\n",
      "Epoch: 10 - Batch: 822, Training Loss: 0.07099627759029616\n",
      "Epoch: 10 - Batch: 823, Training Loss: 0.07108740381908851\n",
      "Epoch: 10 - Batch: 824, Training Loss: 0.071174391275071\n",
      "Epoch: 10 - Batch: 825, Training Loss: 0.07125815242578339\n",
      "Epoch: 10 - Batch: 826, Training Loss: 0.07134019318430578\n",
      "Epoch: 10 - Batch: 827, Training Loss: 0.07141771070523247\n",
      "Epoch: 10 - Batch: 828, Training Loss: 0.0715037795157476\n",
      "Epoch: 10 - Batch: 829, Training Loss: 0.07158896806425914\n",
      "Epoch: 10 - Batch: 830, Training Loss: 0.07167067581395407\n",
      "Epoch: 10 - Batch: 831, Training Loss: 0.07175438517510001\n",
      "Epoch: 10 - Batch: 832, Training Loss: 0.07184218172953892\n",
      "Epoch: 10 - Batch: 833, Training Loss: 0.07191678987905556\n",
      "Epoch: 10 - Batch: 834, Training Loss: 0.0720078354179365\n",
      "Epoch: 10 - Batch: 835, Training Loss: 0.07208928731494084\n",
      "Epoch: 10 - Batch: 836, Training Loss: 0.07217748669496618\n",
      "Epoch: 10 - Batch: 837, Training Loss: 0.07225766454268846\n",
      "Epoch: 10 - Batch: 838, Training Loss: 0.0723440644839411\n",
      "Epoch: 10 - Batch: 839, Training Loss: 0.07242298596360988\n",
      "Epoch: 10 - Batch: 840, Training Loss: 0.07250814370279683\n",
      "Epoch: 10 - Batch: 841, Training Loss: 0.07260021084848525\n",
      "Epoch: 10 - Batch: 842, Training Loss: 0.07267794405321774\n",
      "Epoch: 10 - Batch: 843, Training Loss: 0.07276465083977476\n",
      "Epoch: 10 - Batch: 844, Training Loss: 0.07286976715225485\n",
      "Epoch: 10 - Batch: 845, Training Loss: 0.07296733354924133\n",
      "Epoch: 10 - Batch: 846, Training Loss: 0.07305091080776296\n",
      "Epoch: 10 - Batch: 847, Training Loss: 0.07313584741421204\n",
      "Epoch: 10 - Batch: 848, Training Loss: 0.07322094642601994\n",
      "Epoch: 10 - Batch: 849, Training Loss: 0.07331116270786692\n",
      "Epoch: 10 - Batch: 850, Training Loss: 0.07339924687572182\n",
      "Epoch: 10 - Batch: 851, Training Loss: 0.07348813664532618\n",
      "Epoch: 10 - Batch: 852, Training Loss: 0.0735705805877548\n",
      "Epoch: 10 - Batch: 853, Training Loss: 0.07365750078464028\n",
      "Epoch: 10 - Batch: 854, Training Loss: 0.07374349293325276\n",
      "Epoch: 10 - Batch: 855, Training Loss: 0.07382627233714606\n",
      "Epoch: 10 - Batch: 856, Training Loss: 0.07390649701973692\n",
      "Epoch: 10 - Batch: 857, Training Loss: 0.07399889923150267\n",
      "Epoch: 10 - Batch: 858, Training Loss: 0.07408501677351013\n",
      "Epoch: 10 - Batch: 859, Training Loss: 0.07416920975517871\n",
      "Epoch: 10 - Batch: 860, Training Loss: 0.07425079660900988\n",
      "Epoch: 10 - Batch: 861, Training Loss: 0.07433385026963987\n",
      "Epoch: 10 - Batch: 862, Training Loss: 0.0744122360303232\n",
      "Epoch: 10 - Batch: 863, Training Loss: 0.07448724492346469\n",
      "Epoch: 10 - Batch: 864, Training Loss: 0.07457662245280312\n",
      "Epoch: 10 - Batch: 865, Training Loss: 0.07465882467763935\n",
      "Epoch: 10 - Batch: 866, Training Loss: 0.07473665319543771\n",
      "Epoch: 10 - Batch: 867, Training Loss: 0.07482116217809925\n",
      "Epoch: 10 - Batch: 868, Training Loss: 0.07491372506540411\n",
      "Epoch: 10 - Batch: 869, Training Loss: 0.07499813283507899\n",
      "Epoch: 10 - Batch: 870, Training Loss: 0.07507748206209384\n",
      "Epoch: 10 - Batch: 871, Training Loss: 0.07516668018441691\n",
      "Epoch: 10 - Batch: 872, Training Loss: 0.07525685967670546\n",
      "Epoch: 10 - Batch: 873, Training Loss: 0.07533888254394974\n",
      "Epoch: 10 - Batch: 874, Training Loss: 0.07542258030912571\n",
      "Epoch: 10 - Batch: 875, Training Loss: 0.07550476000923818\n",
      "Epoch: 10 - Batch: 876, Training Loss: 0.07558627078189185\n",
      "Epoch: 10 - Batch: 877, Training Loss: 0.07567298485840336\n",
      "Epoch: 10 - Batch: 878, Training Loss: 0.07576437514417404\n",
      "Epoch: 10 - Batch: 879, Training Loss: 0.07585402466294976\n",
      "Epoch: 10 - Batch: 880, Training Loss: 0.07594639518194729\n",
      "Epoch: 10 - Batch: 881, Training Loss: 0.07603688348412119\n",
      "Epoch: 10 - Batch: 882, Training Loss: 0.07611915673659018\n",
      "Epoch: 10 - Batch: 883, Training Loss: 0.0762089962451711\n",
      "Epoch: 10 - Batch: 884, Training Loss: 0.07629561981504435\n",
      "Epoch: 10 - Batch: 885, Training Loss: 0.07638987767854535\n",
      "Epoch: 10 - Batch: 886, Training Loss: 0.07648362260725763\n",
      "Epoch: 10 - Batch: 887, Training Loss: 0.0765825323884068\n",
      "Epoch: 10 - Batch: 888, Training Loss: 0.07666244165072987\n",
      "Epoch: 10 - Batch: 889, Training Loss: 0.07674397866104175\n",
      "Epoch: 10 - Batch: 890, Training Loss: 0.07682921504524612\n",
      "Epoch: 10 - Batch: 891, Training Loss: 0.07691252453385498\n",
      "Epoch: 10 - Batch: 892, Training Loss: 0.07698872540251137\n",
      "Epoch: 10 - Batch: 893, Training Loss: 0.07707228642543948\n",
      "Epoch: 10 - Batch: 894, Training Loss: 0.07715070736324214\n",
      "Epoch: 10 - Batch: 895, Training Loss: 0.07723075620965973\n",
      "Epoch: 10 - Batch: 896, Training Loss: 0.07731076462154167\n",
      "Epoch: 10 - Batch: 897, Training Loss: 0.07740888584584343\n",
      "Epoch: 10 - Batch: 898, Training Loss: 0.07750385784687687\n",
      "Epoch: 10 - Batch: 899, Training Loss: 0.07758420456824809\n",
      "Epoch: 10 - Batch: 900, Training Loss: 0.07766312604791688\n",
      "Epoch: 10 - Batch: 901, Training Loss: 0.07775143533945084\n",
      "Epoch: 10 - Batch: 902, Training Loss: 0.07783860568062187\n",
      "Epoch: 10 - Batch: 903, Training Loss: 0.07792687754627088\n",
      "Epoch: 10 - Batch: 904, Training Loss: 0.07801399497349266\n",
      "Epoch: 10 - Batch: 905, Training Loss: 0.07810206612445426\n",
      "Epoch: 10 - Batch: 906, Training Loss: 0.0782087780967676\n",
      "Epoch: 10 - Batch: 907, Training Loss: 0.07829642837641647\n",
      "Epoch: 10 - Batch: 908, Training Loss: 0.07838257281712038\n",
      "Epoch: 10 - Batch: 909, Training Loss: 0.07845612795742392\n",
      "Epoch: 10 - Batch: 910, Training Loss: 0.07854182984228948\n",
      "Epoch: 10 - Batch: 911, Training Loss: 0.07863056906197795\n",
      "Epoch: 10 - Batch: 912, Training Loss: 0.07871472777468252\n",
      "Epoch: 10 - Batch: 913, Training Loss: 0.07879743052285108\n",
      "Epoch: 10 - Batch: 914, Training Loss: 0.07888061824450841\n",
      "Epoch: 10 - Batch: 915, Training Loss: 0.07896336656724834\n",
      "Epoch: 10 - Batch: 916, Training Loss: 0.07904649377610554\n",
      "Epoch: 10 - Batch: 917, Training Loss: 0.07912817175436772\n",
      "Epoch: 10 - Batch: 918, Training Loss: 0.0792179632048504\n",
      "Epoch: 10 - Batch: 919, Training Loss: 0.07930220741389403\n",
      "Epoch: 10 - Batch: 920, Training Loss: 0.07939060647689287\n",
      "Epoch: 10 - Batch: 921, Training Loss: 0.0794860163694294\n",
      "Epoch: 10 - Batch: 922, Training Loss: 0.0795690938027245\n",
      "Epoch: 10 - Batch: 923, Training Loss: 0.07965020177450347\n",
      "Epoch: 10 - Batch: 924, Training Loss: 0.07973178419258266\n",
      "Epoch: 10 - Batch: 925, Training Loss: 0.07981559691663405\n",
      "Epoch: 10 - Batch: 926, Training Loss: 0.07990678656185246\n",
      "Epoch: 10 - Batch: 927, Training Loss: 0.07999247151919661\n",
      "Epoch: 10 - Batch: 928, Training Loss: 0.08008346479898859\n",
      "Epoch: 10 - Batch: 929, Training Loss: 0.08017633668151661\n",
      "Epoch: 10 - Batch: 930, Training Loss: 0.0802636002661478\n",
      "Epoch: 10 - Batch: 931, Training Loss: 0.08034897732176195\n",
      "Epoch: 10 - Batch: 932, Training Loss: 0.08043295513239271\n",
      "Epoch: 10 - Batch: 933, Training Loss: 0.08052101849298769\n",
      "Epoch: 10 - Batch: 934, Training Loss: 0.08060463096569624\n",
      "Epoch: 10 - Batch: 935, Training Loss: 0.08069841476592851\n",
      "Epoch: 10 - Batch: 936, Training Loss: 0.0807773579919022\n",
      "Epoch: 10 - Batch: 937, Training Loss: 0.08086284980869807\n",
      "Epoch: 10 - Batch: 938, Training Loss: 0.08095014627225956\n",
      "Epoch: 10 - Batch: 939, Training Loss: 0.08104126784907249\n",
      "Epoch: 10 - Batch: 940, Training Loss: 0.0811295089671762\n",
      "Epoch: 10 - Batch: 941, Training Loss: 0.08121281421772679\n",
      "Epoch: 10 - Batch: 942, Training Loss: 0.08131127480794344\n",
      "Epoch: 10 - Batch: 943, Training Loss: 0.08139713747337288\n",
      "Epoch: 10 - Batch: 944, Training Loss: 0.08148189645204972\n",
      "Epoch: 10 - Batch: 945, Training Loss: 0.0815640564075058\n",
      "Epoch: 10 - Batch: 946, Training Loss: 0.08165428702832257\n",
      "Epoch: 10 - Batch: 947, Training Loss: 0.08174461696550225\n",
      "Epoch: 10 - Batch: 948, Training Loss: 0.08182818816347699\n",
      "Epoch: 10 - Batch: 949, Training Loss: 0.08190963653288473\n",
      "Epoch: 10 - Batch: 950, Training Loss: 0.08199436105725975\n",
      "Epoch: 10 - Batch: 951, Training Loss: 0.08207931449856133\n",
      "Epoch: 10 - Batch: 952, Training Loss: 0.082170250607812\n",
      "Epoch: 10 - Batch: 953, Training Loss: 0.0822577574606954\n",
      "Epoch: 10 - Batch: 954, Training Loss: 0.08234627410818886\n",
      "Epoch: 10 - Batch: 955, Training Loss: 0.08243556227380561\n",
      "Epoch: 10 - Batch: 956, Training Loss: 0.08251946879733063\n",
      "Epoch: 10 - Batch: 957, Training Loss: 0.082608216635228\n",
      "Epoch: 10 - Batch: 958, Training Loss: 0.08270563772118111\n",
      "Epoch: 10 - Batch: 959, Training Loss: 0.08279045270277098\n",
      "Epoch: 10 - Batch: 960, Training Loss: 0.08287876166069685\n",
      "Epoch: 10 - Batch: 961, Training Loss: 0.08296007655598038\n",
      "Epoch: 10 - Batch: 962, Training Loss: 0.08304524634212601\n",
      "Epoch: 10 - Batch: 963, Training Loss: 0.08312520128404521\n",
      "Epoch: 10 - Batch: 964, Training Loss: 0.08320382779856424\n",
      "Epoch: 10 - Batch: 965, Training Loss: 0.08328794378472205\n",
      "Epoch: 10 - Batch: 966, Training Loss: 0.08336931208201111\n",
      "Epoch: 10 - Batch: 967, Training Loss: 0.08345317734315819\n",
      "Epoch: 10 - Batch: 968, Training Loss: 0.08353761012726162\n",
      "Epoch: 10 - Batch: 969, Training Loss: 0.08362142864620903\n",
      "Epoch: 10 - Batch: 970, Training Loss: 0.08370659526925578\n",
      "Epoch: 10 - Batch: 971, Training Loss: 0.08379309720191394\n",
      "Epoch: 10 - Batch: 972, Training Loss: 0.0838860155733466\n",
      "Epoch: 10 - Batch: 973, Training Loss: 0.08397269738872055\n",
      "Epoch: 10 - Batch: 974, Training Loss: 0.08406160522258499\n",
      "Epoch: 10 - Batch: 975, Training Loss: 0.08414941835240346\n",
      "Epoch: 10 - Batch: 976, Training Loss: 0.08424093977086382\n",
      "Epoch: 10 - Batch: 977, Training Loss: 0.08432666623152509\n",
      "Epoch: 10 - Batch: 978, Training Loss: 0.08441300523987852\n",
      "Epoch: 10 - Batch: 979, Training Loss: 0.08450539647964499\n",
      "Epoch: 10 - Batch: 980, Training Loss: 0.08458826101537961\n",
      "Epoch: 10 - Batch: 981, Training Loss: 0.08467352129580764\n",
      "Epoch: 10 - Batch: 982, Training Loss: 0.08476428648033743\n",
      "Epoch: 10 - Batch: 983, Training Loss: 0.08485310343688796\n",
      "Epoch: 10 - Batch: 984, Training Loss: 0.0849437135060134\n",
      "Epoch: 10 - Batch: 985, Training Loss: 0.08503062716665158\n",
      "Epoch: 10 - Batch: 986, Training Loss: 0.08511536566549865\n",
      "Epoch: 10 - Batch: 987, Training Loss: 0.08519729310847436\n",
      "Epoch: 10 - Batch: 988, Training Loss: 0.0852920180974315\n",
      "Epoch: 10 - Batch: 989, Training Loss: 0.08537495275882148\n",
      "Epoch: 10 - Batch: 990, Training Loss: 0.08546104924027402\n",
      "Epoch: 10 - Batch: 991, Training Loss: 0.08554425633591206\n",
      "Epoch: 10 - Batch: 992, Training Loss: 0.08563331234736822\n",
      "Epoch: 10 - Batch: 993, Training Loss: 0.0857202990434656\n",
      "Epoch: 10 - Batch: 994, Training Loss: 0.08579863838937943\n",
      "Epoch: 10 - Batch: 995, Training Loss: 0.08588663740068131\n",
      "Epoch: 10 - Batch: 996, Training Loss: 0.08596940764888603\n",
      "Epoch: 10 - Batch: 997, Training Loss: 0.08605446625086996\n",
      "Epoch: 10 - Batch: 998, Training Loss: 0.08613880327091882\n",
      "Epoch: 10 - Batch: 999, Training Loss: 0.08623231506935795\n",
      "Epoch: 10 - Batch: 1000, Training Loss: 0.08632629467900317\n",
      "Epoch: 10 - Batch: 1001, Training Loss: 0.08641278911165733\n",
      "Epoch: 10 - Batch: 1002, Training Loss: 0.08649191688468207\n",
      "Epoch: 10 - Batch: 1003, Training Loss: 0.0865883483222468\n",
      "Epoch: 10 - Batch: 1004, Training Loss: 0.08666812766893191\n",
      "Epoch: 10 - Batch: 1005, Training Loss: 0.086751835738891\n",
      "Epoch: 10 - Batch: 1006, Training Loss: 0.08683892872005355\n",
      "Epoch: 10 - Batch: 1007, Training Loss: 0.08693248571621047\n",
      "Epoch: 10 - Batch: 1008, Training Loss: 0.0870236401719537\n",
      "Epoch: 10 - Batch: 1009, Training Loss: 0.08710969019539123\n",
      "Epoch: 10 - Batch: 1010, Training Loss: 0.08719350200510935\n",
      "Epoch: 10 - Batch: 1011, Training Loss: 0.08727534719515796\n",
      "Epoch: 10 - Batch: 1012, Training Loss: 0.0873561596519516\n",
      "Epoch: 10 - Batch: 1013, Training Loss: 0.08744655340094472\n",
      "Epoch: 10 - Batch: 1014, Training Loss: 0.08753242878054901\n",
      "Epoch: 10 - Batch: 1015, Training Loss: 0.08761924332869586\n",
      "Epoch: 10 - Batch: 1016, Training Loss: 0.08770208969514563\n",
      "Epoch: 10 - Batch: 1017, Training Loss: 0.0877837263739623\n",
      "Epoch: 10 - Batch: 1018, Training Loss: 0.08786956029349496\n",
      "Epoch: 10 - Batch: 1019, Training Loss: 0.08795404695530436\n",
      "Epoch: 10 - Batch: 1020, Training Loss: 0.08803969055760164\n",
      "Epoch: 10 - Batch: 1021, Training Loss: 0.08812041066673462\n",
      "Epoch: 10 - Batch: 1022, Training Loss: 0.08820289498191963\n",
      "Epoch: 10 - Batch: 1023, Training Loss: 0.08829065085470578\n",
      "Epoch: 10 - Batch: 1024, Training Loss: 0.08837242780336693\n",
      "Epoch: 10 - Batch: 1025, Training Loss: 0.08846610672092359\n",
      "Epoch: 10 - Batch: 1026, Training Loss: 0.088556235164997\n",
      "Epoch: 10 - Batch: 1027, Training Loss: 0.08864929515924026\n",
      "Epoch: 10 - Batch: 1028, Training Loss: 0.08873949739865798\n",
      "Epoch: 10 - Batch: 1029, Training Loss: 0.08881923041088664\n",
      "Epoch: 10 - Batch: 1030, Training Loss: 0.08889595590914857\n",
      "Epoch: 10 - Batch: 1031, Training Loss: 0.08898541323877686\n",
      "Epoch: 10 - Batch: 1032, Training Loss: 0.08908097962439554\n",
      "Epoch: 10 - Batch: 1033, Training Loss: 0.08916837276016697\n",
      "Epoch: 10 - Batch: 1034, Training Loss: 0.08924894043511616\n",
      "Epoch: 10 - Batch: 1035, Training Loss: 0.08933490941650041\n",
      "Epoch: 10 - Batch: 1036, Training Loss: 0.08941566680942602\n",
      "Epoch: 10 - Batch: 1037, Training Loss: 0.08950613287723876\n",
      "Epoch: 10 - Batch: 1038, Training Loss: 0.08959682718240602\n",
      "Epoch: 10 - Batch: 1039, Training Loss: 0.08967751325733626\n",
      "Epoch: 10 - Batch: 1040, Training Loss: 0.08977253358211881\n",
      "Epoch: 10 - Batch: 1041, Training Loss: 0.08985435683434678\n",
      "Epoch: 10 - Batch: 1042, Training Loss: 0.08993613736002797\n",
      "Epoch: 10 - Batch: 1043, Training Loss: 0.09001799426342717\n",
      "Epoch: 10 - Batch: 1044, Training Loss: 0.09011405103439914\n",
      "Epoch: 10 - Batch: 1045, Training Loss: 0.09020002755285496\n",
      "Epoch: 10 - Batch: 1046, Training Loss: 0.09028392537785802\n",
      "Epoch: 10 - Batch: 1047, Training Loss: 0.09036966122279119\n",
      "Epoch: 10 - Batch: 1048, Training Loss: 0.09046027074825902\n",
      "Epoch: 10 - Batch: 1049, Training Loss: 0.09053989032756037\n",
      "Epoch: 10 - Batch: 1050, Training Loss: 0.09062881854240772\n",
      "Epoch: 10 - Batch: 1051, Training Loss: 0.09071515265784255\n",
      "Epoch: 10 - Batch: 1052, Training Loss: 0.09079586477273732\n",
      "Epoch: 10 - Batch: 1053, Training Loss: 0.09088645370099477\n",
      "Epoch: 10 - Batch: 1054, Training Loss: 0.09096925042161894\n",
      "Epoch: 10 - Batch: 1055, Training Loss: 0.0910467428910495\n",
      "Epoch: 10 - Batch: 1056, Training Loss: 0.09113099683032898\n",
      "Epoch: 10 - Batch: 1057, Training Loss: 0.09121073435244473\n",
      "Epoch: 10 - Batch: 1058, Training Loss: 0.09130054457147126\n",
      "Epoch: 10 - Batch: 1059, Training Loss: 0.09138784620595809\n",
      "Epoch: 10 - Batch: 1060, Training Loss: 0.0914806535167876\n",
      "Epoch: 10 - Batch: 1061, Training Loss: 0.09156409272334073\n",
      "Epoch: 10 - Batch: 1062, Training Loss: 0.09164605946411343\n",
      "Epoch: 10 - Batch: 1063, Training Loss: 0.0917306708397458\n",
      "Epoch: 10 - Batch: 1064, Training Loss: 0.09181685785872624\n",
      "Epoch: 10 - Batch: 1065, Training Loss: 0.09191039263905577\n",
      "Epoch: 10 - Batch: 1066, Training Loss: 0.0919933713676028\n",
      "Epoch: 10 - Batch: 1067, Training Loss: 0.0920861528423394\n",
      "Epoch: 10 - Batch: 1068, Training Loss: 0.09217218422079165\n",
      "Epoch: 10 - Batch: 1069, Training Loss: 0.09226696104873867\n",
      "Epoch: 10 - Batch: 1070, Training Loss: 0.09235627266576832\n",
      "Epoch: 10 - Batch: 1071, Training Loss: 0.09244917624106455\n",
      "Epoch: 10 - Batch: 1072, Training Loss: 0.09254023802789488\n",
      "Epoch: 10 - Batch: 1073, Training Loss: 0.0926271661325276\n",
      "Epoch: 10 - Batch: 1074, Training Loss: 0.09271021478285837\n",
      "Epoch: 10 - Batch: 1075, Training Loss: 0.09279610127655429\n",
      "Epoch: 10 - Batch: 1076, Training Loss: 0.09288251422234435\n",
      "Epoch: 10 - Batch: 1077, Training Loss: 0.092963349951391\n",
      "Epoch: 10 - Batch: 1078, Training Loss: 0.0930532800592791\n",
      "Epoch: 10 - Batch: 1079, Training Loss: 0.0931371498364912\n",
      "Epoch: 10 - Batch: 1080, Training Loss: 0.09322607157069257\n",
      "Epoch: 10 - Batch: 1081, Training Loss: 0.09331737217148936\n",
      "Epoch: 10 - Batch: 1082, Training Loss: 0.09339808009157133\n",
      "Epoch: 10 - Batch: 1083, Training Loss: 0.09347793857750805\n",
      "Epoch: 10 - Batch: 1084, Training Loss: 0.09355980049738441\n",
      "Epoch: 10 - Batch: 1085, Training Loss: 0.09364669825603712\n",
      "Epoch: 10 - Batch: 1086, Training Loss: 0.09372995064206187\n",
      "Epoch: 10 - Batch: 1087, Training Loss: 0.09382490195939394\n",
      "Epoch: 10 - Batch: 1088, Training Loss: 0.09391351307357722\n",
      "Epoch: 10 - Batch: 1089, Training Loss: 0.09399468326944221\n",
      "Epoch: 10 - Batch: 1090, Training Loss: 0.09408498935982165\n",
      "Epoch: 10 - Batch: 1091, Training Loss: 0.09417441620137759\n",
      "Epoch: 10 - Batch: 1092, Training Loss: 0.09425983323436075\n",
      "Epoch: 10 - Batch: 1093, Training Loss: 0.09434588865730695\n",
      "Epoch: 10 - Batch: 1094, Training Loss: 0.09443518275373412\n",
      "Epoch: 10 - Batch: 1095, Training Loss: 0.09452401820082174\n",
      "Epoch: 10 - Batch: 1096, Training Loss: 0.09461153086713495\n",
      "Epoch: 10 - Batch: 1097, Training Loss: 0.09469706497184475\n",
      "Epoch: 10 - Batch: 1098, Training Loss: 0.09478345910584551\n",
      "Epoch: 10 - Batch: 1099, Training Loss: 0.09487097402092434\n",
      "Epoch: 10 - Batch: 1100, Training Loss: 0.0949592655570946\n",
      "Epoch: 10 - Batch: 1101, Training Loss: 0.09505374032763106\n",
      "Epoch: 10 - Batch: 1102, Training Loss: 0.09513453645616227\n",
      "Epoch: 10 - Batch: 1103, Training Loss: 0.09522150448874059\n",
      "Epoch: 10 - Batch: 1104, Training Loss: 0.09530600290714607\n",
      "Epoch: 10 - Batch: 1105, Training Loss: 0.09539404678873557\n",
      "Epoch: 10 - Batch: 1106, Training Loss: 0.09548129390301198\n",
      "Epoch: 10 - Batch: 1107, Training Loss: 0.09555553189582294\n",
      "Epoch: 10 - Batch: 1108, Training Loss: 0.09564734042728719\n",
      "Epoch: 10 - Batch: 1109, Training Loss: 0.09572513458345265\n",
      "Epoch: 10 - Batch: 1110, Training Loss: 0.09581545340654071\n",
      "Epoch: 10 - Batch: 1111, Training Loss: 0.09589563219330798\n",
      "Epoch: 10 - Batch: 1112, Training Loss: 0.09598238703796322\n",
      "Epoch: 10 - Batch: 1113, Training Loss: 0.09605780402646334\n",
      "Epoch: 10 - Batch: 1114, Training Loss: 0.09614246616612619\n",
      "Epoch: 10 - Batch: 1115, Training Loss: 0.0962280218194472\n",
      "Epoch: 10 - Batch: 1116, Training Loss: 0.09631462196261927\n",
      "Epoch: 10 - Batch: 1117, Training Loss: 0.09639718636845672\n",
      "Epoch: 10 - Batch: 1118, Training Loss: 0.09648642456699565\n",
      "Epoch: 10 - Batch: 1119, Training Loss: 0.09657182939239402\n",
      "Epoch: 10 - Batch: 1120, Training Loss: 0.09665960844476425\n",
      "Epoch: 10 - Batch: 1121, Training Loss: 0.09674766062948834\n",
      "Epoch: 10 - Batch: 1122, Training Loss: 0.09683175832280275\n",
      "Epoch: 10 - Batch: 1123, Training Loss: 0.09691755926455826\n",
      "Epoch: 10 - Batch: 1124, Training Loss: 0.09700853591954728\n",
      "Epoch: 10 - Batch: 1125, Training Loss: 0.0970917626733507\n",
      "Epoch: 10 - Batch: 1126, Training Loss: 0.09717934304357168\n",
      "Epoch: 10 - Batch: 1127, Training Loss: 0.09726231445745251\n",
      "Epoch: 10 - Batch: 1128, Training Loss: 0.09734648047246743\n",
      "Epoch: 10 - Batch: 1129, Training Loss: 0.09742876816275306\n",
      "Epoch: 10 - Batch: 1130, Training Loss: 0.09752357995159196\n",
      "Epoch: 10 - Batch: 1131, Training Loss: 0.09760659990544936\n",
      "Epoch: 10 - Batch: 1132, Training Loss: 0.09768924084345303\n",
      "Epoch: 10 - Batch: 1133, Training Loss: 0.09777391889745718\n",
      "Epoch: 10 - Batch: 1134, Training Loss: 0.09785942334193692\n",
      "Epoch: 10 - Batch: 1135, Training Loss: 0.09794404574516993\n",
      "Epoch: 10 - Batch: 1136, Training Loss: 0.09802332732387839\n",
      "Epoch: 10 - Batch: 1137, Training Loss: 0.0981199882220273\n",
      "Epoch: 10 - Batch: 1138, Training Loss: 0.09821706848407463\n",
      "Epoch: 10 - Batch: 1139, Training Loss: 0.09830524673558784\n",
      "Epoch: 10 - Batch: 1140, Training Loss: 0.09839264110076684\n",
      "Epoch: 10 - Batch: 1141, Training Loss: 0.09848125159097944\n",
      "Epoch: 10 - Batch: 1142, Training Loss: 0.09856952081247547\n",
      "Epoch: 10 - Batch: 1143, Training Loss: 0.09866177333305724\n",
      "Epoch: 10 - Batch: 1144, Training Loss: 0.09874603656393971\n",
      "Epoch: 10 - Batch: 1145, Training Loss: 0.09882758114221282\n",
      "Epoch: 10 - Batch: 1146, Training Loss: 0.09891696961883882\n",
      "Epoch: 10 - Batch: 1147, Training Loss: 0.09900537943143156\n",
      "Epoch: 10 - Batch: 1148, Training Loss: 0.09909290441653226\n",
      "Epoch: 10 - Batch: 1149, Training Loss: 0.09918410615195485\n",
      "Epoch: 10 - Batch: 1150, Training Loss: 0.09926683397633124\n",
      "Epoch: 10 - Batch: 1151, Training Loss: 0.09934916795782782\n",
      "Epoch: 10 - Batch: 1152, Training Loss: 0.09942885374597847\n",
      "Epoch: 10 - Batch: 1153, Training Loss: 0.0995158526373048\n",
      "Epoch: 10 - Batch: 1154, Training Loss: 0.09959784545510958\n",
      "Epoch: 10 - Batch: 1155, Training Loss: 0.09968567060099708\n",
      "Epoch: 10 - Batch: 1156, Training Loss: 0.099770471255973\n",
      "Epoch: 10 - Batch: 1157, Training Loss: 0.09985886701995855\n",
      "Epoch: 10 - Batch: 1158, Training Loss: 0.09994427819626643\n",
      "Epoch: 10 - Batch: 1159, Training Loss: 0.10003104584158752\n",
      "Epoch: 10 - Batch: 1160, Training Loss: 0.10011871125098089\n",
      "Epoch: 10 - Batch: 1161, Training Loss: 0.1002127215155025\n",
      "Epoch: 10 - Batch: 1162, Training Loss: 0.10029544738147587\n",
      "Epoch: 10 - Batch: 1163, Training Loss: 0.10038565733094713\n",
      "Epoch: 10 - Batch: 1164, Training Loss: 0.10047936247380614\n",
      "Epoch: 10 - Batch: 1165, Training Loss: 0.10056549617109409\n",
      "Epoch: 10 - Batch: 1166, Training Loss: 0.10064599858859483\n",
      "Epoch: 10 - Batch: 1167, Training Loss: 0.10073391636014974\n",
      "Epoch: 10 - Batch: 1168, Training Loss: 0.10082874180220847\n",
      "Epoch: 10 - Batch: 1169, Training Loss: 0.10093374932138481\n",
      "Epoch: 10 - Batch: 1170, Training Loss: 0.10100953445876416\n",
      "Epoch: 10 - Batch: 1171, Training Loss: 0.10109046526611543\n",
      "Epoch: 10 - Batch: 1172, Training Loss: 0.10117723782906683\n",
      "Epoch: 10 - Batch: 1173, Training Loss: 0.10126492764745186\n",
      "Epoch: 10 - Batch: 1174, Training Loss: 0.10134232872680052\n",
      "Epoch: 10 - Batch: 1175, Training Loss: 0.10142508946099685\n",
      "Epoch: 10 - Batch: 1176, Training Loss: 0.10150746257348638\n",
      "Epoch: 10 - Batch: 1177, Training Loss: 0.10159838849533455\n",
      "Epoch: 10 - Batch: 1178, Training Loss: 0.10168580300055728\n",
      "Epoch: 10 - Batch: 1179, Training Loss: 0.10177423814883081\n",
      "Epoch: 10 - Batch: 1180, Training Loss: 0.1018575097790998\n",
      "Epoch: 10 - Batch: 1181, Training Loss: 0.10194573849192504\n",
      "Epoch: 10 - Batch: 1182, Training Loss: 0.10203062956087032\n",
      "Epoch: 10 - Batch: 1183, Training Loss: 0.10211237408776781\n",
      "Epoch: 10 - Batch: 1184, Training Loss: 0.1021997041901447\n",
      "Epoch: 10 - Batch: 1185, Training Loss: 0.10228850859932441\n",
      "Epoch: 10 - Batch: 1186, Training Loss: 0.10236756688302034\n",
      "Epoch: 10 - Batch: 1187, Training Loss: 0.1024567853863263\n",
      "Epoch: 10 - Batch: 1188, Training Loss: 0.10254719817569205\n",
      "Epoch: 10 - Batch: 1189, Training Loss: 0.10263447770555419\n",
      "Epoch: 10 - Batch: 1190, Training Loss: 0.10272071131228609\n",
      "Epoch: 10 - Batch: 1191, Training Loss: 0.10281139036415031\n",
      "Epoch: 10 - Batch: 1192, Training Loss: 0.10290125808114832\n",
      "Epoch: 10 - Batch: 1193, Training Loss: 0.10298467827821846\n",
      "Epoch: 10 - Batch: 1194, Training Loss: 0.10307210695560694\n",
      "Epoch: 10 - Batch: 1195, Training Loss: 0.10315889812647605\n",
      "Epoch: 10 - Batch: 1196, Training Loss: 0.10324324786687768\n",
      "Epoch: 10 - Batch: 1197, Training Loss: 0.10333600314102363\n",
      "Epoch: 10 - Batch: 1198, Training Loss: 0.10342477328741728\n",
      "Epoch: 10 - Batch: 1199, Training Loss: 0.10350829024318835\n",
      "Epoch: 10 - Batch: 1200, Training Loss: 0.1035977501490124\n",
      "Epoch: 10 - Batch: 1201, Training Loss: 0.10368364859699808\n",
      "Epoch: 10 - Batch: 1202, Training Loss: 0.10376761691833214\n",
      "Epoch: 10 - Batch: 1203, Training Loss: 0.10385448509088994\n",
      "Epoch: 10 - Batch: 1204, Training Loss: 0.10394017681924266\n",
      "Epoch: 10 - Batch: 1205, Training Loss: 0.10402476280977081\n",
      "Epoch: 10 - Batch: 1206, Training Loss: 0.10411059006331967\n",
      "Epoch: 10 - Batch: 1207, Training Loss: 0.10419638533373772\n",
      "Epoch: 10 - Batch: 1208, Training Loss: 0.10428093495480655\n",
      "Epoch: 10 - Batch: 1209, Training Loss: 0.10436315215493909\n",
      "Epoch: 10 - Batch: 1210, Training Loss: 0.10444887430958487\n",
      "Epoch: 10 - Batch: 1211, Training Loss: 0.10453845042840362\n",
      "Epoch: 10 - Batch: 1212, Training Loss: 0.10462639523456939\n",
      "Epoch: 10 - Batch: 1213, Training Loss: 0.10470808749002208\n",
      "Epoch: 10 - Batch: 1214, Training Loss: 0.10479469944539158\n",
      "Epoch: 10 - Batch: 1215, Training Loss: 0.10488382924601411\n",
      "Epoch: 10 - Batch: 1216, Training Loss: 0.10497816657001897\n",
      "Epoch: 10 - Batch: 1217, Training Loss: 0.10505921082265342\n",
      "Epoch: 10 - Batch: 1218, Training Loss: 0.10514752593997304\n",
      "Epoch: 10 - Batch: 1219, Training Loss: 0.1052377605764427\n",
      "Epoch: 10 - Batch: 1220, Training Loss: 0.10532453568470024\n",
      "Epoch: 10 - Batch: 1221, Training Loss: 0.10540833390915572\n",
      "Epoch: 10 - Batch: 1222, Training Loss: 0.10549424882131825\n",
      "Epoch: 10 - Batch: 1223, Training Loss: 0.10557914349199528\n",
      "Epoch: 10 - Batch: 1224, Training Loss: 0.10566757281448315\n",
      "Epoch: 10 - Batch: 1225, Training Loss: 0.10575723243417036\n",
      "Epoch: 10 - Batch: 1226, Training Loss: 0.10583853113092791\n",
      "Epoch: 10 - Batch: 1227, Training Loss: 0.10592064223751105\n",
      "Epoch: 10 - Batch: 1228, Training Loss: 0.10601025664094668\n",
      "Epoch: 10 - Batch: 1229, Training Loss: 0.10608701394935746\n",
      "Epoch: 10 - Batch: 1230, Training Loss: 0.1061752194578869\n",
      "Epoch: 10 - Batch: 1231, Training Loss: 0.10626511757028834\n",
      "Epoch: 10 - Batch: 1232, Training Loss: 0.1063494187954251\n",
      "Epoch: 10 - Batch: 1233, Training Loss: 0.10643601487969878\n",
      "Epoch: 10 - Batch: 1234, Training Loss: 0.1065178822237955\n",
      "Epoch: 10 - Batch: 1235, Training Loss: 0.10660460212609266\n",
      "Epoch: 10 - Batch: 1236, Training Loss: 0.10668797047181706\n",
      "Epoch: 10 - Batch: 1237, Training Loss: 0.1067769047657451\n",
      "Epoch: 10 - Batch: 1238, Training Loss: 0.10685848470029743\n",
      "Epoch: 10 - Batch: 1239, Training Loss: 0.10694644292492178\n",
      "Epoch: 10 - Batch: 1240, Training Loss: 0.10702764355325778\n",
      "Epoch: 10 - Batch: 1241, Training Loss: 0.10712016159498672\n",
      "Epoch: 10 - Batch: 1242, Training Loss: 0.10720583111368998\n",
      "Epoch: 10 - Batch: 1243, Training Loss: 0.1072941429319963\n",
      "Epoch: 10 - Batch: 1244, Training Loss: 0.1073724779224712\n",
      "Epoch: 10 - Batch: 1245, Training Loss: 0.10745645629529336\n",
      "Epoch: 10 - Batch: 1246, Training Loss: 0.10754188342301012\n",
      "Epoch: 10 - Batch: 1247, Training Loss: 0.10762566316928436\n",
      "Epoch: 10 - Batch: 1248, Training Loss: 0.10770742349932047\n",
      "Epoch: 10 - Batch: 1249, Training Loss: 0.10779823970117577\n",
      "Epoch: 10 - Batch: 1250, Training Loss: 0.10788199094249241\n",
      "Epoch: 10 - Batch: 1251, Training Loss: 0.1079647167219748\n",
      "Epoch: 10 - Batch: 1252, Training Loss: 0.10805251234354664\n",
      "Epoch: 10 - Batch: 1253, Training Loss: 0.10813371814280798\n",
      "Epoch: 10 - Batch: 1254, Training Loss: 0.10823293539941015\n",
      "Epoch: 10 - Batch: 1255, Training Loss: 0.1083221083157889\n",
      "Epoch: 10 - Batch: 1256, Training Loss: 0.10841280903364493\n",
      "Epoch: 10 - Batch: 1257, Training Loss: 0.10849554049682064\n",
      "Epoch: 10 - Batch: 1258, Training Loss: 0.10858209817909681\n",
      "Epoch: 10 - Batch: 1259, Training Loss: 0.10866773053756598\n",
      "Epoch: 10 - Batch: 1260, Training Loss: 0.10875238731478774\n",
      "Epoch: 10 - Batch: 1261, Training Loss: 0.10884512114534727\n",
      "Epoch: 10 - Batch: 1262, Training Loss: 0.10893060015529937\n",
      "Epoch: 10 - Batch: 1263, Training Loss: 0.109012816922977\n",
      "Epoch: 10 - Batch: 1264, Training Loss: 0.10909952545788751\n",
      "Epoch: 10 - Batch: 1265, Training Loss: 0.10919329392450366\n",
      "Epoch: 10 - Batch: 1266, Training Loss: 0.10928065422459028\n",
      "Epoch: 10 - Batch: 1267, Training Loss: 0.1093611163990711\n",
      "Epoch: 10 - Batch: 1268, Training Loss: 0.1094467321512711\n",
      "Epoch: 10 - Batch: 1269, Training Loss: 0.1095311380613898\n",
      "Epoch: 10 - Batch: 1270, Training Loss: 0.10961565853499655\n",
      "Epoch: 10 - Batch: 1271, Training Loss: 0.10969936314531623\n",
      "Epoch: 10 - Batch: 1272, Training Loss: 0.10978240457982764\n",
      "Epoch: 10 - Batch: 1273, Training Loss: 0.10986545201928462\n",
      "Epoch: 10 - Batch: 1274, Training Loss: 0.10995902630746068\n",
      "Epoch: 10 - Batch: 1275, Training Loss: 0.11004102186208736\n",
      "Epoch: 10 - Batch: 1276, Training Loss: 0.11012231376312463\n",
      "Epoch: 10 - Batch: 1277, Training Loss: 0.1102066923297361\n",
      "Epoch: 10 - Batch: 1278, Training Loss: 0.11028655051295437\n",
      "Epoch: 10 - Batch: 1279, Training Loss: 0.11037436678733795\n",
      "Epoch: 10 - Batch: 1280, Training Loss: 0.11046464284681166\n",
      "Epoch: 10 - Batch: 1281, Training Loss: 0.11055311240614152\n",
      "Epoch: 10 - Batch: 1282, Training Loss: 0.11063604261324576\n",
      "Epoch: 10 - Batch: 1283, Training Loss: 0.11072032138161596\n",
      "Epoch: 10 - Batch: 1284, Training Loss: 0.1108124920879035\n",
      "Epoch: 10 - Batch: 1285, Training Loss: 0.11089112436642892\n",
      "Epoch: 10 - Batch: 1286, Training Loss: 0.11098160316695029\n",
      "Epoch: 10 - Batch: 1287, Training Loss: 0.11106748712151798\n",
      "Epoch: 10 - Batch: 1288, Training Loss: 0.1111564203343087\n",
      "Epoch: 10 - Batch: 1289, Training Loss: 0.11123796587016056\n",
      "Epoch: 10 - Batch: 1290, Training Loss: 0.11132435592055123\n",
      "Epoch: 10 - Batch: 1291, Training Loss: 0.1114105419387074\n",
      "Epoch: 10 - Batch: 1292, Training Loss: 0.11148891749345445\n",
      "Epoch: 10 - Batch: 1293, Training Loss: 0.11157323149454534\n",
      "Epoch: 10 - Batch: 1294, Training Loss: 0.11164880723107118\n",
      "Epoch: 10 - Batch: 1295, Training Loss: 0.11173762583095043\n",
      "Epoch: 10 - Batch: 1296, Training Loss: 0.1118289725499762\n",
      "Epoch: 10 - Batch: 1297, Training Loss: 0.11191845760614322\n",
      "Epoch: 10 - Batch: 1298, Training Loss: 0.11199231948672637\n",
      "Epoch: 10 - Batch: 1299, Training Loss: 0.11207165561035695\n",
      "Epoch: 10 - Batch: 1300, Training Loss: 0.11215757551428493\n",
      "Epoch: 10 - Batch: 1301, Training Loss: 0.11224406908820715\n",
      "Epoch: 10 - Batch: 1302, Training Loss: 0.11233044091371161\n",
      "Epoch: 10 - Batch: 1303, Training Loss: 0.1124248931101009\n",
      "Epoch: 10 - Batch: 1304, Training Loss: 0.11251432536969928\n",
      "Epoch: 10 - Batch: 1305, Training Loss: 0.11260122360405242\n",
      "Epoch: 10 - Batch: 1306, Training Loss: 0.11269168691033155\n",
      "Epoch: 10 - Batch: 1307, Training Loss: 0.11277046045334778\n",
      "Epoch: 10 - Batch: 1308, Training Loss: 0.1128575153166975\n",
      "Epoch: 10 - Batch: 1309, Training Loss: 0.11294632766451408\n",
      "Epoch: 10 - Batch: 1310, Training Loss: 0.11303415338494885\n",
      "Epoch: 10 - Batch: 1311, Training Loss: 0.11312157118918488\n",
      "Epoch: 10 - Batch: 1312, Training Loss: 0.1132118320077114\n",
      "Epoch: 10 - Batch: 1313, Training Loss: 0.11330094489316837\n",
      "Epoch: 10 - Batch: 1314, Training Loss: 0.11338869342039869\n",
      "Epoch: 10 - Batch: 1315, Training Loss: 0.11347574348349872\n",
      "Epoch: 10 - Batch: 1316, Training Loss: 0.1135679918092677\n",
      "Epoch: 10 - Batch: 1317, Training Loss: 0.1136584547942946\n",
      "Epoch: 10 - Batch: 1318, Training Loss: 0.11374523233665558\n",
      "Epoch: 10 - Batch: 1319, Training Loss: 0.11382798153048329\n",
      "Epoch: 10 - Batch: 1320, Training Loss: 0.11390924875070009\n",
      "Epoch: 10 - Batch: 1321, Training Loss: 0.1139950409136206\n",
      "Epoch: 10 - Batch: 1322, Training Loss: 0.11408851038090032\n",
      "Epoch: 10 - Batch: 1323, Training Loss: 0.11418202321723128\n",
      "Epoch: 10 - Batch: 1324, Training Loss: 0.11426050177038606\n",
      "Epoch: 10 - Batch: 1325, Training Loss: 0.11435321410175778\n",
      "Epoch: 10 - Batch: 1326, Training Loss: 0.11444150458768035\n",
      "Epoch: 10 - Batch: 1327, Training Loss: 0.11452802178599744\n",
      "Epoch: 10 - Batch: 1328, Training Loss: 0.11461443948261378\n",
      "Epoch: 10 - Batch: 1329, Training Loss: 0.11469922704656128\n",
      "Epoch: 10 - Batch: 1330, Training Loss: 0.11478289385414242\n",
      "Epoch: 10 - Batch: 1331, Training Loss: 0.11487063867461622\n",
      "Epoch: 10 - Batch: 1332, Training Loss: 0.11495912104078984\n",
      "Epoch: 10 - Batch: 1333, Training Loss: 0.11505342706741385\n",
      "Epoch: 10 - Batch: 1334, Training Loss: 0.11514276460084946\n",
      "Epoch: 10 - Batch: 1335, Training Loss: 0.11522913989217128\n",
      "Epoch: 10 - Batch: 1336, Training Loss: 0.11531562371114593\n",
      "Epoch: 10 - Batch: 1337, Training Loss: 0.11540507483813497\n",
      "Epoch: 10 - Batch: 1338, Training Loss: 0.11548972077309394\n",
      "Epoch: 10 - Batch: 1339, Training Loss: 0.11556941935573249\n",
      "Epoch: 10 - Batch: 1340, Training Loss: 0.11564675243463286\n",
      "Epoch: 10 - Batch: 1341, Training Loss: 0.11574207523074712\n",
      "Epoch: 10 - Batch: 1342, Training Loss: 0.11583471217286923\n",
      "Epoch: 10 - Batch: 1343, Training Loss: 0.11591927513208358\n",
      "Epoch: 10 - Batch: 1344, Training Loss: 0.11601395081500114\n",
      "Epoch: 10 - Batch: 1345, Training Loss: 0.11610674350267619\n",
      "Epoch: 10 - Batch: 1346, Training Loss: 0.11618625469023909\n",
      "Epoch: 10 - Batch: 1347, Training Loss: 0.11627163227097707\n",
      "Epoch: 10 - Batch: 1348, Training Loss: 0.116360428222574\n",
      "Epoch: 10 - Batch: 1349, Training Loss: 0.11644806504051879\n",
      "Epoch: 10 - Batch: 1350, Training Loss: 0.11652572898450578\n",
      "Epoch: 10 - Batch: 1351, Training Loss: 0.11661136096612137\n",
      "Epoch: 10 - Batch: 1352, Training Loss: 0.11670682354402384\n",
      "Epoch: 10 - Batch: 1353, Training Loss: 0.11679009389546183\n",
      "Epoch: 10 - Batch: 1354, Training Loss: 0.11687736946527243\n",
      "Epoch: 10 - Batch: 1355, Training Loss: 0.11696895355880754\n",
      "Epoch: 10 - Batch: 1356, Training Loss: 0.11705533851858593\n",
      "Epoch: 10 - Batch: 1357, Training Loss: 0.11714032698255866\n",
      "Epoch: 10 - Batch: 1358, Training Loss: 0.11723127159881552\n",
      "Epoch: 10 - Batch: 1359, Training Loss: 0.1173202114467004\n",
      "Epoch: 10 - Batch: 1360, Training Loss: 0.11740087288405567\n",
      "Epoch: 10 - Batch: 1361, Training Loss: 0.11748424733357248\n",
      "Epoch: 10 - Batch: 1362, Training Loss: 0.11756879438462345\n",
      "Epoch: 10 - Batch: 1363, Training Loss: 0.1176536751673194\n",
      "Epoch: 10 - Batch: 1364, Training Loss: 0.11773871227011554\n",
      "Epoch: 10 - Batch: 1365, Training Loss: 0.11782769691415292\n",
      "Epoch: 10 - Batch: 1366, Training Loss: 0.11791101072113312\n",
      "Epoch: 10 - Batch: 1367, Training Loss: 0.11799308080627748\n",
      "Epoch: 10 - Batch: 1368, Training Loss: 0.11808229590554538\n",
      "Epoch: 10 - Batch: 1369, Training Loss: 0.11817301746155097\n",
      "Epoch: 10 - Batch: 1370, Training Loss: 0.11826469980266754\n",
      "Epoch: 10 - Batch: 1371, Training Loss: 0.11834954073155302\n",
      "Epoch: 10 - Batch: 1372, Training Loss: 0.11843554167753428\n",
      "Epoch: 10 - Batch: 1373, Training Loss: 0.1185244278268731\n",
      "Epoch: 10 - Batch: 1374, Training Loss: 0.11861312561737958\n",
      "Epoch: 10 - Batch: 1375, Training Loss: 0.11869316373891499\n",
      "Epoch: 10 - Batch: 1376, Training Loss: 0.11877813845082104\n",
      "Epoch: 10 - Batch: 1377, Training Loss: 0.11886828877711375\n",
      "Epoch: 10 - Batch: 1378, Training Loss: 0.11895330360230325\n",
      "Epoch: 10 - Batch: 1379, Training Loss: 0.11903299822489025\n",
      "Epoch: 10 - Batch: 1380, Training Loss: 0.11912237608165883\n",
      "Epoch: 10 - Batch: 1381, Training Loss: 0.11920564121274806\n",
      "Epoch: 10 - Batch: 1382, Training Loss: 0.11928580881128857\n",
      "Epoch: 10 - Batch: 1383, Training Loss: 0.11937627435397746\n",
      "Epoch: 10 - Batch: 1384, Training Loss: 0.11945964099459387\n",
      "Epoch: 10 - Batch: 1385, Training Loss: 0.11954107061399157\n",
      "Epoch: 10 - Batch: 1386, Training Loss: 0.11963359117631493\n",
      "Epoch: 10 - Batch: 1387, Training Loss: 0.11971249354765387\n",
      "Epoch: 10 - Batch: 1388, Training Loss: 0.1197991206821913\n",
      "Epoch: 10 - Batch: 1389, Training Loss: 0.11987966733983105\n",
      "Epoch: 10 - Batch: 1390, Training Loss: 0.119964428610519\n",
      "Epoch: 10 - Batch: 1391, Training Loss: 0.12005265289377019\n",
      "Epoch: 10 - Batch: 1392, Training Loss: 0.12014347430979633\n",
      "Epoch: 10 - Batch: 1393, Training Loss: 0.12023143376382825\n",
      "Epoch: 10 - Batch: 1394, Training Loss: 0.12031880849555357\n",
      "Epoch: 10 - Batch: 1395, Training Loss: 0.12040162814864472\n",
      "Epoch: 10 - Batch: 1396, Training Loss: 0.1204889273675619\n",
      "Epoch: 10 - Batch: 1397, Training Loss: 0.12057717422496027\n",
      "Epoch: 10 - Batch: 1398, Training Loss: 0.1206721271238418\n",
      "Epoch: 10 - Batch: 1399, Training Loss: 0.12075466531055484\n",
      "Epoch: 10 - Batch: 1400, Training Loss: 0.12084071833028723\n",
      "Epoch: 10 - Batch: 1401, Training Loss: 0.12092606737517797\n",
      "Epoch: 10 - Batch: 1402, Training Loss: 0.12100846750374457\n",
      "Epoch: 10 - Batch: 1403, Training Loss: 0.12109911434389466\n",
      "Epoch: 10 - Batch: 1404, Training Loss: 0.12118339580584125\n",
      "Epoch: 10 - Batch: 1405, Training Loss: 0.12127403155661143\n",
      "Epoch: 10 - Batch: 1406, Training Loss: 0.1213534833310453\n",
      "Epoch: 10 - Batch: 1407, Training Loss: 0.12143760827519803\n",
      "Epoch: 10 - Batch: 1408, Training Loss: 0.1215243876338697\n",
      "Epoch: 10 - Batch: 1409, Training Loss: 0.12160465763773688\n",
      "Epoch: 10 - Batch: 1410, Training Loss: 0.12169965018037936\n",
      "Epoch: 10 - Batch: 1411, Training Loss: 0.12177987514097695\n",
      "Epoch: 10 - Batch: 1412, Training Loss: 0.12185879667006914\n",
      "Epoch: 10 - Batch: 1413, Training Loss: 0.12193947767292089\n",
      "Epoch: 10 - Batch: 1414, Training Loss: 0.12203958561986833\n",
      "Epoch: 10 - Batch: 1415, Training Loss: 0.12212101418901834\n",
      "Epoch: 10 - Batch: 1416, Training Loss: 0.12220515063647212\n",
      "Epoch: 10 - Batch: 1417, Training Loss: 0.12228829798537305\n",
      "Epoch: 10 - Batch: 1418, Training Loss: 0.12236855738173867\n",
      "Epoch: 10 - Batch: 1419, Training Loss: 0.12245351392435988\n",
      "Epoch: 10 - Batch: 1420, Training Loss: 0.12254510548083146\n",
      "Epoch: 10 - Batch: 1421, Training Loss: 0.12262231848553243\n",
      "Epoch: 10 - Batch: 1422, Training Loss: 0.12270946942801104\n",
      "Epoch: 10 - Batch: 1423, Training Loss: 0.1227923540790777\n",
      "Epoch: 10 - Batch: 1424, Training Loss: 0.12287240125745486\n",
      "Epoch: 10 - Batch: 1425, Training Loss: 0.12296995169685453\n",
      "Epoch: 10 - Batch: 1426, Training Loss: 0.12305583324820842\n",
      "Epoch: 10 - Batch: 1427, Training Loss: 0.12314678780475066\n",
      "Epoch: 10 - Batch: 1428, Training Loss: 0.12322738567123168\n",
      "Epoch: 10 - Batch: 1429, Training Loss: 0.1233124633012324\n",
      "Epoch: 10 - Batch: 1430, Training Loss: 0.12339982214332813\n",
      "Epoch: 10 - Batch: 1431, Training Loss: 0.12348734870380035\n",
      "Epoch: 10 - Batch: 1432, Training Loss: 0.1235781190901451\n",
      "Epoch: 10 - Batch: 1433, Training Loss: 0.12366347895876488\n",
      "Epoch: 10 - Batch: 1434, Training Loss: 0.12375512889665158\n",
      "Epoch: 10 - Batch: 1435, Training Loss: 0.12383894026155891\n",
      "Epoch: 10 - Batch: 1436, Training Loss: 0.12393137054582734\n",
      "Epoch: 10 - Batch: 1437, Training Loss: 0.12401705790638529\n",
      "Epoch: 10 - Batch: 1438, Training Loss: 0.12410400817742197\n",
      "Epoch: 10 - Batch: 1439, Training Loss: 0.12418987408626336\n",
      "Epoch: 10 - Batch: 1440, Training Loss: 0.12427394375032058\n",
      "Epoch: 10 - Batch: 1441, Training Loss: 0.1243606894516431\n",
      "Epoch: 10 - Batch: 1442, Training Loss: 0.12444723617427582\n",
      "Epoch: 10 - Batch: 1443, Training Loss: 0.12452722405072666\n",
      "Epoch: 10 - Batch: 1444, Training Loss: 0.12460780866555314\n",
      "Epoch: 10 - Batch: 1445, Training Loss: 0.12469837307979416\n",
      "Epoch: 10 - Batch: 1446, Training Loss: 0.12478719620809428\n",
      "Epoch: 10 - Batch: 1447, Training Loss: 0.12487943431557115\n",
      "Epoch: 10 - Batch: 1448, Training Loss: 0.12497153952964898\n",
      "Epoch: 10 - Batch: 1449, Training Loss: 0.1250561338603793\n",
      "Epoch: 10 - Batch: 1450, Training Loss: 0.1251345335275775\n",
      "Epoch: 10 - Batch: 1451, Training Loss: 0.12522756198350074\n",
      "Epoch: 10 - Batch: 1452, Training Loss: 0.1253186754425169\n",
      "Epoch: 10 - Batch: 1453, Training Loss: 0.12540221955645736\n",
      "Epoch: 10 - Batch: 1454, Training Loss: 0.1254884045552555\n",
      "Epoch: 10 - Batch: 1455, Training Loss: 0.1255682710477863\n",
      "Epoch: 10 - Batch: 1456, Training Loss: 0.12565276144724186\n",
      "Epoch: 10 - Batch: 1457, Training Loss: 0.12574167923992546\n",
      "Epoch: 10 - Batch: 1458, Training Loss: 0.12583650877795013\n",
      "Epoch: 10 - Batch: 1459, Training Loss: 0.1259181133624333\n",
      "Epoch: 10 - Batch: 1460, Training Loss: 0.12600229526707782\n",
      "Epoch: 10 - Batch: 1461, Training Loss: 0.12608884950207042\n",
      "Epoch: 10 - Batch: 1462, Training Loss: 0.12617135177031283\n",
      "Epoch: 10 - Batch: 1463, Training Loss: 0.12625478144432378\n",
      "Epoch: 10 - Batch: 1464, Training Loss: 0.126341175794552\n",
      "Epoch: 10 - Batch: 1465, Training Loss: 0.12643020127469035\n",
      "Epoch: 10 - Batch: 1466, Training Loss: 0.12650509195318863\n",
      "Epoch: 10 - Batch: 1467, Training Loss: 0.1265817730445075\n",
      "Epoch: 10 - Batch: 1468, Training Loss: 0.12665787578296306\n",
      "Epoch: 10 - Batch: 1469, Training Loss: 0.1267431043706229\n",
      "Epoch: 10 - Batch: 1470, Training Loss: 0.12682529066876194\n",
      "Epoch: 10 - Batch: 1471, Training Loss: 0.12691406206309697\n",
      "Epoch: 10 - Batch: 1472, Training Loss: 0.12700168204618922\n",
      "Epoch: 10 - Batch: 1473, Training Loss: 0.12708963801587003\n",
      "Epoch: 10 - Batch: 1474, Training Loss: 0.1271774327169307\n",
      "Epoch: 10 - Batch: 1475, Training Loss: 0.1272632702629068\n",
      "Epoch: 10 - Batch: 1476, Training Loss: 0.1273566767894609\n",
      "Epoch: 10 - Batch: 1477, Training Loss: 0.12743467578716935\n",
      "Epoch: 10 - Batch: 1478, Training Loss: 0.12753355883642611\n",
      "Epoch: 10 - Batch: 1479, Training Loss: 0.12762511981215643\n",
      "Epoch: 10 - Batch: 1480, Training Loss: 0.12770729232733918\n",
      "Epoch: 10 - Batch: 1481, Training Loss: 0.12779747202967728\n",
      "Epoch: 10 - Batch: 1482, Training Loss: 0.12788822466683625\n",
      "Epoch: 10 - Batch: 1483, Training Loss: 0.12797237342072165\n",
      "Epoch: 10 - Batch: 1484, Training Loss: 0.12805538725225288\n",
      "Epoch: 10 - Batch: 1485, Training Loss: 0.12814031386899316\n",
      "Epoch: 10 - Batch: 1486, Training Loss: 0.12822851188987444\n",
      "Epoch: 10 - Batch: 1487, Training Loss: 0.1283107740714973\n",
      "Epoch: 10 - Batch: 1488, Training Loss: 0.12839256191802262\n",
      "Epoch: 10 - Batch: 1489, Training Loss: 0.12847538594508645\n",
      "Epoch: 10 - Batch: 1490, Training Loss: 0.1285606674343397\n",
      "Epoch: 10 - Batch: 1491, Training Loss: 0.12864180528257615\n",
      "Epoch: 10 - Batch: 1492, Training Loss: 0.12872839742533207\n",
      "Epoch: 10 - Batch: 1493, Training Loss: 0.1288122454624172\n",
      "Epoch: 10 - Batch: 1494, Training Loss: 0.12890435228893413\n",
      "Epoch: 10 - Batch: 1495, Training Loss: 0.1289910470841338\n",
      "Epoch: 10 - Batch: 1496, Training Loss: 0.12907478191687496\n",
      "Epoch: 10 - Batch: 1497, Training Loss: 0.12916121495946328\n",
      "Epoch: 10 - Batch: 1498, Training Loss: 0.1292449553338054\n",
      "Epoch: 10 - Batch: 1499, Training Loss: 0.1293348844532251\n",
      "Epoch: 10 - Batch: 1500, Training Loss: 0.12941768435548193\n",
      "Epoch: 10 - Batch: 1501, Training Loss: 0.12950740650815157\n",
      "Epoch: 10 - Batch: 1502, Training Loss: 0.12959995914627465\n",
      "Epoch: 10 - Batch: 1503, Training Loss: 0.12968873545206205\n",
      "Epoch: 10 - Batch: 1504, Training Loss: 0.12977299893623956\n",
      "Epoch: 10 - Batch: 1505, Training Loss: 0.12985719915843918\n",
      "Epoch: 10 - Batch: 1506, Training Loss: 0.12994313441451708\n",
      "Epoch: 10 - Batch: 1507, Training Loss: 0.13002361389955083\n",
      "Epoch: 10 - Batch: 1508, Training Loss: 0.13010732095015187\n",
      "Epoch: 10 - Batch: 1509, Training Loss: 0.1301989956924176\n",
      "Epoch: 10 - Batch: 1510, Training Loss: 0.13029352884436918\n",
      "Epoch: 10 - Batch: 1511, Training Loss: 0.13038142480783993\n",
      "Epoch: 10 - Batch: 1512, Training Loss: 0.130475435288589\n",
      "Epoch: 10 - Batch: 1513, Training Loss: 0.13055422192263366\n",
      "Epoch: 10 - Batch: 1514, Training Loss: 0.13064049165788574\n",
      "Epoch: 10 - Batch: 1515, Training Loss: 0.13072387108681213\n",
      "Epoch: 10 - Batch: 1516, Training Loss: 0.13080803622456136\n",
      "Epoch: 10 - Batch: 1517, Training Loss: 0.13089928603043802\n",
      "Epoch: 10 - Batch: 1518, Training Loss: 0.13098610223426946\n",
      "Epoch: 10 - Batch: 1519, Training Loss: 0.13108285516500473\n",
      "Epoch: 10 - Batch: 1520, Training Loss: 0.13117039985769424\n",
      "Epoch: 10 - Batch: 1521, Training Loss: 0.13125559754336058\n",
      "Epoch: 10 - Batch: 1522, Training Loss: 0.1313444726959884\n",
      "Epoch: 10 - Batch: 1523, Training Loss: 0.13142906209673258\n",
      "Epoch: 10 - Batch: 1524, Training Loss: 0.13151029250885718\n",
      "Epoch: 10 - Batch: 1525, Training Loss: 0.1315963094989162\n",
      "Epoch: 10 - Batch: 1526, Training Loss: 0.13167598300534694\n",
      "Epoch: 10 - Batch: 1527, Training Loss: 0.13177045251228917\n",
      "Epoch: 10 - Batch: 1528, Training Loss: 0.13186549472546893\n",
      "Epoch: 10 - Batch: 1529, Training Loss: 0.13194480283219817\n",
      "Epoch: 10 - Batch: 1530, Training Loss: 0.132031725302561\n",
      "Epoch: 10 - Batch: 1531, Training Loss: 0.13212205915036881\n",
      "Epoch: 10 - Batch: 1532, Training Loss: 0.13221959180969306\n",
      "Epoch: 10 - Batch: 1533, Training Loss: 0.13230264406175557\n",
      "Epoch: 10 - Batch: 1534, Training Loss: 0.132392228224236\n",
      "Epoch: 10 - Batch: 1535, Training Loss: 0.1324870862548624\n",
      "Epoch: 10 - Batch: 1536, Training Loss: 0.1325827971978073\n",
      "Epoch: 10 - Batch: 1537, Training Loss: 0.13267214273783698\n",
      "Epoch: 10 - Batch: 1538, Training Loss: 0.13276334014253235\n",
      "Epoch: 10 - Batch: 1539, Training Loss: 0.13285258997410881\n",
      "Epoch: 10 - Batch: 1540, Training Loss: 0.13294322700988792\n",
      "Epoch: 10 - Batch: 1541, Training Loss: 0.13302607766381938\n",
      "Epoch: 10 - Batch: 1542, Training Loss: 0.13311953073487945\n",
      "Epoch: 10 - Batch: 1543, Training Loss: 0.1331994855532401\n",
      "Epoch: 10 - Batch: 1544, Training Loss: 0.13329147089699014\n",
      "Epoch: 10 - Batch: 1545, Training Loss: 0.133367449323188\n",
      "Epoch: 10 - Batch: 1546, Training Loss: 0.13346227647035475\n",
      "Epoch: 10 - Batch: 1547, Training Loss: 0.13355001969074531\n",
      "Epoch: 10 - Batch: 1548, Training Loss: 0.13364149985918358\n",
      "Epoch: 10 - Batch: 1549, Training Loss: 0.1337235154426513\n",
      "Epoch: 10 - Batch: 1550, Training Loss: 0.13381313014262747\n",
      "Epoch: 10 - Batch: 1551, Training Loss: 0.13390887377298688\n",
      "Epoch: 10 - Batch: 1552, Training Loss: 0.1340023849350995\n",
      "Epoch: 10 - Batch: 1553, Training Loss: 0.13408774859696676\n",
      "Epoch: 10 - Batch: 1554, Training Loss: 0.13418231404094555\n",
      "Epoch: 10 - Batch: 1555, Training Loss: 0.13425992526867694\n",
      "Epoch: 10 - Batch: 1556, Training Loss: 0.1343529655553314\n",
      "Epoch: 10 - Batch: 1557, Training Loss: 0.13443697932150037\n",
      "Epoch: 10 - Batch: 1558, Training Loss: 0.13452064213814033\n",
      "Epoch: 10 - Batch: 1559, Training Loss: 0.134609497101301\n",
      "Epoch: 10 - Batch: 1560, Training Loss: 0.1346941965741799\n",
      "Epoch: 10 - Batch: 1561, Training Loss: 0.1347869776967746\n",
      "Epoch: 10 - Batch: 1562, Training Loss: 0.13487578242720655\n",
      "Epoch: 10 - Batch: 1563, Training Loss: 0.13495660528145223\n",
      "Epoch: 10 - Batch: 1564, Training Loss: 0.13503567516111814\n",
      "Epoch: 10 - Batch: 1565, Training Loss: 0.13511491451369192\n",
      "Epoch: 10 - Batch: 1566, Training Loss: 0.1352015991153112\n",
      "Epoch: 10 - Batch: 1567, Training Loss: 0.13528854869442003\n",
      "Epoch: 10 - Batch: 1568, Training Loss: 0.13537808761593714\n",
      "Epoch: 10 - Batch: 1569, Training Loss: 0.13546173395604438\n",
      "Epoch: 10 - Batch: 1570, Training Loss: 0.13554283492205352\n",
      "Epoch: 10 - Batch: 1571, Training Loss: 0.13563658278528137\n",
      "Epoch: 10 - Batch: 1572, Training Loss: 0.1357235662565006\n",
      "Epoch: 10 - Batch: 1573, Training Loss: 0.13581252471848113\n",
      "Epoch: 10 - Batch: 1574, Training Loss: 0.1359048383572999\n",
      "Epoch: 10 - Batch: 1575, Training Loss: 0.13598110629971547\n",
      "Epoch: 10 - Batch: 1576, Training Loss: 0.13607361714415883\n",
      "Epoch: 10 - Batch: 1577, Training Loss: 0.1361587374512829\n",
      "Epoch: 10 - Batch: 1578, Training Loss: 0.136246575502861\n",
      "Epoch: 10 - Batch: 1579, Training Loss: 0.13633554069878254\n",
      "Epoch: 10 - Batch: 1580, Training Loss: 0.1364279454311427\n",
      "Epoch: 10 - Batch: 1581, Training Loss: 0.13651839915545624\n",
      "Epoch: 10 - Batch: 1582, Training Loss: 0.13660692591621706\n",
      "Epoch: 10 - Batch: 1583, Training Loss: 0.1366858036438031\n",
      "Epoch: 10 - Batch: 1584, Training Loss: 0.13677689383674418\n",
      "Epoch: 10 - Batch: 1585, Training Loss: 0.13686313082280246\n",
      "Epoch: 10 - Batch: 1586, Training Loss: 0.13694491690861843\n",
      "Epoch: 10 - Batch: 1587, Training Loss: 0.13703652212448778\n",
      "Epoch: 10 - Batch: 1588, Training Loss: 0.13711994592946758\n",
      "Epoch: 10 - Batch: 1589, Training Loss: 0.13720008126687055\n",
      "Epoch: 10 - Batch: 1590, Training Loss: 0.13728452552956333\n",
      "Epoch: 10 - Batch: 1591, Training Loss: 0.13737314427966502\n",
      "Epoch: 10 - Batch: 1592, Training Loss: 0.1374613716086345\n",
      "Epoch: 10 - Batch: 1593, Training Loss: 0.13754875638567\n",
      "Epoch: 10 - Batch: 1594, Training Loss: 0.1376344610629588\n",
      "Epoch: 10 - Batch: 1595, Training Loss: 0.13772315993460257\n",
      "Epoch: 10 - Batch: 1596, Training Loss: 0.13780413628563556\n",
      "Epoch: 10 - Batch: 1597, Training Loss: 0.13788632714308513\n",
      "Epoch: 10 - Batch: 1598, Training Loss: 0.13797248219746855\n",
      "Epoch: 10 - Batch: 1599, Training Loss: 0.13806555007474736\n",
      "Epoch: 10 - Batch: 1600, Training Loss: 0.1381506509399335\n",
      "Epoch: 10 - Batch: 1601, Training Loss: 0.13822974533979374\n",
      "Epoch: 10 - Batch: 1602, Training Loss: 0.1383137117274365\n",
      "Epoch: 10 - Batch: 1603, Training Loss: 0.13839276033032\n",
      "Epoch: 10 - Batch: 1604, Training Loss: 0.13847488221120874\n",
      "Epoch: 10 - Batch: 1605, Training Loss: 0.13856017067534215\n",
      "Epoch: 10 - Batch: 1606, Training Loss: 0.1386473240519242\n",
      "Epoch: 10 - Batch: 1607, Training Loss: 0.13874103526150805\n",
      "Epoch: 10 - Batch: 1608, Training Loss: 0.13882593365129745\n",
      "Epoch: 10 - Batch: 1609, Training Loss: 0.13890880018250265\n",
      "Epoch: 10 - Batch: 1610, Training Loss: 0.13899130264843873\n",
      "Epoch: 10 - Batch: 1611, Training Loss: 0.1390832505601259\n",
      "Epoch: 10 - Batch: 1612, Training Loss: 0.1391681157991562\n",
      "Epoch: 10 - Batch: 1613, Training Loss: 0.1392571467529383\n",
      "Epoch: 10 - Batch: 1614, Training Loss: 0.13934043815257538\n",
      "Epoch: 10 - Batch: 1615, Training Loss: 0.13942864506967229\n",
      "Epoch: 10 - Batch: 1616, Training Loss: 0.13950885583957037\n",
      "Epoch: 10 - Batch: 1617, Training Loss: 0.139588312834353\n",
      "Epoch: 10 - Batch: 1618, Training Loss: 0.13967968130586159\n",
      "Epoch: 10 - Batch: 1619, Training Loss: 0.1397719046357358\n",
      "Epoch: 10 - Batch: 1620, Training Loss: 0.1398543676679605\n",
      "Epoch: 10 - Batch: 1621, Training Loss: 0.13993011198159475\n",
      "Epoch: 10 - Batch: 1622, Training Loss: 0.14001522558566745\n",
      "Epoch: 10 - Batch: 1623, Training Loss: 0.14009756161205805\n",
      "Epoch: 10 - Batch: 1624, Training Loss: 0.14019823558072544\n",
      "Epoch: 10 - Batch: 1625, Training Loss: 0.14029022719507195\n",
      "Epoch: 10 - Batch: 1626, Training Loss: 0.1403690277418094\n",
      "Epoch: 10 - Batch: 1627, Training Loss: 0.1404609300603914\n",
      "Epoch: 10 - Batch: 1628, Training Loss: 0.14054988660103646\n",
      "Epoch: 10 - Batch: 1629, Training Loss: 0.1406391166756007\n",
      "Epoch: 10 - Batch: 1630, Training Loss: 0.14072857934913627\n",
      "Epoch: 10 - Batch: 1631, Training Loss: 0.14081218736138115\n",
      "Epoch: 10 - Batch: 1632, Training Loss: 0.1409091808111909\n",
      "Epoch: 10 - Batch: 1633, Training Loss: 0.14098704380800278\n",
      "Epoch: 10 - Batch: 1634, Training Loss: 0.14107768471734244\n",
      "Epoch: 10 - Batch: 1635, Training Loss: 0.14116009122153025\n",
      "Epoch: 10 - Batch: 1636, Training Loss: 0.14124857560788617\n",
      "Epoch: 10 - Batch: 1637, Training Loss: 0.14132783459391365\n",
      "Epoch: 10 - Batch: 1638, Training Loss: 0.14140906642078366\n",
      "Epoch: 10 - Batch: 1639, Training Loss: 0.14149273927037792\n",
      "Epoch: 10 - Batch: 1640, Training Loss: 0.14158411904131596\n",
      "Epoch: 10 - Batch: 1641, Training Loss: 0.14166132662179656\n",
      "Epoch: 10 - Batch: 1642, Training Loss: 0.14174046039358892\n",
      "Epoch: 10 - Batch: 1643, Training Loss: 0.14182362856828357\n",
      "Epoch: 10 - Batch: 1644, Training Loss: 0.1419138904679474\n",
      "Epoch: 10 - Batch: 1645, Training Loss: 0.14200479260477458\n",
      "Epoch: 10 - Batch: 1646, Training Loss: 0.1420944669711748\n",
      "Epoch: 10 - Batch: 1647, Training Loss: 0.14217895657985563\n",
      "Epoch: 10 - Batch: 1648, Training Loss: 0.14225967091262637\n",
      "Epoch: 10 - Batch: 1649, Training Loss: 0.14233881099208276\n",
      "Epoch: 10 - Batch: 1650, Training Loss: 0.14241408153281085\n",
      "Epoch: 10 - Batch: 1651, Training Loss: 0.14249614075715863\n",
      "Epoch: 10 - Batch: 1652, Training Loss: 0.14258696714641642\n",
      "Epoch: 10 - Batch: 1653, Training Loss: 0.1426706583197437\n",
      "Epoch: 10 - Batch: 1654, Training Loss: 0.14276003836967657\n",
      "Epoch: 10 - Batch: 1655, Training Loss: 0.142849522282927\n",
      "Epoch: 10 - Batch: 1656, Training Loss: 0.14294533450681574\n",
      "Epoch: 10 - Batch: 1657, Training Loss: 0.14303112849222488\n",
      "Epoch: 10 - Batch: 1658, Training Loss: 0.1431129210525089\n",
      "Epoch: 10 - Batch: 1659, Training Loss: 0.1431925213443205\n",
      "Epoch: 10 - Batch: 1660, Training Loss: 0.14327827732311949\n",
      "Epoch: 10 - Batch: 1661, Training Loss: 0.1433647819802437\n",
      "Epoch: 10 - Batch: 1662, Training Loss: 0.14344661074318302\n",
      "Epoch: 10 - Batch: 1663, Training Loss: 0.1435266997646633\n",
      "Epoch: 10 - Batch: 1664, Training Loss: 0.14361436151054566\n",
      "Epoch: 10 - Batch: 1665, Training Loss: 0.14369781980689486\n",
      "Epoch: 10 - Batch: 1666, Training Loss: 0.1437819979244698\n",
      "Epoch: 10 - Batch: 1667, Training Loss: 0.14386750550733673\n",
      "Epoch: 10 - Batch: 1668, Training Loss: 0.14395025207554524\n",
      "Epoch: 10 - Batch: 1669, Training Loss: 0.14403985769396793\n",
      "Epoch: 10 - Batch: 1670, Training Loss: 0.14413381285755394\n",
      "Epoch: 10 - Batch: 1671, Training Loss: 0.1442056894487706\n",
      "Epoch: 10 - Batch: 1672, Training Loss: 0.14429287540774244\n",
      "Epoch: 10 - Batch: 1673, Training Loss: 0.14437717767694894\n",
      "Epoch: 10 - Batch: 1674, Training Loss: 0.1444608946245899\n",
      "Epoch: 10 - Batch: 1675, Training Loss: 0.144541711134104\n",
      "Epoch: 10 - Batch: 1676, Training Loss: 0.14463889327017623\n",
      "Epoch: 10 - Batch: 1677, Training Loss: 0.14472237750764314\n",
      "Epoch: 10 - Batch: 1678, Training Loss: 0.14481809777308063\n",
      "Epoch: 10 - Batch: 1679, Training Loss: 0.14489561124746478\n",
      "Epoch: 10 - Batch: 1680, Training Loss: 0.14498715771742127\n",
      "Epoch: 10 - Batch: 1681, Training Loss: 0.1450714682897624\n",
      "Epoch: 10 - Batch: 1682, Training Loss: 0.14515769128899272\n",
      "Epoch: 10 - Batch: 1683, Training Loss: 0.14524574870642146\n",
      "Epoch: 10 - Batch: 1684, Training Loss: 0.14533653656394524\n",
      "Epoch: 10 - Batch: 1685, Training Loss: 0.1454251360265572\n",
      "Epoch: 10 - Batch: 1686, Training Loss: 0.1455150289680333\n",
      "Epoch: 10 - Batch: 1687, Training Loss: 0.14559789350376792\n",
      "Epoch: 10 - Batch: 1688, Training Loss: 0.14568757848384764\n",
      "Epoch: 10 - Batch: 1689, Training Loss: 0.14577618749753554\n",
      "Epoch: 10 - Batch: 1690, Training Loss: 0.14586300326273413\n",
      "Epoch: 10 - Batch: 1691, Training Loss: 0.14594731183342674\n",
      "Epoch: 10 - Batch: 1692, Training Loss: 0.14603465658366976\n",
      "Epoch: 10 - Batch: 1693, Training Loss: 0.14612754653045787\n",
      "Epoch: 10 - Batch: 1694, Training Loss: 0.1462168016503403\n",
      "Epoch: 10 - Batch: 1695, Training Loss: 0.1463027510106267\n",
      "Epoch: 10 - Batch: 1696, Training Loss: 0.14639039792330505\n",
      "Epoch: 10 - Batch: 1697, Training Loss: 0.14648221995971886\n",
      "Epoch: 10 - Batch: 1698, Training Loss: 0.14657106042328363\n",
      "Epoch: 10 - Batch: 1699, Training Loss: 0.14665748851117408\n",
      "Epoch: 10 - Batch: 1700, Training Loss: 0.14674739587193303\n",
      "Epoch: 10 - Batch: 1701, Training Loss: 0.14683886907166904\n",
      "Epoch: 10 - Batch: 1702, Training Loss: 0.1469280696589852\n",
      "Epoch: 10 - Batch: 1703, Training Loss: 0.1470128217430949\n",
      "Epoch: 10 - Batch: 1704, Training Loss: 0.1470965452825845\n",
      "Epoch: 10 - Batch: 1705, Training Loss: 0.147187058086419\n",
      "Epoch: 10 - Batch: 1706, Training Loss: 0.14727540246240337\n",
      "Epoch: 10 - Batch: 1707, Training Loss: 0.14736856305184057\n",
      "Epoch: 10 - Batch: 1708, Training Loss: 0.1474488172216795\n",
      "Epoch: 10 - Batch: 1709, Training Loss: 0.14753341959607147\n",
      "Epoch: 10 - Batch: 1710, Training Loss: 0.14761952016113408\n",
      "Epoch: 10 - Batch: 1711, Training Loss: 0.1476992292153598\n",
      "Epoch: 10 - Batch: 1712, Training Loss: 0.1477786613378062\n",
      "Epoch: 10 - Batch: 1713, Training Loss: 0.14786556340247442\n",
      "Epoch: 10 - Batch: 1714, Training Loss: 0.14795447147727803\n",
      "Epoch: 10 - Batch: 1715, Training Loss: 0.14804522078453408\n",
      "Epoch: 10 - Batch: 1716, Training Loss: 0.14812798943265557\n",
      "Epoch: 10 - Batch: 1717, Training Loss: 0.14821575323172273\n",
      "Epoch: 10 - Batch: 1718, Training Loss: 0.14829907913508503\n",
      "Epoch: 10 - Batch: 1719, Training Loss: 0.14838264743561175\n",
      "Epoch: 10 - Batch: 1720, Training Loss: 0.1484685759701045\n",
      "Epoch: 10 - Batch: 1721, Training Loss: 0.14855309037080847\n",
      "Epoch: 10 - Batch: 1722, Training Loss: 0.14863849371350424\n",
      "Epoch: 10 - Batch: 1723, Training Loss: 0.14873105375896242\n",
      "Epoch: 10 - Batch: 1724, Training Loss: 0.14881379862824087\n",
      "Epoch: 10 - Batch: 1725, Training Loss: 0.14889965185997497\n",
      "Epoch: 10 - Batch: 1726, Training Loss: 0.14898253935906622\n",
      "Epoch: 10 - Batch: 1727, Training Loss: 0.14907649448558466\n",
      "Epoch: 10 - Batch: 1728, Training Loss: 0.14916320211233983\n",
      "Epoch: 10 - Batch: 1729, Training Loss: 0.14924991996974296\n",
      "Epoch: 10 - Batch: 1730, Training Loss: 0.1493381440553005\n",
      "Epoch: 10 - Batch: 1731, Training Loss: 0.14942137483093473\n",
      "Epoch: 10 - Batch: 1732, Training Loss: 0.14950753529718266\n",
      "Epoch: 10 - Batch: 1733, Training Loss: 0.14959012172115382\n",
      "Epoch: 10 - Batch: 1734, Training Loss: 0.14968126323413888\n",
      "Epoch: 10 - Batch: 1735, Training Loss: 0.14977014997655874\n",
      "Epoch: 10 - Batch: 1736, Training Loss: 0.14985430654552248\n",
      "Epoch: 10 - Batch: 1737, Training Loss: 0.14994401023854467\n",
      "Epoch: 10 - Batch: 1738, Training Loss: 0.15003733857724796\n",
      "Epoch: 10 - Batch: 1739, Training Loss: 0.15012233113100873\n",
      "Epoch: 10 - Batch: 1740, Training Loss: 0.15020674055019026\n",
      "Epoch: 10 - Batch: 1741, Training Loss: 0.15029582206413125\n",
      "Epoch: 10 - Batch: 1742, Training Loss: 0.15038295281096478\n",
      "Epoch: 10 - Batch: 1743, Training Loss: 0.15047342829393906\n",
      "Epoch: 10 - Batch: 1744, Training Loss: 0.15055833195967855\n",
      "Epoch: 10 - Batch: 1745, Training Loss: 0.15064502536484456\n",
      "Epoch: 10 - Batch: 1746, Training Loss: 0.1507374123356631\n",
      "Epoch: 10 - Batch: 1747, Training Loss: 0.1508242507738557\n",
      "Epoch: 10 - Batch: 1748, Training Loss: 0.1509143017837855\n",
      "Epoch: 10 - Batch: 1749, Training Loss: 0.15100269626894006\n",
      "Epoch: 10 - Batch: 1750, Training Loss: 0.15107990543714803\n",
      "Epoch: 10 - Batch: 1751, Training Loss: 0.15116100186238043\n",
      "Epoch: 10 - Batch: 1752, Training Loss: 0.15123954884843843\n",
      "Epoch: 10 - Batch: 1753, Training Loss: 0.15131871254611173\n",
      "Epoch: 10 - Batch: 1754, Training Loss: 0.15140285407182194\n",
      "Epoch: 10 - Batch: 1755, Training Loss: 0.15149446179592985\n",
      "Epoch: 10 - Batch: 1756, Training Loss: 0.15158512852900657\n",
      "Epoch: 10 - Batch: 1757, Training Loss: 0.1516757246113041\n",
      "Epoch: 10 - Batch: 1758, Training Loss: 0.15176677813824532\n",
      "Epoch: 10 - Batch: 1759, Training Loss: 0.15184691456914146\n",
      "Epoch: 10 - Batch: 1760, Training Loss: 0.15193902798132913\n",
      "Epoch: 10 - Batch: 1761, Training Loss: 0.15201898276880013\n",
      "Epoch: 10 - Batch: 1762, Training Loss: 0.15210040983671375\n",
      "Epoch: 10 - Batch: 1763, Training Loss: 0.15219503897512532\n",
      "Epoch: 10 - Batch: 1764, Training Loss: 0.1522791705481943\n",
      "Epoch: 10 - Batch: 1765, Training Loss: 0.15237057792829045\n",
      "Epoch: 10 - Batch: 1766, Training Loss: 0.15246264999160916\n",
      "Epoch: 10 - Batch: 1767, Training Loss: 0.1525454733885244\n",
      "Epoch: 10 - Batch: 1768, Training Loss: 0.15263474331072116\n",
      "Epoch: 10 - Batch: 1769, Training Loss: 0.15272282726852654\n",
      "Epoch: 10 - Batch: 1770, Training Loss: 0.15280021635651786\n",
      "Epoch: 10 - Batch: 1771, Training Loss: 0.1528891037661539\n",
      "Epoch: 10 - Batch: 1772, Training Loss: 0.15296896705851823\n",
      "Epoch: 10 - Batch: 1773, Training Loss: 0.15305279848933417\n",
      "Epoch: 10 - Batch: 1774, Training Loss: 0.15313017921241165\n",
      "Epoch: 10 - Batch: 1775, Training Loss: 0.15321171960204988\n",
      "Epoch: 10 - Batch: 1776, Training Loss: 0.1532971294439254\n",
      "Epoch: 10 - Batch: 1777, Training Loss: 0.15337955715693843\n",
      "Epoch: 10 - Batch: 1778, Training Loss: 0.15346409095015692\n",
      "Epoch: 10 - Batch: 1779, Training Loss: 0.15354611190224365\n",
      "Epoch: 10 - Batch: 1780, Training Loss: 0.15362649186704289\n",
      "Epoch: 10 - Batch: 1781, Training Loss: 0.153705929043034\n",
      "Epoch: 10 - Batch: 1782, Training Loss: 0.1537917032220075\n",
      "Epoch: 10 - Batch: 1783, Training Loss: 0.1538753023256809\n",
      "Epoch: 10 - Batch: 1784, Training Loss: 0.15396323083447383\n",
      "Epoch: 10 - Batch: 1785, Training Loss: 0.1540392375432239\n",
      "Epoch: 10 - Batch: 1786, Training Loss: 0.15412520641958338\n",
      "Epoch: 10 - Batch: 1787, Training Loss: 0.1542094662526057\n",
      "Epoch: 10 - Batch: 1788, Training Loss: 0.1542956937679011\n",
      "Epoch: 10 - Batch: 1789, Training Loss: 0.15437804471398664\n",
      "Epoch: 10 - Batch: 1790, Training Loss: 0.154469980078303\n",
      "Epoch: 10 - Batch: 1791, Training Loss: 0.15455544028882165\n",
      "Epoch: 10 - Batch: 1792, Training Loss: 0.1546448888149032\n",
      "Epoch: 10 - Batch: 1793, Training Loss: 0.15473254674282635\n",
      "Epoch: 10 - Batch: 1794, Training Loss: 0.15482025267324637\n",
      "Epoch: 10 - Batch: 1795, Training Loss: 0.1548989713439103\n",
      "Epoch: 10 - Batch: 1796, Training Loss: 0.1549900131945685\n",
      "Epoch: 10 - Batch: 1797, Training Loss: 0.1550766110766191\n",
      "Epoch: 10 - Batch: 1798, Training Loss: 0.15516279084889054\n",
      "Epoch: 10 - Batch: 1799, Training Loss: 0.15524812567837004\n",
      "Epoch: 10 - Batch: 1800, Training Loss: 0.15533766982641387\n",
      "Epoch: 10 - Batch: 1801, Training Loss: 0.155423497141742\n",
      "Epoch: 10 - Batch: 1802, Training Loss: 0.1555081923086054\n",
      "Epoch: 10 - Batch: 1803, Training Loss: 0.15559716292256937\n",
      "Epoch: 10 - Batch: 1804, Training Loss: 0.1556922885254149\n",
      "Epoch: 10 - Batch: 1805, Training Loss: 0.15577587673122412\n",
      "Epoch: 10 - Batch: 1806, Training Loss: 0.1558624477701796\n",
      "Epoch: 10 - Batch: 1807, Training Loss: 0.155945415810912\n",
      "Epoch: 10 - Batch: 1808, Training Loss: 0.156031778215077\n",
      "Epoch: 10 - Batch: 1809, Training Loss: 0.15611366999287707\n",
      "Epoch: 10 - Batch: 1810, Training Loss: 0.15619786424719873\n",
      "Epoch: 10 - Batch: 1811, Training Loss: 0.1562912892890016\n",
      "Epoch: 10 - Batch: 1812, Training Loss: 0.15637123495167365\n",
      "Epoch: 10 - Batch: 1813, Training Loss: 0.15645223952347959\n",
      "Epoch: 10 - Batch: 1814, Training Loss: 0.15653272080629027\n",
      "Epoch: 10 - Batch: 1815, Training Loss: 0.15662839845020576\n",
      "Epoch: 10 - Batch: 1816, Training Loss: 0.15670757795719562\n",
      "Epoch: 10 - Batch: 1817, Training Loss: 0.15679396560437248\n",
      "Epoch: 10 - Batch: 1818, Training Loss: 0.15688410157316163\n",
      "Epoch: 10 - Batch: 1819, Training Loss: 0.15696669785092718\n",
      "Epoch: 10 - Batch: 1820, Training Loss: 0.15706753095065185\n",
      "Epoch: 10 - Batch: 1821, Training Loss: 0.1571569965401692\n",
      "Epoch: 10 - Batch: 1822, Training Loss: 0.1572514258226253\n",
      "Epoch: 10 - Batch: 1823, Training Loss: 0.15733986557963278\n",
      "Epoch: 10 - Batch: 1824, Training Loss: 0.15743254215315997\n",
      "Epoch: 10 - Batch: 1825, Training Loss: 0.1575228184103274\n",
      "Epoch: 10 - Batch: 1826, Training Loss: 0.15760949311490677\n",
      "Epoch: 10 - Batch: 1827, Training Loss: 0.15770538636849293\n",
      "Epoch: 10 - Batch: 1828, Training Loss: 0.15779203287083315\n",
      "Epoch: 10 - Batch: 1829, Training Loss: 0.15787683501468963\n",
      "Epoch: 10 - Batch: 1830, Training Loss: 0.1579652366168167\n",
      "Epoch: 10 - Batch: 1831, Training Loss: 0.15804537110166567\n",
      "Epoch: 10 - Batch: 1832, Training Loss: 0.15814091150292117\n",
      "Epoch: 10 - Batch: 1833, Training Loss: 0.15822463570344902\n",
      "Epoch: 10 - Batch: 1834, Training Loss: 0.158310216167327\n",
      "Epoch: 10 - Batch: 1835, Training Loss: 0.15839567083006673\n",
      "Epoch: 10 - Batch: 1836, Training Loss: 0.15847976590393986\n",
      "Epoch: 10 - Batch: 1837, Training Loss: 0.15857414617071894\n",
      "Epoch: 10 - Batch: 1838, Training Loss: 0.15865183448663003\n",
      "Epoch: 10 - Batch: 1839, Training Loss: 0.15873664632365478\n",
      "Epoch: 10 - Batch: 1840, Training Loss: 0.15881634581444867\n",
      "Epoch: 10 - Batch: 1841, Training Loss: 0.15889817725243063\n",
      "Epoch: 10 - Batch: 1842, Training Loss: 0.15897543964966218\n",
      "Epoch: 10 - Batch: 1843, Training Loss: 0.15905814203951094\n",
      "Epoch: 10 - Batch: 1844, Training Loss: 0.15914680379416615\n",
      "Epoch: 10 - Batch: 1845, Training Loss: 0.15922951491960444\n",
      "Epoch: 10 - Batch: 1846, Training Loss: 0.15931307317482102\n",
      "Epoch: 10 - Batch: 1847, Training Loss: 0.15940370205573576\n",
      "Epoch: 10 - Batch: 1848, Training Loss: 0.15948814640491954\n",
      "Epoch: 10 - Batch: 1849, Training Loss: 0.1595776197971594\n",
      "Epoch: 10 - Batch: 1850, Training Loss: 0.15966205916693357\n",
      "Epoch: 10 - Batch: 1851, Training Loss: 0.15975036737115228\n",
      "Epoch: 10 - Batch: 1852, Training Loss: 0.15983067009950158\n",
      "Epoch: 10 - Batch: 1853, Training Loss: 0.1599116130588363\n",
      "Epoch: 10 - Batch: 1854, Training Loss: 0.1599944242172771\n",
      "Epoch: 10 - Batch: 1855, Training Loss: 0.16008324177926453\n",
      "Epoch: 10 - Batch: 1856, Training Loss: 0.16017072585412914\n",
      "Epoch: 10 - Batch: 1857, Training Loss: 0.16024943228014074\n",
      "Epoch: 10 - Batch: 1858, Training Loss: 0.16034473634477872\n",
      "Epoch: 10 - Batch: 1859, Training Loss: 0.1604284971557051\n",
      "Epoch: 10 - Batch: 1860, Training Loss: 0.16051370149499938\n",
      "Epoch: 10 - Batch: 1861, Training Loss: 0.16060794991862715\n",
      "Epoch: 10 - Batch: 1862, Training Loss: 0.16070223003502312\n",
      "Epoch: 10 - Batch: 1863, Training Loss: 0.16079020172752947\n",
      "Epoch: 10 - Batch: 1864, Training Loss: 0.16087114129518199\n",
      "Epoch: 10 - Batch: 1865, Training Loss: 0.16096289284762064\n",
      "Epoch: 10 - Batch: 1866, Training Loss: 0.16104332727702894\n",
      "Epoch: 10 - Batch: 1867, Training Loss: 0.16112805745420763\n",
      "Epoch: 10 - Batch: 1868, Training Loss: 0.1612140520183898\n",
      "Epoch: 10 - Batch: 1869, Training Loss: 0.1612953126220462\n",
      "Epoch: 10 - Batch: 1870, Training Loss: 0.1613831413387758\n",
      "Epoch: 10 - Batch: 1871, Training Loss: 0.16146823805857258\n",
      "Epoch: 10 - Batch: 1872, Training Loss: 0.16155056374929042\n",
      "Epoch: 10 - Batch: 1873, Training Loss: 0.16163201417594802\n",
      "Epoch: 10 - Batch: 1874, Training Loss: 0.16172398797932944\n",
      "Epoch: 10 - Batch: 1875, Training Loss: 0.16180654867223246\n",
      "Epoch: 10 - Batch: 1876, Training Loss: 0.16189106571708944\n",
      "Epoch: 10 - Batch: 1877, Training Loss: 0.16198612491339198\n",
      "Epoch: 10 - Batch: 1878, Training Loss: 0.16207027554536735\n",
      "Epoch: 10 - Batch: 1879, Training Loss: 0.16215400368741298\n",
      "Epoch: 10 - Batch: 1880, Training Loss: 0.16224313096126317\n",
      "Epoch: 10 - Batch: 1881, Training Loss: 0.16232622400618113\n",
      "Epoch: 10 - Batch: 1882, Training Loss: 0.16241017675913783\n",
      "Epoch: 10 - Batch: 1883, Training Loss: 0.16250113888981924\n",
      "Epoch: 10 - Batch: 1884, Training Loss: 0.16259885945698713\n",
      "Epoch: 10 - Batch: 1885, Training Loss: 0.16268698780320176\n",
      "Epoch: 10 - Batch: 1886, Training Loss: 0.16278068732414672\n",
      "Epoch: 10 - Batch: 1887, Training Loss: 0.16287394066280986\n",
      "Epoch: 10 - Batch: 1888, Training Loss: 0.1629639315120814\n",
      "Epoch: 10 - Batch: 1889, Training Loss: 0.16304564363574903\n",
      "Epoch: 10 - Batch: 1890, Training Loss: 0.16312910503959577\n",
      "Epoch: 10 - Batch: 1891, Training Loss: 0.16321925257964315\n",
      "Epoch: 10 - Batch: 1892, Training Loss: 0.1633024665636704\n",
      "Epoch: 10 - Batch: 1893, Training Loss: 0.1633875888971547\n",
      "Epoch: 10 - Batch: 1894, Training Loss: 0.16348011369135843\n",
      "Epoch: 10 - Batch: 1895, Training Loss: 0.16356675596799622\n",
      "Epoch: 10 - Batch: 1896, Training Loss: 0.16365011222351644\n",
      "Epoch: 10 - Batch: 1897, Training Loss: 0.16373190076196964\n",
      "Epoch: 10 - Batch: 1898, Training Loss: 0.1638141639011713\n",
      "Epoch: 10 - Batch: 1899, Training Loss: 0.16390823129916665\n",
      "Epoch: 10 - Batch: 1900, Training Loss: 0.16400009391220846\n",
      "Epoch: 10 - Batch: 1901, Training Loss: 0.16408060863614082\n",
      "Epoch: 10 - Batch: 1902, Training Loss: 0.1641598767284335\n",
      "Epoch: 10 - Batch: 1903, Training Loss: 0.16424956536584628\n",
      "Epoch: 10 - Batch: 1904, Training Loss: 0.16433966056649166\n",
      "Epoch: 10 - Batch: 1905, Training Loss: 0.16442443484171707\n",
      "Epoch: 10 - Batch: 1906, Training Loss: 0.16451438175356803\n",
      "Epoch: 10 - Batch: 1907, Training Loss: 0.1645999423801207\n",
      "Epoch: 10 - Batch: 1908, Training Loss: 0.16469740708224215\n",
      "Epoch: 10 - Batch: 1909, Training Loss: 0.1647736059863176\n",
      "Epoch: 10 - Batch: 1910, Training Loss: 0.1648587241435229\n",
      "Epoch: 10 - Batch: 1911, Training Loss: 0.16495001084398275\n",
      "Epoch: 10 - Batch: 1912, Training Loss: 0.16503379354330636\n",
      "Epoch: 10 - Batch: 1913, Training Loss: 0.16512410620082274\n",
      "Epoch: 10 - Batch: 1914, Training Loss: 0.16520514114949833\n",
      "Epoch: 10 - Batch: 1915, Training Loss: 0.16528658188298764\n",
      "Epoch: 10 - Batch: 1916, Training Loss: 0.16536944259458514\n",
      "Epoch: 10 - Batch: 1917, Training Loss: 0.1654571388874382\n",
      "Epoch: 10 - Batch: 1918, Training Loss: 0.16555096711932527\n",
      "Epoch: 10 - Batch: 1919, Training Loss: 0.16563651856547762\n",
      "Epoch: 10 - Batch: 1920, Training Loss: 0.1657210732956925\n",
      "Epoch: 10 - Batch: 1921, Training Loss: 0.16581565346785052\n",
      "Epoch: 10 - Batch: 1922, Training Loss: 0.16589844529555606\n",
      "Epoch: 10 - Batch: 1923, Training Loss: 0.16597904855536783\n",
      "Epoch: 10 - Batch: 1924, Training Loss: 0.1660619704222699\n",
      "Epoch: 10 - Batch: 1925, Training Loss: 0.16614377140924705\n",
      "Epoch: 10 - Batch: 1926, Training Loss: 0.16622668685728242\n",
      "Epoch: 10 - Batch: 1927, Training Loss: 0.16631672853854165\n",
      "Epoch: 10 - Batch: 1928, Training Loss: 0.16639698134923653\n",
      "Epoch: 10 - Batch: 1929, Training Loss: 0.1664889712770088\n",
      "Epoch: 10 - Batch: 1930, Training Loss: 0.1665722658706344\n",
      "Epoch: 10 - Batch: 1931, Training Loss: 0.16665957292316366\n",
      "Epoch: 10 - Batch: 1932, Training Loss: 0.16675049314278473\n",
      "Epoch: 10 - Batch: 1933, Training Loss: 0.16683794485766495\n",
      "Epoch: 10 - Batch: 1934, Training Loss: 0.16692769559560525\n",
      "Epoch: 10 - Batch: 1935, Training Loss: 0.16701032491910517\n",
      "Epoch: 10 - Batch: 1936, Training Loss: 0.1671049475645149\n",
      "Epoch: 10 - Batch: 1937, Training Loss: 0.1671896556741365\n",
      "Epoch: 10 - Batch: 1938, Training Loss: 0.16728222740823356\n",
      "Epoch: 10 - Batch: 1939, Training Loss: 0.16737036870956223\n",
      "Epoch: 10 - Batch: 1940, Training Loss: 0.16746499632202572\n",
      "Epoch: 10 - Batch: 1941, Training Loss: 0.16754276152224484\n",
      "Epoch: 10 - Batch: 1942, Training Loss: 0.16761919921557503\n",
      "Epoch: 10 - Batch: 1943, Training Loss: 0.16770489310620237\n",
      "Epoch: 10 - Batch: 1944, Training Loss: 0.16779275012401798\n",
      "Epoch: 10 - Batch: 1945, Training Loss: 0.1678847151732761\n",
      "Epoch: 10 - Batch: 1946, Training Loss: 0.16797175768077077\n",
      "Epoch: 10 - Batch: 1947, Training Loss: 0.16806259916888344\n",
      "Epoch: 10 - Batch: 1948, Training Loss: 0.16814937842253033\n",
      "Epoch: 10 - Batch: 1949, Training Loss: 0.16823806994861829\n",
      "Epoch: 10 - Batch: 1950, Training Loss: 0.16832726302975248\n",
      "Epoch: 10 - Batch: 1951, Training Loss: 0.16841077364079196\n",
      "Epoch: 10 - Batch: 1952, Training Loss: 0.1684931907648372\n",
      "Epoch: 10 - Batch: 1953, Training Loss: 0.16858535572565214\n",
      "Epoch: 10 - Batch: 1954, Training Loss: 0.16867150998308292\n",
      "Epoch: 10 - Batch: 1955, Training Loss: 0.16875387726360885\n",
      "Epoch: 10 - Batch: 1956, Training Loss: 0.16884749563747575\n",
      "Epoch: 10 - Batch: 1957, Training Loss: 0.1689315697743525\n",
      "Epoch: 10 - Batch: 1958, Training Loss: 0.16901546129786948\n",
      "Epoch: 10 - Batch: 1959, Training Loss: 0.16910234270972596\n",
      "Epoch: 10 - Batch: 1960, Training Loss: 0.16918718554141313\n",
      "Epoch: 10 - Batch: 1961, Training Loss: 0.16927765214670554\n",
      "Epoch: 10 - Batch: 1962, Training Loss: 0.16936410625602674\n",
      "Epoch: 10 - Batch: 1963, Training Loss: 0.16944600261167111\n",
      "Epoch: 10 - Batch: 1964, Training Loss: 0.16952573887348965\n",
      "Epoch: 10 - Batch: 1965, Training Loss: 0.16960498266799334\n",
      "Epoch: 10 - Batch: 1966, Training Loss: 0.16969233647507814\n",
      "Epoch: 10 - Batch: 1967, Training Loss: 0.1697833491412067\n",
      "Epoch: 10 - Batch: 1968, Training Loss: 0.1698676974174395\n",
      "Epoch: 10 - Batch: 1969, Training Loss: 0.16994646611078265\n",
      "Epoch: 10 - Batch: 1970, Training Loss: 0.1700485010645283\n",
      "Epoch: 10 - Batch: 1971, Training Loss: 0.17012730116027702\n",
      "Epoch: 10 - Batch: 1972, Training Loss: 0.17020803130854223\n",
      "Epoch: 10 - Batch: 1973, Training Loss: 0.17029716762070038\n",
      "Epoch: 10 - Batch: 1974, Training Loss: 0.17038221222967848\n",
      "Epoch: 10 - Batch: 1975, Training Loss: 0.17046398941927882\n",
      "Epoch: 10 - Batch: 1976, Training Loss: 0.1705430079480111\n",
      "Epoch: 10 - Batch: 1977, Training Loss: 0.17063480904742853\n",
      "Epoch: 10 - Batch: 1978, Training Loss: 0.17072031662411752\n",
      "Epoch: 10 - Batch: 1979, Training Loss: 0.1708102563757505\n",
      "Epoch: 10 - Batch: 1980, Training Loss: 0.17089311018042502\n",
      "Epoch: 10 - Batch: 1981, Training Loss: 0.17099394871054796\n",
      "Epoch: 10 - Batch: 1982, Training Loss: 0.17108491762459377\n",
      "Epoch: 10 - Batch: 1983, Training Loss: 0.17117906996909263\n",
      "Epoch: 10 - Batch: 1984, Training Loss: 0.1712599678048447\n",
      "Epoch: 10 - Batch: 1985, Training Loss: 0.17135412543764952\n",
      "Epoch: 10 - Batch: 1986, Training Loss: 0.17144526505401084\n",
      "Epoch: 10 - Batch: 1987, Training Loss: 0.1715356833911555\n",
      "Epoch: 10 - Batch: 1988, Training Loss: 0.17161855605704274\n",
      "Epoch: 10 - Batch: 1989, Training Loss: 0.1717060986307031\n",
      "Epoch: 10 - Batch: 1990, Training Loss: 0.17179836631191903\n",
      "Epoch: 10 - Batch: 1991, Training Loss: 0.17187960224711085\n",
      "Epoch: 10 - Batch: 1992, Training Loss: 0.17196676890623708\n",
      "Epoch: 10 - Batch: 1993, Training Loss: 0.17205479547109573\n",
      "Epoch: 10 - Batch: 1994, Training Loss: 0.17213795500904766\n",
      "Epoch: 10 - Batch: 1995, Training Loss: 0.1722292084660795\n",
      "Epoch: 10 - Batch: 1996, Training Loss: 0.17231024020223276\n",
      "Epoch: 10 - Batch: 1997, Training Loss: 0.17239686904805612\n",
      "Epoch: 10 - Batch: 1998, Training Loss: 0.17248122835806748\n",
      "Epoch: 10 - Batch: 1999, Training Loss: 0.17257205940548262\n",
      "Epoch: 10 - Batch: 2000, Training Loss: 0.17265403845268695\n",
      "Epoch: 10 - Batch: 2001, Training Loss: 0.17274066803368368\n",
      "Epoch: 10 - Batch: 2002, Training Loss: 0.17282164896256097\n",
      "Epoch: 10 - Batch: 2003, Training Loss: 0.17291244296712266\n",
      "Epoch: 10 - Batch: 2004, Training Loss: 0.17298836400655174\n",
      "Epoch: 10 - Batch: 2005, Training Loss: 0.17307583896897324\n",
      "Epoch: 10 - Batch: 2006, Training Loss: 0.17315789438771767\n",
      "Epoch: 10 - Batch: 2007, Training Loss: 0.17324280232858302\n",
      "Epoch: 10 - Batch: 2008, Training Loss: 0.17332358142456802\n",
      "Epoch: 10 - Batch: 2009, Training Loss: 0.17340210909224663\n",
      "Epoch: 10 - Batch: 2010, Training Loss: 0.1734887983582012\n",
      "Epoch: 10 - Batch: 2011, Training Loss: 0.1735756878322531\n",
      "Epoch: 10 - Batch: 2012, Training Loss: 0.17365183985203653\n",
      "Epoch: 10 - Batch: 2013, Training Loss: 0.1737383487842866\n",
      "Epoch: 10 - Batch: 2014, Training Loss: 0.1738214228753229\n",
      "Epoch: 10 - Batch: 2015, Training Loss: 0.17390604653885314\n",
      "Epoch: 10 - Batch: 2016, Training Loss: 0.17400251247396517\n",
      "Epoch: 10 - Batch: 2017, Training Loss: 0.17408010674946345\n",
      "Epoch: 10 - Batch: 2018, Training Loss: 0.17416394889898362\n",
      "Epoch: 10 - Batch: 2019, Training Loss: 0.1742399516291484\n",
      "Epoch: 10 - Batch: 2020, Training Loss: 0.17432486941145228\n",
      "Epoch: 10 - Batch: 2021, Training Loss: 0.17440267325460812\n",
      "Epoch: 10 - Batch: 2022, Training Loss: 0.17448326461573146\n",
      "Epoch: 10 - Batch: 2023, Training Loss: 0.17455944111246374\n",
      "Epoch: 10 - Batch: 2024, Training Loss: 0.1746431437890921\n",
      "Epoch: 10 - Batch: 2025, Training Loss: 0.1747255465432779\n",
      "Epoch: 10 - Batch: 2026, Training Loss: 0.17481421341943504\n",
      "Epoch: 10 - Batch: 2027, Training Loss: 0.1748897636905436\n",
      "Epoch: 10 - Batch: 2028, Training Loss: 0.17497637059854632\n",
      "Epoch: 10 - Batch: 2029, Training Loss: 0.1750648709115994\n",
      "Epoch: 10 - Batch: 2030, Training Loss: 0.17514856878797808\n",
      "Epoch: 10 - Batch: 2031, Training Loss: 0.1752398299427076\n",
      "Epoch: 10 - Batch: 2032, Training Loss: 0.17533240622375934\n",
      "Epoch: 10 - Batch: 2033, Training Loss: 0.175427158018744\n",
      "Epoch: 10 - Batch: 2034, Training Loss: 0.17550982709720755\n",
      "Epoch: 10 - Batch: 2035, Training Loss: 0.1755890714847923\n",
      "Epoch: 10 - Batch: 2036, Training Loss: 0.17567912781391767\n",
      "Epoch: 10 - Batch: 2037, Training Loss: 0.1757697738941827\n",
      "Epoch: 10 - Batch: 2038, Training Loss: 0.17585467584863626\n",
      "Epoch: 10 - Batch: 2039, Training Loss: 0.1759397056985455\n",
      "Epoch: 10 - Batch: 2040, Training Loss: 0.17602101969802952\n",
      "Epoch: 10 - Batch: 2041, Training Loss: 0.17611043390572367\n",
      "Epoch: 10 - Batch: 2042, Training Loss: 0.17619783262016367\n",
      "Epoch: 10 - Batch: 2043, Training Loss: 0.17628281844616134\n",
      "Epoch: 10 - Batch: 2044, Training Loss: 0.17637663775094312\n",
      "Epoch: 10 - Batch: 2045, Training Loss: 0.17646708286322565\n",
      "Epoch: 10 - Batch: 2046, Training Loss: 0.1765536811406636\n",
      "Epoch: 10 - Batch: 2047, Training Loss: 0.17663750526916921\n",
      "Epoch: 10 - Batch: 2048, Training Loss: 0.17671927076395275\n",
      "Epoch: 10 - Batch: 2049, Training Loss: 0.17680149069472925\n",
      "Epoch: 10 - Batch: 2050, Training Loss: 0.1768850359701201\n",
      "Epoch: 10 - Batch: 2051, Training Loss: 0.17696944160840997\n",
      "Epoch: 10 - Batch: 2052, Training Loss: 0.17705641259403768\n",
      "Epoch: 10 - Batch: 2053, Training Loss: 0.17713379156258371\n",
      "Epoch: 10 - Batch: 2054, Training Loss: 0.17722677475900991\n",
      "Epoch: 10 - Batch: 2055, Training Loss: 0.17731894721365093\n",
      "Epoch: 10 - Batch: 2056, Training Loss: 0.17741456715572335\n",
      "Epoch: 10 - Batch: 2057, Training Loss: 0.17751061052059258\n",
      "Epoch: 10 - Batch: 2058, Training Loss: 0.1775948585723071\n",
      "Epoch: 10 - Batch: 2059, Training Loss: 0.17767870050559986\n",
      "Epoch: 10 - Batch: 2060, Training Loss: 0.1777503644401952\n",
      "Epoch: 10 - Batch: 2061, Training Loss: 0.17783441406923345\n",
      "Epoch: 10 - Batch: 2062, Training Loss: 0.17792687980862795\n",
      "Epoch: 10 - Batch: 2063, Training Loss: 0.17802031987515057\n",
      "Epoch: 10 - Batch: 2064, Training Loss: 0.1781112716207358\n",
      "Epoch: 10 - Batch: 2065, Training Loss: 0.1781951699893965\n",
      "Epoch: 10 - Batch: 2066, Training Loss: 0.1782807748190206\n",
      "Epoch: 10 - Batch: 2067, Training Loss: 0.1783680914349817\n",
      "Epoch: 10 - Batch: 2068, Training Loss: 0.17845327589370522\n",
      "Epoch: 10 - Batch: 2069, Training Loss: 0.17854089128659734\n",
      "Epoch: 10 - Batch: 2070, Training Loss: 0.1786273320145275\n",
      "Epoch: 10 - Batch: 2071, Training Loss: 0.17870572453386352\n",
      "Epoch: 10 - Batch: 2072, Training Loss: 0.17878203551790015\n",
      "Epoch: 10 - Batch: 2073, Training Loss: 0.17886031924393245\n",
      "Epoch: 10 - Batch: 2074, Training Loss: 0.17895946468830504\n",
      "Epoch: 10 - Batch: 2075, Training Loss: 0.17904011956470128\n",
      "Epoch: 10 - Batch: 2076, Training Loss: 0.1791281026060605\n",
      "Epoch: 10 - Batch: 2077, Training Loss: 0.17920788531353818\n",
      "Epoch: 10 - Batch: 2078, Training Loss: 0.17929534887150547\n",
      "Epoch: 10 - Batch: 2079, Training Loss: 0.17937896175813517\n",
      "Epoch: 10 - Batch: 2080, Training Loss: 0.1794607530890116\n",
      "Epoch: 10 - Batch: 2081, Training Loss: 0.17955239499559253\n",
      "Epoch: 10 - Batch: 2082, Training Loss: 0.17964207261776055\n",
      "Epoch: 10 - Batch: 2083, Training Loss: 0.179723289061591\n",
      "Epoch: 10 - Batch: 2084, Training Loss: 0.17981349916551045\n",
      "Epoch: 10 - Batch: 2085, Training Loss: 0.17989451231845774\n",
      "Epoch: 10 - Batch: 2086, Training Loss: 0.1799783795997871\n",
      "Epoch: 10 - Batch: 2087, Training Loss: 0.18005795588417234\n",
      "Epoch: 10 - Batch: 2088, Training Loss: 0.18014474749985224\n",
      "Epoch: 10 - Batch: 2089, Training Loss: 0.18023946865849433\n",
      "Epoch: 10 - Batch: 2090, Training Loss: 0.18032228655705404\n",
      "Epoch: 10 - Batch: 2091, Training Loss: 0.1804100550266344\n",
      "Epoch: 10 - Batch: 2092, Training Loss: 0.18049846074943915\n",
      "Epoch: 10 - Batch: 2093, Training Loss: 0.18058085237717747\n",
      "Epoch: 10 - Batch: 2094, Training Loss: 0.18066122723900857\n",
      "Epoch: 10 - Batch: 2095, Training Loss: 0.1807520983873513\n",
      "Epoch: 10 - Batch: 2096, Training Loss: 0.1808458617509993\n",
      "Epoch: 10 - Batch: 2097, Training Loss: 0.18092852385458266\n",
      "Epoch: 10 - Batch: 2098, Training Loss: 0.1810068164150513\n",
      "Epoch: 10 - Batch: 2099, Training Loss: 0.1810930547046523\n",
      "Epoch: 10 - Batch: 2100, Training Loss: 0.18118356386968745\n",
      "Epoch: 10 - Batch: 2101, Training Loss: 0.18127559415168232\n",
      "Epoch: 10 - Batch: 2102, Training Loss: 0.18136107137621338\n",
      "Epoch: 10 - Batch: 2103, Training Loss: 0.18144653459538274\n",
      "Epoch: 10 - Batch: 2104, Training Loss: 0.18153721855869934\n",
      "Epoch: 10 - Batch: 2105, Training Loss: 0.18161997524635312\n",
      "Epoch: 10 - Batch: 2106, Training Loss: 0.18171212218615349\n",
      "Epoch: 10 - Batch: 2107, Training Loss: 0.18179581806706158\n",
      "Epoch: 10 - Batch: 2108, Training Loss: 0.1818845389259494\n",
      "Epoch: 10 - Batch: 2109, Training Loss: 0.18196716086682593\n",
      "Epoch: 10 - Batch: 2110, Training Loss: 0.1820475682863351\n",
      "Epoch: 10 - Batch: 2111, Training Loss: 0.18213123836821782\n",
      "Epoch: 10 - Batch: 2112, Training Loss: 0.18222182752505858\n",
      "Epoch: 10 - Batch: 2113, Training Loss: 0.1823024807828378\n",
      "Epoch: 10 - Batch: 2114, Training Loss: 0.18237836421722203\n",
      "Epoch: 10 - Batch: 2115, Training Loss: 0.1824609811169097\n",
      "Epoch: 10 - Batch: 2116, Training Loss: 0.18255295139269448\n",
      "Epoch: 10 - Batch: 2117, Training Loss: 0.18264317521779097\n",
      "Epoch: 10 - Batch: 2118, Training Loss: 0.18272797531675344\n",
      "Epoch: 10 - Batch: 2119, Training Loss: 0.18281182669609736\n",
      "Epoch: 10 - Batch: 2120, Training Loss: 0.18289650715331532\n",
      "Epoch: 10 - Batch: 2121, Training Loss: 0.18298250663512777\n",
      "Epoch: 10 - Batch: 2122, Training Loss: 0.18307129157765786\n",
      "Epoch: 10 - Batch: 2123, Training Loss: 0.18315615883069253\n",
      "Epoch: 10 - Batch: 2124, Training Loss: 0.1832439831178481\n",
      "Epoch: 10 - Batch: 2125, Training Loss: 0.18332742371443492\n",
      "Epoch: 10 - Batch: 2126, Training Loss: 0.18342220895054132\n",
      "Epoch: 10 - Batch: 2127, Training Loss: 0.18350015841362685\n",
      "Epoch: 10 - Batch: 2128, Training Loss: 0.18357978105100234\n",
      "Epoch: 10 - Batch: 2129, Training Loss: 0.18366253129507773\n",
      "Epoch: 10 - Batch: 2130, Training Loss: 0.1837588274298517\n",
      "Epoch: 10 - Batch: 2131, Training Loss: 0.18384956317037887\n",
      "Epoch: 10 - Batch: 2132, Training Loss: 0.18393225314259332\n",
      "Epoch: 10 - Batch: 2133, Training Loss: 0.18401602432626002\n",
      "Epoch: 10 - Batch: 2134, Training Loss: 0.18409884768610768\n",
      "Epoch: 10 - Batch: 2135, Training Loss: 0.18418652102796\n",
      "Epoch: 10 - Batch: 2136, Training Loss: 0.18426999242769349\n",
      "Epoch: 10 - Batch: 2137, Training Loss: 0.18435666712609491\n",
      "Epoch: 10 - Batch: 2138, Training Loss: 0.18444085526426832\n",
      "Epoch: 10 - Batch: 2139, Training Loss: 0.1845303750342101\n",
      "Epoch: 10 - Batch: 2140, Training Loss: 0.1846120033048674\n",
      "Epoch: 10 - Batch: 2141, Training Loss: 0.18471139745406845\n",
      "Epoch: 10 - Batch: 2142, Training Loss: 0.1848053359990294\n",
      "Epoch: 10 - Batch: 2143, Training Loss: 0.18489197792970324\n",
      "Epoch: 10 - Batch: 2144, Training Loss: 0.1849786814048911\n",
      "Epoch: 10 - Batch: 2145, Training Loss: 0.18506992517493256\n",
      "Epoch: 10 - Batch: 2146, Training Loss: 0.18514726749601254\n",
      "Epoch: 10 - Batch: 2147, Training Loss: 0.18523305189327813\n",
      "Epoch: 10 - Batch: 2148, Training Loss: 0.18530999297649903\n",
      "Epoch: 10 - Batch: 2149, Training Loss: 0.1853950318523604\n",
      "Epoch: 10 - Batch: 2150, Training Loss: 0.18548132193770575\n",
      "Epoch: 10 - Batch: 2151, Training Loss: 0.1855656094725551\n",
      "Epoch: 10 - Batch: 2152, Training Loss: 0.1856559048009848\n",
      "Epoch: 10 - Batch: 2153, Training Loss: 0.1857362218312363\n",
      "Epoch: 10 - Batch: 2154, Training Loss: 0.1858234924587049\n",
      "Epoch: 10 - Batch: 2155, Training Loss: 0.18591555404425852\n",
      "Epoch: 10 - Batch: 2156, Training Loss: 0.1860032960537753\n",
      "Epoch: 10 - Batch: 2157, Training Loss: 0.18608501762967206\n",
      "Epoch: 10 - Batch: 2158, Training Loss: 0.18616647568607014\n",
      "Epoch: 10 - Batch: 2159, Training Loss: 0.18625483090431733\n",
      "Epoch: 10 - Batch: 2160, Training Loss: 0.1863429300351127\n",
      "Epoch: 10 - Batch: 2161, Training Loss: 0.1864405129086319\n",
      "Epoch: 10 - Batch: 2162, Training Loss: 0.1865236527760626\n",
      "Epoch: 10 - Batch: 2163, Training Loss: 0.1866046450908603\n",
      "Epoch: 10 - Batch: 2164, Training Loss: 0.18668899427524846\n",
      "Epoch: 10 - Batch: 2165, Training Loss: 0.18677654741934283\n",
      "Epoch: 10 - Batch: 2166, Training Loss: 0.18685841440511974\n",
      "Epoch: 10 - Batch: 2167, Training Loss: 0.1869505055458787\n",
      "Epoch: 10 - Batch: 2168, Training Loss: 0.18703119508662627\n",
      "Epoch: 10 - Batch: 2169, Training Loss: 0.18711859184266322\n",
      "Epoch: 10 - Batch: 2170, Training Loss: 0.18719705374012538\n",
      "Epoch: 10 - Batch: 2171, Training Loss: 0.18728664056529257\n",
      "Epoch: 10 - Batch: 2172, Training Loss: 0.18737892686060412\n",
      "Epoch: 10 - Batch: 2173, Training Loss: 0.18746103835021877\n",
      "Epoch: 10 - Batch: 2174, Training Loss: 0.1875507309044376\n",
      "Epoch: 10 - Batch: 2175, Training Loss: 0.18763440198096668\n",
      "Epoch: 10 - Batch: 2176, Training Loss: 0.18771262883917608\n",
      "Epoch: 10 - Batch: 2177, Training Loss: 0.18779916083980752\n",
      "Epoch: 10 - Batch: 2178, Training Loss: 0.18788321899438576\n",
      "Epoch: 10 - Batch: 2179, Training Loss: 0.18796639743679594\n",
      "Epoch: 10 - Batch: 2180, Training Loss: 0.18805659108271647\n",
      "Epoch: 10 - Batch: 2181, Training Loss: 0.18814428318075674\n",
      "Epoch: 10 - Batch: 2182, Training Loss: 0.1882393578898353\n",
      "Epoch: 10 - Batch: 2183, Training Loss: 0.1883202487259955\n",
      "Epoch: 10 - Batch: 2184, Training Loss: 0.18840867240803555\n",
      "Epoch: 10 - Batch: 2185, Training Loss: 0.18849184253495527\n",
      "Epoch: 10 - Batch: 2186, Training Loss: 0.18857656556427183\n",
      "Epoch: 10 - Batch: 2187, Training Loss: 0.18865631531250615\n",
      "Epoch: 10 - Batch: 2188, Training Loss: 0.18874004008758127\n",
      "Epoch: 10 - Batch: 2189, Training Loss: 0.18882235993397967\n",
      "Epoch: 10 - Batch: 2190, Training Loss: 0.1889119395433275\n",
      "Epoch: 10 - Batch: 2191, Training Loss: 0.1890006160632295\n",
      "Epoch: 10 - Batch: 2192, Training Loss: 0.1890869306255237\n",
      "Epoch: 10 - Batch: 2193, Training Loss: 0.18918284013323722\n",
      "Epoch: 10 - Batch: 2194, Training Loss: 0.18926263396716236\n",
      "Epoch: 10 - Batch: 2195, Training Loss: 0.1893439295687782\n",
      "Epoch: 10 - Batch: 2196, Training Loss: 0.18942818046851737\n",
      "Epoch: 10 - Batch: 2197, Training Loss: 0.1895118179432986\n",
      "Epoch: 10 - Batch: 2198, Training Loss: 0.18959673901225996\n",
      "Epoch: 10 - Batch: 2199, Training Loss: 0.18967635547788583\n",
      "Epoch: 10 - Batch: 2200, Training Loss: 0.1897626890805527\n",
      "Epoch: 10 - Batch: 2201, Training Loss: 0.1898436602730162\n",
      "Epoch: 10 - Batch: 2202, Training Loss: 0.18993260177658566\n",
      "Epoch: 10 - Batch: 2203, Training Loss: 0.19001772038477965\n",
      "Epoch: 10 - Batch: 2204, Training Loss: 0.19010065148768338\n",
      "Epoch: 10 - Batch: 2205, Training Loss: 0.1901896877174156\n",
      "Epoch: 10 - Batch: 2206, Training Loss: 0.19027278668696607\n",
      "Epoch: 10 - Batch: 2207, Training Loss: 0.19036808120052812\n",
      "Epoch: 10 - Batch: 2208, Training Loss: 0.1904496661515576\n",
      "Epoch: 10 - Batch: 2209, Training Loss: 0.19052867674783094\n",
      "Epoch: 10 - Batch: 2210, Training Loss: 0.19062433728506514\n",
      "Epoch: 10 - Batch: 2211, Training Loss: 0.1907160770994415\n",
      "Epoch: 10 - Batch: 2212, Training Loss: 0.1908027847138408\n",
      "Epoch: 10 - Batch: 2213, Training Loss: 0.19088616081286425\n",
      "Epoch: 10 - Batch: 2214, Training Loss: 0.19097874566270145\n",
      "Epoch: 10 - Batch: 2215, Training Loss: 0.19106413202227446\n",
      "Epoch: 10 - Batch: 2216, Training Loss: 0.19115279900345636\n",
      "Epoch: 10 - Batch: 2217, Training Loss: 0.19124047186343032\n",
      "Epoch: 10 - Batch: 2218, Training Loss: 0.19132349043343197\n",
      "Epoch: 10 - Batch: 2219, Training Loss: 0.19142101341194379\n",
      "Epoch: 10 - Batch: 2220, Training Loss: 0.19151070536072576\n",
      "Epoch: 10 - Batch: 2221, Training Loss: 0.19160875219660217\n",
      "Epoch: 10 - Batch: 2222, Training Loss: 0.19169882833834115\n",
      "Epoch: 10 - Batch: 2223, Training Loss: 0.19177858292389272\n",
      "Epoch: 10 - Batch: 2224, Training Loss: 0.1918673303849365\n",
      "Epoch: 10 - Batch: 2225, Training Loss: 0.1919483587850089\n",
      "Epoch: 10 - Batch: 2226, Training Loss: 0.19203517835581085\n",
      "Epoch: 10 - Batch: 2227, Training Loss: 0.1921171683379469\n",
      "Epoch: 10 - Batch: 2228, Training Loss: 0.19220196450774746\n",
      "Epoch: 10 - Batch: 2229, Training Loss: 0.19229069554785985\n",
      "Epoch: 10 - Batch: 2230, Training Loss: 0.19238499852258767\n",
      "Epoch: 10 - Batch: 2231, Training Loss: 0.19247629245740067\n",
      "Epoch: 10 - Batch: 2232, Training Loss: 0.1925601892877574\n",
      "Epoch: 10 - Batch: 2233, Training Loss: 0.19264975963639186\n",
      "Epoch: 10 - Batch: 2234, Training Loss: 0.1927261846835044\n",
      "Epoch: 10 - Batch: 2235, Training Loss: 0.19280680290007868\n",
      "Epoch: 10 - Batch: 2236, Training Loss: 0.19289589038189767\n",
      "Epoch: 10 - Batch: 2237, Training Loss: 0.1929872715181577\n",
      "Epoch: 10 - Batch: 2238, Training Loss: 0.19308279833415057\n",
      "Epoch: 10 - Batch: 2239, Training Loss: 0.1931650692822526\n",
      "Epoch: 10 - Batch: 2240, Training Loss: 0.19324951106759644\n",
      "Epoch: 10 - Batch: 2241, Training Loss: 0.19333506712866066\n",
      "Epoch: 10 - Batch: 2242, Training Loss: 0.1934192472849517\n",
      "Epoch: 10 - Batch: 2243, Training Loss: 0.193509708274508\n",
      "Epoch: 10 - Batch: 2244, Training Loss: 0.19359547499672294\n",
      "Epoch: 10 - Batch: 2245, Training Loss: 0.19367441630136117\n",
      "Epoch: 10 - Batch: 2246, Training Loss: 0.19376760780514768\n",
      "Epoch: 10 - Batch: 2247, Training Loss: 0.1938504761526636\n",
      "Epoch: 10 - Batch: 2248, Training Loss: 0.1939391411136632\n",
      "Epoch: 10 - Batch: 2249, Training Loss: 0.19402466705115282\n",
      "Epoch: 10 - Batch: 2250, Training Loss: 0.19410577994056208\n",
      "Epoch: 10 - Batch: 2251, Training Loss: 0.19419524229284543\n",
      "Epoch: 10 - Batch: 2252, Training Loss: 0.1942885157035951\n",
      "Epoch: 10 - Batch: 2253, Training Loss: 0.1943652757982512\n",
      "Epoch: 10 - Batch: 2254, Training Loss: 0.1944473712314322\n",
      "Epoch: 10 - Batch: 2255, Training Loss: 0.19453331485860187\n",
      "Epoch: 10 - Batch: 2256, Training Loss: 0.1946286553606564\n",
      "Epoch: 10 - Batch: 2257, Training Loss: 0.19470795617125322\n",
      "Epoch: 10 - Batch: 2258, Training Loss: 0.19479135660513322\n",
      "Epoch: 10 - Batch: 2259, Training Loss: 0.19487551390309238\n",
      "Epoch: 10 - Batch: 2260, Training Loss: 0.19495745635225406\n",
      "Epoch: 10 - Batch: 2261, Training Loss: 0.1950399921419312\n",
      "Epoch: 10 - Batch: 2262, Training Loss: 0.1951233982347928\n",
      "Epoch: 10 - Batch: 2263, Training Loss: 0.19520454261309866\n",
      "Epoch: 10 - Batch: 2264, Training Loss: 0.19529522842979352\n",
      "Epoch: 10 - Batch: 2265, Training Loss: 0.19538181030483387\n",
      "Epoch: 10 - Batch: 2266, Training Loss: 0.19547576590087481\n",
      "Epoch: 10 - Batch: 2267, Training Loss: 0.19555748279390248\n",
      "Epoch: 10 - Batch: 2268, Training Loss: 0.19563104533536319\n",
      "Epoch: 10 - Batch: 2269, Training Loss: 0.19572123689932214\n",
      "Epoch: 10 - Batch: 2270, Training Loss: 0.19580375563791932\n",
      "Epoch: 10 - Batch: 2271, Training Loss: 0.19588740905175359\n",
      "Epoch: 10 - Batch: 2272, Training Loss: 0.1959762283373828\n",
      "Epoch: 10 - Batch: 2273, Training Loss: 0.196063808262793\n",
      "Epoch: 10 - Batch: 2274, Training Loss: 0.1961536637474945\n",
      "Epoch: 10 - Batch: 2275, Training Loss: 0.1962450126473287\n",
      "Epoch: 10 - Batch: 2276, Training Loss: 0.1963291052500308\n",
      "Epoch: 10 - Batch: 2277, Training Loss: 0.19641358698803196\n",
      "Epoch: 10 - Batch: 2278, Training Loss: 0.1965025950525926\n",
      "Epoch: 10 - Batch: 2279, Training Loss: 0.1965932047572203\n",
      "Epoch: 10 - Batch: 2280, Training Loss: 0.19667471042072793\n",
      "Epoch: 10 - Batch: 2281, Training Loss: 0.19675625470863845\n",
      "Epoch: 10 - Batch: 2282, Training Loss: 0.1968508556077433\n",
      "Epoch: 10 - Batch: 2283, Training Loss: 0.19693182621330368\n",
      "Epoch: 10 - Batch: 2284, Training Loss: 0.19702273188390543\n",
      "Epoch: 10 - Batch: 2285, Training Loss: 0.19711207453884297\n",
      "Epoch: 10 - Batch: 2286, Training Loss: 0.19721120146178883\n",
      "Epoch: 10 - Batch: 2287, Training Loss: 0.19729224391664638\n",
      "Epoch: 10 - Batch: 2288, Training Loss: 0.19739451759168955\n",
      "Epoch: 10 - Batch: 2289, Training Loss: 0.19747630545674866\n",
      "Epoch: 10 - Batch: 2290, Training Loss: 0.19755048218923027\n",
      "Epoch: 10 - Batch: 2291, Training Loss: 0.19763639244941336\n",
      "Epoch: 10 - Batch: 2292, Training Loss: 0.1977141831397615\n",
      "Epoch: 10 - Batch: 2293, Training Loss: 0.19780296239859824\n",
      "Epoch: 10 - Batch: 2294, Training Loss: 0.19789692958443122\n",
      "Epoch: 10 - Batch: 2295, Training Loss: 0.19797978510039166\n",
      "Epoch: 10 - Batch: 2296, Training Loss: 0.19806545898688965\n",
      "Epoch: 10 - Batch: 2297, Training Loss: 0.19815171128803027\n",
      "Epoch: 10 - Batch: 2298, Training Loss: 0.1982398464299652\n",
      "Epoch: 10 - Batch: 2299, Training Loss: 0.198334387149878\n",
      "Epoch: 10 - Batch: 2300, Training Loss: 0.1984200634087002\n",
      "Epoch: 10 - Batch: 2301, Training Loss: 0.1985018244368421\n",
      "Epoch: 10 - Batch: 2302, Training Loss: 0.19858321422096312\n",
      "Epoch: 10 - Batch: 2303, Training Loss: 0.19866390856813831\n",
      "Epoch: 10 - Batch: 2304, Training Loss: 0.19875858440550406\n",
      "Epoch: 10 - Batch: 2305, Training Loss: 0.1988495589785315\n",
      "Epoch: 10 - Batch: 2306, Training Loss: 0.19893715058640263\n",
      "Epoch: 10 - Batch: 2307, Training Loss: 0.19902808706015102\n",
      "Epoch: 10 - Batch: 2308, Training Loss: 0.1991137047213306\n",
      "Epoch: 10 - Batch: 2309, Training Loss: 0.199195213047525\n",
      "Epoch: 10 - Batch: 2310, Training Loss: 0.19929223355047937\n",
      "Epoch: 10 - Batch: 2311, Training Loss: 0.19938347420066743\n",
      "Epoch: 10 - Batch: 2312, Training Loss: 0.19947518161799185\n",
      "Epoch: 10 - Batch: 2313, Training Loss: 0.19956285047466876\n",
      "Epoch: 10 - Batch: 2314, Training Loss: 0.19964621461322454\n",
      "Epoch: 10 - Batch: 2315, Training Loss: 0.19973526498423286\n",
      "Epoch: 10 - Batch: 2316, Training Loss: 0.19982616312394094\n",
      "Epoch: 10 - Batch: 2317, Training Loss: 0.19991614909264974\n",
      "Epoch: 10 - Batch: 2318, Training Loss: 0.19999726160520542\n",
      "Epoch: 10 - Batch: 2319, Training Loss: 0.20009245895850125\n",
      "Epoch: 10 - Batch: 2320, Training Loss: 0.20017400028100654\n",
      "Epoch: 10 - Batch: 2321, Training Loss: 0.20026502868849444\n",
      "Epoch: 10 - Batch: 2322, Training Loss: 0.2003506048834245\n",
      "Epoch: 10 - Batch: 2323, Training Loss: 0.20043678619399988\n",
      "Epoch: 10 - Batch: 2324, Training Loss: 0.20052250779263217\n",
      "Epoch: 10 - Batch: 2325, Training Loss: 0.20060488993725176\n",
      "Epoch: 10 - Batch: 2326, Training Loss: 0.20069769883580865\n",
      "Epoch: 10 - Batch: 2327, Training Loss: 0.200791838435243\n",
      "Epoch: 10 - Batch: 2328, Training Loss: 0.20087793247023625\n",
      "Epoch: 10 - Batch: 2329, Training Loss: 0.20095957136075096\n",
      "Epoch: 10 - Batch: 2330, Training Loss: 0.20104760115048778\n",
      "Epoch: 10 - Batch: 2331, Training Loss: 0.20114812770144858\n",
      "Epoch: 10 - Batch: 2332, Training Loss: 0.20123388116583105\n",
      "Epoch: 10 - Batch: 2333, Training Loss: 0.20131536476795947\n",
      "Epoch: 10 - Batch: 2334, Training Loss: 0.20139779898015223\n",
      "Epoch: 10 - Batch: 2335, Training Loss: 0.20148301335240082\n",
      "Epoch: 10 - Batch: 2336, Training Loss: 0.20156615367082023\n",
      "Epoch: 10 - Batch: 2337, Training Loss: 0.20165945990489886\n",
      "Epoch: 10 - Batch: 2338, Training Loss: 0.2017470913974701\n",
      "Epoch: 10 - Batch: 2339, Training Loss: 0.20183591271563747\n",
      "Epoch: 10 - Batch: 2340, Training Loss: 0.20192410059853969\n",
      "Epoch: 10 - Batch: 2341, Training Loss: 0.20200957901541075\n",
      "Epoch: 10 - Batch: 2342, Training Loss: 0.20209586061228368\n",
      "Epoch: 10 - Batch: 2343, Training Loss: 0.20219590623629824\n",
      "Epoch: 10 - Batch: 2344, Training Loss: 0.20228748323757256\n",
      "Epoch: 10 - Batch: 2345, Training Loss: 0.20237467067306908\n",
      "Epoch: 10 - Batch: 2346, Training Loss: 0.20246770551696938\n",
      "Epoch: 10 - Batch: 2347, Training Loss: 0.20255678564705462\n",
      "Epoch: 10 - Batch: 2348, Training Loss: 0.20264954383718237\n",
      "Epoch: 10 - Batch: 2349, Training Loss: 0.20274091605869296\n",
      "Epoch: 10 - Batch: 2350, Training Loss: 0.20282115839162276\n",
      "Epoch: 10 - Batch: 2351, Training Loss: 0.2029184059457696\n",
      "Epoch: 10 - Batch: 2352, Training Loss: 0.20300074288031553\n",
      "Epoch: 10 - Batch: 2353, Training Loss: 0.20308803894848965\n",
      "Epoch: 10 - Batch: 2354, Training Loss: 0.20318625356452186\n",
      "Epoch: 10 - Batch: 2355, Training Loss: 0.2032792766641819\n",
      "Epoch: 10 - Batch: 2356, Training Loss: 0.20336485888259132\n",
      "Epoch: 10 - Batch: 2357, Training Loss: 0.2034638258589935\n",
      "Epoch: 10 - Batch: 2358, Training Loss: 0.2035525372347626\n",
      "Epoch: 10 - Batch: 2359, Training Loss: 0.20363649083409538\n",
      "Epoch: 10 - Batch: 2360, Training Loss: 0.203719789943786\n",
      "Epoch: 10 - Batch: 2361, Training Loss: 0.20380759472104645\n",
      "Epoch: 10 - Batch: 2362, Training Loss: 0.20389147281992692\n",
      "Epoch: 10 - Batch: 2363, Training Loss: 0.2039716688678237\n",
      "Epoch: 10 - Batch: 2364, Training Loss: 0.2040596012254655\n",
      "Epoch: 10 - Batch: 2365, Training Loss: 0.2041442408218708\n",
      "Epoch: 10 - Batch: 2366, Training Loss: 0.20422967652455096\n",
      "Epoch: 10 - Batch: 2367, Training Loss: 0.2043122038690012\n",
      "Epoch: 10 - Batch: 2368, Training Loss: 0.2043933581752366\n",
      "Epoch: 10 - Batch: 2369, Training Loss: 0.20448200830921012\n",
      "Epoch: 10 - Batch: 2370, Training Loss: 0.20457020886921962\n",
      "Epoch: 10 - Batch: 2371, Training Loss: 0.20466035839238175\n",
      "Epoch: 10 - Batch: 2372, Training Loss: 0.2047528353879602\n",
      "Epoch: 10 - Batch: 2373, Training Loss: 0.20483500146574246\n",
      "Epoch: 10 - Batch: 2374, Training Loss: 0.20493074664058378\n",
      "Epoch: 10 - Batch: 2375, Training Loss: 0.20502393554346282\n",
      "Epoch: 10 - Batch: 2376, Training Loss: 0.2051048052449329\n",
      "Epoch: 10 - Batch: 2377, Training Loss: 0.20519789559751206\n",
      "Epoch: 10 - Batch: 2378, Training Loss: 0.20528931600067943\n",
      "Epoch: 10 - Batch: 2379, Training Loss: 0.20538120013144281\n",
      "Epoch: 10 - Batch: 2380, Training Loss: 0.20547200235882604\n",
      "Epoch: 10 - Batch: 2381, Training Loss: 0.20555543963168788\n",
      "Epoch: 10 - Batch: 2382, Training Loss: 0.2056371337343409\n",
      "Epoch: 10 - Batch: 2383, Training Loss: 0.2057285150250491\n",
      "Epoch: 10 - Batch: 2384, Training Loss: 0.205820075525079\n",
      "Epoch: 10 - Batch: 2385, Training Loss: 0.20590696025448257\n",
      "Epoch: 10 - Batch: 2386, Training Loss: 0.2059986596652129\n",
      "Epoch: 10 - Batch: 2387, Training Loss: 0.2060810869704827\n",
      "Epoch: 10 - Batch: 2388, Training Loss: 0.20616376417292093\n",
      "Epoch: 10 - Batch: 2389, Training Loss: 0.20625151331176608\n",
      "Epoch: 10 - Batch: 2390, Training Loss: 0.20633846156262048\n",
      "Epoch: 10 - Batch: 2391, Training Loss: 0.20642313820187924\n",
      "Epoch: 10 - Batch: 2392, Training Loss: 0.20650842387976734\n",
      "Epoch: 10 - Batch: 2393, Training Loss: 0.20659887346486944\n",
      "Epoch: 10 - Batch: 2394, Training Loss: 0.20668816077793217\n",
      "Epoch: 10 - Batch: 2395, Training Loss: 0.20676746030352008\n",
      "Epoch: 10 - Batch: 2396, Training Loss: 0.20685737430555112\n",
      "Epoch: 10 - Batch: 2397, Training Loss: 0.2069362963659451\n",
      "Epoch: 10 - Batch: 2398, Training Loss: 0.20702396463571893\n",
      "Epoch: 10 - Batch: 2399, Training Loss: 0.20711418168980683\n",
      "Epoch: 10 - Batch: 2400, Training Loss: 0.20719835359856462\n",
      "Epoch: 10 - Batch: 2401, Training Loss: 0.20728316256285306\n",
      "Epoch: 10 - Batch: 2402, Training Loss: 0.20736510295476487\n",
      "Epoch: 10 - Batch: 2403, Training Loss: 0.2074469317856614\n",
      "Epoch: 10 - Batch: 2404, Training Loss: 0.2075313556735492\n",
      "Epoch: 10 - Batch: 2405, Training Loss: 0.20762234010037103\n",
      "Epoch: 10 - Batch: 2406, Training Loss: 0.20770523459905416\n",
      "Epoch: 10 - Batch: 2407, Training Loss: 0.20778159551927897\n",
      "Epoch: 10 - Batch: 2408, Training Loss: 0.20787925794597092\n",
      "Epoch: 10 - Batch: 2409, Training Loss: 0.20796860396170103\n",
      "Epoch: 10 - Batch: 2410, Training Loss: 0.20805362333384517\n",
      "Epoch: 10 - Batch: 2411, Training Loss: 0.2081374786670627\n",
      "Epoch: 10 - Batch: 2412, Training Loss: 0.2082110559675923\n",
      "Epoch 10 - Batch 2412, Training Loss: 0.2082110559675923, Validation Loss: 0.20813929911078902\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    epochs = 10,\n",
    "    early_stopping = early_stopping,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/nickprock_fine_tuned/tokenizer_config.json',\n",
       " '../models/nickprock_fine_tuned/special_tokens_map.json',\n",
       " '../models/nickprock_fine_tuned/vocab.txt',\n",
       " '../models/nickprock_fine_tuned/added_tokens.json',\n",
       " '../models/nickprock_fine_tuned/tokenizer.json')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('../models/nickprock_fine_tuned')\n",
    "tokenizer.save_pretrained('../models/nickprock_fine_tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"stsb_multi_mt\", name=\"it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datasets = pd.DataFrame()\n",
    "for _, dataset in dataset.items():\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    datasets = pd.concat([datasets, dataset], axis=0). reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence1', 'sentence2', 'similarity_score'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Come Ã¨ stato detto, il problema del formaggio Ã¨ il potenziale della Listeria.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[datasets['sentence1'].apply(lambda x: x.find('gravidanza') != -1)]['sentence2'][7940]#[3077]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
